{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marzinouri/AzeriPipeline/blob/main/Notebooks/MachineTranslation/Translation_fa2azb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k051L-Y3rTu"
      },
      "source": [
        "Acknowledgment: This portion of the code is based on the work available at [JoeyNMT](https://github.com/joeynmt/joeynmt).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRRatWnCZFdd",
        "outputId": "74ddedfe-e3ed-4bc3-84c2-47da6e41f6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/translation"
      ],
      "metadata": {
        "id": "x-YYYitPmhZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites"
      ],
      "metadata": {
        "id": "KqHP3tuOmPr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/drive/MyDrive/translation\""
      ],
      "metadata": {
        "id": "kPQ3tbJ-mfaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "K7ZOEq3znIGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pQwoOS-OvMLf",
        "outputId": "e98babc2-56d3-4d19-af57-f7b11e53d859"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJBa6lx26Hdx"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "def load_data_to_df(path):\n",
        "    \"\"\"\n",
        "    Load dataset\n",
        "    \"\"\"\n",
        "    lines = Path(path).open(encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
        "\n",
        "    data = {\n",
        "        \"id\": range(len(lines)),\n",
        "    }\n",
        "\n",
        "    #load data into a DataFrame object:\n",
        "    df = pd.DataFrame(data)\n",
        "    sents = []\n",
        "    for i, l in enumerate(lines):\n",
        "        lang1, lang2 = l.split(\"\\t\")\n",
        "        sents.append({\"azb\": lang1, \"fa\": lang2})\n",
        "    random.seed(42)\n",
        "    random.shuffle(sents)\n",
        "    df[\"translation\"] = sents\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "1ZzzL9FF_UXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1p32slBTthm"
      },
      "outputs": [],
      "source": [
        "df = load_data_to_df(\"/content/drive/MyDrive/Preprocessed_Datasets/Bilingual/ALL_v2.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTPhy7M8vWw0",
        "outputId": "594d97fc-e447-40df-c8d5-fcb2c12f92dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['id', 'translation'],\n",
              "     num_rows: 11978\n",
              " }), Dataset({\n",
              "     features: ['id', 'translation'],\n",
              "     num_rows: 1497\n",
              " }), Dataset({\n",
              "     features: ['id', 'translation'],\n",
              "     num_rows: 1497\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_frac = 0.8\n",
        "dev_frac = 0.1\n",
        "test_frac = 0.1\n",
        "\n",
        "total = len(df)\n",
        "train_num = int(train_frac * len(df))\n",
        "dev_num = int(dev_frac * len(df))\n",
        "\n",
        "data_train = Dataset.from_pandas(df[:train_num+1])\n",
        "data_dev = Dataset.from_pandas(df[train_num+1:train_num+1+dev_num])\n",
        "data_test = Dataset.from_pandas(df[train_num+1+dev_num:])\n",
        "\n",
        "data_train, data_dev, data_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the data\n",
        "\n"
      ],
      "metadata": {
        "id": "_sQkopZtneD9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr_YM040Tthp",
        "outputId": "721f3a24-eb07-402a-923d-ee5081dc68d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'azb': 'اونا یئشآیا پیغمبرین کیتابینی وئردیلر . ایسا تومارێ آچێب بو سؤزلر یازیلان هیسسهنی تاپدێ : ',\n",
              "  'fa': 'طومار اشعْیای نبی به او داده شد ، او طومار را گشود و بخشی را یافت که چنین نوشته شده است : '},\n",
              " {'azb': 'وه اؤزو ده جهنهمه آتیلاجاقدیر . ',\n",
              "  'fa': 'و فرجامش\\u200c درافتادن به جهنم است . '},\n",
              " {'azb': 'البته عيلتی ده بودور کی', 'fa': 'البته علتش هم اين است كه '}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data_train['translation'][:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUFSJnWYvWzq",
        "outputId": "6110666a-b210-445f-cc2b-d0138121a62d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'azb': 'بئلهجه یهودا بیر آلای اسگرله باشچێ کاهنلرین وه فاریسئیلرین گؤندردیگی بزی مۆحافظهچیلری گؤتوروب چێراقلار ، مشلر وه سلاحلارلا اورایا گلدی . ',\n",
              "  'fa': 'پس یهودا گروهی از سربازان و مأموران سران کاهنان و فریسیان را با خود همراه کرد و آنان با مشعل و چراغ و سلاح به آنجا رسیدند . '},\n",
              " {'azb': 'دئدیم : ائله بیل یاتمیشدین آییلدین هه ! ',\n",
              "  'fa': 'گفتم\\u200c : خوابت\\u200c پرید ! '},\n",
              " {'azb': 'بیز کیتابدان سونرا زبوردا دا تورپاغا یالنیز منیم سالئه بندهلریمین داخل اولاجاغێنێ یازمیشدیق . ',\n",
              "  'fa': 'و در حقیقت ، در زبور پس از تورات نوشتیم که زمین را بندگان شایسته ما به ارث خواهند برد . '}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data_dev['translation'][:3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test['translation'][:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECqj2TRfAjHX",
        "outputId": "643dad6e-62db-4d4f-ec69-84af03fe610e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'azb': 'ائشیتدیک کی ، بیزلردن بزی آداملار یانینیزا گلیب سؤزلری ایله سیزی لرزهیه سالاراق دۆشۆنجهلرینیزی قارێشدێرێبلار . لاکین بونو اونلارا بیز تاپشێرمامێشدێق . ',\n",
              "  'fa': 'شنیده\\u200cایم که بعضی از میان ما ، هرچند که ما به آنان حکم نکرده بودیم ، شما را با سخنان خود مضطرب ساخته\\u200cاند و در تلاش بوده\\u200cاند ذهنتان را آشفته سازند . '},\n",
              " {'azb': 'آمما اؤز دینینی آرالارێندا پارچالاییب فرقه فرقه اولدولار . هر فرقه اؤز دینینه سئوینیر ',\n",
              "  'fa': 'تا کار دین\\u200c شان را میان خود قطعه قطعه کردند و دسته دسته شدند : هر دسته\\u200cای به آنچه نزدشان بود ، دل خوش کردند . '},\n",
              " {'azb': 'گۆندهلیک چؤرهییمیزی بیزه بو گۆن وئر . ',\n",
              "  'fa': 'نان روزانهٔ ما را امروز به ما عطا کن . '}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlCBTXOH6KSw"
      },
      "source": [
        "Save the train-dev-test splits in local dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz97rpCkvW7w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "121f09611d5b4737b3cb8fb465d84599",
            "4bfa21cdecc14374a1bdf01554fff916",
            "cc26dbb86aa8456e9ef5e515263b5811",
            "7bd0582440244b8e8ffb70ed7e9c94b9",
            "db20d278b31546888f19c07ef80b92cd",
            "a13430b6a83c428cb767ae9ed8c2da3d",
            "4e92eea5d0174995b62c2cc24cd06c3b",
            "5ab3dcfa2f5c44c783f661bc716bde1d",
            "b29ebb14d2ec409b9d1ec8702d915d9e",
            "53bd855d8d5e42afbf9e79acdaaf5c50",
            "546a204f976148278ba0dc2c1d1df958",
            "103312696eb14c95a81a212cb00f6c5f",
            "e1422d0dd9e34f7b997ea28e35022444",
            "1a886e5736ac42f895a4bd3f8431522f",
            "c53c622071d548be85e10903e30a1b6d",
            "ba1309e170fd4fc69031656b3a9faef8",
            "c0a1fa06c81942b18d7f3d9e9f304990",
            "bdda6420da584a62ae4ac1a6e180e338",
            "818d390f94044bccb7183b98fd428f8a",
            "882a29f154c042e8813e7f74953ab527",
            "44b777ab0b704c9cbd35f3ba79b33a33",
            "6481faf34a8e4a5faa64a4ad48af95da",
            "a452dcf3cf794d4ca39b26610f614cb1",
            "f41c6f9c2ce74f6c98e0651f195a00d2",
            "33f3bad294294e04b3c02c857a6483f5",
            "c9c4b5f8ac1c4faf8fd21b17d9dd2a0c",
            "ca40d9400f00406db1bcd03b287bc63e",
            "c6b8e53cd8f845eea5bf3b48e2758258",
            "d52d9c92b8804004af8d8dd6b66275fd",
            "a227d83015a94b65ba1f6ba90204b676",
            "8072b31667614e71aa5b47751abb6478",
            "5a046a5f1c344fba909d0654200d012e",
            "943ac4e671994f789bb7413e4dea10bc"
          ]
        },
        "outputId": "1ae4bb32-6dd6-4e34-e530-604d5028cfdb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/11978 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "121f09611d5b4737b3cb8fb465d84599"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1497 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "103312696eb14c95a81a212cb00f6c5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1497 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a452dcf3cf794d4ca39b26610f614cb1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets.dataset_dict import DatasetDict\n",
        "\n",
        "dataset_dict = DatasetDict({\n",
        "  \"train\": data_train,\n",
        "  \"validation\": data_dev,\n",
        "  \"test\": data_test\n",
        "})\n",
        "\n",
        "data_dir = \"RESULTS_fa2azb/data\"\n",
        "dataset_dict.save_to_disk(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1RMfXeT-V1m"
      },
      "source": [
        "## Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = 'fa'\n",
        "trg = 'azb'"
      ],
      "metadata": {
        "id": "5wY9pGMekRjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJML2jYR1PlG"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create the config\n",
        "config = f\"\"\"\n",
        "name: \"data_sp\"\n",
        "joeynmt_version: \"2.0.0\"\n",
        "\n",
        "data:\n",
        "    train: \"{data_dir}/train\"\n",
        "    dev: \"{data_dir}/validation\"\n",
        "    test: \"{data_dir}/test\"\n",
        "    dataset_type: \"huggingface\"\n",
        "    sample_dev_subset: 200\n",
        "    src:\n",
        "        lang: \"{src}\"\n",
        "        max_length: 100\n",
        "        lowercase: False\n",
        "        normalize: False\n",
        "        level: \"bpe\"\n",
        "        voc_limit: 2000\n",
        "        voc_min_freq: 1\n",
        "        voc_file: \"{data_dir}/vocab.txt\"\n",
        "        tokenizer_type: \"sentencepiece\"\n",
        "        tokenizer_cfg:\n",
        "            model_file: \"{data_dir}/sp.model\"\n",
        "\n",
        "    trg:\n",
        "        lang: \"{trg}\"\n",
        "        max_length: 100\n",
        "        lowercase: False\n",
        "        normalize: False\n",
        "        level: \"bpe\"\n",
        "        voc_limit: 2000\n",
        "        voc_min_freq: 1\n",
        "        voc_file: \"{data_dir}/vocab.txt\"\n",
        "        tokenizer_type: \"sentencepiece\"\n",
        "        tokenizer_cfg:\n",
        "            model_file: \"{data_dir}/sp.model\"\n",
        "\n",
        "\"\"\".format(data_dir=data_dir)\n",
        "with (Path(data_dir) / \"config.yaml\").open('w') as f:\n",
        "    f.write(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMkbAmPz1Pnx",
        "outputId": "975c7f2a-80e4-4bfd-d54e-28c75c0dfa4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
            "  warnings.warn(\n",
            "Dropping NaN...: 100% 12/12 [00:00<00:00, 100.90ba/s]\n",
            "Preprocessing...: 100% 11978/11978 [00:01<00:00, 10124.44ex/s]\n",
            "### Training sentencepiece...\n",
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/tmp/sentencepiece_9ur21_0_.txt --model_prefix=RESULTS_fa2azb/data/sp --model_type=unigram --vocab_size=2000 --character_coverage=1.0 --accept_language=fa,azb --unk_piece=<unk> --bos_piece=<s> --eos_piece=</s> --pad_piece=<pad> --unk_id=0 --bos_id=2 --eos_id=3 --pad_id=1 --vocabulary_output_piece_score=false\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: /tmp/sentencepiece_9ur21_0_.txt\n",
            "  input_format: \n",
            "  model_prefix: RESULTS_fa2azb/data/sp\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  accept_language: fa\n",
            "  accept_language: azb\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 1\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 0\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 2\n",
            "  eos_id: 3\n",
            "  pad_id: 1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(181) LOG(INFO) Loading corpus: /tmp/sentencepiece_9ur21_0_.txt\n",
            "trainer_interface.cc(406) LOG(INFO) Loaded all 23956 sentences\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <pad>\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(536) LOG(INFO) all chars count=2630293\n",
            "trainer_interface.cc(557) LOG(INFO) Alphabet size=67\n",
            "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 23956 sentences.\n",
            "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
            "unigram_model_trainer.cc(201) LOG(INFO) Initialized 72314 seed sentencepieces\n",
            "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 23956\n",
            "trainer_interface.cc(607) LOG(INFO) Done! 38166\n",
            "unigram_model_trainer.cc(491) LOG(INFO) Using 38166 sentences for EM training\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=25918 obj=10.0248 num_tokens=66708 num_tokens/piece=2.57381\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=20870 obj=8.50938 num_tokens=67358 num_tokens/piece=3.2275\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=15646 obj=8.45861 num_tokens=71367 num_tokens/piece=4.56136\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=15627 obj=8.42387 num_tokens=71513 num_tokens/piece=4.57625\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=11720 obj=8.50191 num_tokens=78263 num_tokens/piece=6.67773\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=11720 obj=8.47385 num_tokens=78429 num_tokens/piece=6.69189\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=8790 obj=8.59031 num_tokens=85496 num_tokens/piece=9.72651\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=8790 obj=8.5585 num_tokens=85477 num_tokens/piece=9.72435\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=6592 obj=8.71571 num_tokens=92974 num_tokens/piece=14.1041\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=6592 obj=8.67856 num_tokens=92940 num_tokens/piece=14.0989\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=4944 obj=8.87261 num_tokens=100260 num_tokens/piece=20.2791\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=4944 obj=8.827 num_tokens=100230 num_tokens/piece=20.2731\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=3708 obj=9.06084 num_tokens=107921 num_tokens/piece=29.1049\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=3708 obj=9.00735 num_tokens=107907 num_tokens/piece=29.1011\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2781 obj=9.27944 num_tokens=115910 num_tokens/piece=41.6793\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2781 obj=9.21734 num_tokens=115903 num_tokens/piece=41.6767\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2200 obj=9.46212 num_tokens=122258 num_tokens/piece=55.5718\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2200 obj=9.40818 num_tokens=122252 num_tokens/piece=55.5691\n",
            "trainer_interface.cc(685) LOG(INFO) Saving model: RESULTS_fa2azb/data/sp.model\n",
            "trainer_interface.cc(697) LOG(INFO) Saving vocabs: RESULTS_fa2azb/data/sp.vocab\n",
            "### Copying RESULTS_fa2azb/data/sp.vocab to RESULTS_fa2azb/data/vocab.txt ...\n",
            "### Done.\n"
          ]
        }
      ],
      "source": [
        "!python3 scripts/build_vocab.py {data_dir}/config.yaml --joint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN1n1gTKARR7",
        "outputId": "2bfdd839-5de4-403d-83e4-3e162ccb54ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk>\n",
            "<pad>\n",
            "<s>\n",
            "</s>\n",
            "▁،\n",
            "ی\n",
            "▁.\n",
            "ه\n",
            "ا\n",
            "▁و\n"
          ]
        }
      ],
      "source": [
        "!head -10 {data_dir}/vocab.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6cN9CPtAaPl"
      },
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJcLOr_S2BTD"
      },
      "outputs": [],
      "source": [
        "model_dir = \"RESULTS_fa2azb/model\"\n",
        "config += \"\"\"\n",
        "testing:\n",
        "    n_best: 1\n",
        "    beam_size: 5\n",
        "    beam_alpha: 1.0\n",
        "    batch_size: 512\n",
        "    batch_type: \"token\"\n",
        "    max_output_length: 100\n",
        "    eval_metrics: [\"bleu\"]\n",
        "    #return_prob: \"hyp\"\n",
        "    #return_attention: False\n",
        "    sacrebleu_cfg:\n",
        "        tokenize: \"13a\"\n",
        "\n",
        "training:\n",
        "    #load_model: \"{model_dir}/latest.ckpt\"\n",
        "    #reset_best_ckpt: False\n",
        "    #reset_scheduler: False\n",
        "    #reset_optimizer: False\n",
        "    #reset_iter_state: False\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999]\n",
        "    scheduling: \"warmupinversesquareroot\"\n",
        "    learning_rate_warmup: 2000\n",
        "    learning_rate: 0.0002\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    loss: \"crossentropy\"\n",
        "    batch_size: 512\n",
        "    batch_type: \"token\"\n",
        "    batch_multiplier: 4\n",
        "    early_stopping_metric: \"loss\"\n",
        "    epochs: 500\n",
        "    updates: 2000000000\n",
        "    validation_freq: 1000\n",
        "    logging_freq: 100\n",
        "    model_dir: \"{model_dir}\"\n",
        "    overwrite: True\n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_best_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 2\n",
        "        num_heads: 4\n",
        "        embeddings:\n",
        "            embedding_dim: 256\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256\n",
        "        ff_size: 1024\n",
        "        dropout: 0.1\n",
        "        layer_norm: \"pre\"\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 2\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 256\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256\n",
        "        ff_size: 1024\n",
        "        dropout: 0.1\n",
        "        layer_norm: \"pre\"\n",
        "\n",
        "\"\"\".format(model_dir=model_dir)\n",
        "with (Path(data_dir) / \"config.yaml\").open('w') as f:\n",
        "    f.write(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w45HbBfeMW38"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTbfgVOq2BfB",
        "outputId": "5e61df86-f9e3-4b20-dcff-a1416706d299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2023-01-19 23:21:56,815 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:22:01,214 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:22:01,214 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:22:01,214 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:22:01,215 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:22:01,218 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.53, loss:   2.75, ppl:  15.58, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3584[sec], evaluation: 0.0374[sec]\n",
            "2023-01-19 23:22:01,220 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:22:01,223 - INFO - joeynmt.training - \tSource:     همچنین شمعون ایشان را برکت داد و به مریم ، مادر طفل گفت : این طفل ، برای افتادن و برخاستن بسیاری در اسرائیل برگزیده شده است . او نشانی خواهد بود که علیه او سخن خواهند گفت\n",
            "2023-01-19 23:22:01,224 - INFO - joeynmt.training - \tReference:  شیمئون اونلارا خئیر دوعا وئریب کؤرپهنین آناسێ مریمه بئله دئدی : بو کؤرپه اسراعلده بیر چوخ آدامێن یێخیلماسینین یا اوجالماسێنێن سبهبکارێ وه اینسانلارین ردد ائتدیگی بیر علامت اولماق اۆچۆن تعیین اولونوب . \n",
            "2023-01-19 23:22:01,224 - INFO - joeynmt.training - \tHypothesis: شیمونو اونلارێ باکلندیرهرک دئدی : مریم اوغلوما ، بو آنالارێ ، اسراعل اؤؤلادلارین چوخو ایسه سئچیلمیشدیلر .\n",
            "2023-01-19 23:22:01,224 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:22:01,226 - INFO - joeynmt.training - \tSource:     او به آنچه دیده و شنیده است ، شهادت می‌دهد ، اما هیچ کس شهادت او را نمی‌پذیرد\n",
            "2023-01-19 23:22:01,226 - INFO - joeynmt.training - \tReference:  نه گؤروب ائشیدیبسه ، اونا شهادت ائدر ، آمما شهادتینی هئچ کس قبول ائتمز . \n",
            "2023-01-19 23:22:01,226 - INFO - joeynmt.training - \tHypothesis: اونون ائتدیگی گؤردوگو هر کس اونون شهادتی ائشیتمز ، آمما هئچ کس اونون شهادتینی قبول ائتمز .\n",
            "2023-01-19 23:22:01,226 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:22:01,228 - INFO - joeynmt.training - \tSource:      همچنین ، دیگر قضاوت مکنید تا در مورد شما قضاوت نشود ، دیگر محکوم مکنید تا محکوم نشوید . همواره گذشت کنید تا نسبت به شما گذشت شود . \n",
            "2023-01-19 23:22:01,229 - INFO - joeynmt.training - \tReference:  مۆحاکمه ائتمهیین ، سیز ده مۆحاکمه اولونمازسێنێز . هؤکم چێخارمایین ، سیزه ده هؤکم چێخارێلماز . باشقالارینی باغیشلایین ، سیز ده باغیشلاناجاقسینزینز . \n",
            "2023-01-19 23:22:01,229 - INFO - joeynmt.training - \tHypothesis: مۆحاکمه ائتمهیین کی ، مۆحاکمه اولونسون ، آمما باشقاسێنا مۆحاکمه اولون . بلکه سیزه محکوم اولون کی ، بلکه ، کئچیب دولاشێن .\n",
            "2023-01-19 23:22:01,229 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:22:01,231 - INFO - joeynmt.training - \tSource:     آن خائن به همراهان خود چنین نشانه‌ای داده بود : کسی را که ببوسم ، همان اوست . دستگیرش کنید . \n",
            "2023-01-19 23:22:01,231 - INFO - joeynmt.training - \tReference:  ایسهآیا خیانت ائدن یهودا اونلارا اشاره ایله بیلدیریب دئمیشدی : کیمی اؤپسم ، ایسا اودور ، اونو توتون . \n",
            "2023-01-19 23:22:01,231 - INFO - joeynmt.training - \tHypothesis: الیندهکیلا بیرلیکده الامتلر اولاندا بئله دئمیشدی : الی ایله آلێب آلێن . او دا اونون الینده اولان الیندهدیر .\n",
            "2023-01-19 23:22:09,283 - INFO - joeynmt.training - Epoch 148, Step:    51100, Batch Loss:     1.891089, Batch Acc: 0.528531, Tokens per Sec:    13257, Lr: 0.000040\n",
            "2023-01-19 23:22:14,224 - INFO - joeynmt.training - Epoch 148: total training loss 648.81\n",
            "2023-01-19 23:22:14,224 - INFO - joeynmt.training - EPOCH 149\n",
            "2023-01-19 23:22:17,252 - INFO - joeynmt.training - Epoch 149, Step:    51200, Batch Loss:     1.874115, Batch Acc: 0.534081, Tokens per Sec:    13925, Lr: 0.000040\n",
            "2023-01-19 23:22:25,120 - INFO - joeynmt.training - Epoch 149, Step:    51300, Batch Loss:     1.913098, Batch Acc: 0.533988, Tokens per Sec:    14035, Lr: 0.000039\n",
            "2023-01-19 23:22:33,067 - INFO - joeynmt.training - Epoch 149, Step:    51400, Batch Loss:     1.807049, Batch Acc: 0.528806, Tokens per Sec:    13704, Lr: 0.000039\n",
            "2023-01-19 23:22:40,903 - INFO - joeynmt.training - Epoch 149, Step:    51500, Batch Loss:     1.807658, Batch Acc: 0.526701, Tokens per Sec:    14098, Lr: 0.000039\n",
            "2023-01-19 23:22:41,483 - INFO - joeynmt.training - Epoch 149: total training loss 646.14\n",
            "2023-01-19 23:22:41,483 - INFO - joeynmt.training - EPOCH 150\n",
            "2023-01-19 23:22:48,899 - INFO - joeynmt.training - Epoch 150, Step:    51600, Batch Loss:     1.774909, Batch Acc: 0.536848, Tokens per Sec:    13899, Lr: 0.000039\n",
            "2023-01-19 23:22:56,915 - INFO - joeynmt.training - Epoch 150, Step:    51700, Batch Loss:     1.782462, Batch Acc: 0.532120, Tokens per Sec:    13752, Lr: 0.000039\n",
            "2023-01-19 23:23:06,144 - INFO - joeynmt.training - Epoch 150, Step:    51800, Batch Loss:     1.927970, Batch Acc: 0.527233, Tokens per Sec:    11931, Lr: 0.000039\n",
            "2023-01-19 23:23:10,331 - INFO - joeynmt.training - Epoch 150: total training loss 645.08\n",
            "2023-01-19 23:23:10,331 - INFO - joeynmt.training - EPOCH 151\n",
            "2023-01-19 23:23:14,225 - INFO - joeynmt.training - Epoch 151, Step:    51900, Batch Loss:     1.897743, Batch Acc: 0.531289, Tokens per Sec:    13661, Lr: 0.000039\n",
            "2023-01-19 23:23:22,093 - INFO - joeynmt.training - Epoch 151, Step:    52000, Batch Loss:     2.031995, Batch Acc: 0.530889, Tokens per Sec:    13745, Lr: 0.000039\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.77ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10154.45ex/s]\n",
            "2023-01-19 23:23:22,366 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=52000\n",
            "2023-01-19 23:23:22,366 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:23:26,897 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:23:26,898 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:23:26,898 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:23:26,899 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:23:26,901 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.67, loss:   2.77, ppl:  15.91, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4926[sec], evaluation: 0.0358[sec]\n",
            "2023-01-19 23:23:26,905 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:23:26,907 - INFO - joeynmt.training - \tSource:     به علاوه ، نجات از طریق هیچ کس دیگری میسر نیست ؛ زیرا هیچ نام دیگری زیر آسمان به انسان‌ها داده نشده است که از طریق آن نجات یابیم . \n",
            "2023-01-19 23:23:26,907 - INFO - joeynmt.training - \tReference:  اوندان باشقا هئچ کیمده خلاص یوخدور . چۆنکی سما آلتێندا بیز اینسانلارا وئریلن باشقا بیر آد یوخدور کی ، اونون واسطهسیله خلاص اولاق . \n",
            "2023-01-19 23:23:26,908 - INFO - joeynmt.training - \tHypothesis: باشقالارینا خلاص دئییل ، چۆنکی سماوی آدێ ایله خلاص دئییل ، چۆنکی سماوی آدێ ایله خلاص دئییل ، چۆنکی بیز ده خلاص اولدوق .\n",
            "2023-01-19 23:23:26,908 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:23:26,910 - INFO - joeynmt.training - \tSource:     دیر میرسی . \n",
            "2023-01-19 23:23:26,910 - INFO - joeynmt.training - \tReference:  بئواختا قالیرسان . \n",
            "2023-01-19 23:23:26,910 - INFO - joeynmt.training - \tHypothesis: چوخ دیر .\n",
            "2023-01-19 23:23:26,910 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:23:26,912 - INFO - joeynmt.training - \tSource:     سپس او را از شهر بیرون انداختند و سنگسارش کردند . شاهدان ، رداهای خود را پیش پاهای مردی جوان به نام سولس گذاشتند . \n",
            "2023-01-19 23:23:26,912 - INFO - joeynmt.training - \tReference:  اونو شهردن کنارا چێخارێب داشقالاق ائتدیلر . استئفانێن الئیهینه شاهدلیک ائدنلر اؤز اۆست گئییملرینی شاعل آدلێ بیر گنجین آیاقلاری آلتێنا قویدولار . \n",
            "2023-01-19 23:23:26,912 - INFO - joeynmt.training - \tHypothesis: سونرا اونو داشقالاقلایاراق یئروسهلیمه قوودولار . او ، آیاقلارینین کنارێندا یووولوب میصیر آدێنێ تمینلیلین ائتدی .\n",
            "2023-01-19 23:23:26,913 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:23:26,915 - INFO - joeynmt.training - \tSource:     اگر به‌راستی خطاکارم و جرمی سزاوار مرگ مرتکب شده‌ام ، از مرگ نمی‌گریزم . اما اگر اتهاماتی که این مردان به من نسبت می‌دهند ، بی‌اساس است ، هیچ کس حق ندارد برای خشنودی آنان مرا به دستشان تسلیم کند . من از قیصر درخواست فرجام می‌کنم ! \n",
            "2023-01-19 23:23:26,915 - INFO - joeynmt.training - \tReference:  اگر حاقسێزلێق ائتمیشمسه ، اؤلوم جزاسێنا لایق بیر ایش توتموشامسا ، اؤلومدن قاچمێرام . یوخ ، اگر بونلارێن اتحاملاری اساسسێزدێرسا ، منی اونلارێن الینه تسلیم ائتمک اۆچۆن هئچ کیمین صلاحیتی یوخدور . من قئیسرین محکمهسینه مۆراجعت ائتمک ایستهییرم . \n",
            "2023-01-19 23:23:26,915 - INFO - joeynmt.training - \tHypothesis: اگر سنه اؤلوم وه گۆناه ایش گؤرسهیدی ، اؤلومدن اوزاقلاشدێم . آمما اگر بو کیشی کیشی سنین رازێلێغێنێن آلماق اۆچۆن اؤلومو پوزماغێمدان اوزاقلاشدێرسا ، اؤزومدن رازێ سالماق ایستهسه ، منه تسلیم ائتمک ایستهمیرم .\n",
            "2023-01-19 23:23:34,920 - INFO - joeynmt.training - Epoch 151, Step:    52100, Batch Loss:     1.974628, Batch Acc: 0.531531, Tokens per Sec:    13301, Lr: 0.000039\n",
            "2023-01-19 23:23:42,613 - INFO - joeynmt.training - Epoch 151: total training loss 644.28\n",
            "2023-01-19 23:23:42,613 - INFO - joeynmt.training - EPOCH 152\n",
            "2023-01-19 23:23:42,855 - INFO - joeynmt.training - Epoch 152, Step:    52200, Batch Loss:     2.117224, Batch Acc: 0.513443, Tokens per Sec:    12811, Lr: 0.000039\n",
            "2023-01-19 23:23:50,805 - INFO - joeynmt.training - Epoch 152, Step:    52300, Batch Loss:     2.001903, Batch Acc: 0.534253, Tokens per Sec:    13863, Lr: 0.000039\n",
            "2023-01-19 23:23:58,926 - INFO - joeynmt.training - Epoch 152, Step:    52400, Batch Loss:     1.963881, Batch Acc: 0.531027, Tokens per Sec:    13782, Lr: 0.000039\n",
            "2023-01-19 23:24:07,942 - INFO - joeynmt.training - Epoch 152, Step:    52500, Batch Loss:     1.974290, Batch Acc: 0.530620, Tokens per Sec:    12134, Lr: 0.000039\n",
            "2023-01-19 23:24:11,257 - INFO - joeynmt.training - Epoch 152: total training loss 642.97\n",
            "2023-01-19 23:24:11,258 - INFO - joeynmt.training - EPOCH 153\n",
            "2023-01-19 23:24:15,805 - INFO - joeynmt.training - Epoch 153, Step:    52600, Batch Loss:     1.899405, Batch Acc: 0.532721, Tokens per Sec:    13858, Lr: 0.000039\n",
            "2023-01-19 23:24:23,744 - INFO - joeynmt.training - Epoch 153, Step:    52700, Batch Loss:     1.964639, Batch Acc: 0.533638, Tokens per Sec:    13870, Lr: 0.000039\n",
            "2023-01-19 23:24:31,741 - INFO - joeynmt.training - Epoch 153, Step:    52800, Batch Loss:     1.838649, Batch Acc: 0.531053, Tokens per Sec:    13802, Lr: 0.000039\n",
            "2023-01-19 23:24:38,770 - INFO - joeynmt.training - Epoch 153: total training loss 644.52\n",
            "2023-01-19 23:24:38,770 - INFO - joeynmt.training - EPOCH 154\n",
            "2023-01-19 23:24:39,723 - INFO - joeynmt.training - Epoch 154, Step:    52900, Batch Loss:     1.664963, Batch Acc: 0.538601, Tokens per Sec:    13950, Lr: 0.000039\n",
            "2023-01-19 23:24:47,624 - INFO - joeynmt.training - Epoch 154, Step:    53000, Batch Loss:     1.801428, Batch Acc: 0.536944, Tokens per Sec:    13852, Lr: 0.000039\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 128.27ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8824.68ex/s]\n",
            "2023-01-19 23:24:47,923 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=53000\n",
            "2023-01-19 23:24:47,924 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:24:52,394 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:24:52,395 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:24:52,395 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:24:52,396 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:24:52,399 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.06, loss:   2.78, ppl:  16.08, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4287[sec], evaluation: 0.0382[sec]\n",
            "2023-01-19 23:24:52,402 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:24:52,416 - INFO - joeynmt.training - \tSource:     در او همهٔ گنج‌های حکمت و شناخت نهفته است . \n",
            "2023-01-19 23:24:52,416 - INFO - joeynmt.training - \tReference:  چۆنکی هکمت وه بیلیگین بۆتۆن خزینهلری مصیحده گیزلهدیلمیشدیر . \n",
            "2023-01-19 23:24:52,416 - INFO - joeynmt.training - \tHypothesis: او ، هامێسێ اونون هکمت صاحبدر ، هامێسێ تانێمێر .\n",
            "2023-01-19 23:24:52,416 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:24:52,420 - INFO - joeynmt.training - \tSource:     در جواب گفت : به شما می‌گویم ، به هر که دارد ، بیشتر داده می‌شود ، اما آن که ندارد ، حتی آنچه دارد هم از او گرفته خواهد شد . \n",
            "2023-01-19 23:24:52,420 - INFO - joeynmt.training - \tReference:  او ، سؤزونه بئله داوام ائتدی : سیزه دئییرم : کیمین وارێدێرسا ، داها چوخ وئریلهجک ، کیمین یوخودورسا ، الینده اولان دا آلێناجاق . \n",
            "2023-01-19 23:24:52,420 - INFO - joeynmt.training - \tHypothesis: اونلارا جاواب وئردی : سیزه دوغروسونو دئییرم : هانسێنا کۆفر ائدن هر کسه نه یوخ ، نه ده اوندان آلاجاق .\n",
            "2023-01-19 23:24:52,420 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:24:52,423 - INFO - joeynmt.training - \tSource:     فرمود : به زودی سخت پشیمان خواهند شد . \n",
            "2023-01-19 23:24:52,423 - INFO - joeynmt.training - \tReference:   بویوردو : بیر آزدان پئشمان اولاجاقلار ! \n",
            "2023-01-19 23:24:52,423 - INFO - joeynmt.training - \tHypothesis: بویوردو : بیز مۆتلق مۆتلق شدتلی بیر ازابلا دۆچار اولاجاقلار !\n",
            "2023-01-19 23:24:52,424 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:24:52,426 - INFO - joeynmt.training - \tSource:     این قرصو که بخوری دیگه الکی نمی‌شاشی . \n",
            "2023-01-19 23:24:52,426 - INFO - joeynmt.training - \tReference:  بوحبی یئیه ر سن کی تئز تئز یئرینه ایشه مه سن . \n",
            "2023-01-19 23:24:52,426 - INFO - joeynmt.training - \tHypothesis: بو ، یئیهجهییمیز یئین یئمز !\n",
            "2023-01-19 23:25:00,422 - INFO - joeynmt.training - Epoch 154, Step:    53100, Batch Loss:     1.900776, Batch Acc: 0.534346, Tokens per Sec:    13229, Lr: 0.000039\n",
            "2023-01-19 23:25:08,418 - INFO - joeynmt.training - Epoch 154, Step:    53200, Batch Loss:     1.867531, Batch Acc: 0.532641, Tokens per Sec:    13587, Lr: 0.000039\n",
            "2023-01-19 23:25:11,209 - INFO - joeynmt.training - Epoch 154: total training loss 641.87\n",
            "2023-01-19 23:25:11,209 - INFO - joeynmt.training - EPOCH 155\n",
            "2023-01-19 23:25:16,585 - INFO - joeynmt.training - Epoch 155, Step:    53300, Batch Loss:     1.875280, Batch Acc: 0.535848, Tokens per Sec:    13581, Lr: 0.000039\n",
            "2023-01-19 23:25:25,660 - INFO - joeynmt.training - Epoch 155, Step:    53400, Batch Loss:     1.804997, Batch Acc: 0.532653, Tokens per Sec:    12191, Lr: 0.000039\n",
            "2023-01-19 23:25:33,655 - INFO - joeynmt.training - Epoch 155, Step:    53500, Batch Loss:     1.694618, Batch Acc: 0.534323, Tokens per Sec:    13697, Lr: 0.000039\n",
            "2023-01-19 23:25:39,990 - INFO - joeynmt.training - Epoch 155: total training loss 642.98\n",
            "2023-01-19 23:25:39,990 - INFO - joeynmt.training - EPOCH 156\n",
            "2023-01-19 23:25:41,584 - INFO - joeynmt.training - Epoch 156, Step:    53600, Batch Loss:     1.877576, Batch Acc: 0.540277, Tokens per Sec:    13590, Lr: 0.000039\n",
            "2023-01-19 23:25:49,493 - INFO - joeynmt.training - Epoch 156, Step:    53700, Batch Loss:     1.967853, Batch Acc: 0.533938, Tokens per Sec:    14015, Lr: 0.000039\n",
            "2023-01-19 23:25:57,409 - INFO - joeynmt.training - Epoch 156, Step:    53800, Batch Loss:     1.810020, Batch Acc: 0.536635, Tokens per Sec:    13850, Lr: 0.000039\n",
            "2023-01-19 23:26:05,332 - INFO - joeynmt.training - Epoch 156, Step:    53900, Batch Loss:     1.782104, Batch Acc: 0.533916, Tokens per Sec:    13903, Lr: 0.000039\n",
            "2023-01-19 23:26:07,296 - INFO - joeynmt.training - Epoch 156: total training loss 638.43\n",
            "2023-01-19 23:26:07,297 - INFO - joeynmt.training - EPOCH 157\n",
            "2023-01-19 23:26:13,235 - INFO - joeynmt.training - Epoch 157, Step:    54000, Batch Loss:     1.938191, Batch Acc: 0.537643, Tokens per Sec:    13822, Lr: 0.000038\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.58ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10293.08ex/s]\n",
            "2023-01-19 23:26:13,502 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=54000\n",
            "2023-01-19 23:26:13,503 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:26:19,079 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:26:19,079 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:26:19,079 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:26:19,080 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:26:19,083 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.02, loss:   2.73, ppl:  15.31, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.5388[sec], evaluation: 0.0343[sec]\n",
            "2023-01-19 23:26:19,086 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:26:19,089 - INFO - joeynmt.training - \tSource:     این امر که خدا او را از مردگان رستاخیز داد و دیگر هرگز به فسادپذیری بازنمی‌گردد ، چنین بیان شده است : من به شما احسان‌هایی خواهم کرد که به داوود وعده داده شده بود . این وعده قطعی است . \n",
            "2023-01-19 23:26:19,089 - INFO - joeynmt.training - \tReference:  آللاه اونو اؤلولر آراسێندان دیریلدرک هئچ واخت چۆرۆمهیه قویمایاجاق . اونا گؤره دئمیشدی : داوودا ود ائتدیگیم صادق محبتی سیزه گؤستهرهجهیم . \n",
            "2023-01-19 23:26:19,089 - INFO - joeynmt.training - \tHypothesis: آللاه اونو اؤلولر آراسێندان دیریلتمهیه یئنه ده دیریلتمک ایستهدی . بئلهجه سیز منه وئریلهجک : داوود وه د اولونموش ود ائدیلن وه د اولوندوغونوز حالدا ،\n",
            "2023-01-19 23:26:19,089 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:26:19,092 - INFO - joeynmt.training - \tSource:     روز بعد ، دو نفر را دید که نزاع می‌کنند ، پس سعی کرد آنان را آشتی دهد و گفت : ای مردان ، شما با هم برادرید . چرا با یکدیگر چنین بدرفتاری می‌کنید ؟ \n",
            "2023-01-19 23:26:19,092 - INFO - joeynmt.training - \tReference:  ائرتهسی گۆن موسا دالاشان ایکی ایسرایللینی گؤرنده اونلارێ بارێشدێرماق ایستهدی وه دئدی : آی کیشیلر ، سیز سویداشسینیز ، نیه بیر بیرینیزه زرر وورورسونوز ؟ \n",
            "2023-01-19 23:26:19,092 - INFO - joeynmt.training - \tHypothesis: ائرتهسی گۆندن سونرا اونلارێ گؤردۆکده اونلارا بئله دئدی : ائی کیشیلر ، سیزه بیر بیر بیرینیزی قارداشلا گئدین ، نیه پیسلهشیرسینیز ؟ بو آداملارلا بئله پیس بیر بیرینیزلهشیرسینیز ؟\n",
            "2023-01-19 23:26:19,092 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:26:19,094 - INFO - joeynmt.training - \tSource:     چراغ را نمی‌افروزند که زیر سرپوش قرار دهند ، بلکه آن را روی پایه می‌گذارند تا نورش بر همهٔ اهالی خانه بتابد . \n",
            "2023-01-19 23:26:19,094 - INFO - joeynmt.training - \tReference:  هئچ کیم چێراغێ یاندیریب قابێن آلتێنا قویماز . اکسینه ، چێراقدانا قویار کی ، ائودهکیلرین هامیسینی ایشیقلاندیرسین . \n",
            "2023-01-19 23:26:19,095 - INFO - joeynmt.training - \tHypothesis: چۆنکی چێراقدان چێراق یاندیرمایانلار دئییل ، بۆتۆن ایشیغیشدیریر . بۆتۆن ائوه ایشلهنننن ائو ائوه ساخلاسێن .\n",
            "2023-01-19 23:26:19,095 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:26:19,097 - INFO - joeynmt.training - \tSource:     این زن گاهی این قدر از من دور می‌شود \n",
            "2023-01-19 23:26:19,097 - INFO - joeynmt.training - \tReference:  بوقادین گاهدان او قدر من دن اوزاقلاشیر\n",
            "2023-01-19 23:26:19,097 - INFO - joeynmt.training - \tHypothesis: بو قادین لاریم بوندان اوزاق بیر آروادين\n",
            "2023-01-19 23:26:26,976 - INFO - joeynmt.training - Epoch 157, Step:    54100, Batch Loss:     1.783123, Batch Acc: 0.536846, Tokens per Sec:    13571, Lr: 0.000038\n",
            "2023-01-19 23:26:34,936 - INFO - joeynmt.training - Epoch 157, Step:    54200, Batch Loss:     1.775630, Batch Acc: 0.529404, Tokens per Sec:    13725, Lr: 0.000038\n",
            "2023-01-19 23:26:40,521 - INFO - joeynmt.training - Epoch 157: total training loss 640.54\n",
            "2023-01-19 23:26:40,522 - INFO - joeynmt.training - EPOCH 158\n",
            "2023-01-19 23:26:42,821 - INFO - joeynmt.training - Epoch 158, Step:    54300, Batch Loss:     1.868119, Batch Acc: 0.540361, Tokens per Sec:    14008, Lr: 0.000038\n",
            "2023-01-19 23:26:50,750 - INFO - joeynmt.training - Epoch 158, Step:    54400, Batch Loss:     1.941207, Batch Acc: 0.540845, Tokens per Sec:    14071, Lr: 0.000038\n",
            "2023-01-19 23:26:58,741 - INFO - joeynmt.training - Epoch 158, Step:    54500, Batch Loss:     1.701103, Batch Acc: 0.535815, Tokens per Sec:    13727, Lr: 0.000038\n",
            "2023-01-19 23:27:06,638 - INFO - joeynmt.training - Epoch 158, Step:    54600, Batch Loss:     1.806868, Batch Acc: 0.535134, Tokens per Sec:    13781, Lr: 0.000038\n",
            "2023-01-19 23:27:07,962 - INFO - joeynmt.training - Epoch 158: total training loss 637.32\n",
            "2023-01-19 23:27:07,962 - INFO - joeynmt.training - EPOCH 159\n",
            "2023-01-19 23:27:14,621 - INFO - joeynmt.training - Epoch 159, Step:    54700, Batch Loss:     1.883198, Batch Acc: 0.539515, Tokens per Sec:    14033, Lr: 0.000038\n",
            "2023-01-19 23:27:22,520 - INFO - joeynmt.training - Epoch 159, Step:    54800, Batch Loss:     1.899172, Batch Acc: 0.538344, Tokens per Sec:    13831, Lr: 0.000038\n",
            "2023-01-19 23:27:30,388 - INFO - joeynmt.training - Epoch 159, Step:    54900, Batch Loss:     1.678067, Batch Acc: 0.533051, Tokens per Sec:    14040, Lr: 0.000038\n",
            "2023-01-19 23:27:36,535 - INFO - joeynmt.training - Epoch 159: total training loss 634.57\n",
            "2023-01-19 23:27:36,536 - INFO - joeynmt.training - EPOCH 160\n",
            "2023-01-19 23:27:39,743 - INFO - joeynmt.training - Epoch 160, Step:    55000, Batch Loss:     1.910674, Batch Acc: 0.535254, Tokens per Sec:    13877, Lr: 0.000038\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 124.16ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9524.21ex/s]\n",
            "2023-01-19 23:27:40,040 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=55000\n",
            "2023-01-19 23:27:40,040 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:27:44,486 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:27:44,486 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:27:44,487 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:27:44,488 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:27:44,491 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.36, loss:   2.69, ppl:  14.76, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4055[sec], evaluation: 0.0377[sec]\n",
            "2023-01-19 23:27:44,493 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:27:44,496 - INFO - joeynmt.training - \tSource:     بگو : من شما را فقط به وسیله وحی هشدار می‌دهم . و لی‌ چون کران بیم داده شوند ، دعوت را نمی‌شنوند . \n",
            "2023-01-19 23:27:44,496 - INFO - joeynmt.training - \tReference:   دئ : من سیزی آنجاق وحی ایله قورخودورام . کارلار قورخودولدوغو زامان چاغێرێشێ ائشیتمزلر ! \n",
            "2023-01-19 23:27:44,496 - INFO - joeynmt.training - \tHypothesis: دئ : من سیزی یالنیز من آنجاق قورخودان بیر پیغمبرم . لاکین من اونوتدورلار . اونلار ائشیتمزلر .\n",
            "2023-01-19 23:27:44,496 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:27:44,498 - INFO - joeynmt.training - \tSource:     ای برادران ، مثلی از زندگی روزمره برایتان می‌آورم : وقتی عهدی معتبر می‌شود ، حتی اگر فقط از طرف یک انسان باشد ، هیچ کس نمی‌تواند آن را لغو کند یا چیزی بر آن بیفزاید . \n",
            "2023-01-19 23:27:44,499 - INFO - joeynmt.training - \tReference:  قارداشلار ، اینسان دۆشۆنجهسی اساسێندا دئییرم : هتا اینسانین بئله تصدیق ائتدیگی اهدی هئچ کس لغو ائده بیلمز یاخود دا اونا هئچ بیر شئی الاوه ائده بیلمز . \n",
            "2023-01-19 23:27:44,499 - INFO - joeynmt.training - \tHypothesis: قارداشلار ، سیزه بیر مسل چکیرم : بیر اهد آلدێغێمێز بیر مۆبتهور اولماسا ، بیر اینسانین بیرینی هئچ کسه ائده بیلمز .\n",
            "2023-01-19 23:27:44,499 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:27:44,501 - INFO - joeynmt.training - \tSource:     سپس پولس و برنابا با شهامت به آنان گفتند : لازم بود که کلام خدا نخست به شما گفته شود . حال که آن را رد می‌کنید و خود را شایستهٔ زندگی ابدی نمی‌دانید ، ما نیز آن را به غیریهودیان موعظه می‌کنیم . \n",
            "2023-01-19 23:27:44,502 - INFO - joeynmt.training - \tReference:  پاوللا بارنابا ایسه جصارتله دئدی : آللاهێن کلامێ اولجه سیزه بیان اولونمالێ ایدی . سیزین اوندان امتنا ائتدیگینیز وه اؤزونوزو ابدی هیاتا لایق گؤرمهدیگینیز اۆچۆن بیز ایندی باشقا ملتلره اۆز توتوروق . \n",
            "2023-01-19 23:27:44,502 - INFO - joeynmt.training - \tHypothesis: پاوللا بارنابا ایله بیرلیکده ماتئدونیایایایا گؤره دئدی : سیز آللاهێن کلامێنێن آلێب اؤز کلامێنێن هیاتی یایدیغیمیزی ، باشقا ملتلری ده خلاص ائده بیلهریک .\n",
            "2023-01-19 23:27:44,502 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:27:44,504 - INFO - joeynmt.training - \tSource:     زیرا اگر آن پیام که از طریق فرشتگان گفته شد ، استوار می‌مانْد و مجازات هر خطا و نافرمانی مطابق عدالت می‌بود\n",
            "2023-01-19 23:27:44,504 - INFO - joeynmt.training - \tReference:  چۆنکی ملکلر واسطهسیله بیان ائدیلن سؤز قۆۆهیه میندی ، هر جۆر تقصیر وه ایتااائسیزلیه حاقلێ جزا وئریلدیسه ، \n",
            "2023-01-19 23:27:44,505 - INFO - joeynmt.training - \tHypothesis: چۆنکی ملکلر واسطهسیله اؤیرنولورسا ، هر شئیه گؤره هر شئیه گؤره سالئهلیک وه عدالتلی احمیت چکیرسه ، عدالتلی ایدی .\n",
            "2023-01-19 23:27:52,426 - INFO - joeynmt.training - Epoch 160, Step:    55100, Batch Loss:     1.952050, Batch Acc: 0.535197, Tokens per Sec:    13210, Lr: 0.000038\n",
            "2023-01-19 23:28:00,255 - INFO - joeynmt.training - Epoch 160, Step:    55200, Batch Loss:     1.788925, Batch Acc: 0.537974, Tokens per Sec:    13997, Lr: 0.000038\n",
            "2023-01-19 23:28:08,379 - INFO - joeynmt.training - Epoch 160, Step:    55300, Batch Loss:     1.661772, Batch Acc: 0.538818, Tokens per Sec:    13504, Lr: 0.000038\n",
            "2023-01-19 23:28:08,964 - INFO - joeynmt.training - Epoch 160: total training loss 638.85\n",
            "2023-01-19 23:28:08,965 - INFO - joeynmt.training - EPOCH 161\n",
            "2023-01-19 23:28:16,473 - INFO - joeynmt.training - Epoch 161, Step:    55400, Batch Loss:     1.854880, Batch Acc: 0.540952, Tokens per Sec:    13598, Lr: 0.000038\n",
            "2023-01-19 23:28:24,517 - INFO - joeynmt.training - Epoch 161, Step:    55500, Batch Loss:     1.864387, Batch Acc: 0.534483, Tokens per Sec:    13526, Lr: 0.000038\n",
            "2023-01-19 23:28:33,620 - INFO - joeynmt.training - Epoch 161, Step:    55600, Batch Loss:     1.777163, Batch Acc: 0.539649, Tokens per Sec:    12125, Lr: 0.000038\n",
            "2023-01-19 23:28:38,197 - INFO - joeynmt.training - Epoch 161: total training loss 638.10\n",
            "2023-01-19 23:28:38,197 - INFO - joeynmt.training - EPOCH 162\n",
            "2023-01-19 23:28:41,896 - INFO - joeynmt.training - Epoch 162, Step:    55700, Batch Loss:     1.647151, Batch Acc: 0.540115, Tokens per Sec:    13778, Lr: 0.000038\n",
            "2023-01-19 23:28:49,805 - INFO - joeynmt.training - Epoch 162, Step:    55800, Batch Loss:     1.867071, Batch Acc: 0.539393, Tokens per Sec:    13915, Lr: 0.000038\n",
            "2023-01-19 23:28:57,715 - INFO - joeynmt.training - Epoch 162, Step:    55900, Batch Loss:     1.824353, Batch Acc: 0.535604, Tokens per Sec:    13897, Lr: 0.000038\n",
            "2023-01-19 23:29:05,577 - INFO - joeynmt.training - Epoch 162: total training loss 633.72\n",
            "2023-01-19 23:29:05,577 - INFO - joeynmt.training - EPOCH 163\n",
            "2023-01-19 23:29:05,653 - INFO - joeynmt.training - Epoch 163, Step:    56000, Batch Loss:     1.940454, Batch Acc: 0.511494, Tokens per Sec:    13845, Lr: 0.000038\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 126.63ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8824.26ex/s]\n",
            "2023-01-19 23:29:05,948 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=56000\n",
            "2023-01-19 23:29:05,948 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:29:10,308 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:29:10,309 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:29:10,309 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:29:10,310 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:29:10,313 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.17, loss:   2.64, ppl:  13.96, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3211[sec], evaluation: 0.0367[sec]\n",
            "2023-01-19 23:29:10,510 - INFO - joeynmt.helpers - delete RESULTS_fa2azb/model/45000.ckpt\n",
            "2023-01-19 23:29:10,522 - INFO - joeynmt.helpers - delete /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/model/45000.ckpt\n",
            "2023-01-19 23:29:10,522 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/model/45000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/model/45000.ckpt')\n",
            "2023-01-19 23:29:10,524 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:29:10,528 - INFO - joeynmt.training - \tSource:     این زن مطرود را بیشتر اوقات حذف می‌كنم . \n",
            "2023-01-19 23:29:10,528 - INFO - joeynmt.training - \tReference:  البته بو دیل دئیه ن قادینی چوخ واخت لار حیاتیم دان سیلیره م . \n",
            "2023-01-19 23:29:10,528 - INFO - joeynmt.training - \tHypothesis: بو قادین ین دان داها چوخ حوصی حاجی نین ائله م .\n",
            "2023-01-19 23:29:10,529 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:29:10,531 - INFO - joeynmt.training - \tSource:     آنگاه که زمین به لرزش شدید خود لرزانیده شود ، \n",
            "2023-01-19 23:29:10,531 - INFO - joeynmt.training - \tReference:  یئر اؤزونه مخصوص بیر شدتله لرزهیه گلیب تیترهیهجهیی زامان ؛ \n",
            "2023-01-19 23:29:10,531 - INFO - joeynmt.training - \tHypothesis: او زامان کی ، یئر اۆزۆنۆن سیررینی سارسێلدێ .\n",
            "2023-01-19 23:29:10,531 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:29:10,533 - INFO - joeynmt.training - \tSource:     ابراهیم نه یهودی بود و نه نصرانی ، بلکه حق گرایی فرمانبردار بود ، و از مشرکان نبود . \n",
            "2023-01-19 23:29:10,533 - INFO - joeynmt.training - \tReference:  ابراهیم نه یهودی ، نه ده خاچپرست ایدی . او آنجاق حنیف مۆسلمان ایدی وه شریک قوشانلاردان دئییلدی . \n",
            "2023-01-19 23:29:10,533 - INFO - joeynmt.training - \tHypothesis: ابراهیمین وه ابادت ائتمهین دئییل ، مۆشرکلردن اولمادێ . مۆشرکلردن اولمادێ . مۆشرکلردن اولمادێ .\n",
            "2023-01-19 23:29:10,534 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:29:10,535 - INFO - joeynmt.training - \tSource:     آن دو همان دم تورهای خود را رها کردند و به دنبال او رفتند . \n",
            "2023-01-19 23:29:10,536 - INFO - joeynmt.training - \tReference:  اونلار درحال تورلارێ کنارا آتێب اونون آردێنجا گئتدیلر . \n",
            "2023-01-19 23:29:10,536 - INFO - joeynmt.training - \tHypothesis: او آندا اونلارێن ایکیسینین یانینا قویدولار وه اونون آردێنجا گئتدی .\n",
            "2023-01-19 23:29:18,703 - INFO - joeynmt.training - Epoch 163, Step:    56100, Batch Loss:     1.809700, Batch Acc: 0.542949, Tokens per Sec:    12545, Lr: 0.000038\n",
            "2023-01-19 23:29:26,642 - INFO - joeynmt.training - Epoch 163, Step:    56200, Batch Loss:     1.836908, Batch Acc: 0.539459, Tokens per Sec:    14020, Lr: 0.000038\n",
            "2023-01-19 23:29:34,639 - INFO - joeynmt.training - Epoch 163, Step:    56300, Batch Loss:     1.772364, Batch Acc: 0.538994, Tokens per Sec:    13722, Lr: 0.000038\n",
            "2023-01-19 23:29:38,146 - INFO - joeynmt.training - Epoch 163: total training loss 634.09\n",
            "2023-01-19 23:29:38,147 - INFO - joeynmt.training - EPOCH 164\n",
            "2023-01-19 23:29:42,483 - INFO - joeynmt.training - Epoch 164, Step:    56400, Batch Loss:     1.765897, Batch Acc: 0.541884, Tokens per Sec:    14111, Lr: 0.000038\n",
            "2023-01-19 23:29:51,287 - INFO - joeynmt.training - Epoch 164, Step:    56500, Batch Loss:     1.848182, Batch Acc: 0.539844, Tokens per Sec:    12477, Lr: 0.000038\n",
            "2023-01-19 23:29:59,705 - INFO - joeynmt.training - Epoch 164, Step:    56600, Batch Loss:     2.019758, Batch Acc: 0.534551, Tokens per Sec:    13062, Lr: 0.000038\n",
            "2023-01-19 23:30:06,890 - INFO - joeynmt.training - Epoch 164: total training loss 632.42\n",
            "2023-01-19 23:30:06,890 - INFO - joeynmt.training - EPOCH 165\n",
            "2023-01-19 23:30:07,705 - INFO - joeynmt.training - Epoch 165, Step:    56700, Batch Loss:     1.819633, Batch Acc: 0.539956, Tokens per Sec:    13466, Lr: 0.000038\n",
            "2023-01-19 23:30:15,707 - INFO - joeynmt.training - Epoch 165, Step:    56800, Batch Loss:     1.707772, Batch Acc: 0.540520, Tokens per Sec:    13654, Lr: 0.000038\n",
            "2023-01-19 23:30:23,758 - INFO - joeynmt.training - Epoch 165, Step:    56900, Batch Loss:     1.856703, Batch Acc: 0.539994, Tokens per Sec:    13520, Lr: 0.000037\n",
            "2023-01-19 23:30:31,602 - INFO - joeynmt.training - Epoch 165, Step:    57000, Batch Loss:     1.890613, Batch Acc: 0.537715, Tokens per Sec:    14227, Lr: 0.000037\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 78.31ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9873.71ex/s]\n",
            "2023-01-19 23:30:31,900 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=57000\n",
            "2023-01-19 23:30:31,900 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:30:36,401 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:30:36,402 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:30:36,402 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:30:36,403 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:30:36,406 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.23, loss:   2.77, ppl:  15.90, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4630[sec], evaluation: 0.0357[sec]\n",
            "2023-01-19 23:30:36,409 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:30:36,412 - INFO - joeynmt.training - \tSource:     آیا فرمانروایی آسمانها و زمین و آنچه میان آن دو است از آن ایشان است ؟ اگر چنین است‌ پس با چنگ زدن‌ در آن اسباب به بالا روند . \n",
            "2023-01-19 23:30:36,412 - INFO - joeynmt.training - \tReference:  یاخود گؤیلرین ، یئرین وه اونلارێن آراسێندا اولان هر شئین اختیاری اونلارێن الیندهدیر ائله ایسه قوی ایپلردن یاپیشیب قالخسێنلار ! \n",
            "2023-01-19 23:30:36,412 - INFO - joeynmt.training - \tHypothesis: مگر گؤیلرین وه یئرین هؤکمو اونلارێن آراسێندان هؤکمو وارمێ ؟ اونلار دا بئلهدیرلر . اگر اونلار مۆلکۆبلنده اولسایدی ، اونلار دا مۆختلف چکرلر .\n",
            "2023-01-19 23:30:36,412 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:30:36,414 - INFO - joeynmt.training - \tSource:     و او آن را در پی خود سخنی جاویدان کرد ، باشد که آنان به توحید بازگردند . \n",
            "2023-01-19 23:30:36,415 - INFO - joeynmt.training - \tReference:   اونو اؤز نسلی آراسێندا همیشهلیک قالان بیر سؤز ائتدی . بلکه ، قاییدالار ! \n",
            "2023-01-19 23:30:36,415 - INFO - joeynmt.training - \tHypothesis: اونو آنجاق اؤزونه دانێشێردێ کی ، اونلار سنین سؤزونه قاییداجاقلار .\n",
            "2023-01-19 23:30:36,415 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:30:36,417 - INFO - joeynmt.training - \tSource:     گاهی‌ وقت‌ غروب‌ . \n",
            "2023-01-19 23:30:36,417 - INFO - joeynmt.training - \tReference:  گون باتا بات دا . \n",
            "2023-01-19 23:30:36,417 - INFO - joeynmt.training - \tHypothesis: و و آز بیر واخت لار .\n",
            "2023-01-19 23:30:36,417 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:30:36,419 - INFO - joeynmt.training - \tSource:     در آن روز ، شما به پیشگاه خدا عرضه می‌شوید ، و پوشیده‌ای از شما پوشیده نمی‌ماند . \n",
            "2023-01-19 23:30:36,419 - INFO - joeynmt.training - \tReference:  او گۆن سیز گتیریلهجکسینزینز . سیزین هئچ بیر سیررینیز گیزلی قالمایاجاقدیر ! \n",
            "2023-01-19 23:30:36,420 - INFO - joeynmt.training - \tHypothesis: او گۆن سیز آللاهێن هۆزورونا قایداجاقسینیز . سیز سیزه نه ده بیر یوواز اولون !\n",
            "2023-01-19 23:30:39,292 - INFO - joeynmt.training - Epoch 165: total training loss 632.38\n",
            "2023-01-19 23:30:39,293 - INFO - joeynmt.training - EPOCH 166\n",
            "2023-01-19 23:30:44,366 - INFO - joeynmt.training - Epoch 166, Step:    57100, Batch Loss:     1.836706, Batch Acc: 0.546819, Tokens per Sec:    13867, Lr: 0.000037\n",
            "2023-01-19 23:30:53,694 - INFO - joeynmt.training - Epoch 166, Step:    57200, Batch Loss:     1.803476, Batch Acc: 0.543462, Tokens per Sec:    11731, Lr: 0.000037\n",
            "2023-01-19 23:31:01,704 - INFO - joeynmt.training - Epoch 166, Step:    57300, Batch Loss:     1.839152, Batch Acc: 0.538849, Tokens per Sec:    13821, Lr: 0.000037\n",
            "2023-01-19 23:31:08,167 - INFO - joeynmt.training - Epoch 166: total training loss 629.80\n",
            "2023-01-19 23:31:08,167 - INFO - joeynmt.training - EPOCH 167\n",
            "2023-01-19 23:31:09,667 - INFO - joeynmt.training - Epoch 167, Step:    57400, Batch Loss:     1.988717, Batch Acc: 0.542234, Tokens per Sec:    13464, Lr: 0.000037\n",
            "2023-01-19 23:31:17,712 - INFO - joeynmt.training - Epoch 167, Step:    57500, Batch Loss:     1.868931, Batch Acc: 0.540473, Tokens per Sec:    13865, Lr: 0.000037\n",
            "2023-01-19 23:31:25,714 - INFO - joeynmt.training - Epoch 167, Step:    57600, Batch Loss:     1.770920, Batch Acc: 0.540895, Tokens per Sec:    13713, Lr: 0.000037\n",
            "2023-01-19 23:31:33,629 - INFO - joeynmt.training - Epoch 167, Step:    57700, Batch Loss:     1.773368, Batch Acc: 0.540227, Tokens per Sec:    13851, Lr: 0.000037\n",
            "2023-01-19 23:31:35,777 - INFO - joeynmt.training - Epoch 167: total training loss 631.34\n",
            "2023-01-19 23:31:35,777 - INFO - joeynmt.training - EPOCH 168\n",
            "2023-01-19 23:31:41,519 - INFO - joeynmt.training - Epoch 168, Step:    57800, Batch Loss:     1.935252, Batch Acc: 0.542243, Tokens per Sec:    13891, Lr: 0.000037\n",
            "2023-01-19 23:31:49,377 - INFO - joeynmt.training - Epoch 168, Step:    57900, Batch Loss:     1.986969, Batch Acc: 0.541179, Tokens per Sec:    13889, Lr: 0.000037\n",
            "2023-01-19 23:31:57,340 - INFO - joeynmt.training - Epoch 168, Step:    58000, Batch Loss:     1.834887, Batch Acc: 0.542328, Tokens per Sec:    13801, Lr: 0.000037\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 119.40ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8997.94ex/s]\n",
            "2023-01-19 23:31:57,640 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=58000\n",
            "2023-01-19 23:31:57,640 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:32:02,213 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:32:02,213 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:32:02,213 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:32:02,214 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:32:02,217 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.75, loss:   2.71, ppl:  14.96, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5347[sec], evaluation: 0.0351[sec]\n",
            "2023-01-19 23:32:02,220 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:32:02,223 - INFO - joeynmt.training - \tSource:     برای الفت‌دادن قریش ، \n",
            "2023-01-19 23:32:02,223 - INFO - joeynmt.training - \tReference:   قۆرئیشین اۆلفتی خاطرینه ، \n",
            "2023-01-19 23:32:02,223 - INFO - joeynmt.training - \tHypothesis: اله سالدی\n",
            "2023-01-19 23:32:02,224 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:32:02,226 - INFO - joeynmt.training - \tSource:     لباس کامل رزم را که از خداست بر تن کنید تا بتوانید در برابر حیله‌های ابلیس استوار بایستید ؛ \n",
            "2023-01-19 23:32:02,226 - INFO - joeynmt.training - \tReference:  ایبلیسین هییلهلرینه قارشێ دورا بیلمک اۆچۆن آللاهێن وئردیگی بۆتۆن زیرئه وه سلاحلارا بۆرۆنۆن . \n",
            "2023-01-19 23:32:02,226 - INFO - joeynmt.training - \tHypothesis: ربیمیزین مۆحکم یارپادن آللاها شۆکۆر ائدهسینیز .\n",
            "2023-01-19 23:32:02,226 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:32:02,228 - INFO - joeynmt.training - \tSource:     مگر کسانی که پروردگار تو به آنان رحم کرده ، و برای همین آنان را آفریده است . و وعده پروردگارت چنین‌ تحقق پذیرفته است که : البته جهنم را از جن و انس یکسره پر خواهم کرد . \n",
            "2023-01-19 23:32:02,228 - INFO - joeynmt.training - \tReference:  ربینین مرحمت ائتدیگی کیمسهلر استصنادیر . اونلارێ بونون اۆچۆن یاراتمیشدیر آرتێق ربینین : من جهنمی بۆتۆن جینلر وه اینسانلارلا دولدورآجاغام ! سؤزو تامامیله یئرینه یئتدی ! \n",
            "2023-01-19 23:32:02,228 - INFO - joeynmt.training - \tHypothesis: یالنیز ربینه رحم ائلهسی اولاراق یاراتدیغینا گؤرهدیر . بو ، ربینی قبول ائدن وه د اولونموشدور . دئینلر ده : جینلر بیر جین وه وه د اولونموشدور !\n",
            "2023-01-19 23:32:02,228 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:32:02,230 - INFO - joeynmt.training - \tSource:     اما آنان به او اصرار ورزیدند که بماند و گفتند : با ما بمان ؛ زیرا چیزی به پایان روز نمانده و نزدیک غروب است . سپس با آنان به خانه رفت تا نزدشان بماند . \n",
            "2023-01-19 23:32:02,230 - INFO - joeynmt.training - \tReference:  آمما اونلار ایسهآدان تکیدله خاهش ائدیب دئدیلر : بیزیمله قال ، چۆنکی آخشام دۆشۆر وه گۆن باتماق اۆزرهدیر . او دا اونلارلا قالماق اۆچۆن ایچری گیردی . \n",
            "2023-01-19 23:32:02,231 - INFO - joeynmt.training - \tHypothesis: آمما اونلار اونا گؤره کی ، او بیزی قالسێن . چۆنکی بیزه بیر شئی اؤیرهدیردی ، آخشام گۆنۆنده قالدێ . او گۆن قالدێ .\n",
            "2023-01-19 23:32:09,498 - INFO - joeynmt.training - Epoch 168: total training loss 632.41\n",
            "2023-01-19 23:32:09,498 - INFO - joeynmt.training - EPOCH 169\n",
            "2023-01-19 23:32:12,192 - INFO - joeynmt.training - Epoch 169, Step:    58100, Batch Loss:     1.912836, Batch Acc: 0.539348, Tokens per Sec:    10861, Lr: 0.000037\n",
            "2023-01-19 23:32:20,804 - INFO - joeynmt.training - Epoch 169, Step:    58200, Batch Loss:     1.823429, Batch Acc: 0.543497, Tokens per Sec:    12876, Lr: 0.000037\n",
            "2023-01-19 23:32:28,786 - INFO - joeynmt.training - Epoch 169, Step:    58300, Batch Loss:     1.695550, Batch Acc: 0.542967, Tokens per Sec:    13677, Lr: 0.000037\n",
            "2023-01-19 23:32:36,774 - INFO - joeynmt.training - Epoch 169, Step:    58400, Batch Loss:     1.751860, Batch Acc: 0.541810, Tokens per Sec:    13756, Lr: 0.000037\n",
            "2023-01-19 23:32:38,298 - INFO - joeynmt.training - Epoch 169: total training loss 627.01\n",
            "2023-01-19 23:32:38,298 - INFO - joeynmt.training - EPOCH 170\n",
            "2023-01-19 23:32:44,747 - INFO - joeynmt.training - Epoch 170, Step:    58500, Batch Loss:     1.863457, Batch Acc: 0.542732, Tokens per Sec:    13898, Lr: 0.000037\n",
            "2023-01-19 23:32:52,690 - INFO - joeynmt.training - Epoch 170, Step:    58600, Batch Loss:     1.793160, Batch Acc: 0.544427, Tokens per Sec:    13747, Lr: 0.000037\n",
            "2023-01-19 23:33:00,611 - INFO - joeynmt.training - Epoch 170, Step:    58700, Batch Loss:     1.706138, Batch Acc: 0.539098, Tokens per Sec:    13812, Lr: 0.000037\n",
            "2023-01-19 23:33:05,896 - INFO - joeynmt.training - Epoch 170: total training loss 629.01\n",
            "2023-01-19 23:33:05,897 - INFO - joeynmt.training - EPOCH 171\n",
            "2023-01-19 23:33:08,666 - INFO - joeynmt.training - Epoch 171, Step:    58800, Batch Loss:     1.874771, Batch Acc: 0.549160, Tokens per Sec:    14150, Lr: 0.000037\n",
            "2023-01-19 23:33:16,567 - INFO - joeynmt.training - Epoch 171, Step:    58900, Batch Loss:     1.744673, Batch Acc: 0.545804, Tokens per Sec:    13880, Lr: 0.000037\n",
            "2023-01-19 23:33:24,569 - INFO - joeynmt.training - Epoch 171, Step:    59000, Batch Loss:     1.886859, Batch Acc: 0.545017, Tokens per Sec:    13790, Lr: 0.000037\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.59ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10385.77ex/s]\n",
            "2023-01-19 23:33:24,861 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=59000\n",
            "2023-01-19 23:33:24,862 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:33:29,668 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:33:29,669 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:33:29,669 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:33:29,670 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:33:29,673 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   5.74, loss:   2.85, ppl:  17.25, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7678[sec], evaluation: 0.0362[sec]\n",
            "2023-01-19 23:33:29,676 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:33:29,679 - INFO - joeynmt.training - \tSource:     خدا مسیح را همچون قربانی کفاره تقدیم کرد تا انسان‌ها را از طریق ایمان به خون او با خود آشتی دهد . او با انجام دادن این کار می‌خواست عدالت خود را نشان دهد ؛ زیرا به سبب شکیبایی خود گناهانی را که در گذشته انجام می‌شد ، می‌بخشید . \n",
            "2023-01-19 23:33:29,679 - INFO - joeynmt.training - \tReference:  آللاه ایسنانی کفاره قوربانێ اولاراق تقدیم ائتدی کی ، ایمان ائدنلرین گۆناهلارێ اونون قانێ واسطهسیله باغیشلانسین . بونو اؤز سالئهلیگینی گؤسترمک اۆچۆن ائتدی . چۆنکی سبیرلی اولوب قاباقجا ائدیلن گۆناهلارێ جزاسێز قویموشدو . \n",
            "2023-01-19 23:33:29,679 - INFO - joeynmt.training - \tHypothesis: آللاه مصیح واسطهسیله بیر قوربانلار تقدیم ائتمک اۆچۆن تقدیم اولوندو کی ، اینسان واسطهسیله سالئهلیکله سالئهلیکله اؤز قانێنا گؤره عدالتلی وه عدالتلی اولدوغونا گؤره عدالتله عدالتله رفتار ائدن هر بیرگه ایرس اولاراق آلدێ .\n",
            "2023-01-19 23:33:29,680 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:33:29,682 - INFO - joeynmt.training - \tSource:     آری ، خدا همین موسی را که آنان رد کرده ، به او گفته بودند : چه کسی تو را حاکم و داور ساخته است ، توسط فرشته‌ای که در بوته بر او ظاهر شد ، فرستاد تا هم حاکم و هم رهاننده باشد . \n",
            "2023-01-19 23:33:29,682 - INFO - joeynmt.training - \tReference:  بو ، همین موسادێر کی ، اونو ردد ائدیب دئمیشدیلر : سنی بیزیم اۆستۆمۆزه کیم باشچێ وه حاکم قویوب ؟ آمما آللاه موسانێ بیر کول آراسێندا گؤرۆنن ملهین الی ایله باشچێ وه قورتارێجێ اولاراق گؤندردی . \n",
            "2023-01-19 23:33:29,682 - INFO - joeynmt.training - \tHypothesis: اونلار آللاه موسایا اؤز پالتارلارێنێن بئلینهییندکی بئله دئمیشدی : سنین حاکمیته وه سنین اۆزهرینده اولان بیر ملک گؤردۆگون وه اونون اۆزهرینده اۆزهرینده حاکمیته دؤنمک اۆچۆن اؤز حاکمیته گؤره اوجالدێردێ .\n",
            "2023-01-19 23:33:29,682 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:33:29,684 - INFO - joeynmt.training - \tSource:     پس همگی ، سوستنیس ، مسئول کنیسه را گرفتند و او را در مقابل مسند داوری زدند . اما گالیو به این امور کاملا بی‌اعتنا بود . \n",
            "2023-01-19 23:33:29,685 - INFO - joeynmt.training - \tReference:  هامێ سیناقوق رعیسی سوستئنی توتوب هؤکم کۆرسۆسۆنۆن قارشیسیندا دؤیدو . قاللیو ایسه بو حادثهلره احمیت وئرمهدی . \n",
            "2023-01-19 23:33:29,685 - INFO - joeynmt.training - \tHypothesis: هامێ شاعل تپۆردۆ ، سیناقوقدا اولاندا هؤکم کۆرسۆسۆسۆسۆ ایسنانین قارشیسیندا قارشیسیندا قارشیسیندا دیز چؤکوب . آمما بو شئیلری درین قارشیسیندا گؤرۆردۆ .\n",
            "2023-01-19 23:33:29,685 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:33:29,687 - INFO - joeynmt.training - \tSource:     گفتاری پسندیده در برابر نیازمندان‌ و گذشت از اصرار و تندی آنان‌ بهتر از صدقه‌ای است که آزاری به دنبال آن باشد ، و خداوند بی‌نیاز بردبار است . \n",
            "2023-01-19 23:33:29,687 - INFO - joeynmt.training - \tReference:  خوش بیر سؤز وه گۆناهلارێ باغێشلاماق ازییتله وئریلن سدهقدن داها یاخشیدیر . آللاه احتیاجسیزدیر ، هلیمدیر ! \n",
            "2023-01-19 23:33:29,687 - INFO - joeynmt.training - \tHypothesis: اونلار دئدی : اۆز چئویریب اۆز چئویریب دولاشێب دۆزلتدیکلری اۆچۆن داها یاخشیدیر . آللاه اونلارێن تقیبهسینی طقیب ائدیر . آللاه احتیاجسیزدیر ، احتیاجسیزدیر !\n",
            "2023-01-19 23:33:37,716 - INFO - joeynmt.training - Epoch 171, Step:    59100, Batch Loss:     1.889057, Batch Acc: 0.539539, Tokens per Sec:    13218, Lr: 0.000037\n",
            "2023-01-19 23:33:38,562 - INFO - joeynmt.training - Epoch 171: total training loss 625.86\n",
            "2023-01-19 23:33:38,562 - INFO - joeynmt.training - EPOCH 172\n",
            "2023-01-19 23:33:45,712 - INFO - joeynmt.training - Epoch 172, Step:    59200, Batch Loss:     1.835663, Batch Acc: 0.543650, Tokens per Sec:    14006, Lr: 0.000037\n",
            "2023-01-19 23:33:53,620 - INFO - joeynmt.training - Epoch 172, Step:    59300, Batch Loss:     1.931997, Batch Acc: 0.548733, Tokens per Sec:    13837, Lr: 0.000037\n",
            "2023-01-19 23:34:01,601 - INFO - joeynmt.training - Epoch 172, Step:    59400, Batch Loss:     1.760206, Batch Acc: 0.542443, Tokens per Sec:    13931, Lr: 0.000037\n",
            "2023-01-19 23:34:06,039 - INFO - joeynmt.training - Epoch 172: total training loss 622.77\n",
            "2023-01-19 23:34:06,039 - INFO - joeynmt.training - EPOCH 173\n",
            "2023-01-19 23:34:09,707 - INFO - joeynmt.training - Epoch 173, Step:    59500, Batch Loss:     1.817000, Batch Acc: 0.545906, Tokens per Sec:    13729, Lr: 0.000037\n",
            "2023-01-19 23:34:17,690 - INFO - joeynmt.training - Epoch 173, Step:    59600, Batch Loss:     1.847588, Batch Acc: 0.547193, Tokens per Sec:    13798, Lr: 0.000037\n",
            "2023-01-19 23:34:26,967 - INFO - joeynmt.training - Epoch 173, Step:    59700, Batch Loss:     1.796033, Batch Acc: 0.541106, Tokens per Sec:    11857, Lr: 0.000037\n",
            "2023-01-19 23:34:36,215 - INFO - joeynmt.training - Epoch 173: total training loss 623.44\n",
            "2023-01-19 23:34:36,215 - INFO - joeynmt.training - EPOCH 174\n",
            "2023-01-19 23:34:36,294 - INFO - joeynmt.training - Epoch 174, Step:    59800, Batch Loss:     1.640644, Batch Acc: 0.583019, Tokens per Sec:    13512, Lr: 0.000037\n",
            "2023-01-19 23:34:44,237 - INFO - joeynmt.training - Epoch 174, Step:    59900, Batch Loss:     1.784100, Batch Acc: 0.546533, Tokens per Sec:    13776, Lr: 0.000037\n",
            "2023-01-19 23:34:52,052 - INFO - joeynmt.training - Epoch 174, Step:    60000, Batch Loss:     1.804960, Batch Acc: 0.545436, Tokens per Sec:    14075, Lr: 0.000037\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.52ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9316.53ex/s]\n",
            "2023-01-19 23:34:52,345 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=60000\n",
            "2023-01-19 23:34:52,345 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:34:57,816 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:34:57,816 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:34:57,817 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:34:57,817 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:34:57,821 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.66, loss:   2.86, ppl:  17.39, acc:   0.40, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.4271[sec], evaluation: 0.0396[sec]\n",
            "2023-01-19 23:34:57,823 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:34:57,826 - INFO - joeynmt.training - \tSource:     اینان همان کسانی هستند که خود را با زنان آلوده نکردند ، آری باکره‌اند . اینان همان کسانی هستند که هر جا بره می‌رود ، همچنان او را دنبال می‌کنند و از میان انسان‌ها خریده شدند تا به عنوان نوبر به خدا و بره تقدیم شوند . \n",
            "2023-01-19 23:34:57,827 - INFO - joeynmt.training - \tReference:  بونلار اؤزلرینی قادێنلارلا لکهلهمهمیش شخصلردیر ، چۆنکی باکردیرلر . قوزو هارا گئتسه ، اونلار دا اونون آردێنجا گئدیر . اونلار آللاه وه قوزو اۆچۆن نۆبار اولاراق اینسانلار آراسێندان ساتێن آلێندێ . \n",
            "2023-01-19 23:34:57,827 - INFO - joeynmt.training - \tHypothesis: اونلار اؤز قادێنلارێ ایله دول قادێنلارێ آرادان چێخانلار . اونلارێن هتا اینسانلار آراسێندا فرقهسیدیردیلر . بئله کی آللاهێن ایزنی ایله تاپمێشلار وه اینسانلار آراسێندا آللاها آللاها تقدیم ائتمک اۆچۆن تقدیم اولونسون .\n",
            "2023-01-19 23:34:57,827 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:34:57,829 - INFO - joeynmt.training - \tSource:     و ثمود و قوم لوط و اصحاب ایکه نیز به تکذیب پرداختند آنها دسته‌های مخالف بودند . \n",
            "2023-01-19 23:34:57,830 - INFO - joeynmt.training - \tReference:  ائلهجه ده سمود ، لوت قؤومو ، ایکه اهلی بو فرقهلرین\n",
            "2023-01-19 23:34:57,830 - INFO - joeynmt.training - \tHypothesis: سمود قؤومونو وه لوتون تایفاسی دا دا تکذیب ائتمیشدیلر .\n",
            "2023-01-19 23:34:57,830 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:34:57,832 - INFO - joeynmt.training - \tSource:     چشم تو ، چراغ بدنت است . اگر چشمت متمرکز باشد ، تمام وجودت روشن خواهد بود . اما اگر چشمت پر از حسد باشد ، وجودت نیز تاریک خواهد بود . \n",
            "2023-01-19 23:34:57,832 - INFO - joeynmt.training - \tReference:  بدهنینین چێراغێ گؤزوندور . گؤزۆن ساغلام اولاندا بۆتۆن بدهنین ده نورلو اولور . گؤزۆن ظعیف گؤرنده ایسه بۆتۆن بدهنین ده قارانلێق اولور . \n",
            "2023-01-19 23:34:57,832 - INFO - joeynmt.training - \tHypothesis: گؤزوندکی چێراقدێر . اگر گؤزوکدن نورلانێرسا ، بۆتۆن بدهنین نورلو اولاجاق . آمما اگر گؤز یاسک ، قێسقانجلێقدان نورلو اولاجاق .\n",
            "2023-01-19 23:34:57,833 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:34:57,835 - INFO - joeynmt.training - \tSource:     پوشاند بر آن دو شهر ، از باران گوگردی‌ آنچه را پوشاند . \n",
            "2023-01-19 23:34:57,835 - INFO - joeynmt.training - \tReference:  اونلارێ نلر ساردێ ، نلر ! \n",
            "2023-01-19 23:34:57,835 - INFO - joeynmt.training - \tHypothesis: اونلار ایکی شهره ایکی دفه دفهلرله بۆرۆدۆکلرینی شئین اۆستۆنۆ اؤرتدۆ .\n",
            "2023-01-19 23:35:05,892 - INFO - joeynmt.training - Epoch 174, Step:    60100, Batch Loss:     1.780179, Batch Acc: 0.544191, Tokens per Sec:    13182, Lr: 0.000036\n",
            "2023-01-19 23:35:09,463 - INFO - joeynmt.training - Epoch 174: total training loss 623.27\n",
            "2023-01-19 23:35:09,464 - INFO - joeynmt.training - EPOCH 175\n",
            "2023-01-19 23:35:13,904 - INFO - joeynmt.training - Epoch 175, Step:    60200, Batch Loss:     2.023011, Batch Acc: 0.545838, Tokens per Sec:    13945, Lr: 0.000036\n",
            "2023-01-19 23:35:22,006 - INFO - joeynmt.training - Epoch 175, Step:    60300, Batch Loss:     1.780333, Batch Acc: 0.550626, Tokens per Sec:    13573, Lr: 0.000036\n",
            "2023-01-19 23:35:30,066 - INFO - joeynmt.training - Epoch 175, Step:    60400, Batch Loss:     1.767030, Batch Acc: 0.544076, Tokens per Sec:    13606, Lr: 0.000036\n",
            "2023-01-19 23:35:37,363 - INFO - joeynmt.training - Epoch 175: total training loss 624.03\n",
            "2023-01-19 23:35:37,363 - INFO - joeynmt.training - EPOCH 176\n",
            "2023-01-19 23:35:38,173 - INFO - joeynmt.training - Epoch 176, Step:    60500, Batch Loss:     1.799806, Batch Acc: 0.556474, Tokens per Sec:    13308, Lr: 0.000036\n",
            "2023-01-19 23:35:46,066 - INFO - joeynmt.training - Epoch 176, Step:    60600, Batch Loss:     1.756567, Batch Acc: 0.545915, Tokens per Sec:    14069, Lr: 0.000036\n",
            "2023-01-19 23:35:53,975 - INFO - joeynmt.training - Epoch 176, Step:    60700, Batch Loss:     1.784597, Batch Acc: 0.546333, Tokens per Sec:    13801, Lr: 0.000036\n",
            "2023-01-19 23:36:01,897 - INFO - joeynmt.training - Epoch 176, Step:    60800, Batch Loss:     1.869483, Batch Acc: 0.544496, Tokens per Sec:    13897, Lr: 0.000036\n",
            "2023-01-19 23:36:04,744 - INFO - joeynmt.training - Epoch 176: total training loss 620.88\n",
            "2023-01-19 23:36:04,744 - INFO - joeynmt.training - EPOCH 177\n",
            "2023-01-19 23:36:09,921 - INFO - joeynmt.training - Epoch 177, Step:    60900, Batch Loss:     1.874415, Batch Acc: 0.542957, Tokens per Sec:    13661, Lr: 0.000036\n",
            "2023-01-19 23:36:17,824 - INFO - joeynmt.training - Epoch 177, Step:    61000, Batch Loss:     1.661771, Batch Acc: 0.552184, Tokens per Sec:    13927, Lr: 0.000036\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.08ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9709.88ex/s] \n",
            "2023-01-19 23:36:18,114 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=61000\n",
            "2023-01-19 23:36:18,114 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:36:22,196 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:36:22,196 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:36:22,197 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:36:22,198 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:36:22,200 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.58, loss:   2.70, ppl:  14.94, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.0446[sec], evaluation: 0.0346[sec]\n",
            "2023-01-19 23:36:22,203 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:36:22,207 - INFO - joeynmt.training - \tSource:     به همین‌سان ، سرور در خصوص کسانی که بشارت را اعلام می‌کنند ، فرمان داد که روزی خود را از طریق بشارت دریافت کنند . \n",
            "2023-01-19 23:36:22,207 - INFO - joeynmt.training - \tReference:  بئلهجه ده رب مۆژدنی ائلان ائدنلره مۆژدن دولانماغێ امر ائتمیشدیر . \n",
            "2023-01-19 23:36:22,207 - INFO - joeynmt.training - \tHypothesis: بئلهجه ربه یایماق اۆچۆن مۆژده یایماق اۆچۆن مۆژدنی یایماق اۆچۆن مۆژدنی یایماق اۆچۆن مۆژدنی یایماق اۆچۆن مۆژدنی یایدیلار .\n",
            "2023-01-19 23:36:22,207 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:36:22,209 - INFO - joeynmt.training - \tSource:      اکنون به اورشلیم می‌رویم . در آنجا پسر انسان به سران کاهنان و علمای دین سپرده خواهد شد . آنان او را به مرگ محکوم خواهند کرد و به دست غیریهودیان تحویل خواهند داد ، \n",
            "2023-01-19 23:36:22,209 - INFO - joeynmt.training - \tReference:   باخ یئروسهلیمه قالخێرێق ، بشر اوغلو باشچێ کاهنلره وه الاهییاتچیلارا تسلیم اولوناجاق . اونو اؤلومه محکوم ائدیب باشقا ملتلره تسلیم ائدهجکلر . \n",
            "2023-01-19 23:36:22,210 - INFO - joeynmt.training - \tHypothesis: ایندی یئروسهلیمه قالخێر ، بشر اوغلو باشچێ کاهنلره وه الاهییاتچیلارا تسلیم ائدیلهجک . اونلار اونو اؤلومه محکوم ائدهجکلر .\n",
            "2023-01-19 23:36:22,210 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:36:22,212 - INFO - joeynmt.training - \tSource:     و لی‌ دروغزنش خواندند و آن ماده‌شتر را پی کردند ، و پروردگارشان به سزای‌ گناهشان بر سرشان عذاب آورد و آنان را با خاک یکسان کرد . \n",
            "2023-01-19 23:36:22,212 - INFO - joeynmt.training - \tReference:  لاکین اونلار یالانچی سایدیلار وه اونو توتوب کسدیلر . ربی ده بو گۆناهلارێنا گؤره اونلارێن کؤکونو کسیب یئرله یئکسان ائتدی . \n",
            "2023-01-19 23:36:22,212 - INFO - joeynmt.training - \tHypothesis: لاکین یالان حساب ائتدیلر ، بوندان داها چوخ اوجالتدێ . اونلارێ ربینین ازابێنا گؤره اونلارێ گۆناهلارینین اۆستۆنه قویدوقلارێ وه اونلارێ بیر اعذاب گؤزلهییر !\n",
            "2023-01-19 23:36:22,212 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:36:22,214 - INFO - joeynmt.training - \tSource:     پس با اشاره از پدر نوزاد سؤال کردند که می‌خواهد او را چه بنامد . \n",
            "2023-01-19 23:36:22,214 - INFO - joeynmt.training - \tReference:  کؤرپهنین آتاسێندان کؤرپهیه نه آد قویولماسینی اشاره ایله سوروشدولار . \n",
            "2023-01-19 23:36:22,214 - INFO - joeynmt.training - \tHypothesis: بونا گؤره آتا آتا آناسێ اونون کؤلگهنی نئجه آپاراجاق .\n",
            "2023-01-19 23:36:30,241 - INFO - joeynmt.training - Epoch 177, Step:    61100, Batch Loss:     1.707089, Batch Acc: 0.545681, Tokens per Sec:    13226, Lr: 0.000036\n",
            "2023-01-19 23:36:36,719 - INFO - joeynmt.training - Epoch 177: total training loss 622.24\n",
            "2023-01-19 23:36:36,720 - INFO - joeynmt.training - EPOCH 178\n",
            "2023-01-19 23:36:38,262 - INFO - joeynmt.training - Epoch 178, Step:    61200, Batch Loss:     1.756532, Batch Acc: 0.553166, Tokens per Sec:    13357, Lr: 0.000036\n",
            "2023-01-19 23:36:48,103 - INFO - joeynmt.training - Epoch 178, Step:    61300, Batch Loss:     1.673468, Batch Acc: 0.547501, Tokens per Sec:    11275, Lr: 0.000036\n",
            "2023-01-19 23:36:56,910 - INFO - joeynmt.training - Epoch 178, Step:    61400, Batch Loss:     1.821449, Batch Acc: 0.544075, Tokens per Sec:    12387, Lr: 0.000036\n",
            "2023-01-19 23:37:04,902 - INFO - joeynmt.training - Epoch 178, Step:    61500, Batch Loss:     1.750080, Batch Acc: 0.547165, Tokens per Sec:    13767, Lr: 0.000036\n",
            "2023-01-19 23:37:07,060 - INFO - joeynmt.training - Epoch 178: total training loss 621.81\n",
            "2023-01-19 23:37:07,060 - INFO - joeynmt.training - EPOCH 179\n",
            "2023-01-19 23:37:12,854 - INFO - joeynmt.training - Epoch 179, Step:    61600, Batch Loss:     1.824452, Batch Acc: 0.548328, Tokens per Sec:    14048, Lr: 0.000036\n",
            "2023-01-19 23:37:20,792 - INFO - joeynmt.training - Epoch 179, Step:    61700, Batch Loss:     1.700219, Batch Acc: 0.547377, Tokens per Sec:    13893, Lr: 0.000036\n",
            "2023-01-19 23:37:28,704 - INFO - joeynmt.training - Epoch 179, Step:    61800, Batch Loss:     1.820555, Batch Acc: 0.548817, Tokens per Sec:    13890, Lr: 0.000036\n",
            "2023-01-19 23:37:34,545 - INFO - joeynmt.training - Epoch 179: total training loss 621.08\n",
            "2023-01-19 23:37:34,546 - INFO - joeynmt.training - EPOCH 180\n",
            "2023-01-19 23:37:36,689 - INFO - joeynmt.training - Epoch 180, Step:    61900, Batch Loss:     1.855685, Batch Acc: 0.549975, Tokens per Sec:    13761, Lr: 0.000036\n",
            "2023-01-19 23:37:44,641 - INFO - joeynmt.training - Epoch 180, Step:    62000, Batch Loss:     1.689511, Batch Acc: 0.548495, Tokens per Sec:    13874, Lr: 0.000036\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.80ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10400.43ex/s]\n",
            "2023-01-19 23:37:44,904 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=62000\n",
            "2023-01-19 23:37:44,905 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:37:49,211 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:37:49,212 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:37:49,212 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:37:49,213 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:37:49,216 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.71, loss:   2.84, ppl:  17.20, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2682[sec], evaluation: 0.0360[sec]\n",
            "2023-01-19 23:37:49,219 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:37:49,222 - INFO - joeynmt.training - \tSource:     هر که نزد من آید و سخنانم را بشنود و به آن عمل کند ، به شما می‌گویم مانند چه کسی است : \n",
            "2023-01-19 23:37:49,222 - INFO - joeynmt.training - \tReference:  منیم یانیما گلن ، سؤزلریمی ائشیدیب اونلارا امل ائدن هر آدامێن کیمه بنزهدیگینی سیزه اضاح ائدرم : \n",
            "2023-01-19 23:37:49,222 - INFO - joeynmt.training - \tHypothesis: منیمله گلیرم کی ، هر ایشی ائشیدیب اونو امل ائدن هر کس سیزه دئییرم : کیمیننیز ده بیریدیر .\n",
            "2023-01-19 23:37:49,223 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:37:49,225 - INFO - joeynmt.training - \tSource:     کسی که جان خود را دوست بدارد ، آن را تباه خواهد ساخت ، اما کسی که در این دنیا از جان خود نفرت داشته باشد ، آن را برای زندگی جاودان حفظ خواهد کرد . \n",
            "2023-01-19 23:37:49,225 - INFO - joeynmt.training - \tReference:  اؤز جانێنێ سئون کس اونو ایتیرر ، بو دۆنیادا جانێ اۆچۆن قایغی چکمهینسه اونو ابدی هیات اۆچۆن قورویار . \n",
            "2023-01-19 23:37:49,225 - INFO - joeynmt.training - \tHypothesis: جانێنێ سئون جانێنێ سئوهجک ، آمما دۆنیا هیاتیا مالک اولاجاق . آمما کیم بو دۆنیانین جانێنێ ابدی هیاتا مالک اولاجاق .\n",
            "2023-01-19 23:37:49,225 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:37:49,227 - INFO - joeynmt.training - \tSource:     به همین شکل ، مردان آمیزش طبیعی با زنان را ترک کردند و در آتش شهوتی که نسبت به یکدیگر داشتند سوختند ، مرد با مرد مرتکب اعمال قبیح شده ، سزای رفتار زشتشان را به طور کامل در خود دیدند . \n",
            "2023-01-19 23:37:49,228 - INFO - joeynmt.training - \tReference:  ائینی ترزده کیشیلر ده قادێنلا اولان تبیعی الاقهدن ال چکیب بیر بیرلهرینه شهوتله قێزدێلار ؛ کیشیلر کیشیلرله رۆسۆایچیلێق ائدیب اؤز داخللرینده رزیللیکلرینه لایق جزانێ آلدێلار . \n",
            "2023-01-19 23:37:49,228 - INFO - joeynmt.training - \tHypothesis: بئلهلیکله ، کیشیلره تابع اولان کیشیلر وه قادێنلار بیر بیرینی پیسلهره سولانمادێلار . کیشینین امللری ایله یاشادیقلاریندان پیسلهره گؤره اؤزلرینه گؤستردیلر .\n",
            "2023-01-19 23:37:49,228 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:37:49,230 - INFO - joeynmt.training - \tSource:     همچنین دربارهٔ فرشتگان می‌گوید : او فرشتگان خود را چون باد پرقدرت می‌سازد و خادمان خود را همچون شعله‌های آتش . \n",
            "2023-01-19 23:37:49,230 - INFO - joeynmt.training - \tReference:  ملکلر بارهسینده ایسه دئییر : اؤز ملکلرینی کۆلکلره ، اؤز خدمتچیلرینی یانار اودا دؤندهریر ! \n",
            "2023-01-19 23:37:49,230 - INFO - joeynmt.training - \tHypothesis: ملکلر اونا بئله دئییر : او ، ملکلری اوددا اودلا دولدورور ، چۆنکی اودلارێنا اودلودور .\n",
            "2023-01-19 23:37:57,307 - INFO - joeynmt.training - Epoch 180, Step:    62100, Batch Loss:     1.719854, Batch Acc: 0.550270, Tokens per Sec:    13271, Lr: 0.000036\n",
            "2023-01-19 23:38:05,271 - INFO - joeynmt.training - Epoch 180, Step:    62200, Batch Loss:     1.794112, Batch Acc: 0.545026, Tokens per Sec:    13763, Lr: 0.000036\n",
            "2023-01-19 23:38:06,738 - INFO - joeynmt.training - Epoch 180: total training loss 617.25\n",
            "2023-01-19 23:38:06,738 - INFO - joeynmt.training - EPOCH 181\n",
            "2023-01-19 23:38:13,191 - INFO - joeynmt.training - Epoch 181, Step:    62300, Batch Loss:     1.808381, Batch Acc: 0.551343, Tokens per Sec:    14034, Lr: 0.000036\n",
            "2023-01-19 23:38:21,142 - INFO - joeynmt.training - Epoch 181, Step:    62400, Batch Loss:     1.686411, Batch Acc: 0.547902, Tokens per Sec:    13834, Lr: 0.000036\n",
            "2023-01-19 23:38:29,036 - INFO - joeynmt.training - Epoch 181, Step:    62500, Batch Loss:     1.893734, Batch Acc: 0.547961, Tokens per Sec:    14002, Lr: 0.000036\n",
            "2023-01-19 23:38:34,056 - INFO - joeynmt.training - Epoch 181: total training loss 618.55\n",
            "2023-01-19 23:38:34,056 - INFO - joeynmt.training - EPOCH 182\n",
            "2023-01-19 23:38:37,005 - INFO - joeynmt.training - Epoch 182, Step:    62600, Batch Loss:     1.761222, Batch Acc: 0.549146, Tokens per Sec:    13843, Lr: 0.000036\n",
            "2023-01-19 23:38:44,976 - INFO - joeynmt.training - Epoch 182, Step:    62700, Batch Loss:     1.801631, Batch Acc: 0.551859, Tokens per Sec:    14043, Lr: 0.000036\n",
            "2023-01-19 23:38:52,927 - INFO - joeynmt.training - Epoch 182, Step:    62800, Batch Loss:     1.807195, Batch Acc: 0.549194, Tokens per Sec:    13705, Lr: 0.000036\n",
            "2023-01-19 23:39:00,784 - INFO - joeynmt.training - Epoch 182, Step:    62900, Batch Loss:     1.731400, Batch Acc: 0.547994, Tokens per Sec:    14003, Lr: 0.000036\n",
            "2023-01-19 23:39:01,440 - INFO - joeynmt.training - Epoch 182: total training loss 616.27\n",
            "2023-01-19 23:39:01,440 - INFO - joeynmt.training - EPOCH 183\n",
            "2023-01-19 23:39:08,821 - INFO - joeynmt.training - Epoch 183, Step:    63000, Batch Loss:     1.805041, Batch Acc: 0.549320, Tokens per Sec:    13692, Lr: 0.000036\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.36ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10148.66ex/s]\n",
            "2023-01-19 23:39:09,097 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=63000\n",
            "2023-01-19 23:39:09,097 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:39:13,285 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:39:13,286 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:39:13,286 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:39:13,287 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:39:13,290 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.52, loss:   2.63, ppl:  13.93, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.0468[sec], evaluation: 0.1392[sec]\n",
            "2023-01-19 23:39:13,512 - INFO - joeynmt.helpers - delete RESULTS_fa2azb/model/39000.ckpt\n",
            "2023-01-19 23:39:13,525 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:39:13,528 - INFO - joeynmt.training - \tSource:     پس وقتی هر دو تن دردادند و همدیگر را بدرود گفتند و پسر را به پیشانی بر خاک افکند ، \n",
            "2023-01-19 23:39:13,528 - INFO - joeynmt.training - \tReference:  اونلارێن هر ایکیسی تسلیم اولدوغو وه اۆزۆسته یئره یێخدیغی زامان\n",
            "2023-01-19 23:39:13,529 - INFO - joeynmt.training - \tHypothesis: اونلار هر ایکیسی ده آتدێ وه بیر یئره آتێب دئدیلر :\n",
            "2023-01-19 23:39:13,529 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:39:13,531 - INFO - joeynmt.training - \tSource:     پس دیگر در پی این مباشید که چه بخورید و چه بنوشید ؛ دیگر غرق در نگرانی مشوید ؛ \n",
            "2023-01-19 23:39:13,531 - INFO - joeynmt.training - \tReference:   نه یئیهجهییک نه ایچهجهییک ؟ دئیه آختارێب ناراهات اولمایین . \n",
            "2023-01-19 23:39:13,531 - INFO - joeynmt.training - \tHypothesis: بونا گؤره ده یئمک یئمهیه اجازه اولمایین . آرتێق آرتێق گئیینمهسین .\n",
            "2023-01-19 23:39:13,531 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:39:13,533 - INFO - joeynmt.training - \tSource:     و می‌گفتند : آن پادشاه یهودیان که متولد شده است ، کجاست ؟ زیرا ما ستارهٔ او را زمانی که در مشرق بودیم ، دیدیم و آمده‌ایم تا در برابر او سر تعظیم فرود آوریم . \n",
            "2023-01-19 23:39:13,534 - INFO - joeynmt.training - \tReference:  دئدیلر : یهودیلرین آنادان اولموش پادشاهێ هارادادێر ؟ شرقده اونون اولدوزونو گؤردۆک وه اونا سجده قێلماغا گلدیک . \n",
            "2023-01-19 23:39:13,534 - INFO - joeynmt.training - \tHypothesis: یهودیلر اونا دئدی : یهودیلرین پادشاهێدێر ؟ هارادادێر ؟ چۆنکی بیز اونو ملته سالانێناق . بیز اونو گؤردۆک کی ، اونون باشێنا گلنده اونو گؤردۆک .\n",
            "2023-01-19 23:39:13,534 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:39:13,536 - INFO - joeynmt.training - \tSource:     و گفتند : جز روزهایی چند ، هرگز آتش به ما نخواهد رسید . بگو : مگر پیمانی از خدا گرفته‌اید ؟ که خدا پیمان خود را هرگز خلاف نخواهد کرد یا آنچه را نمی‌دانید به دروغ به خدا نسبت می‌دهید ؟ \n",
            "2023-01-19 23:39:13,536 - INFO - joeynmt.training - \tReference:  اونلار : جهنم اودو بیزه بیر نئچه گۆندن آرتێق اعذاب وئرمز دئیرلر . اونلارا سؤیله : سیز آللاهدان بئله بیر وه د آلمیسینیزمی ؟ آللاه هئچ واخت اؤز اهدیندن دؤنمز . یوخسا آللاها قارشێ بیلمهدیگینیزی سؤیلهییرسینیز ؟ \n",
            "2023-01-19 23:39:13,536 - INFO - joeynmt.training - \tHypothesis: اونلار : بیز او گۆنلر بیر مۆدت هئچ بیر شئی آلمایاجاقدیر . دئ : آللاهێن اهدی یئرینه یئتیرمهلییینیزی اسلا ! دئینلره هئچ بیر اهد آلمادێغێنێز حالدا ، یاخود آللاهێ شاهد توتمایدینیز ؟\n",
            "2023-01-19 23:39:21,836 - INFO - joeynmt.training - Epoch 183, Step:    63100, Batch Loss:     1.713556, Batch Acc: 0.548258, Tokens per Sec:    12449, Lr: 0.000036\n",
            "2023-01-19 23:39:32,597 - INFO - joeynmt.training - Epoch 183, Step:    63200, Batch Loss:     1.762510, Batch Acc: 0.549521, Tokens per Sec:    10169, Lr: 0.000036\n",
            "2023-01-19 23:39:36,940 - INFO - joeynmt.training - Epoch 183: total training loss 618.30\n",
            "2023-01-19 23:39:36,941 - INFO - joeynmt.training - EPOCH 184\n",
            "2023-01-19 23:39:40,653 - INFO - joeynmt.training - Epoch 184, Step:    63300, Batch Loss:     1.792832, Batch Acc: 0.553062, Tokens per Sec:    13564, Lr: 0.000036\n",
            "2023-01-19 23:39:48,454 - INFO - joeynmt.training - Epoch 184, Step:    63400, Batch Loss:     1.775787, Batch Acc: 0.551774, Tokens per Sec:    14026, Lr: 0.000036\n",
            "2023-01-19 23:39:56,312 - INFO - joeynmt.training - Epoch 184, Step:    63500, Batch Loss:     1.734115, Batch Acc: 0.548543, Tokens per Sec:    13862, Lr: 0.000035\n",
            "2023-01-19 23:40:04,249 - INFO - joeynmt.training - Epoch 184, Step:    63600, Batch Loss:     1.659318, Batch Acc: 0.546958, Tokens per Sec:    14017, Lr: 0.000035\n",
            "2023-01-19 23:40:04,283 - INFO - joeynmt.training - Epoch 184: total training loss 616.84\n",
            "2023-01-19 23:40:04,283 - INFO - joeynmt.training - EPOCH 185\n",
            "2023-01-19 23:40:12,237 - INFO - joeynmt.training - Epoch 185, Step:    63700, Batch Loss:     1.811239, Batch Acc: 0.551411, Tokens per Sec:    13660, Lr: 0.000035\n",
            "2023-01-19 23:40:20,147 - INFO - joeynmt.training - Epoch 185, Step:    63800, Batch Loss:     1.853155, Batch Acc: 0.549033, Tokens per Sec:    14113, Lr: 0.000035\n",
            "2023-01-19 23:40:28,139 - INFO - joeynmt.training - Epoch 185, Step:    63900, Batch Loss:     1.756623, Batch Acc: 0.548988, Tokens per Sec:    13616, Lr: 0.000035\n",
            "2023-01-19 23:40:31,841 - INFO - joeynmt.training - Epoch 185: total training loss 617.10\n",
            "2023-01-19 23:40:31,841 - INFO - joeynmt.training - EPOCH 186\n",
            "2023-01-19 23:40:36,179 - INFO - joeynmt.training - Epoch 186, Step:    64000, Batch Loss:     1.754332, Batch Acc: 0.554639, Tokens per Sec:    13525, Lr: 0.000035\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.62ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10202.90ex/s]\n",
            "2023-01-19 23:40:36,447 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=64000\n",
            "2023-01-19 23:40:36,448 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:40:41,410 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:40:41,410 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:40:41,410 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:40:41,411 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:40:41,414 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.74, loss:   2.72, ppl:  15.18, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9242[sec], evaluation: 0.0357[sec]\n",
            "2023-01-19 23:40:41,417 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:40:41,421 - INFO - joeynmt.training - \tSource:     و اگر تو را استوار نمی‌داشتیم ، قطعا نزدیک بود کمی به سوی آنان متمایل شوی . \n",
            "2023-01-19 23:40:41,421 - INFO - joeynmt.training - \tReference:  اگر بیز سنه صبات وئرمهسیدیک ، یقین کی ، آز دا اولسا ، اونلارا اویاجاقدین ! \n",
            "2023-01-19 23:40:41,421 - INFO - joeynmt.training - \tHypothesis: اگر سنه قارشێ دورساق ، سؤزسۆز کی ، اونلارا بیر آز قالا بیلهریک .\n",
            "2023-01-19 23:40:41,422 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:40:41,423 - INFO - joeynmt.training - \tSource:     یا فرشتگان را مادینه آفریدیم و آنان شاهد بودند ؟ \n",
            "2023-01-19 23:40:41,424 - INFO - joeynmt.training - \tReference:  یوخسا بیز ملکلری دیشی یاراتمیشیق وه اونلار دا شاهد اولوبلار \n",
            "2023-01-19 23:40:41,424 - INFO - joeynmt.training - \tHypothesis: یاخود بیز ملکلری اونلارێن نظارتچیسینی یاراتدیق\n",
            "2023-01-19 23:40:41,424 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:40:41,426 - INFO - joeynmt.training - \tSource:     و یاد کن‌ آنگاه که ابراهیم گفت : پروردگارا ، به من نشان ده ؛ چگونه مردگان را زنده می‌کنی ؟ فرمود : مگر ایمان نیاورده‌ای ؟ گفت : چرا ، ولی تا دلم آرامش یابد . فرمود : پس ، چهار پرنده برگیر ، و آنها را پیش خود ، ریز ریز گردان ؛ سپس بر هر کوهی پاره‌ای از آنها را قرار ده ؛ آنگاه آنها را فرا خوان ، شتابان به سوی تو می‌آیند ، و بدان که خداوند توانا و حکیم است . \n",
            "2023-01-19 23:40:41,427 - INFO - joeynmt.training - \tReference:   خاطرلا کی ، ابراهیم : ائی ربیم ، اؤلولهری نه جۆر دیریلتدیگینی منه گؤستر ! دئدیکده : مگر اینانمیرسان ؟ بویورموشدو . بلی ، اینانیرام ، لاکین اۆرهییم ساکت اولماق اۆچۆن ، دئیه جاواب وئرمیشدی . بویورموشدو : دؤرد جۆر قوش گؤتوروب اونلارا دقتله باخ ، سونرا هر داغێن باشێنا اونلاردان بیر پارچا آت ، سونرا اونلارێ چاغێر ، تئز یانینا گلهجکلر . بیل کی ، آللاه یئنیلمز غۆۆت ، هکمت صاحبدر ! \n",
            "2023-01-19 23:40:41,427 - INFO - joeynmt.training - \tHypothesis: او زامان ابراهیم بئله دئمیشدی : ائی ربیم ! من اؤلولر منه گؤستردیگی زامان نه دئییرسن ؟ اونلار : ائی ایمان گتیردیم ، نه اۆچۆن اؤلوب گئدیرسن ؟ دئیه سوروشآرلار . اونلار : ائی ایمان گتیرنلر ! نه اۆچۆن بیر قؤؤمه گؤتور ، کیمی ، نه ده بیر داغازێ داغالێقلا گلیب چاتێب قبیرین یانینا قاییدیب دورور . شبههسیز کی ، نه ده اونلاردان اۆز دؤندردی . آللاه ، هۆبیلن\n",
            "2023-01-19 23:40:41,427 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:40:41,429 - INFO - joeynmt.training - \tSource:     شخصی را که در اتحاد با مسیح است می‌شناسم که ۱۴ سال پیش با بدن یا خارج از بدن نمی‌دانم ، خدا می‌داند به آسمان سوم برده شد . \n",
            "2023-01-19 23:40:41,429 - INFO - joeynmt.training - \tReference:  مصیحده اولاراق اون دؤرد ایل بوندان اول بدندمی ، یوخسا بدندن خارجمی ، من بیلمیرم ، آللاه بیلیر گؤتورولوب گؤیون اۆچۆنجۆ قاتێنا آپارێلان بیر اینسانی تانیییرام . \n",
            "2023-01-19 23:40:41,429 - INFO - joeynmt.training - \tHypothesis: مصیح مصیحده اولان بیر نفر ده بیلمیرم کی ، بدهنین دؤرد ایلدن اجلمهتینی بیلمیر ، آللاهێن بدهنینی گؤیه مینیرندا بیلیرم .\n",
            "2023-01-19 23:40:49,473 - INFO - joeynmt.training - Epoch 186, Step:    64100, Batch Loss:     1.697748, Batch Acc: 0.553830, Tokens per Sec:    13295, Lr: 0.000035\n",
            "2023-01-19 23:40:57,306 - INFO - joeynmt.training - Epoch 186, Step:    64200, Batch Loss:     1.758366, Batch Acc: 0.547885, Tokens per Sec:    13881, Lr: 0.000035\n",
            "2023-01-19 23:41:04,707 - INFO - joeynmt.training - Epoch 186: total training loss 615.12\n",
            "2023-01-19 23:41:04,707 - INFO - joeynmt.training - EPOCH 187\n",
            "2023-01-19 23:41:05,376 - INFO - joeynmt.training - Epoch 187, Step:    64300, Batch Loss:     1.719460, Batch Acc: 0.556537, Tokens per Sec:    13549, Lr: 0.000035\n",
            "2023-01-19 23:41:13,240 - INFO - joeynmt.training - Epoch 187, Step:    64400, Batch Loss:     1.885646, Batch Acc: 0.551415, Tokens per Sec:    13944, Lr: 0.000035\n",
            "2023-01-19 23:41:21,150 - INFO - joeynmt.training - Epoch 187, Step:    64500, Batch Loss:     1.847908, Batch Acc: 0.550592, Tokens per Sec:    13911, Lr: 0.000035\n",
            "2023-01-19 23:41:29,035 - INFO - joeynmt.training - Epoch 187, Step:    64600, Batch Loss:     1.792990, Batch Acc: 0.548520, Tokens per Sec:    14003, Lr: 0.000035\n",
            "2023-01-19 23:41:31,969 - INFO - joeynmt.training - Epoch 187: total training loss 613.30\n",
            "2023-01-19 23:41:31,970 - INFO - joeynmt.training - EPOCH 188\n",
            "2023-01-19 23:41:36,980 - INFO - joeynmt.training - Epoch 188, Step:    64700, Batch Loss:     1.732129, Batch Acc: 0.556387, Tokens per Sec:    13837, Lr: 0.000035\n",
            "2023-01-19 23:41:44,858 - INFO - joeynmt.training - Epoch 188, Step:    64800, Batch Loss:     1.768748, Batch Acc: 0.551827, Tokens per Sec:    13986, Lr: 0.000035\n",
            "2023-01-19 23:41:52,837 - INFO - joeynmt.training - Epoch 188, Step:    64900, Batch Loss:     1.798197, Batch Acc: 0.551856, Tokens per Sec:    13753, Lr: 0.000035\n",
            "2023-01-19 23:41:59,433 - INFO - joeynmt.training - Epoch 188: total training loss 612.12\n",
            "2023-01-19 23:41:59,434 - INFO - joeynmt.training - EPOCH 189\n",
            "2023-01-19 23:42:00,887 - INFO - joeynmt.training - Epoch 189, Step:    65000, Batch Loss:     2.063684, Batch Acc: 0.558039, Tokens per Sec:    13677, Lr: 0.000035\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.93ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10447.76ex/s]\n",
            "2023-01-19 23:42:01,163 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=65000\n",
            "2023-01-19 23:42:01,163 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:42:06,286 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:42:06,286 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:42:06,287 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:42:06,287 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:42:06,290 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.86, loss:   2.71, ppl:  14.98, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0819[sec], evaluation: 0.0375[sec]\n",
            "2023-01-19 23:42:06,306 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:42:06,310 - INFO - joeynmt.training - \tSource:     آن خائن به همراهان خود چنین نشانه‌ای داده بود : کسی را که ببوسم ، همان اوست . دستگیرش کنید . \n",
            "2023-01-19 23:42:06,310 - INFO - joeynmt.training - \tReference:  ایسهآیا خیانت ائدن یهودا اونلارا اشاره ایله بیلدیریب دئمیشدی : کیمی اؤپسم ، ایسا اودور ، اونو توتون . \n",
            "2023-01-19 23:42:06,311 - INFO - joeynmt.training - \tHypothesis: ایسا ایله بیرلیکده الامتلری ایله دانێشاندا دئدی : کیمسنی اللهرینه توتمایین . او ، الینده اولان الیندن آلێن !\n",
            "2023-01-19 23:42:06,311 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:42:06,314 - INFO - joeynmt.training - \tSource:     این امر که خدا او را از مردگان رستاخیز داد و دیگر هرگز به فسادپذیری بازنمی‌گردد ، چنین بیان شده است : من به شما احسان‌هایی خواهم کرد که به داوود وعده داده شده بود . این وعده قطعی است . \n",
            "2023-01-19 23:42:06,314 - INFO - joeynmt.training - \tReference:  آللاه اونو اؤلولر آراسێندان دیریلدرک هئچ واخت چۆرۆمهیه قویمایاجاق . اونا گؤره دئمیشدی : داوودا ود ائتدیگیم صادق محبتی سیزه گؤستهرهجهیم . \n",
            "2023-01-19 23:42:06,314 - INFO - joeynmt.training - \tHypothesis: آللاه اونو اؤلولر آراسێندان دیریلتمهیه قادردیر . او ، یئنه ده ود ائدیلن بدنلری ایله منه وئریلهجکدیر . سیز داوودون وه د اولونان ود ائدیلن شهادتی یئرینه یئتیرهجک .\n",
            "2023-01-19 23:42:06,314 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:42:06,316 - INFO - joeynmt.training - \tSource:     پس آنان برای بار دوم آن مرد را که پیش از این نابینا بود ، فراخواندند و به او گفتند : خدا را تمجید کن ؛ ما می‌دانیم که این مرد گناهکار است . \n",
            "2023-01-19 23:42:06,317 - INFO - joeynmt.training - \tReference:  یهودی باشچیلاری اوللر کور اولان آدامێ ایکینجی دفه چاغێرێب اونا دئدیلر : آللاهێن ایزهتی نامنه دوغرو سؤیله . بیز بو آدامێن گۆناهکار اولدوغونو بیلیریک . \n",
            "2023-01-19 23:42:06,317 - INFO - joeynmt.training - \tHypothesis: اونلار ایکی دفه چاغێرانلارێ یانینا چاغێرێب دئدیلر : آللاه بیزه رحم ائله ، بو آداما دا وئر . بو آداما دا گۆناهکارلارا یاخشیدیر .\n",
            "2023-01-19 23:42:06,317 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:42:06,319 - INFO - joeynmt.training - \tSource:     داش آكل خوب یادش بود كه سه روز پیش در قهوه خانه دو میل كاكا رستم برایش خط و نشان كشید ، \n",
            "2023-01-19 23:42:06,319 - INFO - joeynmt.training - \tReference:  داش آکلین یاخشی یادیندا ایدی کی اوچ گون بوندان اول دومیل قفه خاناسیندا ، کاکا رستم اونا خط نشان چکمیشدی ، \n",
            "2023-01-19 23:42:06,320 - INFO - joeynmt.training - \tHypothesis: داش آکل گونده کی گونده کی قیسمین اون بئشی دان بیر قیسم دان دان بیر قیسم داها دان آلتیندا گوندی\n",
            "2023-01-19 23:42:14,289 - INFO - joeynmt.training - Epoch 189, Step:    65100, Batch Loss:     1.621657, Batch Acc: 0.554958, Tokens per Sec:    13337, Lr: 0.000035\n",
            "2023-01-19 23:42:22,240 - INFO - joeynmt.training - Epoch 189, Step:    65200, Batch Loss:     1.809945, Batch Acc: 0.553155, Tokens per Sec:    13833, Lr: 0.000035\n",
            "2023-01-19 23:42:30,131 - INFO - joeynmt.training - Epoch 189, Step:    65300, Batch Loss:     1.762505, Batch Acc: 0.548230, Tokens per Sec:    14112, Lr: 0.000035\n",
            "2023-01-19 23:42:32,215 - INFO - joeynmt.training - Epoch 189: total training loss 609.50\n",
            "2023-01-19 23:42:32,215 - INFO - joeynmt.training - EPOCH 190\n",
            "2023-01-19 23:42:40,562 - INFO - joeynmt.training - Epoch 190, Step:    65400, Batch Loss:     1.754599, Batch Acc: 0.554179, Tokens per Sec:     9806, Lr: 0.000035\n",
            "2023-01-19 23:42:49,034 - INFO - joeynmt.training - Epoch 190, Step:    65500, Batch Loss:     1.771951, Batch Acc: 0.553368, Tokens per Sec:    12908, Lr: 0.000035\n",
            "2023-01-19 23:42:56,878 - INFO - joeynmt.training - Epoch 190, Step:    65600, Batch Loss:     1.827695, Batch Acc: 0.555810, Tokens per Sec:    14193, Lr: 0.000035\n",
            "2023-01-19 23:43:02,512 - INFO - joeynmt.training - Epoch 190: total training loss 611.03\n",
            "2023-01-19 23:43:02,513 - INFO - joeynmt.training - EPOCH 191\n",
            "2023-01-19 23:43:04,838 - INFO - joeynmt.training - Epoch 191, Step:    65700, Batch Loss:     1.746912, Batch Acc: 0.565561, Tokens per Sec:    13596, Lr: 0.000035\n",
            "2023-01-19 23:43:12,778 - INFO - joeynmt.training - Epoch 191, Step:    65800, Batch Loss:     1.864467, Batch Acc: 0.552719, Tokens per Sec:    13789, Lr: 0.000035\n",
            "2023-01-19 23:43:20,589 - INFO - joeynmt.training - Epoch 191, Step:    65900, Batch Loss:     1.771925, Batch Acc: 0.553435, Tokens per Sec:    14149, Lr: 0.000035\n",
            "2023-01-19 23:43:28,360 - INFO - joeynmt.training - Epoch 191, Step:    66000, Batch Loss:     1.907080, Batch Acc: 0.552842, Tokens per Sec:    14181, Lr: 0.000035\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.25ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8206.04ex/s]\n",
            "2023-01-19 23:43:28,667 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=66000\n",
            "2023-01-19 23:43:28,667 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:43:33,034 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:43:33,034 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:43:33,034 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:43:33,036 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:43:33,044 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.39, loss:   2.65, ppl:  14.20, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3265[sec], evaluation: 0.0426[sec]\n",
            "2023-01-19 23:43:33,046 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:43:33,049 - INFO - joeynmt.training - \tSource:     همچون کارگزاران نیکوی لطف ال هی ، لطفی که عطایای گوناگون می‌بخشد ، متناسب با عطیه‌ای که یافته‌اید یکدیگر را خدمت کنید . \n",
            "2023-01-19 23:43:33,050 - INFO - joeynmt.training - \tReference:  هر کس آللاهێن چوخجهتلی لۆتفنۆن یاخشی ادارهچیلری کیمی آلدێغێ اناما گؤره بیر بیرینه خدمت ائتسین . \n",
            "2023-01-19 23:43:33,050 - INFO - joeynmt.training - \tHypothesis: لۆتفکارلارا لۆتف وه لۆتفله بیر یئرده لۆتفله سیزه خدمت ائدیر .\n",
            "2023-01-19 23:43:33,050 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:43:33,052 - INFO - joeynmt.training - \tSource:     برعکس ، آنان متوجه شدند که بشارت دادن به غیریهودیان به من سپرده شده است ، همان طور که بشارت دادن به یهودیان به پطرس واگذار شده بود \n",
            "2023-01-19 23:43:33,052 - INFO - joeynmt.training - \tReference:  اکسینه ، سۆنتلیلره مۆژده یایماق ایشی پئتئره تاپشیریلدیغی کیمی سۆنتسیزلر آراسێندا مۆژده یایماق ایشینین منه تاپشیریلدیغینی گؤردولر . \n",
            "2023-01-19 23:43:33,052 - INFO - joeynmt.training - \tHypothesis: اونلار سیناقوق رۆسۆایچیلێقدا مۆژدهی یایاراق مۆژدنی یایدیغیم مۆژدنی یایماق اۆچۆن اونلارا وز ائدیلدی . پئتئرین یهودیلره وزیگتده قویولانلارینا وزیگتده قویدو .\n",
            "2023-01-19 23:43:33,052 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:43:33,054 - INFO - joeynmt.training - \tSource:     گناهان ما را ببخش ؛ زیرا خود ما نیز هر که را به ما مقروض است ، می‌بخشیم . همچنین ما را در وسوسه میاور . \n",
            "2023-01-19 23:43:33,055 - INFO - joeynmt.training - \tReference:  بیزه بورجلو اولان هر آدامێ باغیشلادیغیمیزا گؤره بیزیم گۆناهلاریمیزی دا باغێشلا . بیزی سێناغا چکمه . \n",
            "2023-01-19 23:43:33,055 - INFO - joeynmt.training - \tHypothesis: گۆناهلاریمیزی باغیشلانماسینی ، چۆنکی بیز ده باغیشلایارێق . بیزسهسه سێناقدان کئچنلری ایله سێناق .\n",
            "2023-01-19 23:43:33,055 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:43:33,057 - INFO - joeynmt.training - \tSource:     هر کاری که می‌کنند برای این است که توجه مردم را به خود جلب کنند ؛ آیه‌دان‌های خود را بزرگ‌تر و حاشیهٔ رداهای خود را پهن‌تر می‌کنند . \n",
            "2023-01-19 23:43:33,057 - INFO - joeynmt.training - \tReference:  اونلار اؤز امللرینین هامیسینی اینسانلار گؤرسۆن دئیه ائدرلر . چۆنکی اونلار دوعا قوتوجوقلارینی گئنیشلندیریب گئییملرینین قوتازلارێنێ اوزادارلار . \n",
            "2023-01-19 23:43:33,057 - INFO - joeynmt.training - \tHypothesis: بو ایشین خالاتلیلارین هامێسێ سنی دینلهمک اۆچۆن ، اؤز تاختلارێنا اۆستۆن توتارلار .\n",
            "2023-01-19 23:43:34,420 - INFO - joeynmt.training - Epoch 191: total training loss 610.79\n",
            "2023-01-19 23:43:34,420 - INFO - joeynmt.training - EPOCH 192\n",
            "2023-01-19 23:43:41,046 - INFO - joeynmt.training - Epoch 192, Step:    66100, Batch Loss:     1.785802, Batch Acc: 0.553298, Tokens per Sec:    13845, Lr: 0.000035\n",
            "2023-01-19 23:43:48,880 - INFO - joeynmt.training - Epoch 192, Step:    66200, Batch Loss:     1.917064, Batch Acc: 0.554616, Tokens per Sec:    14168, Lr: 0.000035\n",
            "2023-01-19 23:43:56,672 - INFO - joeynmt.training - Epoch 192, Step:    66300, Batch Loss:     1.956687, Batch Acc: 0.554821, Tokens per Sec:    13945, Lr: 0.000035\n",
            "2023-01-19 23:44:01,572 - INFO - joeynmt.training - Epoch 192: total training loss 610.60\n",
            "2023-01-19 23:44:01,573 - INFO - joeynmt.training - EPOCH 193\n",
            "2023-01-19 23:44:04,512 - INFO - joeynmt.training - Epoch 193, Step:    66400, Batch Loss:     1.777727, Batch Acc: 0.557615, Tokens per Sec:    13805, Lr: 0.000035\n",
            "2023-01-19 23:44:12,383 - INFO - joeynmt.training - Epoch 193, Step:    66500, Batch Loss:     1.660130, Batch Acc: 0.554350, Tokens per Sec:    14038, Lr: 0.000035\n",
            "2023-01-19 23:44:20,245 - INFO - joeynmt.training - Epoch 193, Step:    66600, Batch Loss:     1.719990, Batch Acc: 0.552113, Tokens per Sec:    14000, Lr: 0.000035\n",
            "2023-01-19 23:44:28,135 - INFO - joeynmt.training - Epoch 193, Step:    66700, Batch Loss:     1.721797, Batch Acc: 0.550436, Tokens per Sec:    13806, Lr: 0.000035\n",
            "2023-01-19 23:44:28,871 - INFO - joeynmt.training - Epoch 193: total training loss 610.48\n",
            "2023-01-19 23:44:28,871 - INFO - joeynmt.training - EPOCH 194\n",
            "2023-01-19 23:44:36,018 - INFO - joeynmt.training - Epoch 194, Step:    66800, Batch Loss:     1.748086, Batch Acc: 0.558630, Tokens per Sec:    13986, Lr: 0.000035\n",
            "2023-01-19 23:44:43,962 - INFO - joeynmt.training - Epoch 194, Step:    66900, Batch Loss:     1.833403, Batch Acc: 0.552544, Tokens per Sec:    13848, Lr: 0.000035\n",
            "2023-01-19 23:44:51,859 - INFO - joeynmt.training - Epoch 194, Step:    67000, Batch Loss:     1.708925, Batch Acc: 0.558357, Tokens per Sec:    13892, Lr: 0.000035\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.05ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10412.48ex/s]\n",
            "2023-01-19 23:44:52,125 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=67000\n",
            "2023-01-19 23:44:52,125 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:44:56,022 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:44:56,022 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:44:56,022 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:44:56,023 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:44:56,027 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.03, loss:   2.71, ppl:  14.98, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 3.8592[sec], evaluation: 0.0352[sec]\n",
            "2023-01-19 23:44:56,029 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:44:56,033 - INFO - joeynmt.training - \tSource:     و در صور دمیده می‌شود ، پس هر که در آسمانها و هر که در زمین است بیهوش درمی‌افتد ، مگر کسی که خدا بخواهد ؛ سپس بار دیگر در آن دمیده می‌شود و بناگاه آنان بر پای ایستاده می‌نگرند . \n",
            "2023-01-19 23:44:56,033 - INFO - joeynmt.training - \tReference:  سور چالیناجاق ، آللاهێن گؤیلرده وه یئرده اولان ایستهدیگی کیمسهلردن باشقا ، درحال هامێ یێخیلیب اؤلهجک . سونرا بیر داها چالێنان کیمی اونلار قالخێب مۆنتزر اولاجاقلار ! \n",
            "2023-01-19 23:44:56,033 - INFO - joeynmt.training - \tHypothesis: سور چالیناجاغی یئر اۆزۆنده اولان هر کس گؤیده وه یاشیلاردا اولان بیرینین آردێنجا گئدن کیمسه کیمین آللاه ایستهینلره گؤیدن ائندیگینی وه اونلار داها دایانیب گئدرلر .\n",
            "2023-01-19 23:44:56,033 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:44:56,035 - INFO - joeynmt.training - \tSource:     وقتی عیسی بازگشت ، مردم به گرمی از او استقبال کردند ؛ زیرا همه منتظرش بودند . \n",
            "2023-01-19 23:44:56,035 - INFO - joeynmt.training - \tReference:  ایسا گئری قاییتدیقدا خالق اونو قبول ائتدی ، چۆنکی هامێ اونو گؤزلهییردی . \n",
            "2023-01-19 23:44:56,036 - INFO - joeynmt.training - \tHypothesis: ایسا یئنه ده آداملار ایسنانین یانینا قاییدیب . چۆنکی یئروسهلیمه گئدیردی .\n",
            "2023-01-19 23:44:56,036 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:44:56,038 - INFO - joeynmt.training - \tSource:     روی صندلی خالی مینشینی . \n",
            "2023-01-19 23:44:56,038 - INFO - joeynmt.training - \tReference:  بوش صندل ده اوتورور سان . \n",
            "2023-01-19 23:44:56,038 - INFO - joeynmt.training - \tHypothesis: و و و  لار اونون اوستونده سینه .\n",
            "2023-01-19 23:44:56,039 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:44:56,040 - INFO - joeynmt.training - \tSource:     یک بار که آپولس در کنیسه با شهامت شروع به سخن گفتن کرد ، پریسکیلا و آکیلا سخنانش را شنیده ، او را نزد خود بردند و طریق خدا را دقیق‌تر به او آموزش دادند . \n",
            "2023-01-19 23:44:56,041 - INFO - joeynmt.training - \tReference:  او ، سیناقوقدا جصارتله دانێشماغا باشلادێ . اونا قولاق آسان پریسکیلا ایله آکیلا اونو یانلارینا چکیب آللاه یولو بارهده داها دقیق اضاهات وئردی . \n",
            "2023-01-19 23:44:56,041 - INFO - joeynmt.training - \tHypothesis: سیناقوق رعیسی ایله بیرگه دانێشاندا بیرگه دانێشماغا باشلادێ . آپولیا ، بزی فئستهیه پاوللا صحبت ائدندن سونرا پاولو پاولون یانیندا اونو اؤیرهدرک آللاهێ مکتوب ائتدی .\n",
            "2023-01-19 23:45:00,387 - INFO - joeynmt.training - Epoch 194: total training loss 608.79\n",
            "2023-01-19 23:45:00,388 - INFO - joeynmt.training - EPOCH 195\n",
            "2023-01-19 23:45:04,000 - INFO - joeynmt.training - Epoch 195, Step:    67100, Batch Loss:     1.632652, Batch Acc: 0.559516, Tokens per Sec:    13513, Lr: 0.000035\n",
            "2023-01-19 23:45:11,864 - INFO - joeynmt.training - Epoch 195, Step:    67200, Batch Loss:     1.882122, Batch Acc: 0.555721, Tokens per Sec:    14002, Lr: 0.000035\n",
            "2023-01-19 23:45:19,774 - INFO - joeynmt.training - Epoch 195, Step:    67300, Batch Loss:     1.871349, Batch Acc: 0.553551, Tokens per Sec:    13960, Lr: 0.000034\n",
            "2023-01-19 23:45:27,626 - INFO - joeynmt.training - Epoch 195, Step:    67400, Batch Loss:     1.936306, Batch Acc: 0.555326, Tokens per Sec:    13985, Lr: 0.000034\n",
            "2023-01-19 23:45:27,719 - INFO - joeynmt.training - Epoch 195: total training loss 609.22\n",
            "2023-01-19 23:45:27,720 - INFO - joeynmt.training - EPOCH 196\n",
            "2023-01-19 23:45:35,446 - INFO - joeynmt.training - Epoch 196, Step:    67500, Batch Loss:     1.670635, Batch Acc: 0.558399, Tokens per Sec:    14007, Lr: 0.000034\n",
            "2023-01-19 23:45:43,438 - INFO - joeynmt.training - Epoch 196, Step:    67600, Batch Loss:     1.729256, Batch Acc: 0.554899, Tokens per Sec:    13600, Lr: 0.000034\n",
            "2023-01-19 23:45:52,625 - INFO - joeynmt.training - Epoch 196, Step:    67700, Batch Loss:     1.802720, Batch Acc: 0.554635, Tokens per Sec:    12068, Lr: 0.000034\n",
            "2023-01-19 23:45:58,039 - INFO - joeynmt.training - Epoch 196: total training loss 609.38\n",
            "2023-01-19 23:45:58,039 - INFO - joeynmt.training - EPOCH 197\n",
            "2023-01-19 23:46:02,103 - INFO - joeynmt.training - Epoch 197, Step:    67800, Batch Loss:     1.691463, Batch Acc: 0.560546, Tokens per Sec:    13984, Lr: 0.000034\n",
            "2023-01-19 23:46:10,058 - INFO - joeynmt.training - Epoch 197, Step:    67900, Batch Loss:     1.789670, Batch Acc: 0.555740, Tokens per Sec:    13836, Lr: 0.000034\n",
            "2023-01-19 23:46:17,881 - INFO - joeynmt.training - Epoch 197, Step:    68000, Batch Loss:     1.824391, Batch Acc: 0.555971, Tokens per Sec:    14203, Lr: 0.000034\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 114.55ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9347.16ex/s]\n",
            "2023-01-19 23:46:18,178 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=68000\n",
            "2023-01-19 23:46:18,178 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:46:22,000 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:46:22,000 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:46:22,001 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:46:22,001 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:46:22,005 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.09, loss:   2.82, ppl:  16.75, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 3.7846[sec], evaluation: 0.0347[sec]\n",
            "2023-01-19 23:46:22,007 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:46:22,010 - INFO - joeynmt.training - \tSource:     و فرعون و کسانی که پیش از او بودند و مردم‌ شهرهای سرنگون شده سدوم و عاموره‌ مرتکب خطا شدند . \n",
            "2023-01-19 23:46:22,010 - INFO - joeynmt.training - \tReference:  فیر اون دا ، اوندان اوهلکیلر ده ، آلت اۆست اولموش معؤ تفیکه اهلی ده گۆناه تؤرتمیشدیلر . \n",
            "2023-01-19 23:46:22,011 - INFO - joeynmt.training - \tHypothesis: فیر اون وه اونون اترافێنداکێ شهرلره گلیب چاتدێ . او ، چوخ اترافێندا یئرله گئتدی .\n",
            "2023-01-19 23:46:22,011 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:46:22,013 - INFO - joeynmt.training - \tSource:     بلکه به مردم ، برای آنچه خدا از فضل خویش به آنان عطا کرده رشک می‌ورزند ؛ در حقیقت ، ما به خاندان ابراهیم کتاب و حکمت دادیم ، و به آنان ملکی بزرگ بخشیدیم . \n",
            "2023-01-19 23:46:22,013 - INFO - joeynmt.training - \tReference:  یوخسا اونلار آللاهێن اؤز نئ متیندن بخش ائتدیگی شئیلره گؤره اینسانلارا حسد آپارێرلار ؟ حالبوکی بیز ابراهیم اؤؤلادێنا دا کیتاب وه هکمت وئرمیشدیک وه اونلارا بؤیوک مۆلک بخش ائتمیشدیک . \n",
            "2023-01-19 23:46:22,014 - INFO - joeynmt.training - \tHypothesis: اینسانلارا اؤز مرحمتندن مرحمت اولاراق آللاهێن لۆتفۆ ایله اونلارا مرحمت گؤستردیگی نئ متلر بخش ائدیر . بیز ابراهیمه هکمت وه ائلم وئردیک ، حقیقتا ، اونلارا ، هکمت وئردیک ، بیز اونلارا بؤیوک هکمت وئردیک .\n",
            "2023-01-19 23:46:22,014 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:46:22,016 - INFO - joeynmt.training - \tSource:     از طرف پولس که به خواست خدا رسول مسیحْ عیسی گشت تا وعدهٔ آن حیاتی را اعلام کند که از طریق مسیحْ عیسی میسر شده است ، \n",
            "2023-01-19 23:46:22,016 - INFO - joeynmt.training - \tReference:  مصیح اسعادا اولان هیات ودینه گؤره آللاهێن ارادهسی ایله مصیح ایسنانین هوارسی من پاولدان\n",
            "2023-01-19 23:46:22,016 - INFO - joeynmt.training - \tHypothesis: آللاهێن ارادهسی ایله مصیح اسعادا مسیحین هوارسی من پاولدان مصیح اسعادا اولان وه اونون ودسو ایله هیات سۆرمک اۆچۆن مصیح اسعادا اولان مصیح اسعادا اولان مصیح اسعادا اولاراق\n",
            "2023-01-19 23:46:22,016 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:46:22,018 - INFO - joeynmt.training - \tSource:     و به کلام حیات‌بخش پای‌بند می‌مانید . به همین خاطر ، من دلیلی برای شادی در روز مسیح خواهم داشت ؛ زیرا آنگاه خواهم دانست که بیهوده ندویده و عبث زحمت نکشیده‌ام . \n",
            "2023-01-19 23:46:22,018 - INFO - joeynmt.training - \tReference:  هیات سؤزونو تقدیم ائدین . قوی مسیحین زۆهور ائدهجهگی گۆنده فخر ائده بیلیم کی ، نه بوش یئره جهد ائتدیم ، نه ده بوش یئره ظهمت چکدیم . \n",
            "2023-01-19 23:46:22,018 - INFO - joeynmt.training - \tHypothesis: هیاتا مالک اولان هیات سۆرمک اۆچۆن بیزه سئویندیلر . بونا گؤره ده منیم حبصلرینیزده سئوینهجهیهم . چۆنکی او ، چوخ آغیلسیزلێق ایچیندهسینیز .\n",
            "2023-01-19 23:46:29,439 - INFO - joeynmt.training - Epoch 197: total training loss 605.35\n",
            "2023-01-19 23:46:29,440 - INFO - joeynmt.training - EPOCH 198\n",
            "2023-01-19 23:46:30,003 - INFO - joeynmt.training - Epoch 198, Step:    68100, Batch Loss:     1.653715, Batch Acc: 0.564046, Tokens per Sec:    13568, Lr: 0.000034\n",
            "2023-01-19 23:46:37,857 - INFO - joeynmt.training - Epoch 198, Step:    68200, Batch Loss:     1.780273, Batch Acc: 0.560241, Tokens per Sec:    14041, Lr: 0.000034\n",
            "2023-01-19 23:46:45,889 - INFO - joeynmt.training - Epoch 198, Step:    68300, Batch Loss:     1.823014, Batch Acc: 0.557645, Tokens per Sec:    13861, Lr: 0.000034\n",
            "2023-01-19 23:46:53,689 - INFO - joeynmt.training - Epoch 198, Step:    68400, Batch Loss:     1.782548, Batch Acc: 0.554199, Tokens per Sec:    13958, Lr: 0.000034\n",
            "2023-01-19 23:46:56,750 - INFO - joeynmt.training - Epoch 198: total training loss 606.62\n",
            "2023-01-19 23:46:56,750 - INFO - joeynmt.training - EPOCH 199\n",
            "2023-01-19 23:47:01,487 - INFO - joeynmt.training - Epoch 199, Step:    68500, Batch Loss:     1.807859, Batch Acc: 0.558064, Tokens per Sec:    14310, Lr: 0.000034\n",
            "2023-01-19 23:47:09,309 - INFO - joeynmt.training - Epoch 199, Step:    68600, Batch Loss:     1.660425, Batch Acc: 0.560687, Tokens per Sec:    13963, Lr: 0.000034\n",
            "2023-01-19 23:47:17,183 - INFO - joeynmt.training - Epoch 199, Step:    68700, Batch Loss:     1.683098, Batch Acc: 0.558270, Tokens per Sec:    13887, Lr: 0.000034\n",
            "2023-01-19 23:47:24,034 - INFO - joeynmt.training - Epoch 199: total training loss 607.65\n",
            "2023-01-19 23:47:24,034 - INFO - joeynmt.training - EPOCH 200\n",
            "2023-01-19 23:47:25,128 - INFO - joeynmt.training - Epoch 200, Step:    68800, Batch Loss:     1.717564, Batch Acc: 0.554130, Tokens per Sec:    13905, Lr: 0.000034\n",
            "2023-01-19 23:47:32,996 - INFO - joeynmt.training - Epoch 200, Step:    68900, Batch Loss:     1.775683, Batch Acc: 0.560717, Tokens per Sec:    13914, Lr: 0.000034\n",
            "2023-01-19 23:47:40,899 - INFO - joeynmt.training - Epoch 200, Step:    69000, Batch Loss:     1.681987, Batch Acc: 0.557765, Tokens per Sec:    13873, Lr: 0.000034\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.46ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9600.11ex/s]\n",
            "2023-01-19 23:47:41,175 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=69000\n",
            "2023-01-19 23:47:41,176 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:47:45,820 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:47:45,821 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:47:45,821 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:47:45,822 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:47:45,825 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.65, loss:   2.81, ppl:  16.63, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6037[sec], evaluation: 0.0381[sec]\n",
            "2023-01-19 23:47:45,827 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:47:45,830 - INFO - joeynmt.training - \tSource:     و در حقیقت ، در زبور پس از تورات نوشتیم که زمین را بندگان شایسته ما به ارث خواهند برد . \n",
            "2023-01-19 23:47:45,831 - INFO - joeynmt.training - \tReference:  بیز کیتابدان سونرا زبوردا دا تورپاغا یالنیز منیم سالئه بندهلریمین داخل اولاجاغێنێ یازمیشدیق . \n",
            "2023-01-19 23:47:45,831 - INFO - joeynmt.training - \tHypothesis: بیز سنه یئر اۆزۆنده فیتنه فصاد تؤرهدرلر . حقیقتا ، سنین بندهنی مۆت ایرهلیسینی آپاراجاق .\n",
            "2023-01-19 23:47:45,831 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:47:45,833 - INFO - joeynmt.training - \tSource:     آیا برای آنان روشن نگردیده که چه بسیار نسلها را پیش از آنها نابود گردانیدیم که اینان‌ در سراهایشان راه می‌روند ؟ قطعا در این امر عبرتهاست ، مگر نمی‌شنوند ؟ \n",
            "2023-01-19 23:47:45,833 - INFO - joeynmt.training - \tReference:  مگر اونلار ایندی یوردلاریندا گزیب دولاشدێقلاری اؤزلریندن اوهلکی نئچه نئچه نسیللری محو ائتدیگیمیزی گؤرمورلرمی ؟ حقیقتا ، بوندا ابرتلر واردێر . مگر قولاق آسمایاجاقلار ؟ \n",
            "2023-01-19 23:47:45,833 - INFO - joeynmt.training - \tHypothesis: مگر اونلارا اؤنجه نئچه نئچه نسیللری محو ائتدیگیمیز نئچه نئچه نئچه نئچه نئچه نئچه نئچه نئچه نئچه نئچه دلیللرمی ؟ حقیقتا ، ائشیدنلر !\n",
            "2023-01-19 23:47:45,834 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:47:45,835 - INFO - joeynmt.training - \tSource:     عیسی دلش به حال او سوخت . پس دستش را دراز کرد ، آن مرد را لمس نمود و به او گفت : می‌خواهم . پاک شو . \n",
            "2023-01-19 23:47:45,836 - INFO - joeynmt.training - \tReference:  ایسنانین اونا رحمی گلدی . الینی اوزادێب اونا توخوندو وه دئدی : ایستهییرهم ، پاک اول ! \n",
            "2023-01-19 23:47:45,836 - INFO - joeynmt.training - \tHypothesis: ایسا اونون اۆرهیینه هئیرت ائتدی . او ، اۆچۆنجۆسۆ ایسا اونا توخوندو وه دئدی : من پاک اول !\n",
            "2023-01-19 23:47:45,836 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:47:45,838 - INFO - joeynmt.training - \tSource:     پروردگارا ، مرا برپادارنده نماز قرار ده ، و از فرزندان من نیز . پروردگارا ، و دعای مرا بپذیر . \n",
            "2023-01-19 23:47:45,838 - INFO - joeynmt.training - \tReference:  ائی ربیم ! منی ده ، نسلیمدن اولانلارێ دا ناماز قێلان ائت . ائی ربیمیز ! دوعامێ قبول بویور ! \n",
            "2023-01-19 23:47:45,838 - INFO - joeynmt.training - \tHypothesis: ائی ربیم ! منی ناماز قێل ، ذکات وئر ! من ده یاخود ربیم ! منی ده دوعا ائتمیشدی .\n",
            "2023-01-19 23:47:53,606 - INFO - joeynmt.training - Epoch 200, Step:    69100, Batch Loss:     1.649231, Batch Acc: 0.557229, Tokens per Sec:    13656, Lr: 0.000034\n",
            "2023-01-19 23:47:56,170 - INFO - joeynmt.training - Epoch 200: total training loss 605.64\n",
            "2023-01-19 23:47:56,170 - INFO - joeynmt.training - EPOCH 201\n",
            "2023-01-19 23:48:01,485 - INFO - joeynmt.training - Epoch 201, Step:    69200, Batch Loss:     1.745575, Batch Acc: 0.560099, Tokens per Sec:    14279, Lr: 0.000034\n",
            "2023-01-19 23:48:09,382 - INFO - joeynmt.training - Epoch 201, Step:    69300, Batch Loss:     1.766333, Batch Acc: 0.559804, Tokens per Sec:    13855, Lr: 0.000034\n",
            "2023-01-19 23:48:17,208 - INFO - joeynmt.training - Epoch 201, Step:    69400, Batch Loss:     1.648178, Batch Acc: 0.555167, Tokens per Sec:    13960, Lr: 0.000034\n",
            "2023-01-19 23:48:23,328 - INFO - joeynmt.training - Epoch 201: total training loss 604.64\n",
            "2023-01-19 23:48:23,328 - INFO - joeynmt.training - EPOCH 202\n",
            "2023-01-19 23:48:25,033 - INFO - joeynmt.training - Epoch 202, Step:    69500, Batch Loss:     1.705894, Batch Acc: 0.559471, Tokens per Sec:    13922, Lr: 0.000034\n",
            "2023-01-19 23:48:32,896 - INFO - joeynmt.training - Epoch 202, Step:    69600, Batch Loss:     1.952685, Batch Acc: 0.558576, Tokens per Sec:    14117, Lr: 0.000034\n",
            "2023-01-19 23:48:40,690 - INFO - joeynmt.training - Epoch 202, Step:    69700, Batch Loss:     1.778004, Batch Acc: 0.557356, Tokens per Sec:    14135, Lr: 0.000034\n",
            "2023-01-19 23:48:48,559 - INFO - joeynmt.training - Epoch 202, Step:    69800, Batch Loss:     1.610044, Batch Acc: 0.556233, Tokens per Sec:    13897, Lr: 0.000034\n",
            "2023-01-19 23:48:50,380 - INFO - joeynmt.training - Epoch 202: total training loss 601.84\n",
            "2023-01-19 23:48:50,381 - INFO - joeynmt.training - EPOCH 203\n",
            "2023-01-19 23:48:56,372 - INFO - joeynmt.training - Epoch 203, Step:    69900, Batch Loss:     1.606824, Batch Acc: 0.562303, Tokens per Sec:    14152, Lr: 0.000034\n",
            "2023-01-19 23:49:05,886 - INFO - joeynmt.training - Epoch 203, Step:    70000, Batch Loss:     1.833151, Batch Acc: 0.554704, Tokens per Sec:    11427, Lr: 0.000034\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 55.07ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 6112.04ex/s]\n",
            "2023-01-19 23:49:06,400 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=70000\n",
            "2023-01-19 23:49:06,401 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:49:13,035 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:49:13,035 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:49:13,036 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:49:13,037 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:49:13,040 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.34, loss:   2.82, ppl:  16.70, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 6.5923[sec], evaluation: 0.0387[sec]\n",
            "2023-01-19 23:49:13,042 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:49:13,045 - INFO - joeynmt.training - \tSource:      ملکه‌ گفت : پادشاهان چون به شهری درآیند ، آن را تباه و عزیزانش را خوار می‌گردانند ، و این گونه می‌کنند . \n",
            "2023-01-19 23:49:13,046 - INFO - joeynmt.training - \tReference:   دئدی : هؤکمدارلار بیر اؤلکهیه گیردیکلری زامان اونو خارابازارا چئویرر ، خالقێنێن بؤیوکلرینی ده زلیل ائدرلر . اونلار محض بئله هرهکت ائدرلر \n",
            "2023-01-19 23:49:13,046 - INFO - joeynmt.training - \tHypothesis: او ملهین پادشاهلارێ ملهیهنه سالان بیر مملکته او شهرده اولان مظمت ائدرلر .\n",
            "2023-01-19 23:49:13,046 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:49:13,048 - INFO - joeynmt.training - \tSource:     آنگاه مادر و برادران عیسی برای دیدنش آمدند ، اما به دلیل ازدحام جمعیت نتوانستند پیش او بروند . \n",
            "2023-01-19 23:49:13,048 - INFO - joeynmt.training - \tReference:  ایسنانین آناسێ وه قارداشلارێ اونون یانینا گلدی . آمما ازدههاما گؤره اونا یاخینلاشا بیلمهدیلر . \n",
            "2023-01-19 23:49:13,048 - INFO - joeynmt.training - \tHypothesis: آناسێ آناسێ وه قارداشلارێ ایسنانین آناسێنا گؤره بیلمیردی . آمما جاماآتدان قاباغێنا گله بیلمهدیلر .\n",
            "2023-01-19 23:49:13,048 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:49:13,050 - INFO - joeynmt.training - \tSource:     اما آنان با بی‌اعتنایی ، یکی پی مزرعهٔ خود رفت ، دیگری پی تجارتش\n",
            "2023-01-19 23:49:13,051 - INFO - joeynmt.training - \tReference:  آمما دوت اولونانلار ائتیناسیزلێق گؤستهرهک کیمی تارلاسێنا ، کیمی تجارتنه گئتدی . \n",
            "2023-01-19 23:49:13,051 - INFO - joeynmt.training - \tHypothesis: آمما اونلارلا بیرگه ایتیرمی ، باشقالارینین داها بتت آنیایا بؤلگهسینده بیر آز ایدی .\n",
            "2023-01-19 23:49:13,051 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:49:13,053 - INFO - joeynmt.training - \tSource:     گذشتن شتر از سوراخ سوزن آسان‌تر است ، از راه یافتن شخص ثروتمند به پادشاهی خدا ! \n",
            "2023-01-19 23:49:13,053 - INFO - joeynmt.training - \tReference:  دوهنین اییینه گؤزوندن کئچمهسی وارلێ آدامێن آللاهێن پادشاهلیغینا داخل اولماسێندان آساندێر . \n",
            "2023-01-19 23:49:13,053 - INFO - joeynmt.training - \tHypothesis: دوغرودان دا ، یولونو آزمێش ، ینی آللاهێن پادشاهلیغینا ان بؤیوکدور !\n",
            "2023-01-19 23:49:20,958 - INFO - joeynmt.training - Epoch 203, Step:    70100, Batch Loss:     1.598181, Batch Acc: 0.559827, Tokens per Sec:    13150, Lr: 0.000034\n",
            "2023-01-19 23:49:26,309 - INFO - joeynmt.training - Epoch 203: total training loss 601.46\n",
            "2023-01-19 23:49:26,309 - INFO - joeynmt.training - EPOCH 204\n",
            "2023-01-19 23:49:28,815 - INFO - joeynmt.training - Epoch 204, Step:    70200, Batch Loss:     1.640955, Batch Acc: 0.558002, Tokens per Sec:    14069, Lr: 0.000034\n",
            "2023-01-19 23:49:36,568 - INFO - joeynmt.training - Epoch 204, Step:    70300, Batch Loss:     1.626372, Batch Acc: 0.558438, Tokens per Sec:    14124, Lr: 0.000034\n",
            "2023-01-19 23:49:44,509 - INFO - joeynmt.training - Epoch 204, Step:    70400, Batch Loss:     1.841527, Batch Acc: 0.558760, Tokens per Sec:    13836, Lr: 0.000034\n",
            "2023-01-19 23:49:52,320 - INFO - joeynmt.training - Epoch 204, Step:    70500, Batch Loss:     1.588282, Batch Acc: 0.559471, Tokens per Sec:    14091, Lr: 0.000034\n",
            "2023-01-19 23:49:53,470 - INFO - joeynmt.training - Epoch 204: total training loss 602.85\n",
            "2023-01-19 23:49:53,470 - INFO - joeynmt.training - EPOCH 205\n",
            "2023-01-19 23:50:00,133 - INFO - joeynmt.training - Epoch 205, Step:    70600, Batch Loss:     1.751869, Batch Acc: 0.560480, Tokens per Sec:    13997, Lr: 0.000034\n",
            "2023-01-19 23:50:08,043 - INFO - joeynmt.training - Epoch 205, Step:    70700, Batch Loss:     1.843357, Batch Acc: 0.559474, Tokens per Sec:    14034, Lr: 0.000034\n",
            "2023-01-19 23:50:15,951 - INFO - joeynmt.training - Epoch 205, Step:    70800, Batch Loss:     1.801921, Batch Acc: 0.557779, Tokens per Sec:    13936, Lr: 0.000034\n",
            "2023-01-19 23:50:20,696 - INFO - joeynmt.training - Epoch 205: total training loss 601.95\n",
            "2023-01-19 23:50:20,696 - INFO - joeynmt.training - EPOCH 206\n",
            "2023-01-19 23:50:23,829 - INFO - joeynmt.training - Epoch 206, Step:    70900, Batch Loss:     1.618743, Batch Acc: 0.563546, Tokens per Sec:    13945, Lr: 0.000034\n",
            "2023-01-19 23:50:31,609 - INFO - joeynmt.training - Epoch 206, Step:    71000, Batch Loss:     1.750921, Batch Acc: 0.562685, Tokens per Sec:    14058, Lr: 0.000034\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.90ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9625.38ex/s]\n",
            "2023-01-19 23:50:31,884 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=71000\n",
            "2023-01-19 23:50:31,885 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:50:36,291 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:50:36,291 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:50:36,291 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:50:36,292 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:50:36,295 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.81, loss:   2.80, ppl:  16.47, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3680[sec], evaluation: 0.0355[sec]\n",
            "2023-01-19 23:50:36,298 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:50:36,301 - INFO - joeynmt.training - \tSource:     اندکی بعد غلام گفت : ارباب فرمانت به اجرا درآمد . اما هنوز هم جا هست . \n",
            "2023-01-19 23:50:36,301 - INFO - joeynmt.training - \tReference:  قول ایسه دئدی : آغا ، سنین امرین یئرینه یئتیریلدی ، هله آرتێق یئر ده قالێب . \n",
            "2023-01-19 23:50:36,301 - INFO - joeynmt.training - \tHypothesis: بیر آز سونرا ایسه آغایا دئدی : هله ده قولدور . آمما هله ده گلیب چاتمادێ .\n",
            "2023-01-19 23:50:36,302 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:50:36,303 - INFO - joeynmt.training - \tSource:      در حقیقت ، تو را بر بندگان من تسلطی نیست ، و حمایتگری چون‌ پروردگارت بس است . \n",
            "2023-01-19 23:50:36,304 - INFO - joeynmt.training - \tReference:  دوغروسو ، منیم بندهلریم اۆزهرینده سنین هئچ بیر هؤکمون اولا بیلمز . ربینین وکیل اولماسێ کفایت ائدر ! \n",
            "2023-01-19 23:50:36,304 - INFO - joeynmt.training - \tHypothesis: سنه بندهلریمهین بندهلریندن هئچ بیر شئی یوخدور . او ، ربینین درگاهێنا کفایت ائدر .\n",
            "2023-01-19 23:50:36,304 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:50:36,306 - INFO - joeynmt.training - \tSource:     آیا برای آنان روشن نگردیده که چه بسیار نسلها را پیش از آنها نابود گردانیدیم که اینان‌ در سراهایشان راه می‌روند ؟ قطعا در این امر عبرتهاست ، مگر نمی‌شنوند ؟ \n",
            "2023-01-19 23:50:36,306 - INFO - joeynmt.training - \tReference:  مگر اونلار ایندی یوردلاریندا گزیب دولاشدێقلاری اؤزلریندن اوهلکی نئچه نئچه نسیللری محو ائتدیگیمیزی گؤرمورلرمی ؟ حقیقتا ، بوندا ابرتلر واردێر . مگر قولاق آسمایاجاقلار ؟ \n",
            "2023-01-19 23:50:36,306 - INFO - joeynmt.training - \tHypothesis: مگر اونلارا اؤنجه نئچه نئچه نسیللری محو ائتدیگیمیز نئچه نئچه نئچه نئچه نئچه نئچه نئچه نئچه نئچه نئچه سؤزلرمی ؟ حقیقتا ، ائشیدنلر !\n",
            "2023-01-19 23:50:36,307 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:50:36,309 - INFO - joeynmt.training - \tSource:     و قوم تو آن قرآن‌ را دروغ شمردند ، در حالی که آن بر حق است . بگو : من بر شما نگهبان نیستم . \n",
            "2023-01-19 23:50:36,309 - INFO - joeynmt.training - \tReference:   سنین جاماآتێن اونو ، حاقق اولدوغو حالدا ، یالان حساب ائتدی . دئ : من سیزین وکیلینیز دئییلم ! \n",
            "2023-01-19 23:50:36,309 - INFO - joeynmt.training - \tHypothesis: او ، قور آنێ سنین قؤوموندور . اوندان سیزه حاقق اولان کیمسه دئییل . دئ : من سیزین اۆستۆنم !\n",
            "2023-01-19 23:50:44,262 - INFO - joeynmt.training - Epoch 206, Step:    71100, Batch Loss:     1.760091, Batch Acc: 0.558600, Tokens per Sec:    13375, Lr: 0.000034\n",
            "2023-01-19 23:50:52,224 - INFO - joeynmt.training - Epoch 206, Step:    71200, Batch Loss:     1.649830, Batch Acc: 0.558830, Tokens per Sec:    13922, Lr: 0.000034\n",
            "2023-01-19 23:50:52,687 - INFO - joeynmt.training - Epoch 206: total training loss 601.40\n",
            "2023-01-19 23:50:52,688 - INFO - joeynmt.training - EPOCH 207\n",
            "2023-01-19 23:51:00,104 - INFO - joeynmt.training - Epoch 207, Step:    71300, Batch Loss:     1.660542, Batch Acc: 0.564953, Tokens per Sec:    14118, Lr: 0.000033\n",
            "2023-01-19 23:51:08,020 - INFO - joeynmt.training - Epoch 207, Step:    71400, Batch Loss:     1.768253, Batch Acc: 0.561395, Tokens per Sec:    13842, Lr: 0.000033\n",
            "2023-01-19 23:51:15,923 - INFO - joeynmt.training - Epoch 207, Step:    71500, Batch Loss:     1.724926, Batch Acc: 0.559614, Tokens per Sec:    13861, Lr: 0.000033\n",
            "2023-01-19 23:51:19,956 - INFO - joeynmt.training - Epoch 207: total training loss 597.96\n",
            "2023-01-19 23:51:19,957 - INFO - joeynmt.training - EPOCH 208\n",
            "2023-01-19 23:51:23,788 - INFO - joeynmt.training - Epoch 208, Step:    71600, Batch Loss:     1.728817, Batch Acc: 0.560429, Tokens per Sec:    14140, Lr: 0.000033\n",
            "2023-01-19 23:51:31,622 - INFO - joeynmt.training - Epoch 208, Step:    71700, Batch Loss:     1.682902, Batch Acc: 0.562257, Tokens per Sec:    13889, Lr: 0.000033\n",
            "2023-01-19 23:51:39,484 - INFO - joeynmt.training - Epoch 208, Step:    71800, Batch Loss:     1.761159, Batch Acc: 0.561380, Tokens per Sec:    13925, Lr: 0.000033\n",
            "2023-01-19 23:51:47,239 - INFO - joeynmt.training - Epoch 208: total training loss 599.82\n",
            "2023-01-19 23:51:47,240 - INFO - joeynmt.training - EPOCH 209\n",
            "2023-01-19 23:51:47,478 - INFO - joeynmt.training - Epoch 209, Step:    71900, Batch Loss:     1.878042, Batch Acc: 0.550278, Tokens per Sec:    13668, Lr: 0.000033\n",
            "2023-01-19 23:51:55,244 - INFO - joeynmt.training - Epoch 209, Step:    72000, Batch Loss:     1.692560, Batch Acc: 0.563702, Tokens per Sec:    14085, Lr: 0.000033\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 125.52ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 7445.52ex/s]\n",
            "2023-01-19 23:51:55,577 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=72000\n",
            "2023-01-19 23:51:55,577 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:51:59,918 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:51:59,918 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:51:59,918 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:51:59,919 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:51:59,922 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.00, loss:   2.74, ppl:  15.55, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3018[sec], evaluation: 0.0358[sec]\n",
            "2023-01-19 23:51:59,924 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:51:59,928 - INFO - joeynmt.training - \tSource:     و باز گفته است : آنان به آرامی من راه نخواهند یافت . \n",
            "2023-01-19 23:51:59,928 - INFO - joeynmt.training - \tReference:  بو بارهده یئنه دئییب : اونلار منیم راحاتلێق دییاریما گیرمهیهجکلر . \n",
            "2023-01-19 23:51:59,928 - INFO - joeynmt.training - \tHypothesis: اونلارا : اونلار منیم یولا گلمزلر ! دئیه جاواب وئرهجکلر .\n",
            "2023-01-19 23:51:59,928 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:51:59,930 - INFO - joeynmt.training - \tSource:     هیچ کس نیست مگر اینکه نگاهبانی بر او گماشته شده‌ است . \n",
            "2023-01-19 23:51:59,931 - INFO - joeynmt.training - \tReference:  ائله بیر کیمسه یوخدور کی ، اونون اۆستۆنده بیر گؤزتچی اولماسێن ! \n",
            "2023-01-19 23:51:59,931 - INFO - joeynmt.training - \tHypothesis: هئچ کس اونون گؤزونون هئچ کسه باخما .\n",
            "2023-01-19 23:51:59,931 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:51:59,933 - INFO - joeynmt.training - \tSource:     پس کدام یک از نعمتهای پروردگارتان را منکرید ؟ \n",
            "2023-01-19 23:51:59,933 - INFO - joeynmt.training - \tReference:  بئله اولدوقدا ربینیزین هانسێ نئ متلرینی یالان سایا بیلرسینیز \n",
            "2023-01-19 23:51:59,933 - INFO - joeynmt.training - \tHypothesis: بئله اولدوقدا ربینیزین هانسێ نئ متلرینی یالان سایا بیلرسینیز\n",
            "2023-01-19 23:51:59,933 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:51:59,935 - INFO - joeynmt.training - \tSource:     و یاد کن‌ روزی را که همه آنان را محشور می‌کند ، آنگاه به فرشتگان می‌فرماید : آیا اینها بودند که شما را می‌پرستیدند ؟ \n",
            "2023-01-19 23:51:59,936 - INFO - joeynmt.training - \tReference:  او گۆن اونلارێ بیر یئره یێغاجاق ، سونرا دا ملکلره بئله دئیهجک : بونلار سیزمی ابادت ائدیردیلر ؟ \n",
            "2023-01-19 23:51:59,936 - INFO - joeynmt.training - \tHypothesis: او گۆن بۆتۆن ملکلری توپلایاجاق ، سونرا ملکلره بئله بویورآجاقدیر : بو سیزه ابادت ائدیرسینیز ؟\n",
            "2023-01-19 23:52:07,908 - INFO - joeynmt.training - Epoch 209, Step:    72100, Batch Loss:     1.735916, Batch Acc: 0.563246, Tokens per Sec:    13129, Lr: 0.000033\n",
            "2023-01-19 23:52:15,869 - INFO - joeynmt.training - Epoch 209, Step:    72200, Batch Loss:     1.726945, Batch Acc: 0.560475, Tokens per Sec:    13775, Lr: 0.000033\n",
            "2023-01-19 23:52:21,600 - INFO - joeynmt.training - Epoch 209: total training loss 600.97\n",
            "2023-01-19 23:52:21,604 - INFO - joeynmt.training - EPOCH 210\n",
            "2023-01-19 23:52:26,689 - INFO - joeynmt.training - Epoch 210, Step:    72300, Batch Loss:     1.826796, Batch Acc: 0.566506, Tokens per Sec:    12170, Lr: 0.000033\n",
            "2023-01-19 23:52:34,507 - INFO - joeynmt.training - Epoch 210, Step:    72400, Batch Loss:     1.676124, Batch Acc: 0.561319, Tokens per Sec:    13959, Lr: 0.000033\n",
            "2023-01-19 23:52:42,313 - INFO - joeynmt.training - Epoch 210, Step:    72500, Batch Loss:     1.823910, Batch Acc: 0.561534, Tokens per Sec:    13987, Lr: 0.000033\n",
            "2023-01-19 23:52:49,581 - INFO - joeynmt.training - Epoch 210: total training loss 598.77\n",
            "2023-01-19 23:52:49,582 - INFO - joeynmt.training - EPOCH 211\n",
            "2023-01-19 23:52:50,393 - INFO - joeynmt.training - Epoch 211, Step:    72600, Batch Loss:     1.708297, Batch Acc: 0.565558, Tokens per Sec:    13383, Lr: 0.000033\n",
            "2023-01-19 23:52:58,246 - INFO - joeynmt.training - Epoch 211, Step:    72700, Batch Loss:     1.743838, Batch Acc: 0.563979, Tokens per Sec:    14132, Lr: 0.000033\n",
            "2023-01-19 23:53:06,013 - INFO - joeynmt.training - Epoch 211, Step:    72800, Batch Loss:     1.739350, Batch Acc: 0.562113, Tokens per Sec:    13983, Lr: 0.000033\n",
            "2023-01-19 23:53:13,867 - INFO - joeynmt.training - Epoch 211, Step:    72900, Batch Loss:     1.688566, Batch Acc: 0.558099, Tokens per Sec:    13929, Lr: 0.000033\n",
            "2023-01-19 23:53:16,783 - INFO - joeynmt.training - Epoch 211: total training loss 599.38\n",
            "2023-01-19 23:53:16,783 - INFO - joeynmt.training - EPOCH 212\n",
            "2023-01-19 23:53:21,781 - INFO - joeynmt.training - Epoch 212, Step:    73000, Batch Loss:     1.770289, Batch Acc: 0.561388, Tokens per Sec:    13918, Lr: 0.000033\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.04ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10281.15ex/s]\n",
            "2023-01-19 23:53:22,056 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=73000\n",
            "2023-01-19 23:53:22,056 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:53:26,647 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:53:26,647 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:53:26,647 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:53:26,648 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:53:26,651 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.29, loss:   2.80, ppl:  16.46, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5521[sec], evaluation: 0.0363[sec]\n",
            "2023-01-19 23:53:26,654 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:53:26,658 - INFO - joeynmt.training - \tSource:      همان گونه‌ که پروردگارت بدان وحی کرده است . \n",
            "2023-01-19 23:53:26,658 - INFO - joeynmt.training - \tReference:  چۆنکی اونا سنین ربین وحی ائتمیشدیر ! \n",
            "2023-01-19 23:53:26,659 - INFO - joeynmt.training - \tHypothesis: او ربین کی ، ربینین وحی ائتدیگینه شاهدیر !\n",
            "2023-01-19 23:53:26,659 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:53:26,661 - INFO - joeynmt.training - \tSource:     او برای سه روز نابینا بود و چیزی نمی‌خورد و نمی‌آشامید . \n",
            "2023-01-19 23:53:26,661 - INFO - joeynmt.training - \tReference:  اۆچ گۆن ارزینده شاعلون گؤزلری گؤرمهدی وه او نه یئدی ، نه ده ایچدی . \n",
            "2023-01-19 23:53:26,661 - INFO - joeynmt.training - \tHypothesis: او گۆن اۆچ آییرد ائتمهدی وه یئمک یئمهین ده یئمهین .\n",
            "2023-01-19 23:53:26,662 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:53:26,664 - INFO - joeynmt.training - \tSource:     نازل شدن این کتاب که هیچ جای‌ شک در آن نیست از طرف پروردگار جهانهاست . \n",
            "2023-01-19 23:53:26,664 - INFO - joeynmt.training - \tReference:  کیتابین آلملرین ربی ترهفیندن نازل ائدیلمهسینده هئچ بیر شک شبههه یوخدور ! \n",
            "2023-01-19 23:53:26,664 - INFO - joeynmt.training - \tHypothesis: کیتابدا هئچ بیر یئرده نازل ائدیلمهمیشدیر . او ، ربینین هۆزوروندا اولانلاردان هئچ بیر طرفه نازل ائدیلمز !\n",
            "2023-01-19 23:53:26,664 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:53:26,666 - INFO - joeynmt.training - \tSource:      و ای قوم من ، پیمانه و ترازو را به داد ، تمام دهید ، و حقوق مردم را کم مدهید ، و در زمین به فساد سر برمدارید . \n",
            "2023-01-19 23:53:26,666 - INFO - joeynmt.training - \tReference:  ائی قؤوموم ! اؤلچوده وه چکیده دۆز اولون . اینسانلارین هئچ بیر شئیده حاققێنێ اسکیلتمهیین . یئر اۆزۆنده گزیب فیتنه فصاد تؤرتمهیین ! \n",
            "2023-01-19 23:53:26,667 - INFO - joeynmt.training - \tHypothesis: ائی قؤوموم ! اهدی آلمادان ، بۆتۆن اینسانلارا حاقق وئرین وه یئر اۆزۆنده فیتنه فصاد تؤرتمهیین ! یئر اۆزۆنده فیتنه فصاد تؤرهدنلرین اۆستۆنۆزه !\n",
            "2023-01-19 23:53:34,561 - INFO - joeynmt.training - Epoch 212, Step:    73100, Batch Loss:     1.676684, Batch Acc: 0.563351, Tokens per Sec:    13519, Lr: 0.000033\n",
            "2023-01-19 23:53:42,475 - INFO - joeynmt.training - Epoch 212, Step:    73200, Batch Loss:     1.712579, Batch Acc: 0.560710, Tokens per Sec:    13950, Lr: 0.000033\n",
            "2023-01-19 23:53:49,022 - INFO - joeynmt.training - Epoch 212: total training loss 593.96\n",
            "2023-01-19 23:53:49,022 - INFO - joeynmt.training - EPOCH 213\n",
            "2023-01-19 23:53:50,513 - INFO - joeynmt.training - Epoch 213, Step:    73300, Batch Loss:     1.631078, Batch Acc: 0.565987, Tokens per Sec:    13990, Lr: 0.000033\n",
            "2023-01-19 23:53:58,418 - INFO - joeynmt.training - Epoch 213, Step:    73400, Batch Loss:     1.664297, Batch Acc: 0.562827, Tokens per Sec:    13988, Lr: 0.000033\n",
            "2023-01-19 23:54:06,292 - INFO - joeynmt.training - Epoch 213, Step:    73500, Batch Loss:     1.701608, Batch Acc: 0.564144, Tokens per Sec:    14011, Lr: 0.000033\n",
            "2023-01-19 23:54:14,196 - INFO - joeynmt.training - Epoch 213, Step:    73600, Batch Loss:     1.711489, Batch Acc: 0.561743, Tokens per Sec:    13975, Lr: 0.000033\n",
            "2023-01-19 23:54:16,260 - INFO - joeynmt.training - Epoch 213: total training loss 594.61\n",
            "2023-01-19 23:54:16,260 - INFO - joeynmt.training - EPOCH 214\n",
            "2023-01-19 23:54:22,157 - INFO - joeynmt.training - Epoch 214, Step:    73700, Batch Loss:     1.822121, Batch Acc: 0.567388, Tokens per Sec:    14099, Lr: 0.000033\n",
            "2023-01-19 23:54:30,005 - INFO - joeynmt.training - Epoch 214, Step:    73800, Batch Loss:     1.752912, Batch Acc: 0.564913, Tokens per Sec:    13873, Lr: 0.000033\n",
            "2023-01-19 23:54:37,891 - INFO - joeynmt.training - Epoch 214, Step:    73900, Batch Loss:     1.801445, Batch Acc: 0.562035, Tokens per Sec:    13950, Lr: 0.000033\n",
            "2023-01-19 23:54:43,411 - INFO - joeynmt.training - Epoch 214: total training loss 593.43\n",
            "2023-01-19 23:54:43,411 - INFO - joeynmt.training - EPOCH 215\n",
            "2023-01-19 23:54:45,759 - INFO - joeynmt.training - Epoch 215, Step:    74000, Batch Loss:     1.684653, Batch Acc: 0.568500, Tokens per Sec:    13362, Lr: 0.000033\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.09ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9914.09ex/s]\n",
            "2023-01-19 23:54:46,061 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=74000\n",
            "2023-01-19 23:54:46,062 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:54:50,833 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:54:50,833 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:54:50,834 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:54:50,834 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:54:50,838 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.67, loss:   2.76, ppl:  15.79, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7289[sec], evaluation: 0.0401[sec]\n",
            "2023-01-19 23:54:50,841 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:54:50,844 - INFO - joeynmt.training - \tSource:     در عین حال ، از گفتن هر آنچه برای شما مفید بود ، دریغ نکردم و شما را چه در جمع و چه خانه به خانه تعلیم دادم . \n",
            "2023-01-19 23:54:50,844 - INFO - joeynmt.training - \tReference:  خئیرلی اولان هر شئیی سیزه بیان ائدهرک ایستر آچێق ، ایسترسه ده ائودن ائوه گزیب دولاشاراق تعلیم وئرمکدن چکینمهدیم . \n",
            "2023-01-19 23:54:50,844 - INFO - joeynmt.training - \tHypothesis: سیزه یازدیغیما گؤره هر شئیه اجازه آلدێنێز . سیزه نه سؤیلهمهدینیز ، ائولر ، ائولره وه ائوده تعلیم اؤیرهدیم .\n",
            "2023-01-19 23:54:50,844 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:54:50,846 - INFO - joeynmt.training - \tSource:     اما ، تو آنچه را از دیگران آموختی و با دلیل و برهان به آن متقاعد شدی ، دنبال کن ؛ زیرا می‌دانی آن‌ها را از چه کسانی آموخته‌ای . \n",
            "2023-01-19 23:54:50,847 - INFO - joeynmt.training - \tReference:  سنسه اؤیرندیگین وه گۆۆندیگین شئیلره صادق قال . چۆنکی بونلارێ کیملردن اؤیرندیگینی بیلیرسن ، \n",
            "2023-01-19 23:54:50,847 - INFO - joeynmt.training - \tHypothesis: آمما سن او بیریلری اؤیرت ، اونا گؤره ده اونو اؤیرت . قورخما ، قورخما ، چۆنکی سندن اؤیرتدیگین اؤیرنلریسن .\n",
            "2023-01-19 23:54:50,847 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:54:50,849 - INFO - joeynmt.training - \tSource:      سرانجام به جرم ایمان کشته شد ، و بدو گفته شد : به بهشت درآی . گفت : ای کاش ، قوم من می‌دانستند ، \n",
            "2023-01-19 23:54:50,849 - INFO - joeynmt.training - \tReference:   . اونا : جنته داخل اول ! دئییلدی . دئدی : کاش قؤوموم بیلهیدی کی ، \n",
            "2023-01-19 23:54:50,849 - INFO - joeynmt.training - \tHypothesis: اوندا ایمان گتیردیم ، گمیه مینباشیادان اؤلدوروب : کاشێمێن ! دئدی . ائی تایفام !\n",
            "2023-01-19 23:54:50,849 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:54:50,851 - INFO - joeynmt.training - \tSource:     خبر فعالیت‌های آنان به گوش جماعت اورشلیم رسید و ایشان برنابا را به انطاکیه فرستادند . \n",
            "2023-01-19 23:54:50,852 - INFO - joeynmt.training - \tReference:  بونلارێن خبری یئروسلیمدکی جمیته چاتدێ وه بارنابانێ آنتاقیایا گؤندردیلر . \n",
            "2023-01-19 23:54:50,852 - INFO - joeynmt.training - \tHypothesis: فئستین یئروسهلیمه گلنده اونلارا آنتاقیایا بارنابایایایا گؤندهرهک\n",
            "2023-01-19 23:54:58,660 - INFO - joeynmt.training - Epoch 215, Step:    74100, Batch Loss:     1.769437, Batch Acc: 0.563453, Tokens per Sec:    13483, Lr: 0.000033\n",
            "2023-01-19 23:55:06,547 - INFO - joeynmt.training - Epoch 215, Step:    74200, Batch Loss:     1.720839, Batch Acc: 0.562390, Tokens per Sec:    14016, Lr: 0.000033\n",
            "2023-01-19 23:55:14,378 - INFO - joeynmt.training - Epoch 215, Step:    74300, Batch Loss:     1.592558, Batch Acc: 0.563726, Tokens per Sec:    14074, Lr: 0.000033\n",
            "2023-01-19 23:55:15,763 - INFO - joeynmt.training - Epoch 215: total training loss 594.65\n",
            "2023-01-19 23:55:15,764 - INFO - joeynmt.training - EPOCH 216\n",
            "2023-01-19 23:55:22,366 - INFO - joeynmt.training - Epoch 216, Step:    74400, Batch Loss:     1.750064, Batch Acc: 0.566108, Tokens per Sec:    13864, Lr: 0.000033\n",
            "2023-01-19 23:55:30,252 - INFO - joeynmt.training - Epoch 216, Step:    74500, Batch Loss:     1.766086, Batch Acc: 0.569832, Tokens per Sec:    13739, Lr: 0.000033\n",
            "2023-01-19 23:55:41,058 - INFO - joeynmt.training - Epoch 216, Step:    74600, Batch Loss:     1.888519, Batch Acc: 0.562394, Tokens per Sec:    10271, Lr: 0.000033\n",
            "2023-01-19 23:55:45,998 - INFO - joeynmt.training - Epoch 216: total training loss 592.56\n",
            "2023-01-19 23:55:45,999 - INFO - joeynmt.training - EPOCH 217\n",
            "2023-01-19 23:55:49,021 - INFO - joeynmt.training - Epoch 217, Step:    74700, Batch Loss:     1.632414, Batch Acc: 0.568749, Tokens per Sec:    14008, Lr: 0.000033\n",
            "2023-01-19 23:55:56,859 - INFO - joeynmt.training - Epoch 217, Step:    74800, Batch Loss:     1.692566, Batch Acc: 0.565950, Tokens per Sec:    14022, Lr: 0.000033\n",
            "2023-01-19 23:56:04,718 - INFO - joeynmt.training - Epoch 217, Step:    74900, Batch Loss:     1.651673, Batch Acc: 0.562617, Tokens per Sec:    13889, Lr: 0.000033\n",
            "2023-01-19 23:56:12,815 - INFO - joeynmt.training - Epoch 217, Step:    75000, Batch Loss:     1.777424, Batch Acc: 0.561435, Tokens per Sec:    13673, Lr: 0.000033\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.25ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10458.23ex/s]\n",
            "2023-01-19 23:56:13,083 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=75000\n",
            "2023-01-19 23:56:13,083 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:56:17,885 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:56:17,885 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:56:17,885 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:56:17,887 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:56:17,892 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.64, loss:   2.72, ppl:  15.17, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7630[sec], evaluation: 0.0385[sec]\n",
            "2023-01-19 23:56:17,895 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:56:17,898 - INFO - joeynmt.training - \tSource:     عیسی از سنگدلی‌شان عمیقا اندوهگین شد و با خشم به آنان نظر افکند . سپس به آن مرد گفت : دستت را دراز کن . او دستش را دراز کرد و دست او شفا یافت . \n",
            "2023-01-19 23:56:17,898 - INFO - joeynmt.training - \tReference:  ایسا اترافینداکیلارا غزبله باخدێ وه اونلارێن اینادکار اۆرکلی اولدوقلارینا گؤره کدرلهنیب او آداما دئدی : الینی اوزات ! او ، الینی اوزاتدێ وه الی اوهلکی هالێنا قایێتدی . \n",
            "2023-01-19 23:56:17,899 - INFO - joeynmt.training - \tHypothesis: ایسا داشقالاقدان داشێشێب کدرلندی . اونلارا دئدی : او ، الینی اوزات ، الینی اوزاتدێ وه الینی اوزاتدێ .\n",
            "2023-01-19 23:56:17,899 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:56:17,901 - INFO - joeynmt.training - \tSource:     به این طریق تشخیص می‌دهید که گفته‌ای الهام‌شده از خداست : هر گفتهٔ الهام‌شده که تصدیق کند عیسی مسیح به صورت انسان آمد ، از جانب خداست . \n",
            "2023-01-19 23:56:17,902 - INFO - joeynmt.training - \tReference:  آللاهێن روحونو بوندان تانییا بیلرسینیز : ایسا مسیحین جسما گلدیگینی اقرار ائدن هر روح آللاهداندێر ، \n",
            "2023-01-19 23:56:17,902 - INFO - joeynmt.training - \tHypothesis: بو شئیلر آللاهێن اؤنونده اسیریه گؤره سؤیلهین آداملارێن اؤنونده هر شئیه گؤره ایسا مصیح واسطهسیله اقرار ائدن هر شئیه گؤره اقرار ائدن آللاهێن ایزهتینین الینده اولاراق گلسین .\n",
            "2023-01-19 23:56:17,902 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:56:17,904 - INFO - joeynmt.training - \tSource:     پس فرعون‌ تصمیم گرفت که آنان را از سرزمین مصر برکند ، پس او و هر که را با وی بود همه را غرق کردیم . \n",
            "2023-01-19 23:56:17,904 - INFO - joeynmt.training - \tReference:   اونلارێ تورپاغێندان قوووب چێخارتماق ایستهدی . بیز ایسه اونو وه اونونلا بیرلیکده اولانلارێن هامیسینی قرق ائتدیک . \n",
            "2023-01-19 23:56:17,904 - INFO - joeynmt.training - \tHypothesis: فیر اون اهلینی میصیردن سونرا اونلارا یئرلشدیردیلر . هر کس اونو بۆتۆن عایلهسینی بیر یئره سالدێق .\n",
            "2023-01-19 23:56:17,904 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:56:17,906 - INFO - joeynmt.training - \tSource:     یهوه روح است و هر جا روح یهوه باشد ، آنجا آزادی است . \n",
            "2023-01-19 23:56:17,907 - INFO - joeynmt.training - \tReference:  رب روحدور ، ربین روحو هارادادێرسا ، آزادلێق دا اورادادێر . \n",
            "2023-01-19 23:56:17,907 - INFO - joeynmt.training - \tHypothesis: رب روح هر شئیده آزادلێدێر .\n",
            "2023-01-19 23:56:18,524 - INFO - joeynmt.training - Epoch 217: total training loss 591.67\n",
            "2023-01-19 23:56:18,524 - INFO - joeynmt.training - EPOCH 218\n",
            "2023-01-19 23:56:25,872 - INFO - joeynmt.training - Epoch 218, Step:    75100, Batch Loss:     1.751130, Batch Acc: 0.570940, Tokens per Sec:    13920, Lr: 0.000033\n",
            "2023-01-19 23:56:33,646 - INFO - joeynmt.training - Epoch 218, Step:    75200, Batch Loss:     1.656193, Batch Acc: 0.566974, Tokens per Sec:    14163, Lr: 0.000033\n",
            "2023-01-19 23:56:41,506 - INFO - joeynmt.training - Epoch 218, Step:    75300, Batch Loss:     1.749858, Batch Acc: 0.561420, Tokens per Sec:    13988, Lr: 0.000033\n",
            "2023-01-19 23:56:45,604 - INFO - joeynmt.training - Epoch 218: total training loss 590.80\n",
            "2023-01-19 23:56:45,604 - INFO - joeynmt.training - EPOCH 219\n",
            "2023-01-19 23:56:49,410 - INFO - joeynmt.training - Epoch 219, Step:    75400, Batch Loss:     1.789503, Batch Acc: 0.560559, Tokens per Sec:    13988, Lr: 0.000033\n",
            "2023-01-19 23:56:57,271 - INFO - joeynmt.training - Epoch 219, Step:    75500, Batch Loss:     1.651067, Batch Acc: 0.568202, Tokens per Sec:    14159, Lr: 0.000033\n",
            "2023-01-19 23:57:05,137 - INFO - joeynmt.training - Epoch 219, Step:    75600, Batch Loss:     1.724453, Batch Acc: 0.566587, Tokens per Sec:    13759, Lr: 0.000033\n",
            "2023-01-19 23:57:12,721 - INFO - joeynmt.training - Epoch 219: total training loss 588.70\n",
            "2023-01-19 23:57:12,722 - INFO - joeynmt.training - EPOCH 220\n",
            "2023-01-19 23:57:13,057 - INFO - joeynmt.training - Epoch 220, Step:    75700, Batch Loss:     1.754298, Batch Acc: 0.565807, Tokens per Sec:    13217, Lr: 0.000033\n",
            "2023-01-19 23:57:20,993 - INFO - joeynmt.training - Epoch 220, Step:    75800, Batch Loss:     1.738344, Batch Acc: 0.568164, Tokens per Sec:    13787, Lr: 0.000032\n",
            "2023-01-19 23:57:28,890 - INFO - joeynmt.training - Epoch 220, Step:    75900, Batch Loss:     1.666286, Batch Acc: 0.567177, Tokens per Sec:    13815, Lr: 0.000032\n",
            "2023-01-19 23:57:36,696 - INFO - joeynmt.training - Epoch 220, Step:    76000, Batch Loss:     1.709803, Batch Acc: 0.560118, Tokens per Sec:    14014, Lr: 0.000032\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 141.88ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10238.25ex/s]\n",
            "2023-01-19 23:57:36,974 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=76000\n",
            "2023-01-19 23:57:36,974 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:57:40,947 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:57:40,948 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:57:40,948 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:57:40,949 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:57:40,952 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.88, loss:   2.79, ppl:  16.26, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 3.9364[sec], evaluation: 0.0335[sec]\n",
            "2023-01-19 23:57:40,954 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:57:40,959 - INFO - joeynmt.training - \tSource:     در حقیقت ، کسانی که اموال یتیمان را به ستم می‌خورند ، جز این نیست که آتشی در شکم خود فرو می‌برند ، و به زودی در آتشی فروزان درآیند . \n",
            "2023-01-19 23:57:40,959 - INFO - joeynmt.training - \tReference:  حقیقتا ، یئتیملرین ماللارێنێ حاقسێزلێقلا یئینلرین یئدیکلری قارینلاریندا اودا چئوریلهجک وه اونلار آلوولو جهنهمه گیرهجکلر . \n",
            "2023-01-19 23:57:40,959 - INFO - joeynmt.training - \tHypothesis: حقیقتا ، ماللارێ ظۆلم ائدنلره آنجاق ظۆلم ائدنلرین حالێ اوددان یئیهجکدیر . اونلار جهنملیکدیرلر .\n",
            "2023-01-19 23:57:40,959 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:57:40,961 - INFO - joeynmt.training - \tSource:     و برای هر یک از این دو گروه‌ ، از آنچه انجام داده‌اند ، در جزا مراتبی خواهد بود ، و پروردگارت از آنچه می‌کنند غافل نیست . \n",
            "2023-01-19 23:57:40,961 - INFO - joeynmt.training - \tReference:  هر کس اۆچۆن ائتدیگی امللره گؤره درهجهلر واردێر . ربین اونلارێن نه ائتدیکلریندن قافل دئییلدیر ! \n",
            "2023-01-19 23:57:40,962 - INFO - joeynmt.training - \tHypothesis: هر ایکیسی ده اونلارێن هر ایکیسیندن باشقا بیر دسته اۆچۆن تعه یین ائتمیشدیر . ربینین درگاهێندا ائتدیکلری امللردن هئچ بیر شئی دئییلدیر !\n",
            "2023-01-19 23:57:40,963 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:57:40,965 - INFO - joeynmt.training - \tSource:     هر که را بخواهد یاری می‌کند ، و اوست شکست‌ناپذیر مهربان . \n",
            "2023-01-19 23:57:40,965 - INFO - joeynmt.training - \tReference:  آللاهێن کؤمگی ایله . ایستهدیگینه کؤمک ائدر . او ، یئنیلمز غۆۆت صاحبی ، مرحمت صاحبدر ! \n",
            "2023-01-19 23:57:40,965 - INFO - joeynmt.training - \tHypothesis: کیم اونا کؤمک ائدرسه ، او ، یئنیلمز غۆۆت ، مرحمت صاحبدر !\n",
            "2023-01-19 23:57:40,966 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:57:40,968 - INFO - joeynmt.training - \tSource:     در آنجا فریسیان نزد عیسی آمدند و با او به بحث پرداختند . آنان برای آزمودن عیسی ، از وی خواستند نشانه‌ای از آسمان برایشان نمایان سازد . \n",
            "2023-01-19 23:57:40,968 - INFO - joeynmt.training - \tReference:  فاریسئیلر ایسنانین یانینا گلیب اونونلا مۆباحصه ائتمهیه باشلادێ . اونو سێناماق اۆچۆن ایستهدیلر کی ، گؤیدن گلن بیر علامت گؤسترسین . \n",
            "2023-01-19 23:57:40,968 - INFO - joeynmt.training - \tHypothesis: فاریسئیلر ایسنانین یانینا گلیب اونلارلا سێناماق اۆچۆن سێناماق اۆچۆن اونلارا سێناقدان کئچیردیلر . ایسا گؤیدن بیر علامت گؤسترمک اۆچۆن گؤیدن بیر علامت آختارێردێ .\n",
            "2023-01-19 23:57:44,398 - INFO - joeynmt.training - Epoch 220: total training loss 594.03\n",
            "2023-01-19 23:57:44,398 - INFO - joeynmt.training - EPOCH 221\n",
            "2023-01-19 23:57:48,911 - INFO - joeynmt.training - Epoch 221, Step:    76100, Batch Loss:     1.665741, Batch Acc: 0.565557, Tokens per Sec:    13853, Lr: 0.000032\n",
            "2023-01-19 23:57:56,822 - INFO - joeynmt.training - Epoch 221, Step:    76200, Batch Loss:     1.618213, Batch Acc: 0.563000, Tokens per Sec:    13817, Lr: 0.000032\n",
            "2023-01-19 23:58:04,817 - INFO - joeynmt.training - Epoch 221, Step:    76300, Batch Loss:     1.770700, Batch Acc: 0.565523, Tokens per Sec:    13692, Lr: 0.000032\n",
            "2023-01-19 23:58:12,114 - INFO - joeynmt.training - Epoch 221: total training loss 593.40\n",
            "2023-01-19 23:58:12,114 - INFO - joeynmt.training - EPOCH 222\n",
            "2023-01-19 23:58:12,938 - INFO - joeynmt.training - Epoch 222, Step:    76400, Batch Loss:     1.690900, Batch Acc: 0.573095, Tokens per Sec:    13852, Lr: 0.000032\n",
            "2023-01-19 23:58:21,051 - INFO - joeynmt.training - Epoch 222, Step:    76500, Batch Loss:     1.727872, Batch Acc: 0.567675, Tokens per Sec:    13681, Lr: 0.000032\n",
            "2023-01-19 23:58:28,919 - INFO - joeynmt.training - Epoch 222, Step:    76600, Batch Loss:     1.728385, Batch Acc: 0.565418, Tokens per Sec:    13878, Lr: 0.000032\n",
            "2023-01-19 23:58:36,744 - INFO - joeynmt.training - Epoch 222, Step:    76700, Batch Loss:     1.673092, Batch Acc: 0.567465, Tokens per Sec:    14189, Lr: 0.000032\n",
            "2023-01-19 23:58:39,465 - INFO - joeynmt.training - Epoch 222: total training loss 587.69\n",
            "2023-01-19 23:58:39,465 - INFO - joeynmt.training - EPOCH 223\n",
            "2023-01-19 23:58:44,633 - INFO - joeynmt.training - Epoch 223, Step:    76800, Batch Loss:     1.730866, Batch Acc: 0.568638, Tokens per Sec:    13851, Lr: 0.000032\n",
            "2023-01-19 23:58:55,446 - INFO - joeynmt.training - Epoch 223, Step:    76900, Batch Loss:     1.814944, Batch Acc: 0.565126, Tokens per Sec:    10313, Lr: 0.000032\n",
            "2023-01-19 23:59:03,218 - INFO - joeynmt.training - Epoch 223, Step:    77000, Batch Loss:     1.626807, Batch Acc: 0.568852, Tokens per Sec:    14010, Lr: 0.000032\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.34ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9785.50ex/s]\n",
            "2023-01-19 23:59:03,497 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=77000\n",
            "2023-01-19 23:59:03,497 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 23:59:07,176 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 23:59:07,177 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 23:59:07,177 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 23:59:07,178 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 23:59:07,181 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.02, loss:   2.87, ppl:  17.63, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 3.6406[sec], evaluation: 0.0364[sec]\n",
            "2023-01-19 23:59:07,184 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 23:59:07,187 - INFO - joeynmt.training - \tSource:     و بخور و بنوش و دیده روشن دار . پس اگر کسی از آدمیان را دیدی ، بگوی : من برای خدای‌ رحمان روزه نذر کرده‌ام ، و امروز مطلقا با انسانی سخن نخواهم گفت . \n",
            "2023-01-19 23:59:07,187 - INFO - joeynmt.training - \tReference:   یئ ایچ ، گؤزۆن آیدین اولسون . اگر بیر آدام گؤرهجک اولسان ، بئله دئ : من رهمان یولوندا اوروج توتماغێ نظیر ائلهمیشم ، اونا گؤره ده بو گۆن هئچ کسله دانیشمایاجاغام ! \n",
            "2023-01-19 23:59:07,188 - INFO - joeynmt.training - \tHypothesis: یئییب ایچمهیه ، دوغانلارێن ایلیاسیندن باشقا بیر نفری گؤردۆکده : اگر آللاه منه رحم ائدیلمهمیشام ، گۆن اینسانین سؤزلرینی هئچ کسه دانێشماز .\n",
            "2023-01-19 23:59:07,188 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 23:59:07,190 - INFO - joeynmt.training - \tSource:     اما او به من گفت : لطف من برای تو کافی است ؛ زیرا قدرت من در ضعف تو کاملا آشکار می‌شود . پس با شادی هر چه بیشتر در مورد ضعف‌های خود فخر خواهم کرد تا قدرت مسیح همچون خیمه‌ای مرا در پناه خود نگاه دارد . \n",
            "2023-01-19 23:59:07,190 - INFO - joeynmt.training - \tReference:  آمما او منه دئدی : لۆتفۆم سنه کفایتدیر ، چۆنکی زیفلیک اولاندا قۆۆه تام اولور . مسیحین قۆۆهسی منده مسکن سالسێن دئیه زیفلیکلریمله داها چوخ سئوینه سئوینه اؤیونهجهیهم . \n",
            "2023-01-19 23:59:07,190 - INFO - joeynmt.training - \tHypothesis: لاکین اونا دئدی : سنه لۆتف وئریرم . سن منیمله اولان لۆتفله هر کسین زۆهور ائدهجهگی تیترهتم کیمی اؤیونرم .\n",
            "2023-01-19 23:59:07,190 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 23:59:07,192 - INFO - joeynmt.training - \tSource:     عیسی گفت : آیا شما نیز مانند آنان درک نمی‌کنید ؟ آیا نمی‌دانید که هیچ چیز نمی‌تواند از بیرون به انسان وارد شود و او را نجس کند ؟ \n",
            "2023-01-19 23:59:07,193 - INFO - joeynmt.training - \tReference:  ایسا اونلارا جاواب وئردی : سیز ده هله درک ائتمهمیسینیز ؟ باشا دۆشمۆرسۆنۆز کی ، کناردان اینسانین داخلینه گیرن هر بیر شئی اینسانی موردار ائده بیلمز ؟ \n",
            "2023-01-19 23:59:07,193 - INFO - joeynmt.training - \tHypothesis: ایسا اونلارا دئدی : اونلار دا کیمی سیز ده بیر شئی بیلمیرسینیزمی ؟ مگر اینسان هئچ بیر شئی بیلمه بیلمهدیگینیز حالدا ، بشری اونون داخلیندن چێخمایا بیلرمی ؟\n",
            "2023-01-19 23:59:07,193 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 23:59:07,195 - INFO - joeynmt.training - \tSource:     پس یکی از خدمتکاران را صدا کرد و از او پرسید : چه خبر است ؟ \n",
            "2023-01-19 23:59:07,195 - INFO - joeynmt.training - \tReference:  او ، نؤکرلردن بیرینی یانینا چاغێرێب سوروشدو : بو ندیر ؟ \n",
            "2023-01-19 23:59:07,195 - INFO - joeynmt.training - \tHypothesis: بوندان سونرا خدمتچیلره اوندان سوروشدو : بس او نه دئیه سوروشدو .\n",
            "2023-01-19 23:59:13,627 - INFO - joeynmt.training - Epoch 223: total training loss 590.01\n",
            "2023-01-19 23:59:13,627 - INFO - joeynmt.training - EPOCH 224\n",
            "2023-01-19 23:59:15,231 - INFO - joeynmt.training - Epoch 224, Step:    77100, Batch Loss:     1.715454, Batch Acc: 0.569355, Tokens per Sec:    14143, Lr: 0.000032\n",
            "2023-01-19 23:59:23,161 - INFO - joeynmt.training - Epoch 224, Step:    77200, Batch Loss:     1.634917, Batch Acc: 0.568969, Tokens per Sec:    13711, Lr: 0.000032\n",
            "2023-01-19 23:59:31,043 - INFO - joeynmt.training - Epoch 224, Step:    77300, Batch Loss:     1.603915, Batch Acc: 0.567339, Tokens per Sec:    14116, Lr: 0.000032\n",
            "2023-01-19 23:59:38,867 - INFO - joeynmt.training - Epoch 224, Step:    77400, Batch Loss:     1.648416, Batch Acc: 0.565809, Tokens per Sec:    14091, Lr: 0.000032\n",
            "2023-01-19 23:59:40,851 - INFO - joeynmt.training - Epoch 224: total training loss 587.77\n",
            "2023-01-19 23:59:40,851 - INFO - joeynmt.training - EPOCH 225\n",
            "2023-01-19 23:59:46,814 - INFO - joeynmt.training - Epoch 225, Step:    77500, Batch Loss:     1.629469, Batch Acc: 0.573382, Tokens per Sec:    13931, Lr: 0.000032\n",
            "2023-01-19 23:59:54,679 - INFO - joeynmt.training - Epoch 225, Step:    77600, Batch Loss:     1.578429, Batch Acc: 0.569709, Tokens per Sec:    13916, Lr: 0.000032\n",
            "2023-01-20 00:00:02,531 - INFO - joeynmt.training - Epoch 225, Step:    77700, Batch Loss:     1.664759, Batch Acc: 0.566051, Tokens per Sec:    14189, Lr: 0.000032\n",
            "2023-01-20 00:00:08,053 - INFO - joeynmt.training - Epoch 225: total training loss 586.33\n",
            "2023-01-20 00:00:08,054 - INFO - joeynmt.training - EPOCH 226\n",
            "2023-01-20 00:00:10,427 - INFO - joeynmt.training - Epoch 226, Step:    77800, Batch Loss:     1.596183, Batch Acc: 0.571942, Tokens per Sec:    13837, Lr: 0.000032\n",
            "2023-01-20 00:00:18,253 - INFO - joeynmt.training - Epoch 226, Step:    77900, Batch Loss:     1.673944, Batch Acc: 0.569304, Tokens per Sec:    14015, Lr: 0.000032\n",
            "2023-01-20 00:00:26,177 - INFO - joeynmt.training - Epoch 226, Step:    78000, Batch Loss:     1.714226, Batch Acc: 0.569621, Tokens per Sec:    14083, Lr: 0.000032\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 126.32ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10502.56ex/s]\n",
            "2023-01-20 00:00:26,451 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=78000\n",
            "2023-01-20 00:00:26,451 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:00:30,507 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:00:30,508 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:00:30,508 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:00:30,509 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:00:30,512 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.53, loss:   2.94, ppl:  18.95, acc:   0.40, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.0170[sec], evaluation: 0.0361[sec]\n",
            "2023-01-20 00:00:30,514 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:00:30,518 - INFO - joeynmt.training - \tSource:     و شما را چه شده است که از آنچه نام خدا بر آن برده شده است نمی‌خورید ؟ با اینکه خدا آنچه را بر شما حرام کرده جز آنچه بدان ناچار شده‌اید برای شما به تفصیل بیان نموده است . و به راستی ، بسیاری از مردم ، دیگران را از روی نادانی ، با هوسهای خود گمراه می‌کنند . آری ، پروردگار تو به حال‌ تجاوزکاران داناتر است . \n",
            "2023-01-20 00:00:30,518 - INFO - joeynmt.training - \tReference:  سیزه نه اولوب کی ، اۆستۆنده آللاهێن آدێ چکیلمیش اتیندن یئمهیهسینیز ؟ حالبوکی مجبوریت قارشیسیندا اولدوغونوز شئیلر استثنا ائدیلمکله ، سیزه هارام بویوردوقلارین آرتێق او مۆفصل شکیلده سیزه بیلدیرمیشدیر . شبههسیز کی ، چوخلارێ بیلمهدیکلریندن نفصلرینین ایستکلرینه اویاراق دۆز یولدان آزدێرارلار . ربین هدی آشانلارێ ان یاخشی تانییاندیر ! \n",
            "2023-01-20 00:00:30,518 - INFO - joeynmt.training - \tHypothesis: سیزه نه اولوب کی ، آللاهێن آدێ ایله یئتیملره حالال ائدیلمهیهلیسینیز ؟ حالبوکی آللاه سیزه هارام بویورموشدور . سیز هدی آشمێش ، اینسانلارین چوخو داها چوخ آزدێرماق ایستهییرسینیز . حقیقتا ، ربین چوخودور ، دوغرو یولدادیر !\n",
            "2023-01-20 00:00:30,518 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:00:30,520 - INFO - joeynmt.training - \tSource:     از طایفهٔ یهودا ۱۲ ۰۰۰ نفر مهر شدند ؛ از طایفهٔ رئوبین ۱۲ ۰۰۰ نفر ؛ از طایفهٔ جاد ۱۲ ۰۰۰ نفر ؛ \n",
            "2023-01-20 00:00:30,520 - INFO - joeynmt.training - \tReference:  یهودا قبیلهسیندن 12 ، 000 مۆهورلنمیش نفر ؛ رووئن قبیلهسیندن 12 ، 000 نفر ؛ قاد قبیلهسیندن 12 ، 000 نفر ؛ \n",
            "2023-01-20 00:00:30,521 - INFO - joeynmt.training - \tHypothesis: قبیلهسیندن 12 ، 0000 نفر ؛ قبیلهسیندن 12 ، 000 نفر ؛ 000 نفر ؛ قبیلهسیندن 12 ، 000 نفر ؛\n",
            "2023-01-20 00:00:30,521 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:00:30,523 - INFO - joeynmt.training - \tSource:     زیرا خدا ، خدای بی‌نظمی نیست ، بلکه خدای آرامش است . همان گونه که در همهٔ جماعت‌های مقدسان مرسوم است ، \n",
            "2023-01-20 00:00:30,523 - INFO - joeynmt.training - \tReference:  چۆنکی آللاه قاریشیقلێق دئییل ، سۆلح قایناغیدیر . مۆقدسلرین بۆتۆن جمیتلرینده بئلهدیر . \n",
            "2023-01-20 00:00:30,523 - INFO - joeynmt.training - \tHypothesis: چۆنکی آللاهسیز دئییل ، مۆقدسلرین هامێسێ ایله دئییل ، مۆقدسلره گؤره اوجادێر .\n",
            "2023-01-20 00:00:30,523 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:00:30,525 - INFO - joeynmt.training - \tSource:     پس ترسی شدید در دل تمامی جماعت و هر که این ماجرا را می‌شنید ، افتاد . \n",
            "2023-01-20 00:00:30,525 - INFO - joeynmt.training - \tReference:  بۆتۆن ایمانلیلار جمیتینی وه بو هاداسنی ائشیدنلرین هامیسینی بؤیوک بیر واهمه بۆرۆدۆ . \n",
            "2023-01-20 00:00:30,525 - INFO - joeynmt.training - \tHypothesis: بو سبدن قورخان بۆتۆن جمیتی وه هر جمیته وه هر ترفی ائشیتدی .\n",
            "2023-01-20 00:00:38,501 - INFO - joeynmt.training - Epoch 226, Step:    78100, Batch Loss:     1.741035, Batch Acc: 0.566087, Tokens per Sec:    13376, Lr: 0.000032\n",
            "2023-01-20 00:00:39,656 - INFO - joeynmt.training - Epoch 226: total training loss 585.04\n",
            "2023-01-20 00:00:39,657 - INFO - joeynmt.training - EPOCH 227\n",
            "2023-01-20 00:00:46,436 - INFO - joeynmt.training - Epoch 227, Step:    78200, Batch Loss:     1.719894, Batch Acc: 0.568892, Tokens per Sec:    13915, Lr: 0.000032\n",
            "2023-01-20 00:00:54,336 - INFO - joeynmt.training - Epoch 227, Step:    78300, Batch Loss:     1.666048, Batch Acc: 0.570150, Tokens per Sec:    13815, Lr: 0.000032\n",
            "2023-01-20 00:01:02,265 - INFO - joeynmt.training - Epoch 227, Step:    78400, Batch Loss:     1.662028, Batch Acc: 0.567208, Tokens per Sec:    13937, Lr: 0.000032\n",
            "2023-01-20 00:01:06,967 - INFO - joeynmt.training - Epoch 227: total training loss 585.90\n",
            "2023-01-20 00:01:06,968 - INFO - joeynmt.training - EPOCH 228\n",
            "2023-01-20 00:01:10,259 - INFO - joeynmt.training - Epoch 228, Step:    78500, Batch Loss:     1.671708, Batch Acc: 0.570785, Tokens per Sec:    13491, Lr: 0.000032\n",
            "2023-01-20 00:01:18,162 - INFO - joeynmt.training - Epoch 228, Step:    78600, Batch Loss:     1.782367, Batch Acc: 0.567589, Tokens per Sec:    13902, Lr: 0.000032\n",
            "2023-01-20 00:01:26,203 - INFO - joeynmt.training - Epoch 228, Step:    78700, Batch Loss:     1.581433, Batch Acc: 0.567133, Tokens per Sec:    13702, Lr: 0.000032\n",
            "2023-01-20 00:01:34,109 - INFO - joeynmt.training - Epoch 228, Step:    78800, Batch Loss:     1.656419, Batch Acc: 0.568695, Tokens per Sec:    13904, Lr: 0.000032\n",
            "2023-01-20 00:01:34,547 - INFO - joeynmt.training - Epoch 228: total training loss 586.93\n",
            "2023-01-20 00:01:34,548 - INFO - joeynmt.training - EPOCH 229\n",
            "2023-01-20 00:01:42,056 - INFO - joeynmt.training - Epoch 229, Step:    78900, Batch Loss:     1.668723, Batch Acc: 0.567116, Tokens per Sec:    13967, Lr: 0.000032\n",
            "2023-01-20 00:01:50,060 - INFO - joeynmt.training - Epoch 229, Step:    79000, Batch Loss:     1.619057, Batch Acc: 0.572159, Tokens per Sec:    13781, Lr: 0.000032\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 109.99ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10267.58ex/s]\n",
            "2023-01-20 00:01:50,342 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=79000\n",
            "2023-01-20 00:01:50,342 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:01:54,872 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:01:54,872 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:01:54,873 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:01:54,873 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:01:54,876 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.26, loss:   2.96, ppl:  19.22, acc:   0.40, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4897[sec], evaluation: 0.0373[sec]\n",
            "2023-01-20 00:01:54,879 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:01:54,883 - INFO - joeynmt.training - \tSource:     بگو : برای خود زیان و سودی در اختیار ندارم ، مگر آنچه را که خدا بخواهد . هر امتی را زمانی محدود است . آنگاه که زمانشان به سر رسد ، پس نه ساعتی از آن‌ تأخیر کنند و نه پیشی گیرند . \n",
            "2023-01-20 00:01:54,883 - INFO - joeynmt.training - \tReference:   دئ : آللاهێن ایستهدیگیندن باشقا ، من اؤزومه نه بیر خئیر ، نه ده بیر زرر وئره بیلهرم . هر اممتین بیر اجل واختێ واردێر . اونلارێن اجهلی گلیب چاتدێقدا بیرجه ساعات بئله نه گئری قالار ، نه ده ایرهلی کئچرلر . \n",
            "2023-01-20 00:01:54,883 - INFO - joeynmt.training - \tHypothesis: دئ : بیر زرر وئره بیلهرم . آللاهدان باشقا بیر شئیه نه بیر خئیر وئرر . سونرا اونلار مۆعین بیر واخت چاتماز ، نه ده بیر ساعات بئله مۆهر وصیته گلرلر .\n",
            "2023-01-20 00:01:54,883 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:01:54,885 - INFO - joeynmt.training - \tSource:     و چون بر ایشان فرو خوانده می‌شود ، می‌گویند : بدان ایمان آوردیم که آن درست است و از طرف پروردگار ماست ؛ ما پیش از آن هم‌ از تسلیم‌شوندگان بودیم . \n",
            "2023-01-20 00:01:54,885 - INFO - joeynmt.training - \tReference:  اونلارا اوخوندوغو زامان : بیز اونا ایناندیق . دوغرودان دا ، ربیمیزدن حاقدێر . بیز اوندان اول ده مۆسلمان ایدیک ! دئییرلر . \n",
            "2023-01-20 00:01:54,886 - INFO - joeynmt.training - \tHypothesis: اونلارا اوخوندوغو زامان : اینانیریق ، اینانیریق ! دئییرلر . بیز ربیمیزدن اول ربیمیزدن اول ربیمیز ! دئیه جاواب وئریرلر .\n",
            "2023-01-20 00:01:54,886 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:01:54,888 - INFO - joeynmt.training - \tSource:     پطرس به او گفت : مفهوم مثلی را که پیش از این گفتی برای ما روشن کن . \n",
            "2023-01-20 00:01:54,888 - INFO - joeynmt.training - \tReference:  پئتئر اونا جاواب وئرهرک دئدی : بو مسهلی بیزه اضاح ائت . \n",
            "2023-01-20 00:01:54,888 - INFO - joeynmt.training - \tHypothesis: پئتئر اونا دئدی : بو قاباغێندا بو ایشی گؤرمک اۆچۆن بو مسللرله آچێق آیدین بیر مسل چک .\n",
            "2023-01-20 00:01:54,888 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:01:54,890 - INFO - joeynmt.training - \tSource:     و برخی از آنان در تقسیم‌ صدقات بر تو خرده می‌گیرند ، پس اگر از آن اموال‌ به ایشان داده شود خشنود می‌گردند ، و اگر از آن به ایشان داده نشود بناگاه به خشم می‌آیند . \n",
            "2023-01-20 00:01:54,891 - INFO - joeynmt.training - \tReference:  اونلاردان سدهقهلر بارهسینده سنه ائییب توتانلار دا وار . اگر اونلارا بیر شئی وئریلسه ، رازێ قالار ، وئریلمهسه ، درحال قزبلهنهرلر . \n",
            "2023-01-20 00:01:54,891 - INFO - joeynmt.training - \tHypothesis: اونلاردان بزیسی تزهلکین وئردیگی مالدان رازێ سالارلار . اگر اونلار مال دؤؤؤلتسه ، اونلار مالدان رازێ اولماسایدی ، اونلار غزبه دۆچار اولاجاقلار .\n",
            "2023-01-20 00:02:03,637 - INFO - joeynmt.training - Epoch 229, Step:    79100, Batch Loss:     1.688017, Batch Acc: 0.570957, Tokens per Sec:    12137, Lr: 0.000032\n",
            "2023-01-20 00:02:10,212 - INFO - joeynmt.training - Epoch 229: total training loss 586.27\n",
            "2023-01-20 00:02:10,213 - INFO - joeynmt.training - EPOCH 230\n",
            "2023-01-20 00:02:14,100 - INFO - joeynmt.training - Epoch 230, Step:    79200, Batch Loss:     1.504335, Batch Acc: 0.571481, Tokens per Sec:    13976, Lr: 0.000032\n",
            "2023-01-20 00:02:22,074 - INFO - joeynmt.training - Epoch 230, Step:    79300, Batch Loss:     1.686463, Batch Acc: 0.569874, Tokens per Sec:    13923, Lr: 0.000032\n",
            "2023-01-20 00:02:29,898 - INFO - joeynmt.training - Epoch 230, Step:    79400, Batch Loss:     1.698873, Batch Acc: 0.570517, Tokens per Sec:    13935, Lr: 0.000032\n",
            "2023-01-20 00:02:37,510 - INFO - joeynmt.training - Epoch 230: total training loss 583.61\n",
            "2023-01-20 00:02:37,511 - INFO - joeynmt.training - EPOCH 231\n",
            "2023-01-20 00:02:37,851 - INFO - joeynmt.training - Epoch 231, Step:    79500, Batch Loss:     1.483823, Batch Acc: 0.583841, Tokens per Sec:    13523, Lr: 0.000032\n",
            "2023-01-20 00:02:45,741 - INFO - joeynmt.training - Epoch 231, Step:    79600, Batch Loss:     1.734892, Batch Acc: 0.570440, Tokens per Sec:    13996, Lr: 0.000032\n",
            "2023-01-20 00:02:53,704 - INFO - joeynmt.training - Epoch 231, Step:    79700, Batch Loss:     1.711838, Batch Acc: 0.568539, Tokens per Sec:    13874, Lr: 0.000032\n",
            "2023-01-20 00:03:01,591 - INFO - joeynmt.training - Epoch 231, Step:    79800, Batch Loss:     1.663329, Batch Acc: 0.570286, Tokens per Sec:    13972, Lr: 0.000032\n",
            "2023-01-20 00:03:04,744 - INFO - joeynmt.training - Epoch 231: total training loss 582.37\n",
            "2023-01-20 00:03:04,744 - INFO - joeynmt.training - EPOCH 232\n",
            "2023-01-20 00:03:09,520 - INFO - joeynmt.training - Epoch 232, Step:    79900, Batch Loss:     1.659287, Batch Acc: 0.566819, Tokens per Sec:    13881, Lr: 0.000032\n",
            "2023-01-20 00:03:17,390 - INFO - joeynmt.training - Epoch 232, Step:    80000, Batch Loss:     1.541800, Batch Acc: 0.573258, Tokens per Sec:    13930, Lr: 0.000032\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.10ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9086.95ex/s]\n",
            "2023-01-20 00:03:17,678 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=80000\n",
            "2023-01-20 00:03:17,678 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:03:21,944 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:03:21,944 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:03:21,945 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:03:21,945 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:03:21,949 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.15, loss:   2.78, ppl:  16.17, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2246[sec], evaluation: 0.0378[sec]\n",
            "2023-01-20 00:03:21,951 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:03:21,955 - INFO - joeynmt.training - \tSource:     کشتی شدیدا گرفتار باد شد و نتوانست خلاف جهت باد پیش رود . پس خود را به باد سپردیم و با آن رانده شدیم . \n",
            "2023-01-20 00:03:21,955 - INFO - joeynmt.training - \tReference:  فێرتینایا دۆشن گمی کۆلهیه مۆقاۆمت گؤستهره بیلمیردی ، الآجسیز قالدێق وه کۆلک بیزی آپارێردێ . \n",
            "2023-01-20 00:03:21,955 - INFO - joeynmt.training - \tHypothesis: گمیه مینباشی هئچ نه یئمهیه بیلمهدی . گمیه مینته اویغونلوقلاندی وه اونو امانتدوق .\n",
            "2023-01-20 00:03:21,955 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:03:21,957 - INFO - joeynmt.training - \tSource:     بله‌تان فقط بله و نه‌تان فقط نه باشد ؛ چون سخنی بیش از این ، از آن شریر است . \n",
            "2023-01-20 00:03:21,957 - INFO - joeynmt.training - \tReference:  آنجاق سؤزونوزده بلی نیز بلی ، خئیر اینیز خئیر اولسون . بوندان قالانێ شر اولانداندێر . \n",
            "2023-01-20 00:03:21,958 - INFO - joeynmt.training - \tHypothesis: یالنیز بو یالنیز بیر دئییل ، یالنیز بوندان گیزلی قالسێن .\n",
            "2023-01-20 00:03:21,958 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:03:21,960 - INFO - joeynmt.training - \tSource:      زیرا اگر شما خطاهای دیگران را ببخشید ، پدر آسمانی شما نیز شما را خواهد بخشید ؛ \n",
            "2023-01-20 00:03:21,960 - INFO - joeynmt.training - \tReference:  اگر باشقا اینسانلارین تقصیرلرینی باغیشلاسانیز ، سماوی آتانێز دا سیزی باغیشلایار . \n",
            "2023-01-20 00:03:21,960 - INFO - joeynmt.training - \tHypothesis: چۆنکی اگر سیزه آتالارینیزی باغێشلاسانێز ، سیز ده آتالارێنێ باغیشلایاجاق .\n",
            "2023-01-20 00:03:21,960 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:03:21,962 - INFO - joeynmt.training - \tSource:     در عین حال ، از گفتن هر آنچه برای شما مفید بود ، دریغ نکردم و شما را چه در جمع و چه خانه به خانه تعلیم دادم . \n",
            "2023-01-20 00:03:21,962 - INFO - joeynmt.training - \tReference:  خئیرلی اولان هر شئیی سیزه بیان ائدهرک ایستر آچێق ، ایسترسه ده ائودن ائوه گزیب دولاشاراق تعلیم وئرمکدن چکینمهدیم . \n",
            "2023-01-20 00:03:21,962 - INFO - joeynmt.training - \tHypothesis: آمما سیزه سؤیلهدیکلریمی هر شئیه گؤره نه بیلمهدیگینیزه ، نه ده ائولره یێغدیم .\n",
            "2023-01-20 00:03:29,975 - INFO - joeynmt.training - Epoch 232, Step:    80100, Batch Loss:     1.758761, Batch Acc: 0.569296, Tokens per Sec:    13223, Lr: 0.000032\n",
            "2023-01-20 00:03:36,699 - INFO - joeynmt.training - Epoch 232: total training loss 582.47\n",
            "2023-01-20 00:03:36,699 - INFO - joeynmt.training - EPOCH 233\n",
            "2023-01-20 00:03:37,883 - INFO - joeynmt.training - Epoch 233, Step:    80200, Batch Loss:     1.697096, Batch Acc: 0.576485, Tokens per Sec:    14326, Lr: 0.000032\n",
            "2023-01-20 00:03:45,881 - INFO - joeynmt.training - Epoch 233, Step:    80300, Batch Loss:     1.709925, Batch Acc: 0.570904, Tokens per Sec:    13716, Lr: 0.000032\n",
            "2023-01-20 00:03:53,771 - INFO - joeynmt.training - Epoch 233, Step:    80400, Batch Loss:     1.625389, Batch Acc: 0.570818, Tokens per Sec:    13876, Lr: 0.000032\n",
            "2023-01-20 00:04:01,724 - INFO - joeynmt.training - Epoch 233, Step:    80500, Batch Loss:     1.727916, Batch Acc: 0.569830, Tokens per Sec:    13757, Lr: 0.000032\n",
            "2023-01-20 00:04:04,200 - INFO - joeynmt.training - Epoch 233: total training loss 584.67\n",
            "2023-01-20 00:04:04,200 - INFO - joeynmt.training - EPOCH 234\n",
            "2023-01-20 00:04:09,680 - INFO - joeynmt.training - Epoch 234, Step:    80600, Batch Loss:     1.655191, Batch Acc: 0.575764, Tokens per Sec:    13899, Lr: 0.000032\n",
            "2023-01-20 00:04:17,513 - INFO - joeynmt.training - Epoch 234, Step:    80700, Batch Loss:     1.769213, Batch Acc: 0.572797, Tokens per Sec:    14004, Lr: 0.000031\n",
            "2023-01-20 00:04:25,359 - INFO - joeynmt.training - Epoch 234, Step:    80800, Batch Loss:     1.750721, Batch Acc: 0.569089, Tokens per Sec:    14016, Lr: 0.000031\n",
            "2023-01-20 00:04:31,515 - INFO - joeynmt.training - Epoch 234: total training loss 583.26\n",
            "2023-01-20 00:04:31,515 - INFO - joeynmt.training - EPOCH 235\n",
            "2023-01-20 00:04:33,293 - INFO - joeynmt.training - Epoch 235, Step:    80900, Batch Loss:     1.737630, Batch Acc: 0.572430, Tokens per Sec:    14366, Lr: 0.000031\n",
            "2023-01-20 00:04:41,078 - INFO - joeynmt.training - Epoch 235, Step:    81000, Batch Loss:     1.650058, Batch Acc: 0.575504, Tokens per Sec:    14072, Lr: 0.000031\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.82ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8964.27ex/s]\n",
            "2023-01-20 00:04:41,366 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=81000\n",
            "2023-01-20 00:04:41,367 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:04:46,022 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:04:46,022 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:04:46,022 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:04:46,023 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:04:46,027 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.36, loss:   2.85, ppl:  17.37, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6145[sec], evaluation: 0.0380[sec]\n",
            "2023-01-20 00:04:46,030 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:04:46,033 - INFO - joeynmt.training - \tSource:     و اندکی از متأخران . \n",
            "2023-01-20 00:04:46,033 - INFO - joeynmt.training - \tReference:  بیز آزێ دا آخێرینجیلارداندیر . \n",
            "2023-01-20 00:04:46,033 - INFO - joeynmt.training - \tHypothesis: بیر آز بیر آز ، چوخ کؤلگهسی ده .\n",
            "2023-01-20 00:04:46,034 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:04:46,036 - INFO - joeynmt.training - \tSource:     اما آن زن پیش عیسی آمد ، در مقابل او به زانو افتاد و گفت : سرورم ، به من کمک کن ! \n",
            "2023-01-20 00:04:46,036 - INFO - joeynmt.training - \tReference:  قادێن ایسه یاخینلاشدی وه اونا سجده قێلێب دئدی : یا رب ، منه امداد ائت . \n",
            "2023-01-20 00:04:46,036 - INFO - joeynmt.training - \tHypothesis: آمما ایسا قادێنا قاباغێندا اونا سجده ائتدی : یا رب ، منه کؤمک ائت !\n",
            "2023-01-20 00:04:46,036 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:04:46,038 - INFO - joeynmt.training - \tSource:     او در راه بازگشت در ارابهٔ خود نشسته بود و با صدای بلند نوشته‌های اشعْیای نبی را می‌خواند . \n",
            "2023-01-20 00:04:46,039 - INFO - joeynmt.training - \tReference:  آراباسێنا اوتوروب گئری قاییدارکن یئشآیا پیغمبرین کیتابینی اوخویوردو . \n",
            "2023-01-20 00:04:46,039 - INFO - joeynmt.training - \tHypothesis: او ، اؤز یولداشلاریندا اوتوروب مۆقدس یازیلاری تاپشێرێب پیغمبرین سسیع تانێدێ .\n",
            "2023-01-20 00:04:46,039 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:04:46,041 - INFO - joeynmt.training - \tSource:     ابراهیم نه یهودی بود و نه نصرانی ، بلکه حق گرایی فرمانبردار بود ، و از مشرکان نبود . \n",
            "2023-01-20 00:04:46,042 - INFO - joeynmt.training - \tReference:  ابراهیم نه یهودی ، نه ده خاچپرست ایدی . او آنجاق حنیف مۆسلمان ایدی وه شریک قوشانلاردان دئییلدی . \n",
            "2023-01-20 00:04:46,042 - INFO - joeynmt.training - \tHypothesis: ابراهیمین وه مۆشرکلردن اولماسێ دئییل ، مۆشرکلردن اولمادێ . مۆشرکلردن اولمادێ وه مۆشرکلردن اولمادێ .\n",
            "2023-01-20 00:04:54,012 - INFO - joeynmt.training - Epoch 235, Step:    81100, Batch Loss:     1.671038, Batch Acc: 0.571054, Tokens per Sec:    13410, Lr: 0.000031\n",
            "2023-01-20 00:05:01,829 - INFO - joeynmt.training - Epoch 235, Step:    81200, Batch Loss:     1.649889, Batch Acc: 0.570901, Tokens per Sec:    13929, Lr: 0.000031\n",
            "2023-01-20 00:05:03,724 - INFO - joeynmt.training - Epoch 235: total training loss 582.14\n",
            "2023-01-20 00:05:03,724 - INFO - joeynmt.training - EPOCH 236\n",
            "2023-01-20 00:05:09,842 - INFO - joeynmt.training - Epoch 236, Step:    81300, Batch Loss:     1.684589, Batch Acc: 0.571839, Tokens per Sec:    13941, Lr: 0.000031\n",
            "2023-01-20 00:05:17,682 - INFO - joeynmt.training - Epoch 236, Step:    81400, Batch Loss:     1.627894, Batch Acc: 0.575900, Tokens per Sec:    14012, Lr: 0.000031\n",
            "2023-01-20 00:05:28,435 - INFO - joeynmt.training - Epoch 236, Step:    81500, Batch Loss:     1.630404, Batch Acc: 0.570180, Tokens per Sec:    10159, Lr: 0.000031\n",
            "2023-01-20 00:05:33,958 - INFO - joeynmt.training - Epoch 236: total training loss 582.29\n",
            "2023-01-20 00:05:33,959 - INFO - joeynmt.training - EPOCH 237\n",
            "2023-01-20 00:05:36,390 - INFO - joeynmt.training - Epoch 237, Step:    81600, Batch Loss:     1.716384, Batch Acc: 0.575529, Tokens per Sec:    14014, Lr: 0.000031\n",
            "2023-01-20 00:05:44,357 - INFO - joeynmt.training - Epoch 237, Step:    81700, Batch Loss:     1.838814, Batch Acc: 0.576540, Tokens per Sec:    13804, Lr: 0.000031\n",
            "2023-01-20 00:05:52,340 - INFO - joeynmt.training - Epoch 237, Step:    81800, Batch Loss:     1.688231, Batch Acc: 0.572824, Tokens per Sec:    13842, Lr: 0.000031\n",
            "2023-01-20 00:06:00,267 - INFO - joeynmt.training - Epoch 237, Step:    81900, Batch Loss:     1.563937, Batch Acc: 0.570364, Tokens per Sec:    13922, Lr: 0.000031\n",
            "2023-01-20 00:06:01,445 - INFO - joeynmt.training - Epoch 237: total training loss 581.42\n",
            "2023-01-20 00:06:01,445 - INFO - joeynmt.training - EPOCH 238\n",
            "2023-01-20 00:06:08,130 - INFO - joeynmt.training - Epoch 238, Step:    82000, Batch Loss:     1.627247, Batch Acc: 0.577338, Tokens per Sec:    13794, Lr: 0.000031\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.52ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10448.23ex/s]\n",
            "2023-01-20 00:06:08,410 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=82000\n",
            "2023-01-20 00:06:08,411 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:06:13,214 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:06:13,215 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:06:13,215 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:06:13,216 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:06:13,219 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.43, loss:   2.81, ppl:  16.65, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7645[sec], evaluation: 0.0365[sec]\n",
            "2023-01-20 00:06:13,221 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:06:13,225 - INFO - joeynmt.training - \tSource:     آنان پس از دریافت مزدشان ، شروع به گله و شکایت از مالک کردند . \n",
            "2023-01-20 00:06:13,225 - INFO - joeynmt.training - \tReference:  پوللارێنێ آلدیقلاری زامان ائو صاحبینه شکایت ائدهرک دئدیلر : \n",
            "2023-01-20 00:06:13,225 - INFO - joeynmt.training - \tHypothesis: اونلار گؤلون اوغدو وه سۆرۆنمهیه باشلادێلار .\n",
            "2023-01-20 00:06:13,225 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:06:13,227 - INFO - joeynmt.training - \tSource:     آنگاه به یقین ، آنان به جهنم درآیند . \n",
            "2023-01-20 00:06:13,227 - INFO - joeynmt.training - \tReference:  سونرا جهنهمه وارد قالاجاقلار . \n",
            "2023-01-20 00:06:13,228 - INFO - joeynmt.training - \tHypothesis: سونرا اونلار جهنمه داخل اولاجاقلار .\n",
            "2023-01-20 00:06:13,229 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:06:13,230 - INFO - joeynmt.training - \tSource:     آن روز ، روز تهیه برای عید پسح و حدود ساعت ششم بود . پیلاتس به یهودیان گفت : ببینید ! این هم پادشاهتان ! \n",
            "2023-01-20 00:06:13,231 - INFO - joeynmt.training - \tReference:  پاسخایا هازێرلێق گۆنۆ ایدی . آلتێنجێ ساعات رادهلری ایدی . پیلات یهودیلره پادشاهینیز بودور ! دئدی . \n",
            "2023-01-20 00:06:13,231 - INFO - joeynmt.training - \tHypothesis: او گۆنلرده بیر ساعات اۆچۆن اۆچۆن خئیلی واختێ آلتێ گۆن یهودیلرین پادشاهێ نئجه گؤرورسونوز ؟ دئیه سوروشدو .\n",
            "2023-01-20 00:06:13,231 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:06:13,233 - INFO - joeynmt.training - \tSource:     اگر کسی بگوید : خدا را دوست دارم ، اما از برادر خود نفرت داشته باشد دروغگوست ؛ زیرا هر که برادر خود را که دیده است دوست نداشته باشد ، نمی‌تواند خدایی را که ندیده است دوست داشته باشد . \n",
            "2023-01-20 00:06:13,236 - INFO - joeynmt.training - \tReference:  کیم من آللاهێ سئویرم دئییر ، آمما اؤز قارداشێنا نفرت ائدیرسه ، یالانچیدیر . آخێ گؤردوگو قارداشێنێ سئومهین گؤرمهدیگی آللاهێ سئوه بیلمز . \n",
            "2023-01-20 00:06:13,236 - INFO - joeynmt.training - \tHypothesis: کیم دئ : اگر بیر کس قارداشێنێ سئوسه ، اؤزونه نفرت ائدیرسه ، چۆنکی قارداشێنێ سئومهینسه ، اؤزونه سئومهین هئچ بیر سئومهدیگینی گؤرمهیهجک .\n",
            "2023-01-20 00:06:21,192 - INFO - joeynmt.training - Epoch 238, Step:    82100, Batch Loss:     1.708093, Batch Acc: 0.572436, Tokens per Sec:    13352, Lr: 0.000031\n",
            "2023-01-20 00:06:29,211 - INFO - joeynmt.training - Epoch 238, Step:    82200, Batch Loss:     1.567316, Batch Acc: 0.572389, Tokens per Sec:    13648, Lr: 0.000031\n",
            "2023-01-20 00:06:34,213 - INFO - joeynmt.training - Epoch 238: total training loss 582.88\n",
            "2023-01-20 00:06:34,214 - INFO - joeynmt.training - EPOCH 239\n",
            "2023-01-20 00:06:37,190 - INFO - joeynmt.training - Epoch 239, Step:    82300, Batch Loss:     1.524186, Batch Acc: 0.575387, Tokens per Sec:    13779, Lr: 0.000031\n",
            "2023-01-20 00:06:45,100 - INFO - joeynmt.training - Epoch 239, Step:    82400, Batch Loss:     1.698071, Batch Acc: 0.573962, Tokens per Sec:    13893, Lr: 0.000031\n",
            "2023-01-20 00:06:52,968 - INFO - joeynmt.training - Epoch 239, Step:    82500, Batch Loss:     1.713457, Batch Acc: 0.574312, Tokens per Sec:    13861, Lr: 0.000031\n",
            "2023-01-20 00:07:00,894 - INFO - joeynmt.training - Epoch 239, Step:    82600, Batch Loss:     1.628940, Batch Acc: 0.572691, Tokens per Sec:    13888, Lr: 0.000031\n",
            "2023-01-20 00:07:01,633 - INFO - joeynmt.training - Epoch 239: total training loss 582.22\n",
            "2023-01-20 00:07:01,634 - INFO - joeynmt.training - EPOCH 240\n",
            "2023-01-20 00:07:08,850 - INFO - joeynmt.training - Epoch 240, Step:    82700, Batch Loss:     1.732282, Batch Acc: 0.574527, Tokens per Sec:    13821, Lr: 0.000031\n",
            "2023-01-20 00:07:16,704 - INFO - joeynmt.training - Epoch 240, Step:    82800, Batch Loss:     1.616859, Batch Acc: 0.572301, Tokens per Sec:    14088, Lr: 0.000031\n",
            "2023-01-20 00:07:24,704 - INFO - joeynmt.training - Epoch 240, Step:    82900, Batch Loss:     1.679148, Batch Acc: 0.572030, Tokens per Sec:    13749, Lr: 0.000031\n",
            "2023-01-20 00:07:29,106 - INFO - joeynmt.training - Epoch 240: total training loss 578.31\n",
            "2023-01-20 00:07:29,107 - INFO - joeynmt.training - EPOCH 241\n",
            "2023-01-20 00:07:32,718 - INFO - joeynmt.training - Epoch 241, Step:    83000, Batch Loss:     1.696886, Batch Acc: 0.577682, Tokens per Sec:    13959, Lr: 0.000031\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.60ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10034.81ex/s]\n",
            "2023-01-20 00:07:33,033 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=83000\n",
            "2023-01-20 00:07:33,033 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:07:37,527 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:07:37,527 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:07:37,527 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:07:37,528 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:07:37,531 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.86, loss:   2.83, ppl:  17.00, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4542[sec], evaluation: 0.0363[sec]\n",
            "2023-01-20 00:07:37,533 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:07:37,537 - INFO - joeynmt.training - \tSource:     بعد از آن که پطرس به طرف در خانه رفت ، زنی دیگر متوجه او شد و به کسانی که آنجا بودند ، گفت : این مرد با عیسای ناصری بود . \n",
            "2023-01-20 00:07:37,537 - INFO - joeynmt.training - \tReference:  سونرا داروازآیا چێخاندا باشقا بیر قاراباش اونو گؤروب اوراداکێلارا دئدی : بو آدام نازارائتلی ایسا ایله بیرلیکده ایدی . \n",
            "2023-01-20 00:07:37,537 - INFO - joeynmt.training - \tHypothesis: پئتئر او ، پئتئرین ائوینه گئدیب او بیری صاحله کئچیب باشقا بیر قادێن ایدی : بو آدامێ نازارائتلی ایسنانین یانیندا ایدی .\n",
            "2023-01-20 00:07:37,537 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:07:37,539 - INFO - joeynmt.training - \tSource:     که در لوحی محفوظ است . \n",
            "2023-01-20 00:07:37,539 - INFO - joeynmt.training - \tReference:  او ، لؤؤهی محفوزدادێر ! \n",
            "2023-01-20 00:07:37,539 - INFO - joeynmt.training - \tHypothesis: لوتون لو دا !\n",
            "2023-01-20 00:07:37,539 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:07:37,541 - INFO - joeynmt.training - \tSource:     باز من به آنان اعلام نمودم و در خلوت و پوشیده نیز به ایشان گفتم . \n",
            "2023-01-20 00:07:37,542 - INFO - joeynmt.training - \tReference:  داها سونرا اونلارا آشکار سؤیلهدیم وه گیزلی بیلدیردیم . \n",
            "2023-01-20 00:07:37,542 - INFO - joeynmt.training - \tHypothesis: من اونلارا سارسێلدێم وه اونلارا دئدی : من ده اونلارا دئدیم .\n",
            "2023-01-20 00:07:37,542 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:07:37,544 - INFO - joeynmt.training - \tSource:     خدا ست که‌ دانای نهان آسمانها و زمین است ، و اوست که به راز دلها داناست . \n",
            "2023-01-20 00:07:37,544 - INFO - joeynmt.training - \tReference:  شبههسیز کی ، آللاه گؤیلرین وه یئرین غیبینی بیلیر . او ، اۆرکلرده اولانلارێ دا بیلندیر ! \n",
            "2023-01-20 00:07:37,544 - INFO - joeynmt.training - \tHypothesis: آللاه گؤیلرین وه یئرین غیبی بیلنلرین ان گؤزل بیلندیر !\n",
            "2023-01-20 00:07:45,485 - INFO - joeynmt.training - Epoch 241, Step:    83100, Batch Loss:     1.577032, Batch Acc: 0.573903, Tokens per Sec:    13154, Lr: 0.000031\n",
            "2023-01-20 00:07:53,404 - INFO - joeynmt.training - Epoch 241, Step:    83200, Batch Loss:     1.683527, Batch Acc: 0.574474, Tokens per Sec:    13927, Lr: 0.000031\n",
            "2023-01-20 00:08:01,256 - INFO - joeynmt.training - Epoch 241, Step:    83300, Batch Loss:     1.723957, Batch Acc: 0.570231, Tokens per Sec:    13983, Lr: 0.000031\n",
            "2023-01-20 00:08:01,354 - INFO - joeynmt.training - Epoch 241: total training loss 581.22\n",
            "2023-01-20 00:08:01,354 - INFO - joeynmt.training - EPOCH 242\n",
            "2023-01-20 00:08:09,204 - INFO - joeynmt.training - Epoch 242, Step:    83400, Batch Loss:     1.666905, Batch Acc: 0.574523, Tokens per Sec:    13973, Lr: 0.000031\n",
            "2023-01-20 00:08:17,021 - INFO - joeynmt.training - Epoch 242, Step:    83500, Batch Loss:     1.603294, Batch Acc: 0.574440, Tokens per Sec:    14055, Lr: 0.000031\n",
            "2023-01-20 00:08:24,886 - INFO - joeynmt.training - Epoch 242, Step:    83600, Batch Loss:     1.630798, Batch Acc: 0.573285, Tokens per Sec:    13912, Lr: 0.000031\n",
            "2023-01-20 00:08:28,674 - INFO - joeynmt.training - Epoch 242: total training loss 578.40\n",
            "2023-01-20 00:08:28,675 - INFO - joeynmt.training - EPOCH 243\n",
            "2023-01-20 00:08:32,896 - INFO - joeynmt.training - Epoch 243, Step:    83700, Batch Loss:     1.615422, Batch Acc: 0.574246, Tokens per Sec:    13637, Lr: 0.000031\n",
            "2023-01-20 00:08:41,185 - INFO - joeynmt.training - Epoch 243, Step:    83800, Batch Loss:     1.736613, Batch Acc: 0.576236, Tokens per Sec:    13352, Lr: 0.000031\n",
            "2023-01-20 00:08:51,651 - INFO - joeynmt.training - Epoch 243, Step:    83900, Batch Loss:     1.586962, Batch Acc: 0.573097, Tokens per Sec:    10489, Lr: 0.000031\n",
            "2023-01-20 00:08:59,074 - INFO - joeynmt.training - Epoch 243: total training loss 578.49\n",
            "2023-01-20 00:08:59,075 - INFO - joeynmt.training - EPOCH 244\n",
            "2023-01-20 00:08:59,651 - INFO - joeynmt.training - Epoch 244, Step:    84000, Batch Loss:     1.624420, Batch Acc: 0.582301, Tokens per Sec:    12758, Lr: 0.000031\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.41ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9097.23ex/s]\n",
            "2023-01-20 00:08:59,939 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=84000\n",
            "2023-01-20 00:08:59,939 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:09:04,601 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:09:04,602 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:09:04,602 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:09:04,603 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:09:04,606 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.46, loss:   2.90, ppl:  18.23, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6228[sec], evaluation: 0.0364[sec]\n",
            "2023-01-20 00:09:04,609 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:09:04,612 - INFO - joeynmt.training - \tSource:     در واقع ، شما نیز فراخوانده شده‌اید تا این راه را دنبال کنید ؛ زیرا حتی مسیح برای شما رنج کشید و سرمشقی برای شما قرار داد تا به‌دقت در جای پای او گام بردارید . \n",
            "2023-01-20 00:09:04,612 - INFO - joeynmt.training - \tReference:  سیز محض بونون اۆچۆن چاغێرێلدێنێز . چۆنکی مصیح ده سیزین اۆچۆن اعذاب چکدی وه سیزه نمونه اولدو کی ، سیز ده اونون ایزی ایله گئدهسینیز . \n",
            "2023-01-20 00:09:04,613 - INFO - joeynmt.training - \tHypothesis: چاغێرێش آلدێغێنێز مصیح ده چاغێرێشێن . چۆنکی سیزه اعذاب چکهرک مصیحه کؤمک ائتمک اۆچۆن اعذاب چکدینیز .\n",
            "2023-01-20 00:09:04,613 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:09:04,615 - INFO - joeynmt.training - \tSource:     وقتی به سالامیس رسیدند ، به اعلام کلام خدا در کنیسه‌های یهودیان پرداختند . یوحنا نیز دستیارشان بود . \n",
            "2023-01-20 00:09:04,615 - INFO - joeynmt.training - \tReference:  سالامیسه چاتاندا یهودیلرین سیناقوقلاریندا آللاهێن کلامێنێ بیان ائتمهیه باشلادێلار . یهیا ایسه اونلارێن کؤمکچیسی ایدی . \n",
            "2023-01-20 00:09:04,615 - INFO - joeynmt.training - \tHypothesis: اونلار آللاهێن کلامێنێ اؤیرهدرک آللاهێن کلامێنێ ائشیدنده یهیا دا اؤز سیناقوقوندا دانێشدێ .\n",
            "2023-01-20 00:09:04,615 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:09:04,617 - INFO - joeynmt.training - \tSource:     آنگاه می‌رود و هفت روح شریرتر از خود را می‌آورد و با آنان به آن خانه داخل گشته و در آن ساکن می‌شوند . در نتیجه ، عاقبت آن شخص از ابتدایش بدتر می‌شود . \n",
            "2023-01-20 00:09:04,617 - INFO - joeynmt.training - \tReference:  بو واخت او گئدیب اؤزوندن داها بئتر یئددی باشقا روحو گؤتورر . اونلار دا اورایا گیریب مسکونلاشار . اوندا بو آدامێن آخێرێ اوهلکیندن داها پیس اولار . \n",
            "2023-01-20 00:09:04,617 - INFO - joeynmt.training - \tHypothesis: اوندا یئددی روحو وه اورا گئدیر ، اونلارێ ائولهنیر وه اورادان آیریلیب گیرهجکلر . سونرا داها پیس آدام کئچیب گئتدی .\n",
            "2023-01-20 00:09:04,618 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:09:04,619 - INFO - joeynmt.training - \tSource:     ولی داش آكل پشت گوش فراخ و گشاد باز بود ، \n",
            "2023-01-20 00:09:04,620 - INFO - joeynmt.training - \tReference:  آمما داش آکل چوخ أل آچیق و مال قدری بیلمز ایدی\n",
            "2023-01-20 00:09:04,620 - INFO - joeynmt.training - \tHypothesis: داش آکل گونده و اونون قولو آچدی\n",
            "2023-01-20 00:09:12,643 - INFO - joeynmt.training - Epoch 244, Step:    84100, Batch Loss:     1.685186, Batch Acc: 0.576373, Tokens per Sec:    13344, Lr: 0.000031\n",
            "2023-01-20 00:09:20,605 - INFO - joeynmt.training - Epoch 244, Step:    84200, Batch Loss:     1.650389, Batch Acc: 0.573605, Tokens per Sec:    13914, Lr: 0.000031\n",
            "2023-01-20 00:09:28,383 - INFO - joeynmt.training - Epoch 244, Step:    84300, Batch Loss:     1.627198, Batch Acc: 0.571684, Tokens per Sec:    13999, Lr: 0.000031\n",
            "2023-01-20 00:09:31,432 - INFO - joeynmt.training - Epoch 244: total training loss 577.09\n",
            "2023-01-20 00:09:31,433 - INFO - joeynmt.training - EPOCH 245\n",
            "2023-01-20 00:09:36,368 - INFO - joeynmt.training - Epoch 245, Step:    84400, Batch Loss:     1.686110, Batch Acc: 0.576369, Tokens per Sec:    13957, Lr: 0.000031\n",
            "2023-01-20 00:09:44,328 - INFO - joeynmt.training - Epoch 245, Step:    84500, Batch Loss:     1.727318, Batch Acc: 0.575872, Tokens per Sec:    13881, Lr: 0.000031\n",
            "2023-01-20 00:09:52,125 - INFO - joeynmt.training - Epoch 245, Step:    84600, Batch Loss:     1.520505, Batch Acc: 0.575698, Tokens per Sec:    14184, Lr: 0.000031\n",
            "2023-01-20 00:09:58,619 - INFO - joeynmt.training - Epoch 245: total training loss 574.46\n",
            "2023-01-20 00:09:58,620 - INFO - joeynmt.training - EPOCH 246\n",
            "2023-01-20 00:10:00,030 - INFO - joeynmt.training - Epoch 246, Step:    84700, Batch Loss:     1.583425, Batch Acc: 0.576726, Tokens per Sec:    13584, Lr: 0.000031\n",
            "2023-01-20 00:10:07,893 - INFO - joeynmt.training - Epoch 246, Step:    84800, Batch Loss:     1.626186, Batch Acc: 0.578822, Tokens per Sec:    14096, Lr: 0.000031\n",
            "2023-01-20 00:10:15,749 - INFO - joeynmt.training - Epoch 246, Step:    84900, Batch Loss:     1.611186, Batch Acc: 0.575252, Tokens per Sec:    13873, Lr: 0.000031\n",
            "2023-01-20 00:10:23,597 - INFO - joeynmt.training - Epoch 246, Step:    85000, Batch Loss:     1.628440, Batch Acc: 0.574163, Tokens per Sec:    13947, Lr: 0.000031\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 105.34ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10089.10ex/s]\n",
            "2023-01-20 00:10:23,873 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=85000\n",
            "2023-01-20 00:10:23,873 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:10:27,747 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:10:27,747 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:10:27,748 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:10:27,749 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:10:27,751 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.48, loss:   2.78, ppl:  16.11, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 3.8356[sec], evaluation: 0.0356[sec]\n",
            "2023-01-20 00:10:27,754 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:10:27,758 - INFO - joeynmt.training - \tSource:     و اگر بخواهیم ، قطعا آنچه را به تو وحی کرده‌ایم می‌بریم ، آنگاه برای حفظ آن ، در برابر ما ، برای خود مدافعی نمی‌یابی ، \n",
            "2023-01-20 00:10:27,758 - INFO - joeynmt.training - \tReference:  اگر ایستهسهیدیک ، سنه وحی ائتدیگیمیزی چێخاردێب آپاراردێق . سونرا بیزه قارشێ اؤزون اۆچۆن بیر مۆدافهچی ده تاپا بیلمزدین \n",
            "2023-01-20 00:10:27,758 - INFO - joeynmt.training - \tHypothesis: اگر سنه وحی ائتدیگیمیز شئیلری سنه وحی ائتدیگیمیزی بیلسک ، سونرا اونا قارشێ هئچ بیر پای وئرمزدیک .\n",
            "2023-01-20 00:10:27,758 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:10:27,760 - INFO - joeynmt.training - \tSource:     او در جواب به آنان گفت : مادر و برادرانم آنانی هستند که کلام خدا را می‌شنوند و به آن عمل می‌کنند . \n",
            "2023-01-20 00:10:27,760 - INFO - joeynmt.training - \tReference:  ایسا اونلارا جاواب وئردی : منیم آنام وه قارداشلارێم آللاهێن کلامێنێ ائشیدیب اونا امل ائدنلردیر . \n",
            "2023-01-20 00:10:27,761 - INFO - joeynmt.training - \tHypothesis: او جاواب وئردی : قارداشلارێم ، آنا وه قارداشلارێم ، اونلار دا ائشیتدیکلری حالدا ائشیتدیکلری حالدا اونلار امل ائدیرلر .\n",
            "2023-01-20 00:10:27,761 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:10:27,763 - INFO - joeynmt.training - \tSource:     و از مادرش و پدرش . \n",
            "2023-01-20 00:10:27,763 - INFO - joeynmt.training - \tReference:  آناسێندان ، آتاسێندان ؛ \n",
            "2023-01-20 00:10:27,763 - INFO - joeynmt.training - \tHypothesis: و آناسیندان کی بیر واهاسی .\n",
            "2023-01-20 00:10:27,763 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:10:27,765 - INFO - joeynmt.training - \tSource:     و کسانی که به نشانه‌های پروردگارشان ایمان می‌آورند ، \n",
            "2023-01-20 00:10:27,765 - INFO - joeynmt.training - \tReference:  ربینین آیهلرینه اینانلار ؛ \n",
            "2023-01-20 00:10:27,765 - INFO - joeynmt.training - \tHypothesis: ایمان گتیرنلرین ربینه اینانیرلار .\n",
            "2023-01-20 00:10:30,125 - INFO - joeynmt.training - Epoch 246: total training loss 578.36\n",
            "2023-01-20 00:10:30,125 - INFO - joeynmt.training - EPOCH 247\n",
            "2023-01-20 00:10:35,839 - INFO - joeynmt.training - Epoch 247, Step:    85100, Batch Loss:     1.669891, Batch Acc: 0.578480, Tokens per Sec:    13530, Lr: 0.000031\n",
            "2023-01-20 00:10:43,662 - INFO - joeynmt.training - Epoch 247, Step:    85200, Batch Loss:     1.690434, Batch Acc: 0.576993, Tokens per Sec:    14039, Lr: 0.000031\n",
            "2023-01-20 00:10:51,553 - INFO - joeynmt.training - Epoch 247, Step:    85300, Batch Loss:     1.752276, Batch Acc: 0.574559, Tokens per Sec:    13893, Lr: 0.000031\n",
            "2023-01-20 00:10:57,534 - INFO - joeynmt.training - Epoch 247: total training loss 577.93\n",
            "2023-01-20 00:10:57,534 - INFO - joeynmt.training - EPOCH 248\n",
            "2023-01-20 00:10:59,427 - INFO - joeynmt.training - Epoch 248, Step:    85400, Batch Loss:     1.587540, Batch Acc: 0.579151, Tokens per Sec:    13940, Lr: 0.000031\n",
            "2023-01-20 00:11:07,326 - INFO - joeynmt.training - Epoch 248, Step:    85500, Batch Loss:     1.695753, Batch Acc: 0.580300, Tokens per Sec:    13896, Lr: 0.000031\n",
            "2023-01-20 00:11:15,110 - INFO - joeynmt.training - Epoch 248, Step:    85600, Batch Loss:     1.517139, Batch Acc: 0.573845, Tokens per Sec:    14156, Lr: 0.000031\n",
            "2023-01-20 00:11:22,936 - INFO - joeynmt.training - Epoch 248, Step:    85700, Batch Loss:     1.709154, Batch Acc: 0.574932, Tokens per Sec:    14026, Lr: 0.000031\n",
            "2023-01-20 00:11:24,691 - INFO - joeynmt.training - Epoch 248: total training loss 575.96\n",
            "2023-01-20 00:11:24,691 - INFO - joeynmt.training - EPOCH 249\n",
            "2023-01-20 00:11:30,860 - INFO - joeynmt.training - Epoch 249, Step:    85800, Batch Loss:     1.499244, Batch Acc: 0.578062, Tokens per Sec:    13778, Lr: 0.000031\n",
            "2023-01-20 00:11:38,815 - INFO - joeynmt.training - Epoch 249, Step:    85900, Batch Loss:     1.672814, Batch Acc: 0.578914, Tokens per Sec:    13759, Lr: 0.000031\n",
            "2023-01-20 00:11:46,756 - INFO - joeynmt.training - Epoch 249, Step:    86000, Batch Loss:     1.623126, Batch Acc: 0.577398, Tokens per Sec:    13957, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 125.08ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9189.93ex/s]\n",
            "2023-01-20 00:11:47,061 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=86000\n",
            "2023-01-20 00:11:47,061 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:11:51,178 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:11:51,178 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:11:51,178 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:11:51,179 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:11:51,182 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.45, loss:   2.75, ppl:  15.63, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.0759[sec], evaluation: 0.0376[sec]\n",
            "2023-01-20 00:11:51,185 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:11:51,188 - INFO - joeynmt.training - \tSource:     میان آتش‌ و میان آب جوشان سرگردان باشند . \n",
            "2023-01-20 00:11:51,188 - INFO - joeynmt.training - \tReference:  اونلار اونونلا قاینار سو آراسێندا دولانێب دوراجاقلار . \n",
            "2023-01-20 00:11:51,189 - INFO - joeynmt.training - \tHypothesis: اونلار جهنهمه جوووچا اولاجاقلار .\n",
            "2023-01-20 00:11:51,189 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:11:51,191 - INFO - joeynmt.training - \tSource:     آیا خدا فقط خدای یهودیان است ؟ مگر خدای غیریهودیان نیز نیست ؟ البته که خدای غیریهودیان نیز است . \n",
            "2023-01-20 00:11:51,191 - INFO - joeynmt.training - \tReference:  مگر آللاه یالنیز یهودیلرین آللاهێدێر ؟ باشقا ملتلرین ده آللاهێ دئییلمی ؟ البته ، باشقا ملتلرین ده آللاهێدێر . \n",
            "2023-01-20 00:11:51,191 - INFO - joeynmt.training - \tHypothesis: مگر آللاه یالنیز یهودیلرین آللاهێ باشقا ملتلرین ربیدیر ؟ باشقا ملتلرین ربیدیر .\n",
            "2023-01-20 00:11:51,191 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:11:51,193 - INFO - joeynmt.training - \tSource:     و چون ساحران پیش فرعون آمدند ، گفتند : آیا اگر ما غالب آییم واقعا برای ما مزدی خواهد بود ؟ \n",
            "2023-01-20 00:11:51,193 - INFO - joeynmt.training - \tReference:  سهربازلار گلن کیمی فیر اونا دئدیلر : اگر بیز غالب گلسک ، یقین کی ، بیزه بیر موزد وئریلهجک ، ائله دئییلمی ؟ \n",
            "2023-01-20 00:11:51,193 - INFO - joeynmt.training - \tHypothesis: اونلار فیر اون یانینا گلیب دئدیلر : اگر بیز قم قصمی ائدهجهییک ، بیز ، مۆتلق گلهجک بیر قؤؤم اۆچۆن بیر قؤؤم اۆچۆن اولاجاق ؟\n",
            "2023-01-20 00:11:51,193 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:11:51,195 - INFO - joeynmt.training - \tSource:     سپس به کفرناحوم ، شهری در جلیل رفت . او در روزهای سبت به مردم تعلیم می‌داد . \n",
            "2023-01-20 00:11:51,196 - INFO - joeynmt.training - \tReference:  ایسا قالیلئیانین کئفئرناهوم شهرینه گلدی وه شنبه گۆنۆ اونلارا تعلیم اؤیرتمهیه باشلادێ . \n",
            "2023-01-20 00:11:51,196 - INFO - joeynmt.training - \tHypothesis: او زامان قالیلئیانین کئفئرناهوما شهرده گزیب دولاشاراق شهرینده تعلیم اؤیرهدیردی .\n",
            "2023-01-20 00:11:56,786 - INFO - joeynmt.training - Epoch 249: total training loss 576.33\n",
            "2023-01-20 00:11:56,786 - INFO - joeynmt.training - EPOCH 250\n",
            "2023-01-20 00:11:59,738 - INFO - joeynmt.training - Epoch 250, Step:    86100, Batch Loss:     1.632853, Batch Acc: 0.583277, Tokens per Sec:    11518, Lr: 0.000030\n",
            "2023-01-20 00:12:10,177 - INFO - joeynmt.training - Epoch 250, Step:    86200, Batch Loss:     1.782801, Batch Acc: 0.579530, Tokens per Sec:    10413, Lr: 0.000030\n",
            "2023-01-20 00:12:18,075 - INFO - joeynmt.training - Epoch 250, Step:    86300, Batch Loss:     1.703164, Batch Acc: 0.576266, Tokens per Sec:    13958, Lr: 0.000030\n",
            "2023-01-20 00:12:25,870 - INFO - joeynmt.training - Epoch 250, Step:    86400, Batch Loss:     1.806995, Batch Acc: 0.574830, Tokens per Sec:    14237, Lr: 0.000030\n",
            "2023-01-20 00:12:27,077 - INFO - joeynmt.training - Epoch 250: total training loss 573.28\n",
            "2023-01-20 00:12:27,078 - INFO - joeynmt.training - EPOCH 251\n",
            "2023-01-20 00:12:33,742 - INFO - joeynmt.training - Epoch 251, Step:    86500, Batch Loss:     1.673516, Batch Acc: 0.579169, Tokens per Sec:    14133, Lr: 0.000030\n",
            "2023-01-20 00:12:41,656 - INFO - joeynmt.training - Epoch 251, Step:    86600, Batch Loss:     1.582650, Batch Acc: 0.577044, Tokens per Sec:    13570, Lr: 0.000030\n",
            "2023-01-20 00:12:49,536 - INFO - joeynmt.training - Epoch 251, Step:    86700, Batch Loss:     1.623322, Batch Acc: 0.575938, Tokens per Sec:    14023, Lr: 0.000030\n",
            "2023-01-20 00:12:54,407 - INFO - joeynmt.training - Epoch 251: total training loss 573.73\n",
            "2023-01-20 00:12:54,407 - INFO - joeynmt.training - EPOCH 252\n",
            "2023-01-20 00:12:57,533 - INFO - joeynmt.training - Epoch 252, Step:    86800, Batch Loss:     1.561225, Batch Acc: 0.582305, Tokens per Sec:    13663, Lr: 0.000030\n",
            "2023-01-20 00:13:05,511 - INFO - joeynmt.training - Epoch 252, Step:    86900, Batch Loss:     1.727541, Batch Acc: 0.579378, Tokens per Sec:    13839, Lr: 0.000030\n",
            "2023-01-20 00:13:13,473 - INFO - joeynmt.training - Epoch 252, Step:    87000, Batch Loss:     1.540494, Batch Acc: 0.574894, Tokens per Sec:    13835, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 142.49ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10089.41ex/s]\n",
            "2023-01-20 00:13:13,747 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=87000\n",
            "2023-01-20 00:13:13,747 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:13:18,455 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:13:18,455 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:13:18,456 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:13:18,456 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:13:18,459 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.24, loss:   2.83, ppl:  16.93, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6692[sec], evaluation: 0.0358[sec]\n",
            "2023-01-20 00:13:18,462 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:13:18,466 - INFO - joeynmt.training - \tSource:     فریسیان وقتی این را دیدند ، به شاگردان عیسی گفتند : چرا استاد شما با خراجگیران و گناهکاران غذا می‌خورد ؟ \n",
            "2023-01-20 00:13:18,466 - INFO - joeynmt.training - \tReference:  فاریسئیلر بونو گؤردۆکده اونون شاگردلرینه دئدیلر : نیه مۆعلمنظ وئرگیییغانلار وه گۆناهکارلارلا بیرگه یئمک یئییر ؟ \n",
            "2023-01-20 00:13:18,466 - INFO - joeynmt.training - \tHypothesis: فاریسئیلر بونو گؤروب شاگردلری ایسنانین شاگردلری اونا دئدیلر : نه اۆچۆن فاریسئیلر وه گۆناهکارلاری ایله یئمهیینی یئییرسینیز ؟\n",
            "2023-01-20 00:13:18,466 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:13:18,468 - INFO - joeynmt.training - \tSource:     همچنین این مثل را برای آنان زد : هیچ کس تکه‌ای از لباس نو را نمی‌برد تا بر لباسی کهنه وصله زند . اگر چنین کند ، نه فقط وصلهٔ نو کنده می‌شود ، بلکه آن تکه از لباس نو نیز وصلهٔ ناجوری برای لباس کهنه است . \n",
            "2023-01-20 00:13:18,469 - INFO - joeynmt.training - \tReference:  ایسا اونلارا بیر مسل ده چکدی : هئچ کیم تزه پالتاردان بیر پارچا قوپارێب کؤهنه پالتارا یاماق وورماز . یوخسا تزه پالتار هم جێرێلار ، هم ده تزه پالتاردان قوپارێلان پارچا کؤهنه پالتارا یاراشماز . \n",
            "2023-01-20 00:13:18,469 - INFO - joeynmt.training - \tHypothesis: یئنه اونلارا مسل چکدی : هئچ کس یئییب ایچمهین ، آمما اۆرهیینده کؤهنه پالتاردان ایچمکدن باشقا بیر پالتار گئییندیر . آمما تلهسیر ، ایچین ده ایچین ده ده ایچسیندن ایچمک اۆچۆن ده گئییندیر .\n",
            "2023-01-20 00:13:18,469 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:13:18,471 - INFO - joeynmt.training - \tSource:     زیرا او در انتظار شهری با بنیاد حقیقی بود ، که طراح و سازندهٔ آن خداست . \n",
            "2023-01-20 00:13:18,471 - INFO - joeynmt.training - \tReference:  چۆنکی ابراهیم مئمارێ وه بانیسی آللاه اولان بۆنؤورهلی شهری گؤزلهییردی . \n",
            "2023-01-20 00:13:18,471 - INFO - joeynmt.training - \tHypothesis: چۆنکی او ، مملکتوب وه حقیقی شهری آللاهێندێر .\n",
            "2023-01-20 00:13:18,471 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:13:18,473 - INFO - joeynmt.training - \tSource:     باز که خودتو کثیف کردی . \n",
            "2023-01-20 00:13:18,473 - INFO - joeynmt.training - \tReference:  گینه کی اؤزون با تیردین . \n",
            "2023-01-20 00:13:18,474 - INFO - joeynmt.training - \tHypothesis: اؤزونه اؤزونه کیچیک اؤزونه خطاب ائتدی .\n",
            "2023-01-20 00:13:26,420 - INFO - joeynmt.training - Epoch 252, Step:    87100, Batch Loss:     1.700694, Batch Acc: 0.576737, Tokens per Sec:    13357, Lr: 0.000030\n",
            "2023-01-20 00:13:26,932 - INFO - joeynmt.training - Epoch 252: total training loss 571.72\n",
            "2023-01-20 00:13:26,933 - INFO - joeynmt.training - EPOCH 253\n",
            "2023-01-20 00:13:34,294 - INFO - joeynmt.training - Epoch 253, Step:    87200, Batch Loss:     1.541472, Batch Acc: 0.579793, Tokens per Sec:    14162, Lr: 0.000030\n",
            "2023-01-20 00:13:42,234 - INFO - joeynmt.training - Epoch 253, Step:    87300, Batch Loss:     1.725360, Batch Acc: 0.574527, Tokens per Sec:    13691, Lr: 0.000030\n",
            "2023-01-20 00:13:50,113 - INFO - joeynmt.training - Epoch 253, Step:    87400, Batch Loss:     1.554145, Batch Acc: 0.578288, Tokens per Sec:    14009, Lr: 0.000030\n",
            "2023-01-20 00:13:54,145 - INFO - joeynmt.training - Epoch 253: total training loss 571.37\n",
            "2023-01-20 00:13:54,146 - INFO - joeynmt.training - EPOCH 254\n",
            "2023-01-20 00:13:58,029 - INFO - joeynmt.training - Epoch 254, Step:    87500, Batch Loss:     1.673129, Batch Acc: 0.581548, Tokens per Sec:    13883, Lr: 0.000030\n",
            "2023-01-20 00:14:05,911 - INFO - joeynmt.training - Epoch 254, Step:    87600, Batch Loss:     1.676629, Batch Acc: 0.579322, Tokens per Sec:    14057, Lr: 0.000030\n",
            "2023-01-20 00:14:13,883 - INFO - joeynmt.training - Epoch 254, Step:    87700, Batch Loss:     1.517489, Batch Acc: 0.575322, Tokens per Sec:    13671, Lr: 0.000030\n",
            "2023-01-20 00:14:21,574 - INFO - joeynmt.training - Epoch 254: total training loss 572.36\n",
            "2023-01-20 00:14:21,575 - INFO - joeynmt.training - EPOCH 255\n",
            "2023-01-20 00:14:21,815 - INFO - joeynmt.training - Epoch 255, Step:    87800, Batch Loss:     1.677621, Batch Acc: 0.567093, Tokens per Sec:    13999, Lr: 0.000030\n",
            "2023-01-20 00:14:29,779 - INFO - joeynmt.training - Epoch 255, Step:    87900, Batch Loss:     1.640381, Batch Acc: 0.582345, Tokens per Sec:    13901, Lr: 0.000030\n",
            "2023-01-20 00:14:37,698 - INFO - joeynmt.training - Epoch 255, Step:    88000, Batch Loss:     1.699961, Batch Acc: 0.581627, Tokens per Sec:    14069, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 75.11ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9321.06ex/s]\n",
            "2023-01-20 00:14:37,996 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=88000\n",
            "2023-01-20 00:14:37,997 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:14:42,959 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:14:42,959 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:14:42,959 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:14:42,960 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:14:42,963 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.47, loss:   2.81, ppl:  16.63, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9225[sec], evaluation: 0.0367[sec]\n",
            "2023-01-20 00:14:42,967 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:14:42,970 - INFO - joeynmt.training - \tSource:     به‌راستی چه فایده دارد که کسی تمام دنیا را به دست آورد ، اما جان خود را از دست بدهد ؟ \n",
            "2023-01-20 00:14:42,970 - INFO - joeynmt.training - \tReference:  اینسانا بۆتۆن دۆنیانی قازانێب جانێنێ ایتیرمهیینین نه خئیری وار ؟ \n",
            "2023-01-20 00:14:42,970 - INFO - joeynmt.training - \tHypothesis: حقیقتا ، اینسان بۆتۆن دۆنیانی قازانماق اۆچۆن دۆنیا الینی ایتیررسه ، جانێنێ ایتیررس اولاراق آلسێن ؟\n",
            "2023-01-20 00:14:42,970 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:14:42,973 - INFO - joeynmt.training - \tSource:      وقتی به اورشلیم بازگشتم ، در معبد مشغول دعا بودم که به حالت خلسه فرو رفتم\n",
            "2023-01-20 00:14:42,973 - INFO - joeynmt.training - \tReference:  من یئروسهلیمه قاییدیب مبده دوعا ائتمکده ایدیم کی ، فکریمی بیر گؤرۆنتۆ آپاردێ\n",
            "2023-01-20 00:14:42,973 - INFO - joeynmt.training - \tHypothesis: من یئروسهلیمه قاییتدیغیم زامان دوعا ائدهرک دوعا ائدرکن\n",
            "2023-01-20 00:14:42,973 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:14:42,975 - INFO - joeynmt.training - \tSource:     پس خواستند به او نیرنگی زنند ؛ و لی‌ ما آنان را پست گردانیدیم . \n",
            "2023-01-20 00:14:42,975 - INFO - joeynmt.training - \tReference:  اونلار اونون اۆچۆن بئله بیر هییه قورماق ایستهدیلر ، بیز ایسه اونلارێ چوخ صفیل بیر وزیته سالدێق \n",
            "2023-01-20 00:14:42,976 - INFO - joeynmt.training - \tHypothesis: بونا گؤره ده قادێنێن هییلهسینی بیر هییله قوردولار . لاکین بیز اونلارێ هییله بۆهتان آتدێق .\n",
            "2023-01-20 00:14:42,976 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:14:42,977 - INFO - joeynmt.training - \tSource:     معبود مردم ، \n",
            "2023-01-20 00:14:42,978 - INFO - joeynmt.training - \tReference:  اینسانلارین تانرێسێنا ؛ \n",
            "2023-01-20 00:14:42,978 - INFO - joeynmt.training - \tHypothesis: اینسانلارین تانرێسێ ایله !\n",
            "2023-01-20 00:14:50,829 - INFO - joeynmt.training - Epoch 255, Step:    88100, Batch Loss:     1.538759, Batch Acc: 0.577300, Tokens per Sec:    13478, Lr: 0.000030\n",
            "2023-01-20 00:14:54,069 - INFO - joeynmt.training - Epoch 255: total training loss 568.49\n",
            "2023-01-20 00:14:54,069 - INFO - joeynmt.training - EPOCH 256\n",
            "2023-01-20 00:14:58,758 - INFO - joeynmt.training - Epoch 256, Step:    88200, Batch Loss:     1.636481, Batch Acc: 0.584372, Tokens per Sec:    13741, Lr: 0.000030\n",
            "2023-01-20 00:15:06,651 - INFO - joeynmt.training - Epoch 256, Step:    88300, Batch Loss:     1.516051, Batch Acc: 0.580494, Tokens per Sec:    13900, Lr: 0.000030\n",
            "2023-01-20 00:15:15,061 - INFO - joeynmt.training - Epoch 256, Step:    88400, Batch Loss:     1.705804, Batch Acc: 0.579628, Tokens per Sec:    12919, Lr: 0.000030\n",
            "2023-01-20 00:15:24,337 - INFO - joeynmt.training - Epoch 256: total training loss 570.40\n",
            "2023-01-20 00:15:24,338 - INFO - joeynmt.training - EPOCH 257\n",
            "2023-01-20 00:15:25,361 - INFO - joeynmt.training - Epoch 257, Step:    88500, Batch Loss:     1.685995, Batch Acc: 0.587785, Tokens per Sec:    14047, Lr: 0.000030\n",
            "2023-01-20 00:15:33,337 - INFO - joeynmt.training - Epoch 257, Step:    88600, Batch Loss:     1.744788, Batch Acc: 0.580574, Tokens per Sec:    13842, Lr: 0.000030\n",
            "2023-01-20 00:15:41,238 - INFO - joeynmt.training - Epoch 257, Step:    88700, Batch Loss:     1.672473, Batch Acc: 0.580100, Tokens per Sec:    13993, Lr: 0.000030\n",
            "2023-01-20 00:15:49,085 - INFO - joeynmt.training - Epoch 257, Step:    88800, Batch Loss:     1.724044, Batch Acc: 0.577077, Tokens per Sec:    13992, Lr: 0.000030\n",
            "2023-01-20 00:15:51,622 - INFO - joeynmt.training - Epoch 257: total training loss 568.96\n",
            "2023-01-20 00:15:51,622 - INFO - joeynmt.training - EPOCH 258\n",
            "2023-01-20 00:15:57,046 - INFO - joeynmt.training - Epoch 258, Step:    88900, Batch Loss:     1.579300, Batch Acc: 0.581830, Tokens per Sec:    13913, Lr: 0.000030\n",
            "2023-01-20 00:16:04,867 - INFO - joeynmt.training - Epoch 258, Step:    89000, Batch Loss:     1.678513, Batch Acc: 0.580617, Tokens per Sec:    14081, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 139.42ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10600.31ex/s]\n",
            "2023-01-20 00:16:05,140 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=89000\n",
            "2023-01-20 00:16:05,140 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:16:09,384 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:16:09,385 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:16:09,385 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:16:09,386 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:16:09,389 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.56, loss:   2.77, ppl:  15.95, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2070[sec], evaluation: 0.0344[sec]\n",
            "2023-01-20 00:16:09,391 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:16:09,395 - INFO - joeynmt.training - \tSource:     وقتی رسولان بازگشتند ، تمام آنچه را کرده بودند برای او تعریف کردند . آنگاه عیسی آنان را با خود به شهری به نام بیت‌صیدا برد تا در آنجا تنها باشند . \n",
            "2023-01-20 00:16:09,395 - INFO - joeynmt.training - \tReference:  هوارلر ایسنانین یانینا قاییدیب گؤردوکلری ایشلر حاققێندا اونا دانێشدێلار . سونرا ایسا آنجاق اونلارێ اؤزو ایله گؤتوروب بئت سایدا آدلانان بیر شهره آپاردێ . \n",
            "2023-01-20 00:16:09,395 - INFO - joeynmt.training - \tHypothesis: هوارلر ایسنانین بۆتۆن شهرین قایغیسینی چکدیلر . ایسا اونلارا شهرین آدێ ایله بیر شهره گئتدی .\n",
            "2023-01-20 00:16:09,395 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:16:09,397 - INFO - joeynmt.training - \tSource:     زیرا پیش از آمدن افرادی از جانب یعقوب ، با غیریهودیان غذا می‌خورد ، اما وقتی آنان آمدند ، از ترس طرفداران ختنه ، دیگر چنین نکرد و خود را از ایشان جدا ساخت . \n",
            "2023-01-20 00:16:09,397 - INFO - joeynmt.training - \tReference:  چۆنکی یاقوبون یانیندان بزی آداملار گلمهمیشدن اول پئتئر باشقا ملتلردن اولانلارلا یئمک یئییردی . همین آداملار گلدیکده ایسه سۆنت ترفدارلاریندان قورخاراق باشقا ملتلرین یانیندان چکیلیب آیری دوردو . \n",
            "2023-01-20 00:16:09,397 - INFO - joeynmt.training - \tHypothesis: چۆنکی سندن اول گلیب باشقا ملتلردن اولان آداملار یاقوب ، باشقا ملتلردن قورخماییب اونلارا دئدی : باشقالارینین باشقالارینین اویغونلوق ائتمهدی ، اونلارا دئدی .\n",
            "2023-01-20 00:16:09,398 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:16:09,400 - INFO - joeynmt.training - \tSource:     سپس به آنان گفت : حال که این‌ها را درک کردید ، بدانید هر معلمی که در مورد پادشاهی آسمان‌ها تعلیم گرفته باشد ، مانند صاحبخانه‌ای است که از گنجینهٔ خود ، هم چیزهای نو و هم چیزهای کهنه بیرون می‌آورد . \n",
            "2023-01-20 00:16:09,400 - INFO - joeynmt.training - \tReference:  ایسا دا اونلارا دئدی : اونا گؤره ده سماوی پادشاهلێق اۆچۆن شاگردلیک ائتمیش هر الاهییاتچی خزینهسیندن تزه وه کؤهنه شئیلر چێخاران ائو صاحبینه بنزهییر . \n",
            "2023-01-20 00:16:09,400 - INFO - joeynmt.training - \tHypothesis: ایسا اونلارا دئدی : ایندیلر تیکین ، بیلین کی ، سماوی پادشاهلێقدا تعلیم اؤیرهدرکن ، سماوی پادشاهلێق تام مۆدرکلرین ایچیندن وه ایچدیگی شئیلره یازیلمیشدیر .\n",
            "2023-01-20 00:16:09,400 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:16:09,402 - INFO - joeynmt.training - \tSource:     واقعا آنچه را که شما سوای خدا می‌پرستید جز بتانی بیش‌ نیستند و دروغی برمی‌سازید . در حقیقت ، کسانی را که جز خدا می‌پرستید اختیار روزی شما را در دست ندارند . پس روزی را پیش خدا بجویید و او را بپرستید و وی را سپاس گویید ، که به سوی او بازگردانیده می‌شوید . \n",
            "2023-01-20 00:16:09,402 - INFO - joeynmt.training - \tReference:  سیز آللاهێ قویوب آنجاق بۆتلره ابادت ائدیر ، یالان اویدورورسونوز . سیزین آللاهدان باشقا ابادت ائتدیکلرینیز سیزه روزی وئرمهیه قادر دئییللر . روزینی آللاهدان دیلهگین . اونا تاپێنێن ، اونا شۆکۆر ائدین . سیز اونون هۆزورونا قایتاریلاجاقسینزیتینز ! \n",
            "2023-01-20 00:16:09,402 - INFO - joeynmt.training - \tHypothesis: سیز آللاهێن دینینه ابادت ائتدیگینیز بۆتلردن باشقا هئچ بیر تانرێ یوخدور . آللاها ابادت ائدین . سیزه ابادت ائتمهیین ! آللاها ابادت ائدین وه شۆکۆر ائدنلره ابادت ائدین ! شۆکۆر ائدین !\n",
            "2023-01-20 00:16:17,263 - INFO - joeynmt.training - Epoch 258, Step:    89100, Batch Loss:     1.668646, Batch Acc: 0.579100, Tokens per Sec:    13352, Lr: 0.000030\n",
            "2023-01-20 00:16:23,432 - INFO - joeynmt.training - Epoch 258: total training loss 568.82\n",
            "2023-01-20 00:16:23,433 - INFO - joeynmt.training - EPOCH 259\n",
            "2023-01-20 00:16:25,260 - INFO - joeynmt.training - Epoch 259, Step:    89200, Batch Loss:     1.672231, Batch Acc: 0.576554, Tokens per Sec:    14146, Lr: 0.000030\n",
            "2023-01-20 00:16:33,086 - INFO - joeynmt.training - Epoch 259, Step:    89300, Batch Loss:     1.601106, Batch Acc: 0.579416, Tokens per Sec:    14024, Lr: 0.000030\n",
            "2023-01-20 00:16:40,928 - INFO - joeynmt.training - Epoch 259, Step:    89400, Batch Loss:     1.586552, Batch Acc: 0.582006, Tokens per Sec:    14094, Lr: 0.000030\n",
            "2023-01-20 00:16:48,858 - INFO - joeynmt.training - Epoch 259, Step:    89500, Batch Loss:     1.568737, Batch Acc: 0.578199, Tokens per Sec:    13982, Lr: 0.000030\n",
            "2023-01-20 00:16:50,612 - INFO - joeynmt.training - Epoch 259: total training loss 566.70\n",
            "2023-01-20 00:16:50,613 - INFO - joeynmt.training - EPOCH 260\n",
            "2023-01-20 00:16:56,878 - INFO - joeynmt.training - Epoch 260, Step:    89600, Batch Loss:     1.665148, Batch Acc: 0.583091, Tokens per Sec:    13695, Lr: 0.000030\n",
            "2023-01-20 00:17:04,814 - INFO - joeynmt.training - Epoch 260, Step:    89700, Batch Loss:     1.752841, Batch Acc: 0.576917, Tokens per Sec:    13871, Lr: 0.000030\n",
            "2023-01-20 00:17:12,734 - INFO - joeynmt.training - Epoch 260, Step:    89800, Batch Loss:     1.672673, Batch Acc: 0.576924, Tokens per Sec:    13942, Lr: 0.000030\n",
            "2023-01-20 00:17:18,122 - INFO - joeynmt.training - Epoch 260: total training loss 571.24\n",
            "2023-01-20 00:17:18,122 - INFO - joeynmt.training - EPOCH 261\n",
            "2023-01-20 00:17:20,659 - INFO - joeynmt.training - Epoch 261, Step:    89900, Batch Loss:     1.651127, Batch Acc: 0.581789, Tokens per Sec:    13958, Lr: 0.000030\n",
            "2023-01-20 00:17:28,475 - INFO - joeynmt.training - Epoch 261, Step:    90000, Batch Loss:     1.464685, Batch Acc: 0.583007, Tokens per Sec:    14116, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.38ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8954.24ex/s]\n",
            "2023-01-20 00:17:28,782 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=90000\n",
            "2023-01-20 00:17:28,783 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:17:33,603 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:17:33,603 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:17:33,604 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:17:33,604 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:17:33,607 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.31, loss:   2.78, ppl:  16.04, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7817[sec], evaluation: 0.0359[sec]\n",
            "2023-01-20 00:17:33,611 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:17:33,614 - INFO - joeynmt.training - \tSource:     و اینکه نماز برپا دارید و از او بترسید ، و هم اوست که نزد وی محشور خواهید گردید . \n",
            "2023-01-20 00:17:33,614 - INFO - joeynmt.training - \tReference:  همچنین : ناماز قێلێن ، آللاهدان قورخون ، هۆزورونا توپلانآجاغینیز محض اودور \n",
            "2023-01-20 00:17:33,614 - INFO - joeynmt.training - \tHypothesis: ناماز قێلێن وه قورخون ، اوندان قورخون وه اونون هۆزورونا قاییداجاقسینیز !\n",
            "2023-01-20 00:17:33,614 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:17:33,616 - INFO - joeynmt.training - \tSource:     پس شاگردان مصمم شدند که هر یک به قدر توانایی خود کمکی برای برادران ساکن یهودیه بفرستد . \n",
            "2023-01-20 00:17:33,617 - INFO - joeynmt.training - \tReference:  شاگردلر قرارا آلدێ کی ، هر بیری گۆجۆ چاتان قدر یهودئیآدا یاشایان ایمانلی باجێ قارداشلارێنا یاردیم گؤندرسین . \n",
            "2023-01-20 00:17:33,617 - INFO - joeynmt.training - \tHypothesis: شاگردلری ایسه هر بیر آز مۆدت اؤز قارداشلارێنێ ایتاااادا اولان یهودی باشچیلاریا گؤندرسین .\n",
            "2023-01-20 00:17:33,617 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:17:33,619 - INFO - joeynmt.training - \tSource:      به‌ همان کسانی که چشمان بصیرت‌ شان از یاد من در پرده بود ، و توانایی شنیدن حق‌ نداشتند . \n",
            "2023-01-20 00:17:33,619 - INFO - joeynmt.training - \tReference:  او کسلر کی ، گؤزلری منی آنماقدان قاپالێ ایدی وه ائشیتمهیه ده قادر دئییلدیلر \n",
            "2023-01-20 00:17:33,620 - INFO - joeynmt.training - \tHypothesis: او کسلر کی ، گؤزلریندن بیر یادا سالا سالێب پاک بیر یووا بیلمهدی .\n",
            "2023-01-20 00:17:33,620 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:17:33,622 - INFO - joeynmt.training - \tSource:     پس با اشاره از پدر نوزاد سؤال کردند که می‌خواهد او را چه بنامد . \n",
            "2023-01-20 00:17:33,622 - INFO - joeynmt.training - \tReference:  کؤرپهنین آتاسێندان کؤرپهیه نه آد قویولماسینی اشاره ایله سوروشدولار . \n",
            "2023-01-20 00:17:33,622 - INFO - joeynmt.training - \tHypothesis: آتا آناسێ اونون سێناغا چکهجک کی ، اونون اناملارێ نه ایستهیهسینیز .\n",
            "2023-01-20 00:17:41,487 - INFO - joeynmt.training - Epoch 261, Step:    90100, Batch Loss:     1.715414, Batch Acc: 0.579458, Tokens per Sec:    13417, Lr: 0.000030\n",
            "2023-01-20 00:17:49,322 - INFO - joeynmt.training - Epoch 261, Step:    90200, Batch Loss:     1.646812, Batch Acc: 0.579224, Tokens per Sec:    13891, Lr: 0.000030\n",
            "2023-01-20 00:17:50,494 - INFO - joeynmt.training - Epoch 261: total training loss 570.16\n",
            "2023-01-20 00:17:50,494 - INFO - joeynmt.training - EPOCH 262\n",
            "2023-01-20 00:17:57,103 - INFO - joeynmt.training - Epoch 262, Step:    90300, Batch Loss:     1.653252, Batch Acc: 0.579663, Tokens per Sec:    14134, Lr: 0.000030\n",
            "2023-01-20 00:18:04,981 - INFO - joeynmt.training - Epoch 262, Step:    90400, Batch Loss:     1.567659, Batch Acc: 0.585310, Tokens per Sec:    13937, Lr: 0.000030\n",
            "2023-01-20 00:18:12,931 - INFO - joeynmt.training - Epoch 262, Step:    90500, Batch Loss:     1.717236, Batch Acc: 0.580704, Tokens per Sec:    13836, Lr: 0.000030\n",
            "2023-01-20 00:18:17,755 - INFO - joeynmt.training - Epoch 262: total training loss 567.56\n",
            "2023-01-20 00:18:17,755 - INFO - joeynmt.training - EPOCH 263\n",
            "2023-01-20 00:18:20,850 - INFO - joeynmt.training - Epoch 263, Step:    90600, Batch Loss:     1.575697, Batch Acc: 0.582266, Tokens per Sec:    13874, Lr: 0.000030\n",
            "2023-01-20 00:18:28,793 - INFO - joeynmt.training - Epoch 263, Step:    90700, Batch Loss:     1.727114, Batch Acc: 0.583004, Tokens per Sec:    13969, Lr: 0.000030\n",
            "2023-01-20 00:18:38,311 - INFO - joeynmt.training - Epoch 263, Step:    90800, Batch Loss:     1.819672, Batch Acc: 0.579722, Tokens per Sec:    11561, Lr: 0.000030\n",
            "2023-01-20 00:18:47,551 - INFO - joeynmt.training - Epoch 263, Step:    90900, Batch Loss:     1.631556, Batch Acc: 0.579568, Tokens per Sec:    11711, Lr: 0.000030\n",
            "2023-01-20 00:18:48,151 - INFO - joeynmt.training - Epoch 263: total training loss 568.01\n",
            "2023-01-20 00:18:48,151 - INFO - joeynmt.training - EPOCH 264\n",
            "2023-01-20 00:18:55,532 - INFO - joeynmt.training - Epoch 264, Step:    91000, Batch Loss:     1.645479, Batch Acc: 0.586283, Tokens per Sec:    13791, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.55ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9961.60ex/s]\n",
            "2023-01-20 00:18:55,819 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=91000\n",
            "2023-01-20 00:18:55,819 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:19:00,324 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:19:00,324 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:19:00,324 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:19:00,325 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:19:00,328 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.09, loss:   2.85, ppl:  17.29, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4651[sec], evaluation: 0.0368[sec]\n",
            "2023-01-20 00:19:00,331 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:19:00,334 - INFO - joeynmt.training - \tSource:     من نشانه‌های رسول بودن خود را با بردباری بسیار ، با نشانه‌ها ، با عجایب و با معجزات در میان شما آشکار ساختم . \n",
            "2023-01-20 00:19:00,334 - INFO - joeynmt.training - \tReference:  هوارلایگیمین نیشانهلری آرانێزدا الامتلر ، خارقهلر وه مۆجزلر واسطهسیله ، بؤیوک دؤزوملولوکله گؤستریلدی . \n",
            "2023-01-20 00:19:00,334 - INFO - joeynmt.training - \tHypothesis: منیمله بیرلیکده علامت اولاراق چوخ مۆجزلر گؤسترمک اۆچۆن الامتلرله گؤستردی .\n",
            "2023-01-20 00:19:00,334 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:19:00,337 - INFO - joeynmt.training - \tSource:     پس ، از آنجا که به این امر اطمینان دارم ، می‌دانم که برای پیشرفت و شادی شما در ایمان ، در جسم خواهم ماند و با همهٔ شما به سر خواهم برد . \n",
            "2023-01-20 00:19:00,337 - INFO - joeynmt.training - \tReference:  بونا امین اولاراق بیلیرم : ساغ قالاجاغام وه هامینیزلا بیرلیکده قالماقدا داوام ائدهجهیم کی ، اماندا هم ایرهلیلهیهسینیز ، هم ده سئوینهسینیز . \n",
            "2023-01-20 00:19:00,337 - INFO - joeynmt.training - \tHypothesis: بونا گؤره ده بیلیرم کی ، منسه یانینیزا گلنده قالێب سیزه وارسا ، بونلاردا قالان حسه یانینیزا آپارێرام .\n",
            "2023-01-20 00:19:00,337 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:19:00,339 - INFO - joeynmt.training - \tSource:     و اگر پیراهن او از پشت دریده شده ، زن دروغ گفته و او از راستگویان است . \n",
            "2023-01-20 00:19:00,339 - INFO - joeynmt.training - \tReference:  یوخ ، اگر اونون کؤینهگی آرخادان جێریلیبسا ، یالان دئییر ، او ایسه دوغرودانیشانلارداندیریندار . \n",
            "2023-01-20 00:19:00,339 - INFO - joeynmt.training - \tHypothesis: اگر اونون کؤینهگی آرخاسێندا دا ، یالانچی اولسا ، یالانچیدیر .\n",
            "2023-01-20 00:19:00,341 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:19:00,343 - INFO - joeynmt.training - \tSource:     و راستی که این قرآن‌ وحی پروردگار جهانیان است . \n",
            "2023-01-20 00:19:00,343 - INFO - joeynmt.training - \tReference:  شبههسیز کی ، بو آلملرین ربی ترهفیندن نازل ائدیلمیشدیر ! \n",
            "2023-01-20 00:19:00,343 - INFO - joeynmt.training - \tHypothesis: حقیقتا ، بو آلملرین ربیدیر !\n",
            "2023-01-20 00:19:08,251 - INFO - joeynmt.training - Epoch 264, Step:    91100, Batch Loss:     1.766277, Batch Acc: 0.581669, Tokens per Sec:    13464, Lr: 0.000030\n",
            "2023-01-20 00:19:16,190 - INFO - joeynmt.training - Epoch 264, Step:    91200, Batch Loss:     1.533068, Batch Acc: 0.579138, Tokens per Sec:    13948, Lr: 0.000030\n",
            "2023-01-20 00:19:20,260 - INFO - joeynmt.training - Epoch 264: total training loss 565.24\n",
            "2023-01-20 00:19:20,261 - INFO - joeynmt.training - EPOCH 265\n",
            "2023-01-20 00:19:24,072 - INFO - joeynmt.training - Epoch 265, Step:    91300, Batch Loss:     1.588141, Batch Acc: 0.584198, Tokens per Sec:    13938, Lr: 0.000030\n",
            "2023-01-20 00:19:31,865 - INFO - joeynmt.training - Epoch 265, Step:    91400, Batch Loss:     1.622520, Batch Acc: 0.584082, Tokens per Sec:    14223, Lr: 0.000030\n",
            "2023-01-20 00:19:39,782 - INFO - joeynmt.training - Epoch 265, Step:    91500, Batch Loss:     1.634745, Batch Acc: 0.580044, Tokens per Sec:    13857, Lr: 0.000030\n",
            "2023-01-20 00:19:47,504 - INFO - joeynmt.training - Epoch 265: total training loss 564.70\n",
            "2023-01-20 00:19:47,504 - INFO - joeynmt.training - EPOCH 266\n",
            "2023-01-20 00:19:47,745 - INFO - joeynmt.training - Epoch 266, Step:    91600, Batch Loss:     1.746851, Batch Acc: 0.549860, Tokens per Sec:    13395, Lr: 0.000030\n",
            "2023-01-20 00:19:55,653 - INFO - joeynmt.training - Epoch 266, Step:    91700, Batch Loss:     1.612211, Batch Acc: 0.583928, Tokens per Sec:    13996, Lr: 0.000030\n",
            "2023-01-20 00:20:03,519 - INFO - joeynmt.training - Epoch 266, Step:    91800, Batch Loss:     1.625522, Batch Acc: 0.583720, Tokens per Sec:    13977, Lr: 0.000030\n",
            "2023-01-20 00:20:11,371 - INFO - joeynmt.training - Epoch 266, Step:    91900, Batch Loss:     1.708868, Batch Acc: 0.582529, Tokens per Sec:    14157, Lr: 0.000030\n",
            "2023-01-20 00:20:14,712 - INFO - joeynmt.training - Epoch 266: total training loss 564.16\n",
            "2023-01-20 00:20:14,713 - INFO - joeynmt.training - EPOCH 267\n",
            "2023-01-20 00:20:19,298 - INFO - joeynmt.training - Epoch 267, Step:    92000, Batch Loss:     1.663753, Batch Acc: 0.583075, Tokens per Sec:    13782, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 119.52ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10275.45ex/s]\n",
            "2023-01-20 00:20:19,598 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=92000\n",
            "2023-01-20 00:20:19,598 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:20:24,781 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:20:24,781 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:20:24,782 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:20:24,782 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:20:24,785 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.39, loss:   2.91, ppl:  18.30, acc:   0.40, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1413[sec], evaluation: 0.0381[sec]\n",
            "2023-01-20 00:20:24,789 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:20:24,792 - INFO - joeynmt.training - \tSource:     همچنین ، از روی طمع با سخنان جعلی از شما بهره‌کشی خواهند کرد . اما محکومیت ایشان که از گذشته‌های دور تعیین شده است ، تأخیر نمی‌کند و نابودی آنان قطعی است . \n",
            "2023-01-20 00:20:24,793 - INFO - joeynmt.training - \tReference:  تاماهکارلێقلارینداندان اویدورما سؤزلرله سیزی استثمار ائدهجکلر . اونلارێن مۆحاکمهسی چوخدان داوام ائدیر وه هلاکێ هازێردێر . \n",
            "2023-01-20 00:20:24,793 - INFO - joeynmt.training - \tHypothesis: سیزه یازیلانلاردان بزیسی داشیاندان سونرا باشقا ملتلره محکوم ائدهجکلر . آمما اونلارا قارشێ چێخمامێش ، اؤیرنمهینلر محو ائتدی .\n",
            "2023-01-20 00:20:24,793 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:20:24,795 - INFO - joeynmt.training - \tSource:     گفت : ای سران کشور در کارم به من نظر دهید که بی‌حضور شما تا به حال‌ کاری را فیصله نداده‌ام . \n",
            "2023-01-20 00:20:24,795 - INFO - joeynmt.training - \tReference:   دئدی : ائی ا یانلار ! بو ایش بارهسینده منه ره یینیزی بیلدیرین . من سیزینله مسلهحتلشمهمیش هئچ بیر ایش گؤرن دئییلم ! \n",
            "2023-01-20 00:20:24,795 - INFO - joeynmt.training - \tHypothesis: دئدی : ائی اؤیرنمیشم ، ایشینیزده ایشچی توتمایین کی ، من ایشیمی گؤرمهیه جانێم .\n",
            "2023-01-20 00:20:24,795 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:20:24,797 - INFO - joeynmt.training - \tSource:     در نزدیکترین سرزمین ، و لی‌ بعد از شکستشان ، در ظرف چند سالی ، به زودی پیروز خواهند گردید . \n",
            "2023-01-20 00:20:24,798 - INFO - joeynmt.training - \tReference:   ان یاخین بیر یئرده . لاکین اونلار مغلوبگتلریندندهندهنالوبال سونرا غالب گلهجکلر . \n",
            "2023-01-20 00:20:24,798 - INFO - joeynmt.training - \tHypothesis: یاخینلاشیب اؤلدوگو یئر اۆزۆنده بیر نئچه ایلدیر . اونلارێن بیر نئچه ایل مۆدت کؤلهلیکده آجێ اولاجاقلار .\n",
            "2023-01-20 00:20:24,798 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:20:24,800 - INFO - joeynmt.training - \tSource:     زنان بسیاری نیز از دور وقایع را نظاره می‌کردند ؛ همان زنانی که برای خدمت به عیسی ، او را از جلیل همراهی کرده بودند . \n",
            "2023-01-20 00:20:24,801 - INFO - joeynmt.training - \tReference:  اورادا اولوب کئچنلره اوزاقدان باخان بیر چوخ قادێن وار ایدی کی ، خدمت ائتمک اۆچۆن قالیلئیادان ایسنانین آردێنجا گلمیشدیلر . \n",
            "2023-01-20 00:20:24,801 - INFO - joeynmt.training - \tHypothesis: قادێنلار دا اسگرلر وه اونونلا بیرلیکده قالیلئیادان نؤقسانسێز قادێنلار ایسنانین آردێنجا گئدیردی .\n",
            "2023-01-20 00:20:32,677 - INFO - joeynmt.training - Epoch 267, Step:    92100, Batch Loss:     1.500905, Batch Acc: 0.585389, Tokens per Sec:    13682, Lr: 0.000029\n",
            "2023-01-20 00:20:40,486 - INFO - joeynmt.training - Epoch 267, Step:    92200, Batch Loss:     1.677257, Batch Acc: 0.581658, Tokens per Sec:    14168, Lr: 0.000029\n",
            "2023-01-20 00:20:47,242 - INFO - joeynmt.training - Epoch 267: total training loss 562.51\n",
            "2023-01-20 00:20:47,242 - INFO - joeynmt.training - EPOCH 268\n",
            "2023-01-20 00:20:48,339 - INFO - joeynmt.training - Epoch 268, Step:    92300, Batch Loss:     1.602397, Batch Acc: 0.583554, Tokens per Sec:    13761, Lr: 0.000029\n",
            "2023-01-20 00:20:56,229 - INFO - joeynmt.training - Epoch 268, Step:    92400, Batch Loss:     1.732169, Batch Acc: 0.585571, Tokens per Sec:    13872, Lr: 0.000029\n",
            "2023-01-20 00:21:04,142 - INFO - joeynmt.training - Epoch 268, Step:    92500, Batch Loss:     1.836924, Batch Acc: 0.582879, Tokens per Sec:    13916, Lr: 0.000029\n",
            "2023-01-20 00:21:12,020 - INFO - joeynmt.training - Epoch 268, Step:    92600, Batch Loss:     1.536160, Batch Acc: 0.579516, Tokens per Sec:    14134, Lr: 0.000029\n",
            "2023-01-20 00:21:14,610 - INFO - joeynmt.training - Epoch 268: total training loss 564.68\n",
            "2023-01-20 00:21:14,610 - INFO - joeynmt.training - EPOCH 269\n",
            "2023-01-20 00:21:19,948 - INFO - joeynmt.training - Epoch 269, Step:    92700, Batch Loss:     1.668836, Batch Acc: 0.582691, Tokens per Sec:    13979, Lr: 0.000029\n",
            "2023-01-20 00:21:27,763 - INFO - joeynmt.training - Epoch 269, Step:    92800, Batch Loss:     1.775517, Batch Acc: 0.586344, Tokens per Sec:    13887, Lr: 0.000029\n",
            "2023-01-20 00:21:35,627 - INFO - joeynmt.training - Epoch 269, Step:    92900, Batch Loss:     1.517887, Batch Acc: 0.582978, Tokens per Sec:    13974, Lr: 0.000029\n",
            "2023-01-20 00:21:41,935 - INFO - joeynmt.training - Epoch 269: total training loss 566.59\n",
            "2023-01-20 00:21:41,935 - INFO - joeynmt.training - EPOCH 270\n",
            "2023-01-20 00:21:43,604 - INFO - joeynmt.training - Epoch 270, Step:    93000, Batch Loss:     1.509857, Batch Acc: 0.589799, Tokens per Sec:    13888, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.41ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9654.62ex/s]\n",
            "2023-01-20 00:21:43,881 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=93000\n",
            "2023-01-20 00:21:43,881 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:21:48,343 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:21:48,344 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:21:48,344 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:21:48,345 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:21:48,348 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.11, loss:   2.76, ppl:  15.79, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4236[sec], evaluation: 0.0357[sec]\n",
            "2023-01-20 00:21:48,351 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:21:48,354 - INFO - joeynmt.training - \tSource:     از این رو ، همان طور که از طریق روح‌القدس نوشته شده است : امروز اگر صدای مرا می‌شنوید ، \n",
            "2023-01-20 00:21:48,354 - INFO - joeynmt.training - \tReference:  بونا گؤره مۆقدس روحون دئدیگی کیمی : بو گۆن اگر اونون سسینی ائشیتسنیز ، \n",
            "2023-01-20 00:21:48,355 - INFO - joeynmt.training - \tHypothesis: مۆقدس روح واسطهسیله بو گۆنده مۆقدس یازیلدیغی کیمی بودور ، اگر منیم سماغیما ائشیدیرسه ،\n",
            "2023-01-20 00:21:48,355 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:21:48,357 - INFO - joeynmt.training - \tSource:     گرگ كه میشوی باید دنبالشان كنی . \n",
            "2023-01-20 00:21:48,357 - INFO - joeynmt.training - \tReference:  قورد اولسان گرگ قاچاسان و قوواسان . \n",
            "2023-01-20 00:21:48,357 - INFO - joeynmt.training - \tHypothesis: آغزی گونده رک لاری آختاریرسان .\n",
            "2023-01-20 00:21:48,357 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:21:48,359 - INFO - joeynmt.training - \tSource:     از طرف پولس که به خواست خدا رسول مسیحْ عیسی گشت تا وعدهٔ آن حیاتی را اعلام کند که از طریق مسیحْ عیسی میسر شده است ، \n",
            "2023-01-20 00:21:48,359 - INFO - joeynmt.training - \tReference:  مصیح اسعادا اولان هیات ودینه گؤره آللاهێن ارادهسی ایله مصیح ایسنانین هوارسی من پاولدان\n",
            "2023-01-20 00:21:48,359 - INFO - joeynmt.training - \tHypothesis: آللاهێن ارادهسی ایله مصیح اسعادا اولان مصیح اسعادا ود ائدیلمیش ودسینده مصیح اسعادا اولان ایمان واسطهسیله مصیح اسعادا اولان ود اولاراق هیات سۆرمک اۆچۆن مصیح اسعادا اولاراق اؤلدو .\n",
            "2023-01-20 00:21:48,359 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:21:48,362 - INFO - joeynmt.training - \tSource:     همچنین ، پسر خدا عیسی مسیح که من و سیلوانوس و تیموتائوس در مورد او به شما موعظه کردیم ، همزمان بله و نه نشد ، بلکه بله در خصوص او بله شده است ؛ \n",
            "2023-01-20 00:21:48,362 - INFO - joeynmt.training - \tReference:  سیلا وه تیموتئیله بیرلیکده سیزه وز ائتدیگیمیز آللاهێن اوغلو ایسا مصیح هم بلی ، هم ده خئیر دئییل . اوندا یالنیز بلی وار . \n",
            "2023-01-20 00:21:48,362 - INFO - joeynmt.training - \tHypothesis: آللاهێن اوغلو ایسا مسیحین اوغلو وه تیموتئیین اوغلو مصیحده وزیگتده وزیگتده وز ائدیریک . خئیر ، اوندان دئییل ، دیریلمهییب .\n",
            "2023-01-20 00:21:56,300 - INFO - joeynmt.training - Epoch 270, Step:    93100, Batch Loss:     1.596700, Batch Acc: 0.585805, Tokens per Sec:    13205, Lr: 0.000029\n",
            "2023-01-20 00:22:07,311 - INFO - joeynmt.training - Epoch 270, Step:    93200, Batch Loss:     1.630483, Batch Acc: 0.583554, Tokens per Sec:    10034, Lr: 0.000029\n",
            "2023-01-20 00:22:15,273 - INFO - joeynmt.training - Epoch 270, Step:    93300, Batch Loss:     1.577069, Batch Acc: 0.582173, Tokens per Sec:    13988, Lr: 0.000029\n",
            "2023-01-20 00:22:17,187 - INFO - joeynmt.training - Epoch 270: total training loss 562.12\n",
            "2023-01-20 00:22:17,187 - INFO - joeynmt.training - EPOCH 271\n",
            "2023-01-20 00:22:23,185 - INFO - joeynmt.training - Epoch 271, Step:    93400, Batch Loss:     1.498811, Batch Acc: 0.586056, Tokens per Sec:    14113, Lr: 0.000029\n",
            "2023-01-20 00:22:31,044 - INFO - joeynmt.training - Epoch 271, Step:    93500, Batch Loss:     1.582939, Batch Acc: 0.584286, Tokens per Sec:    13931, Lr: 0.000029\n",
            "2023-01-20 00:22:38,910 - INFO - joeynmt.training - Epoch 271, Step:    93600, Batch Loss:     1.628790, Batch Acc: 0.581098, Tokens per Sec:    14003, Lr: 0.000029\n",
            "2023-01-20 00:22:44,328 - INFO - joeynmt.training - Epoch 271: total training loss 560.63\n",
            "2023-01-20 00:22:44,328 - INFO - joeynmt.training - EPOCH 272\n",
            "2023-01-20 00:22:46,908 - INFO - joeynmt.training - Epoch 272, Step:    93700, Batch Loss:     1.622585, Batch Acc: 0.589430, Tokens per Sec:    13819, Lr: 0.000029\n",
            "2023-01-20 00:22:54,767 - INFO - joeynmt.training - Epoch 272, Step:    93800, Batch Loss:     1.596966, Batch Acc: 0.586733, Tokens per Sec:    13934, Lr: 0.000029\n",
            "2023-01-20 00:23:02,706 - INFO - joeynmt.training - Epoch 272, Step:    93900, Batch Loss:     1.695112, Batch Acc: 0.581003, Tokens per Sec:    13775, Lr: 0.000029\n",
            "2023-01-20 00:23:10,576 - INFO - joeynmt.training - Epoch 272, Step:    94000, Batch Loss:     1.699978, Batch Acc: 0.585418, Tokens per Sec:    14003, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.32ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 5124.19ex/s]\n",
            "2023-01-20 00:23:11,016 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=94000\n",
            "2023-01-20 00:23:11,016 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:23:15,393 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:23:15,394 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:23:15,394 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:23:15,395 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:23:15,397 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.49, loss:   2.84, ppl:  17.17, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3397[sec], evaluation: 0.0344[sec]\n",
            "2023-01-20 00:23:15,400 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:23:15,403 - INFO - joeynmt.training - \tSource:     مردمی بسیار نزد او آمدند و گفتند : یحیی یک معجزه هم به ظهور نرساند ، اما هر چه در مورد این مرد گفت ، درست بود . \n",
            "2023-01-20 00:23:15,404 - INFO - joeynmt.training - \tReference:  چوخلو آدام اونون یانینا گلیب دئدی : یهیا هئچ بیر علامت گؤسترمهسه ده ، اونون بو آدام بارهده دئدیگی بۆتۆن سؤزلر دوغرو چێخدێ . \n",
            "2023-01-20 00:23:15,404 - INFO - joeynmt.training - \tHypothesis: خالق اونون یانینا گلیب دئدیلر : بیر آدام دا او بیری گؤستریب کی ، بو آدام بارهده دانێشماسێن . آمما بو آدام بارهده نه دئمکدیر .\n",
            "2023-01-20 00:23:15,404 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:23:15,406 - INFO - joeynmt.training - \tSource:     پیش از عید پسح بود . عیسی می‌دانست ساعت او رسیده است که این دنیا را ترک کند و نزد پدر برود و چون پیروانش را که در دنیا بودند ، دوست می‌داشت ، تا به آخر به ایشان محبت کرد . \n",
            "2023-01-20 00:23:15,406 - INFO - joeynmt.training - \tReference:  پاسخا بایرامی ارفسی ایدی . ایسا آرتێق بو دۆنیادان آیریلیب آتانێن یانینا گئدهجهگی مقامێن گلدیگینی بیلیردی . او ، دۆنیادا اؤزونه مخصوص اولانلارێ همیشه سئومیشدی وه آخێرا قدر ده سئودی . \n",
            "2023-01-20 00:23:15,406 - INFO - joeynmt.training - \tHypothesis: بایرامین پاسخا بایرامی یاخینلاشیردی . ایسا دۆنیانین یانیندان یانیندان چێخێب اونو آتا آناسێنێن یانینا گلدی کی ، دۆنیایا سئودی . دۆنیایا سئودیگی اۆچۆن اونلارێ سئویردی .\n",
            "2023-01-20 00:23:15,406 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:23:15,408 - INFO - joeynmt.training - \tSource:     و در این کتاب به یاد ابراهیم پرداز ، زیرا او پیامبری بسیار راستگوی بود . \n",
            "2023-01-20 00:23:15,409 - INFO - joeynmt.training - \tReference:   کیتابدا ابراهیمی ده یاد ائت . حقیقتا ، او ، بۆسبۆتۆن دوغرو دانێشان کیمسه بیر پیغمبر ایدی . \n",
            "2023-01-20 00:23:15,409 - INFO - joeynmt.training - \tHypothesis: کیتابدا ابراهیمه ده یادا سالدێ . چۆنکی او ، دوغرودان دا ، بیر پیغمبر ایدی .\n",
            "2023-01-20 00:23:15,409 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:23:15,411 - INFO - joeynmt.training - \tSource:     و ما او را به مقامی بلند ارتقا دادیم . \n",
            "2023-01-20 00:23:15,411 - INFO - joeynmt.training - \tReference:  بیز اونو یۆکسک بیر مقاما قالدێردێق . \n",
            "2023-01-20 00:23:15,411 - INFO - joeynmt.training - \tHypothesis: بیز اونو اوجالتدێق .\n",
            "2023-01-20 00:23:16,583 - INFO - joeynmt.training - Epoch 272: total training loss 563.04\n",
            "2023-01-20 00:23:16,583 - INFO - joeynmt.training - EPOCH 273\n",
            "2023-01-20 00:23:23,430 - INFO - joeynmt.training - Epoch 273, Step:    94100, Batch Loss:     1.610373, Batch Acc: 0.587282, Tokens per Sec:    14090, Lr: 0.000029\n",
            "2023-01-20 00:23:31,278 - INFO - joeynmt.training - Epoch 273, Step:    94200, Batch Loss:     1.696006, Batch Acc: 0.587371, Tokens per Sec:    13908, Lr: 0.000029\n",
            "2023-01-20 00:23:39,103 - INFO - joeynmt.training - Epoch 273, Step:    94300, Batch Loss:     1.526703, Batch Acc: 0.583674, Tokens per Sec:    14143, Lr: 0.000029\n",
            "2023-01-20 00:23:43,751 - INFO - joeynmt.training - Epoch 273: total training loss 560.46\n",
            "2023-01-20 00:23:43,752 - INFO - joeynmt.training - EPOCH 274\n",
            "2023-01-20 00:23:47,024 - INFO - joeynmt.training - Epoch 274, Step:    94400, Batch Loss:     1.660442, Batch Acc: 0.583691, Tokens per Sec:    13610, Lr: 0.000029\n",
            "2023-01-20 00:23:54,920 - INFO - joeynmt.training - Epoch 274, Step:    94500, Batch Loss:     1.486937, Batch Acc: 0.588617, Tokens per Sec:    13849, Lr: 0.000029\n",
            "2023-01-20 00:24:02,772 - INFO - joeynmt.training - Epoch 274, Step:    94600, Batch Loss:     1.711415, Batch Acc: 0.583585, Tokens per Sec:    14059, Lr: 0.000029\n",
            "2023-01-20 00:24:10,578 - INFO - joeynmt.training - Epoch 274, Step:    94700, Batch Loss:     1.757627, Batch Acc: 0.585246, Tokens per Sec:    14000, Lr: 0.000029\n",
            "2023-01-20 00:24:11,086 - INFO - joeynmt.training - Epoch 274: total training loss 564.02\n",
            "2023-01-20 00:24:11,086 - INFO - joeynmt.training - EPOCH 275\n",
            "2023-01-20 00:24:18,410 - INFO - joeynmt.training - Epoch 275, Step:    94800, Batch Loss:     1.666810, Batch Acc: 0.587106, Tokens per Sec:    14160, Lr: 0.000029\n",
            "2023-01-20 00:24:26,322 - INFO - joeynmt.training - Epoch 275, Step:    94900, Batch Loss:     1.564884, Batch Acc: 0.587677, Tokens per Sec:    13923, Lr: 0.000029\n",
            "2023-01-20 00:24:34,165 - INFO - joeynmt.training - Epoch 275, Step:    95000, Batch Loss:     1.537473, Batch Acc: 0.584785, Tokens per Sec:    14062, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.48ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10106.19ex/s]\n",
            "2023-01-20 00:24:34,441 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=95000\n",
            "2023-01-20 00:24:34,442 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:24:38,312 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:24:38,313 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:24:38,313 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:24:38,314 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:24:38,317 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.37, loss:   2.81, ppl:  16.57, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 3.8337[sec], evaluation: 0.0344[sec]\n",
            "2023-01-20 00:24:38,319 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:24:38,323 - INFO - joeynmt.training - \tSource:     و آنان را در کورهٔ آتش خواهند افکند ؛ جایی که در آن گریه خواهند کرد و دندان بر هم خواهند سایید . \n",
            "2023-01-20 00:24:38,323 - INFO - joeynmt.training - \tReference:  وه اونلارێ اودلو سوبایا آتاجاقلار . اورادا آغلاشما وه دیش قێجێرتێسێ اولاجاق . \n",
            "2023-01-20 00:24:38,323 - INFO - joeynmt.training - \tHypothesis: اونلارێ اوددا کور اولاجاق ، آغلاشما وه آغلاشما آتێلاجاق .\n",
            "2023-01-20 00:24:38,323 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:24:38,325 - INFO - joeynmt.training - \tSource:     و مانند موسی عمل نمی‌کنیم که پوششی بر چهرهٔ خود می‌کشید تا بنی‌اسرائیل به جلال آنچه از میان می‌رفت ، چشم ندوزند . \n",
            "2023-01-20 00:24:38,325 - INFO - joeynmt.training - \tReference:  اسراعل اؤؤلادلارێ اونون اۆزوندکی اؤتری احتشامین کئچیب گئتمهسینه باخماسێن دئیه اۆزۆنه نیگاب چکن موسا کیمی دئییلیک . \n",
            "2023-01-20 00:24:38,325 - INFO - joeynmt.training - \tHypothesis: موسا کیمی موسانێن نؤقسانسێز اولماغێنێزا نه قدر اؤز احتشامینی بیلمک اۆچۆن اسراعلده ایزتلندیرمهین گؤزونه گئتمهسین .\n",
            "2023-01-20 00:24:38,326 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:24:38,328 - INFO - joeynmt.training - \tSource:     آیا ندیده‌اند که خدا چگونه آفرینش را آغاز می‌کند سپس آن را باز می‌گرداند ؟ در حقیقت ، این کار بر خدا آسان است . \n",
            "2023-01-20 00:24:38,328 - INFO - joeynmt.training - \tReference:  مگر اونلار آللاهێن مخلوقاتێ اولجه نئجه یاراتدیغینی ، سونرا دا اونو یئنیدن دیریلدهجهیینی بیلمیرلرمی ؟ حقیقتا ، بو ، آللاه اۆچۆن آساندێر ! \n",
            "2023-01-20 00:24:38,328 - INFO - joeynmt.training - \tHypothesis: مگر آللاهێن یارادیلیشی یارادیلیشی گؤرمورسنمی ؟ سونرا اونو نئجه کی ، بو ایش اۆچۆن آساندێر .\n",
            "2023-01-20 00:24:38,328 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:24:38,330 - INFO - joeynmt.training - \tSource:     ای افعی‌زادگان ! چگونه می‌توانید سخن نیکو بگویید ، در حالی که خود شریرید ؟ زیرا زبان از آنچه دل از آن پر است ، سخن می‌گوید . \n",
            "2023-01-20 00:24:38,330 - INFO - joeynmt.training - \tReference:  ائی گۆرزهلر نسلی ، سیز پیس اولدوغونوز حالدا نئجه یاخشی شئیلر سؤیلهیه بیلرسینیز ؟ چۆنکی اۆرک دولولوغوندان آغێز دانێشار . \n",
            "2023-01-20 00:24:38,330 - INFO - joeynmt.training - \tHypothesis: ائی زئوولو ! سیزه نه سؤیلهییرسینیز ؟ چۆنکی دیللرده دانێشێرسێنێز ؟ چۆنکی اۆرهیینده دانێشێر . چۆنکی اۆرهیینده دانێشان بۆتلره دانێشێر .\n",
            "2023-01-20 00:24:42,374 - INFO - joeynmt.training - Epoch 275: total training loss 560.49\n",
            "2023-01-20 00:24:42,375 - INFO - joeynmt.training - EPOCH 276\n",
            "2023-01-20 00:24:46,186 - INFO - joeynmt.training - Epoch 276, Step:    95100, Batch Loss:     1.543199, Batch Acc: 0.589483, Tokens per Sec:    14037, Lr: 0.000029\n",
            "2023-01-20 00:24:54,061 - INFO - joeynmt.training - Epoch 276, Step:    95200, Batch Loss:     1.579306, Batch Acc: 0.586155, Tokens per Sec:    14107, Lr: 0.000029\n",
            "2023-01-20 00:25:01,992 - INFO - joeynmt.training - Epoch 276, Step:    95300, Batch Loss:     1.669373, Batch Acc: 0.585639, Tokens per Sec:    13805, Lr: 0.000029\n",
            "2023-01-20 00:25:09,634 - INFO - joeynmt.training - Epoch 276: total training loss 559.80\n",
            "2023-01-20 00:25:09,635 - INFO - joeynmt.training - EPOCH 277\n",
            "2023-01-20 00:25:09,956 - INFO - joeynmt.training - Epoch 277, Step:    95400, Batch Loss:     1.586754, Batch Acc: 0.603127, Tokens per Sec:    13569, Lr: 0.000029\n",
            "2023-01-20 00:25:17,877 - INFO - joeynmt.training - Epoch 277, Step:    95500, Batch Loss:     1.552859, Batch Acc: 0.589394, Tokens per Sec:    13782, Lr: 0.000029\n",
            "2023-01-20 00:25:28,728 - INFO - joeynmt.training - Epoch 277, Step:    95600, Batch Loss:     1.591910, Batch Acc: 0.585432, Tokens per Sec:    10157, Lr: 0.000029\n",
            "2023-01-20 00:25:36,575 - INFO - joeynmt.training - Epoch 277, Step:    95700, Batch Loss:     1.509368, Batch Acc: 0.584805, Tokens per Sec:    14199, Lr: 0.000029\n",
            "2023-01-20 00:25:39,805 - INFO - joeynmt.training - Epoch 277: total training loss 559.14\n",
            "2023-01-20 00:25:39,805 - INFO - joeynmt.training - EPOCH 278\n",
            "2023-01-20 00:25:44,470 - INFO - joeynmt.training - Epoch 278, Step:    95800, Batch Loss:     1.561444, Batch Acc: 0.585607, Tokens per Sec:    13978, Lr: 0.000029\n",
            "2023-01-20 00:25:52,364 - INFO - joeynmt.training - Epoch 278, Step:    95900, Batch Loss:     1.607897, Batch Acc: 0.586890, Tokens per Sec:    13879, Lr: 0.000029\n",
            "2023-01-20 00:26:00,169 - INFO - joeynmt.training - Epoch 278, Step:    96000, Batch Loss:     1.511165, Batch Acc: 0.586238, Tokens per Sec:    14060, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.98ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8655.77ex/s]\n",
            "2023-01-20 00:26:00,465 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=96000\n",
            "2023-01-20 00:26:00,465 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:26:05,497 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:26:05,497 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:26:05,497 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:26:05,498 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:26:05,501 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.50, loss:   2.82, ppl:  16.84, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9921[sec], evaluation: 0.0363[sec]\n",
            "2023-01-20 00:26:05,503 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:26:05,507 - INFO - joeynmt.training - \tSource:     و کاهل نشوید ، بلکه آنانی را سرمشق قرار دهید که وعده‌ها را از طریق ایمان و شکیبایی به میراث می‌برند . \n",
            "2023-01-20 00:26:05,508 - INFO - joeynmt.training - \tReference:  سیز تنبل اولمایاسینیز ، آمما ود اولونان شئیلری ایمان وه سبیرله ایرس آلانلاردان نمونه گؤتورهسینیز . \n",
            "2023-01-20 00:26:05,508 - INFO - joeynmt.training - \tHypothesis: کاشیل اولمایین ، اونلارا وه د اولوندوقلارینا گؤره ده ود ائدیلهنه ایمانلا صادق اولمایین .\n",
            "2023-01-20 00:26:05,508 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:26:05,511 - INFO - joeynmt.training - \tSource:     از طرف پولس که به خواست خدا رسول مسیحْ عیسی گشت تا وعدهٔ آن حیاتی را اعلام کند که از طریق مسیحْ عیسی میسر شده است ، \n",
            "2023-01-20 00:26:05,511 - INFO - joeynmt.training - \tReference:  مصیح اسعادا اولان هیات ودینه گؤره آللاهێن ارادهسی ایله مصیح ایسنانین هوارسی من پاولدان\n",
            "2023-01-20 00:26:05,511 - INFO - joeynmt.training - \tHypothesis: آللاهێن ارادهسی ایله مصیح اسعادا اولان مصیح اسعادا ود ائدیلمیش هیات سۆرمک اۆچۆن مصیح اسعادا اولان وه ود ائدیلمیش ود اولان مصیح اسعادا اولاراق اؤلدو .\n",
            "2023-01-20 00:26:05,511 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:26:05,514 - INFO - joeynmt.training - \tSource:     آنگاه به او گفتند : پس به ما بگو تو کیستی تا بتوانیم برای آنان که ما را فرستاده‌اند ، جوابی ببریم ؛ در مورد خود چه می‌گویی ؟ \n",
            "2023-01-20 00:26:05,514 - INFO - joeynmt.training - \tReference:  سونرا اونا دئدیلر : بس سن کیمسن ؟ بیزی گؤندهرنلره نه جاواب وئرک ؟ اؤزون بارهده نه دئییرسن ؟ \n",
            "2023-01-20 00:26:05,515 - INFO - joeynmt.training - \tHypothesis: اوندا ایسهآیا دئدیلر : بس سن ده بیزه دئ کی ، بیز ده سنین پیغمبرلیک سؤزلرینه ایناناق . بس بیز اؤزلری اۆچۆن سنه امر ائدیرسن ؟\n",
            "2023-01-20 00:26:05,516 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:26:05,519 - INFO - joeynmt.training - \tSource:     و به راستی الیاس از فرستادگان ما بود . \n",
            "2023-01-20 00:26:05,519 - INFO - joeynmt.training - \tReference:  حقیقتا ، ایلیاس دا پیغمبرلردندیر ! \n",
            "2023-01-20 00:26:05,519 - INFO - joeynmt.training - \tHypothesis: حقیقتا ، ائلچیلریمیزدن ایدی .\n",
            "2023-01-20 00:26:12,335 - INFO - joeynmt.training - Epoch 278: total training loss 558.16\n",
            "2023-01-20 00:26:12,336 - INFO - joeynmt.training - EPOCH 279\n",
            "2023-01-20 00:26:13,452 - INFO - joeynmt.training - Epoch 279, Step:    96100, Batch Loss:     1.527832, Batch Acc: 0.598551, Tokens per Sec:    13978, Lr: 0.000029\n",
            "2023-01-20 00:26:21,300 - INFO - joeynmt.training - Epoch 279, Step:    96200, Batch Loss:     1.621255, Batch Acc: 0.588535, Tokens per Sec:    14053, Lr: 0.000029\n",
            "2023-01-20 00:26:29,164 - INFO - joeynmt.training - Epoch 279, Step:    96300, Batch Loss:     1.633432, Batch Acc: 0.583845, Tokens per Sec:    14022, Lr: 0.000029\n",
            "2023-01-20 00:26:37,078 - INFO - joeynmt.training - Epoch 279, Step:    96400, Batch Loss:     1.524748, Batch Acc: 0.585013, Tokens per Sec:    14026, Lr: 0.000029\n",
            "2023-01-20 00:26:39,511 - INFO - joeynmt.training - Epoch 279: total training loss 556.30\n",
            "2023-01-20 00:26:39,512 - INFO - joeynmt.training - EPOCH 280\n",
            "2023-01-20 00:26:44,987 - INFO - joeynmt.training - Epoch 280, Step:    96500, Batch Loss:     1.544883, Batch Acc: 0.589533, Tokens per Sec:    14128, Lr: 0.000029\n",
            "2023-01-20 00:26:52,803 - INFO - joeynmt.training - Epoch 280, Step:    96600, Batch Loss:     1.547749, Batch Acc: 0.588533, Tokens per Sec:    13909, Lr: 0.000029\n",
            "2023-01-20 00:27:00,616 - INFO - joeynmt.training - Epoch 280, Step:    96700, Batch Loss:     1.529400, Batch Acc: 0.587092, Tokens per Sec:    14079, Lr: 0.000029\n",
            "2023-01-20 00:27:06,721 - INFO - joeynmt.training - Epoch 280: total training loss 559.83\n",
            "2023-01-20 00:27:06,721 - INFO - joeynmt.training - EPOCH 281\n",
            "2023-01-20 00:27:08,646 - INFO - joeynmt.training - Epoch 281, Step:    96800, Batch Loss:     1.567181, Batch Acc: 0.587487, Tokens per Sec:    13887, Lr: 0.000029\n",
            "2023-01-20 00:27:16,544 - INFO - joeynmt.training - Epoch 281, Step:    96900, Batch Loss:     1.559846, Batch Acc: 0.589186, Tokens per Sec:    13911, Lr: 0.000029\n",
            "2023-01-20 00:27:24,419 - INFO - joeynmt.training - Epoch 281, Step:    97000, Batch Loss:     1.656816, Batch Acc: 0.585267, Tokens per Sec:    13992, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.23ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9647.06ex/s]\n",
            "2023-01-20 00:27:24,701 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=97000\n",
            "2023-01-20 00:27:24,701 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:27:29,299 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:27:29,299 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:27:29,300 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:27:29,301 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:27:29,304 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.33, loss:   2.83, ppl:  16.87, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5599[sec], evaluation: 0.0352[sec]\n",
            "2023-01-20 00:27:29,306 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:27:29,309 - INFO - joeynmt.training - \tSource:     زیرا تصمیم داشتم در مسیر خود به مقدونیه از شما دیدار کنم و از مقدونیه باز نزدتان بیایم و سپس شما مرا از آنجا راهی یهودیه کنید . \n",
            "2023-01-20 00:27:29,310 - INFO - joeynmt.training - \tReference:  هم ماکئدونیایا گئدنده ، هم ده ماکئدونایادان قاییداندا یانینیزا گلمک نیتینده اولدوم . بوندان سونرا سیز منی یهودهیایا یولا سالمالێ ایدینیز . \n",
            "2023-01-20 00:27:29,310 - INFO - joeynmt.training - \tHypothesis: من اؤزومدن آسییییا ولایتنده سیزه مصلحت چکیب دولاشاراق اورادان چێخێب گئدهجهیم . سونرا اورادان دا سیزین اۆچۆن یهودیم .\n",
            "2023-01-20 00:27:29,310 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:27:29,312 - INFO - joeynmt.training - \tSource:     ای ابراهیم ! از این چون و چرا روی برتاب ، که فرمان پروردگارت آمده و برای آنان عذابی که بی‌بازگشت است خواهد آمد . \n",
            "2023-01-20 00:27:29,312 - INFO - joeynmt.training - \tReference:   ائی ابراهیم ! بوندان ال چک ، ربینین امری آرتێق گلمیشدیر . اونلارا مۆتلق قارشیسیآلینماز بیر اعذاب گلهجکدیر . \n",
            "2023-01-20 00:27:29,312 - INFO - joeynmt.training - \tHypothesis: ائی ابراهیم ! بو ، سنین ربینه ایتاعت ائتمهدیگی زامان اۆز چئویر . اونلارا بیر اعذاب گلمیشدیر .\n",
            "2023-01-20 00:27:29,312 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:27:29,314 - INFO - joeynmt.training - \tSource:     بقایای آنان را به شکیم بردند و در آرامگاهی دفن کردند که ابراهیم با پول نقره از پسران حمور در شکیم خریده بود . \n",
            "2023-01-20 00:27:29,315 - INFO - joeynmt.training - \tReference:  اونلارێن جسدلری سونرادان شکئمه گتیریلدی وه شکئمده ابراهیمین خامور اؤؤلادلاریندان بیر نئچه گۆمۆشه ساتێن آلدێغێ قبیرده دفن اولوندو . \n",
            "2023-01-20 00:27:29,315 - INFO - joeynmt.training - \tHypothesis: اونلارێ بزیلری آپاردێق . ابراهیمین بابلهتینی آپاردێلار کی ، لئویس قالێب مئلکیسئمرنای اوغلو لئمرنای اوغلو ایدی .\n",
            "2023-01-20 00:27:29,315 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:27:29,317 - INFO - joeynmt.training - \tSource:     سران قومش که تکبر می‌ورزیدند ، گفتند : ای شعیب ، یا تو و کسانی را که با تو ایمان آورده‌اند ، از شهر خودمان بیرون خواهیم کرد ؛ یا به کیش ما برگردید . گفت : آیا هر چند کراهت داشته باشیم ؟ \n",
            "2023-01-20 00:27:29,317 - INFO - joeynmt.training - \tReference:   تایفاسینین اؤزلرینه سێغیشدیرمایان تکبۆرلۆ ا یانلاری : ائی شۆعیب ! یا سنی وه سنینله بیرلیکده ایمان گتیرنلری مملهکتیمیزدن مۆتلق قوواجاغێق ، یا دا سیز بیزیم دینیمیزه دؤنهجکسینیز ! دئدیلر . بئله جاواب وئردی : نفرت ائتدیگیم حالدا بئلهمی ؟ \n",
            "2023-01-20 00:27:29,317 - INFO - joeynmt.training - \tHypothesis: تایفاسینین کافر ا یان اشیرته ائدهرک دئدیلر : ائی ربیمیز ! سندن ، ائی شهر ! بیزی جهد ائدنلردن چێخارتاجاغێق ، یاخود بیز هر شئییکتته سالاجاقسان ؟ اونلار : بو مۆدتدله بیرلیکده دؤؤؤرتوردونک ! دئدیک .\n",
            "2023-01-20 00:27:37,254 - INFO - joeynmt.training - Epoch 281, Step:    97100, Batch Loss:     1.590497, Batch Acc: 0.587864, Tokens per Sec:    13451, Lr: 0.000029\n",
            "2023-01-20 00:27:38,929 - INFO - joeynmt.training - Epoch 281: total training loss 556.95\n",
            "2023-01-20 00:27:38,929 - INFO - joeynmt.training - EPOCH 282\n",
            "2023-01-20 00:27:45,109 - INFO - joeynmt.training - Epoch 282, Step:    97200, Batch Loss:     1.596018, Batch Acc: 0.587990, Tokens per Sec:    14266, Lr: 0.000029\n",
            "2023-01-20 00:27:52,970 - INFO - joeynmt.training - Epoch 282, Step:    97300, Batch Loss:     1.664470, Batch Acc: 0.588730, Tokens per Sec:    14035, Lr: 0.000029\n",
            "2023-01-20 00:28:00,771 - INFO - joeynmt.training - Epoch 282, Step:    97400, Batch Loss:     1.757680, Batch Acc: 0.587035, Tokens per Sec:    14053, Lr: 0.000029\n",
            "2023-01-20 00:28:06,111 - INFO - joeynmt.training - Epoch 282: total training loss 556.13\n",
            "2023-01-20 00:28:06,112 - INFO - joeynmt.training - EPOCH 283\n",
            "2023-01-20 00:28:08,752 - INFO - joeynmt.training - Epoch 283, Step:    97500, Batch Loss:     1.629878, Batch Acc: 0.587802, Tokens per Sec:    14156, Lr: 0.000029\n",
            "2023-01-20 00:28:16,529 - INFO - joeynmt.training - Epoch 283, Step:    97600, Batch Loss:     1.663984, Batch Acc: 0.590262, Tokens per Sec:    14166, Lr: 0.000029\n",
            "2023-01-20 00:28:24,388 - INFO - joeynmt.training - Epoch 283, Step:    97700, Batch Loss:     1.569210, Batch Acc: 0.586274, Tokens per Sec:    13926, Lr: 0.000029\n",
            "2023-01-20 00:28:32,156 - INFO - joeynmt.training - Epoch 283, Step:    97800, Batch Loss:     1.596115, Batch Acc: 0.588600, Tokens per Sec:    14315, Lr: 0.000029\n",
            "2023-01-20 00:28:33,017 - INFO - joeynmt.training - Epoch 283: total training loss 556.15\n",
            "2023-01-20 00:28:33,017 - INFO - joeynmt.training - EPOCH 284\n",
            "2023-01-20 00:28:40,945 - INFO - joeynmt.training - Epoch 284, Step:    97900, Batch Loss:     1.544171, Batch Acc: 0.588177, Tokens per Sec:    12245, Lr: 0.000029\n",
            "2023-01-20 00:28:50,581 - INFO - joeynmt.training - Epoch 284, Step:    98000, Batch Loss:     1.494001, Batch Acc: 0.590116, Tokens per Sec:    11285, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.51ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9889.92ex/s]\n",
            "2023-01-20 00:28:50,870 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=98000\n",
            "2023-01-20 00:28:50,870 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:28:54,744 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:28:54,744 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:28:54,745 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:28:54,745 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:28:54,749 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.39, loss:   2.76, ppl:  15.80, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 3.8380[sec], evaluation: 0.0336[sec]\n",
            "2023-01-20 00:28:54,754 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:28:54,757 - INFO - joeynmt.training - \tSource:     علاوه بر این ، در میان افراد خارج از جماعت نیز نیکنام باشد تا مورد سرزنش قرار نگیرد و در دام ابلیس نیفتد . \n",
            "2023-01-20 00:28:54,757 - INFO - joeynmt.training - \tReference:  کنار اینسانلار ترهفیندن ده یاخشی بیر اینسان کیمی تانینمالیدیر . بئله کی ابلیسین قوردوغو تورا دۆشۆب اوتاندێرێلماسێن . \n",
            "2023-01-20 00:28:54,757 - INFO - joeynmt.training - \tHypothesis: بو ، جمیتده ده کیشیلر آراسێندان سئچمک اۆچۆن فعالیتلی بیر کیشی وار . او ، ابلیسین وارسا ، ایبلیسلرینه آند اولسون کی ، ابلیسین ایزتلی شکیلده یارادسین .\n",
            "2023-01-20 00:28:54,757 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:28:54,760 - INFO - joeynmt.training - \tSource:      خوشا به حال آنان که دلی پاک دارند ؛ زیرا خدا را خواهند دید . \n",
            "2023-01-20 00:28:54,760 - INFO - joeynmt.training - \tReference:  نه بختیاردیر اۆرهیی تمیز اولانلار ! چۆنکی اونلار آللاهێ گؤرهجک . \n",
            "2023-01-20 00:28:54,760 - INFO - joeynmt.training - \tHypothesis: نه بختیاردیر اونلار آللاهێن اۆرکلرینی پاکدێر ، چۆنکی آللاه گؤرهجک .\n",
            "2023-01-20 00:28:54,760 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:28:54,762 - INFO - joeynmt.training - \tSource:     بی‌شک کسی که بدی کند ، سزای عمل خود را خواهد دید ؛ زیرا خدا تبعیض قائل نمی‌شود . \n",
            "2023-01-20 00:28:54,762 - INFO - joeynmt.training - \tReference:  کیم حاقسێزلێق ائدرسه ، ایشلتدیگی حاقسێزلێغێن اوهزینی آلاجاق ، چۆنکی رب ترفکئشلیک ائتمز . \n",
            "2023-01-20 00:28:54,763 - INFO - joeynmt.training - \tHypothesis: اینسانا ائتدیکلری امللره گؤره آللاهسیزلێق ائدنلرین هامێسێ اونا قارشێدێر .\n",
            "2023-01-20 00:28:54,763 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:28:54,765 - INFO - joeynmt.training - \tSource:     اما فرزندی نداشتند ، چون الیزابت نازا بود و هر دو سالخورده بودند . \n",
            "2023-01-20 00:28:54,765 - INFO - joeynmt.training - \tReference:  آمما ائلیزآوئت سونسوز اولدوغونا گؤره اونلارێن اؤؤلادێ یوخ ایدی . هر ایکیسینین یاشی چوخ ایدی . \n",
            "2023-01-20 00:28:54,765 - INFO - joeynmt.training - \tHypothesis: آمما بیر دوغدو ، چۆنکی بیر دوغاندا ایکی ایل ارهگی دوتدو .\n",
            "2023-01-20 00:29:02,698 - INFO - joeynmt.training - Epoch 284, Step:    98100, Batch Loss:     1.670800, Batch Acc: 0.586695, Tokens per Sec:    13480, Lr: 0.000029\n",
            "2023-01-20 00:29:07,349 - INFO - joeynmt.training - Epoch 284: total training loss 559.26\n",
            "2023-01-20 00:29:07,350 - INFO - joeynmt.training - EPOCH 285\n",
            "2023-01-20 00:29:10,661 - INFO - joeynmt.training - Epoch 285, Step:    98200, Batch Loss:     1.630982, Batch Acc: 0.592825, Tokens per Sec:    13902, Lr: 0.000029\n",
            "2023-01-20 00:29:18,535 - INFO - joeynmt.training - Epoch 285, Step:    98300, Batch Loss:     1.599546, Batch Acc: 0.589962, Tokens per Sec:    14021, Lr: 0.000029\n",
            "2023-01-20 00:29:26,392 - INFO - joeynmt.training - Epoch 285, Step:    98400, Batch Loss:     1.658165, Batch Acc: 0.588249, Tokens per Sec:    14030, Lr: 0.000029\n",
            "2023-01-20 00:29:34,178 - INFO - joeynmt.training - Epoch 285, Step:    98500, Batch Loss:     1.624663, Batch Acc: 0.584219, Tokens per Sec:    13954, Lr: 0.000028\n",
            "2023-01-20 00:29:34,545 - INFO - joeynmt.training - Epoch 285: total training loss 557.22\n",
            "2023-01-20 00:29:34,545 - INFO - joeynmt.training - EPOCH 286\n",
            "2023-01-20 00:29:42,051 - INFO - joeynmt.training - Epoch 286, Step:    98600, Batch Loss:     1.592570, Batch Acc: 0.588288, Tokens per Sec:    14035, Lr: 0.000028\n",
            "2023-01-20 00:29:49,863 - INFO - joeynmt.training - Epoch 286, Step:    98700, Batch Loss:     1.546154, Batch Acc: 0.592360, Tokens per Sec:    14083, Lr: 0.000028\n",
            "2023-01-20 00:29:57,741 - INFO - joeynmt.training - Epoch 286, Step:    98800, Batch Loss:     1.664599, Batch Acc: 0.587856, Tokens per Sec:    13961, Lr: 0.000028\n",
            "2023-01-20 00:30:01,574 - INFO - joeynmt.training - Epoch 286: total training loss 555.00\n",
            "2023-01-20 00:30:01,574 - INFO - joeynmt.training - EPOCH 287\n",
            "2023-01-20 00:30:05,600 - INFO - joeynmt.training - Epoch 287, Step:    98900, Batch Loss:     1.451798, Batch Acc: 0.591887, Tokens per Sec:    13877, Lr: 0.000028\n",
            "2023-01-20 00:30:13,417 - INFO - joeynmt.training - Epoch 287, Step:    99000, Batch Loss:     1.753888, Batch Acc: 0.587218, Tokens per Sec:    14127, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.85ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9871.99ex/s]\n",
            "2023-01-20 00:30:13,713 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=99000\n",
            "2023-01-20 00:30:13,713 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:30:19,272 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:30:19,272 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:30:19,272 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:30:19,273 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:30:19,276 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.04, loss:   3.01, ppl:  20.24, acc:   0.40, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.5180[sec], evaluation: 0.0373[sec]\n",
            "2023-01-20 00:30:19,279 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:30:19,282 - INFO - joeynmt.training - \tSource:     حال برای آن که استوار بایستید ، کمربند حقیقت را به کمر ببندید ، سینه‌پوش عدالت را بر تن کنید\n",
            "2023-01-20 00:30:19,282 - INFO - joeynmt.training - \tReference:  بئلهلیکله ، حقیقتی کمر کیمی بئلینیزه باغلامێش ، سالئهلیگی زیرئه کیمی دؤشونوزه تاخمێش ، بارێشێق مۆژدسنی یایماق حاضرلێغێنێ آیاقلارینیزا گئیینمیش اولاراق یئرینیزده دورون . \n",
            "2023-01-20 00:30:19,283 - INFO - joeynmt.training - \tHypothesis: ایندی ایسه همیشهلی اولمایین . حقیقتا ، بونا گؤره ده عدالتلی اولمانێزێ گئییندیرین ،\n",
            "2023-01-20 00:30:19,283 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:30:19,285 - INFO - joeynmt.training - \tSource:     يا اين‌كه موهاي آن خانم\n",
            "2023-01-20 00:30:19,285 - INFO - joeynmt.training - \tReference:  یادا کی او خانیمین ساچ لاری\n",
            "2023-01-20 00:30:19,285 - INFO - joeynmt.training - \tHypothesis: هامی چاغی اولاندا کی اونو\n",
            "2023-01-20 00:30:19,285 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:30:19,287 - INFO - joeynmt.training - \tSource:     اما نتوانستند با حکمت او و روحی که او را در سخن گفتن هدایت می‌کرد ، مقابله کنند . \n",
            "2023-01-20 00:30:19,287 - INFO - joeynmt.training - \tReference:  آمما استئفانێن سؤزوندکی هیکمتین وه روحون قارشیسیندا دایانا بیلمهدیلر . \n",
            "2023-01-20 00:30:19,287 - INFO - joeynmt.training - \tHypothesis: آمما اونون هکمتی روحلا دولانمادێلار .\n",
            "2023-01-20 00:30:19,288 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:30:19,290 - INFO - joeynmt.training - \tSource:     پس ایشان را نزد والیان شهر حاضر ساختند و گفتند : این مردان نظم شهر ما را برهم زده‌اند . آنان یهودی‌اند\n",
            "2023-01-20 00:30:19,290 - INFO - joeynmt.training - \tReference:  اونلارێ حاکملرین قارشیسینا چێخاراراق دئدیلر : بو آداملار یهودیدیر . شهریمیزه قاریشیقلێق سالێبلار . \n",
            "2023-01-20 00:30:19,290 - INFO - joeynmt.training - \tHypothesis: اونلار ایسنانین یانینا قامچیلارینین یانینا گتیردیلر : بو شهرده یئردن کنارێندا اولان شهرلری یهودئیادان چؤلده ایدی .\n",
            "2023-01-20 00:30:27,190 - INFO - joeynmt.training - Epoch 287, Step:    99100, Batch Loss:     1.578057, Batch Acc: 0.591490, Tokens per Sec:    13365, Lr: 0.000028\n",
            "2023-01-20 00:30:34,652 - INFO - joeynmt.training - Epoch 287: total training loss 554.13\n",
            "2023-01-20 00:30:34,652 - INFO - joeynmt.training - EPOCH 288\n",
            "2023-01-20 00:30:35,135 - INFO - joeynmt.training - Epoch 288, Step:    99200, Batch Loss:     1.507183, Batch Acc: 0.578399, Tokens per Sec:    13705, Lr: 0.000028\n",
            "2023-01-20 00:30:42,948 - INFO - joeynmt.training - Epoch 288, Step:    99300, Batch Loss:     1.688197, Batch Acc: 0.591771, Tokens per Sec:    13985, Lr: 0.000028\n",
            "2023-01-20 00:30:50,789 - INFO - joeynmt.training - Epoch 288, Step:    99400, Batch Loss:     1.544284, Batch Acc: 0.588997, Tokens per Sec:    14145, Lr: 0.000028\n",
            "2023-01-20 00:30:58,611 - INFO - joeynmt.training - Epoch 288, Step:    99500, Batch Loss:     1.542108, Batch Acc: 0.590686, Tokens per Sec:    14002, Lr: 0.000028\n",
            "2023-01-20 00:31:01,781 - INFO - joeynmt.training - Epoch 288: total training loss 555.30\n",
            "2023-01-20 00:31:01,781 - INFO - joeynmt.training - EPOCH 289\n",
            "2023-01-20 00:31:06,555 - INFO - joeynmt.training - Epoch 289, Step:    99600, Batch Loss:     1.597201, Batch Acc: 0.588970, Tokens per Sec:    13728, Lr: 0.000028\n",
            "2023-01-20 00:31:14,369 - INFO - joeynmt.training - Epoch 289, Step:    99700, Batch Loss:     1.582584, Batch Acc: 0.593390, Tokens per Sec:    14171, Lr: 0.000028\n",
            "2023-01-20 00:31:22,215 - INFO - joeynmt.training - Epoch 289, Step:    99800, Batch Loss:     1.548539, Batch Acc: 0.587926, Tokens per Sec:    14033, Lr: 0.000028\n",
            "2023-01-20 00:31:28,921 - INFO - joeynmt.training - Epoch 289: total training loss 553.60\n",
            "2023-01-20 00:31:28,921 - INFO - joeynmt.training - EPOCH 290\n",
            "2023-01-20 00:31:30,102 - INFO - joeynmt.training - Epoch 290, Step:    99900, Batch Loss:     1.557248, Batch Acc: 0.597535, Tokens per Sec:    14019, Lr: 0.000028\n",
            "2023-01-20 00:31:37,876 - INFO - joeynmt.training - Epoch 290, Step:   100000, Batch Loss:     1.601591, Batch Acc: 0.591476, Tokens per Sec:    14193, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.36ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10366.87ex/s]\n",
            "2023-01-20 00:31:38,151 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=100000\n",
            "2023-01-20 00:31:38,151 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:31:42,612 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:31:42,613 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:31:42,613 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:31:42,614 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:31:42,617 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.70, loss:   2.86, ppl:  17.43, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4202[sec], evaluation: 0.0376[sec]\n",
            "2023-01-20 00:31:42,619 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:31:42,623 - INFO - joeynmt.training - \tSource:     و یاد کن‌ آنگاه که ابراهیم گفت : پروردگارا ، به من نشان ده ؛ چگونه مردگان را زنده می‌کنی ؟ فرمود : مگر ایمان نیاورده‌ای ؟ گفت : چرا ، ولی تا دلم آرامش یابد . فرمود : پس ، چهار پرنده برگیر ، و آنها را پیش خود ، ریز ریز گردان ؛ سپس بر هر کوهی پاره‌ای از آنها را قرار ده ؛ آنگاه آنها را فرا خوان ، شتابان به سوی تو می‌آیند ، و بدان که خداوند توانا و حکیم است . \n",
            "2023-01-20 00:31:42,624 - INFO - joeynmt.training - \tReference:   خاطرلا کی ، ابراهیم : ائی ربیم ، اؤلولهری نه جۆر دیریلتدیگینی منه گؤستر ! دئدیکده : مگر اینانمیرسان ؟ بویورموشدو . بلی ، اینانیرام ، لاکین اۆرهییم ساکت اولماق اۆچۆن ، دئیه جاواب وئرمیشدی . بویورموشدو : دؤرد جۆر قوش گؤتوروب اونلارا دقتله باخ ، سونرا هر داغێن باشێنا اونلاردان بیر پارچا آت ، سونرا اونلارێ چاغێر ، تئز یانینا گلهجکلر . بیل کی ، آللاه یئنیلمز غۆۆت ، هکمت صاحبدر ! \n",
            "2023-01-20 00:31:42,624 - INFO - joeynmt.training - \tHypothesis: او زامان ابراهیم بئله دئمیشدی : ائی ربیم ! منیمله بیرلیکده گؤرسنمی ؟ اونلار : ائی ایمانی اؤلوب ! نه اۆچۆن بیر آردێن ؟ دئدی . او : ائی ایمان گتیردیک ، نه ده اوغلانلار دا آرخایینلێقلا گئت ! سن اونلاردان کیمین اۆز دؤندر ، هۆکۆزهن ، ائی ده یئر اۆزۆنۆن اۆز دؤندرسه ، هۆزهر ! شبههسیز کی ، آللاه داغالێق ، هۆلۆکۆکۆک\n",
            "2023-01-20 00:31:42,624 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:31:42,626 - INFO - joeynmt.training - \tSource:      ای یوسف ، از این پیشامد روی بگردان . و تو ای زن‌ برای گناه خود آمرزش بخواه که تو از خطاکاران بوده‌ای . \n",
            "2023-01-20 00:31:42,627 - INFO - joeynmt.training - \tReference:  ائی یوصیف ! سن بو ایشی آچێب آغارتما . سن ده گۆناهێنا گؤره باغیشلانمانی دیله . چۆنکی سن ، حقیقتا ، گۆناه ائدنلردنسن . \n",
            "2023-01-20 00:31:42,627 - INFO - joeynmt.training - \tHypothesis: ائی یوصیفی ! بوندان اول ! سن گۆناهێن باغیشلانماسینی دیله . ائی گۆناهلاریندان ! سنه قارشێ گۆناهکار ایدی .\n",
            "2023-01-20 00:31:42,627 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:31:42,629 - INFO - joeynmt.training - \tSource:     پس ای منافقان ، آیا امید بستید که چون از خدا برگشتید یا سرپرست مردم شدید در روی‌ زمین فساد کنید و خویشاوندیهای خود را از هم بگسلید ؟ \n",
            "2023-01-20 00:31:42,629 - INFO - joeynmt.training - \tReference:  اگر اۆز دؤندرسنیز ، سیزدن یئر اۆزۆنده فیتنه فصاد تؤرتمک وه قوهوملوق تئللرینی قێرماق گؤزلهنیلمزمی \n",
            "2023-01-20 00:31:42,629 - INFO - joeynmt.training - \tHypothesis: بئلهلیکله ، ائی ایمان گتیرنلر ! مگر آللاهلا یاناشی ، یوخسا اینسانلاری یئر اۆزۆنده فیتنه فصاد تؤرهدنلره تکبۆر گؤسترین وه فیتنه فصاد تؤرهدین !\n",
            "2023-01-20 00:31:42,630 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:31:42,632 - INFO - joeynmt.training - \tSource:     عیسی دست مرد نابینا را گرفت و او را از روستا بیرون برد . سپس روی چشمان او آب دهان انداخت ، دست‌های خود را روی او گذاشت و پرسید : آیا چیزی می‌بینی ؟ \n",
            "2023-01-20 00:31:42,632 - INFO - joeynmt.training - \tReference:  ایسا کورون الیندن توتوب کندن کنارا آپاردێ . سونرا اونون گؤزلرینه تۆپۆرۆب اللهرینی اونون اۆزهرینه قویدو وه سوروشدو : بیر شئی گؤرورسنمی ؟ \n",
            "2023-01-20 00:31:42,632 - INFO - joeynmt.training - \tHypothesis: ایسا الینی اوزادێب اوشاغێ گؤتوروب یانینا آپاردێ . او ایسه گؤزلرینه توخوندو وه گؤزونو آچدێ . ایسا اونا دئدی : مگر گؤرنده او هئچ بیر شئی گؤرورسنمی ؟\n",
            "2023-01-20 00:31:50,427 - INFO - joeynmt.training - Epoch 290, Step:   100100, Batch Loss:     1.733517, Batch Acc: 0.589591, Tokens per Sec:    13507, Lr: 0.000028\n",
            "2023-01-20 00:31:58,410 - INFO - joeynmt.training - Epoch 290, Step:   100200, Batch Loss:     1.805456, Batch Acc: 0.588749, Tokens per Sec:    13764, Lr: 0.000028\n",
            "2023-01-20 00:32:00,973 - INFO - joeynmt.training - Epoch 290: total training loss 554.76\n",
            "2023-01-20 00:32:00,974 - INFO - joeynmt.training - EPOCH 291\n",
            "2023-01-20 00:32:09,585 - INFO - joeynmt.training - Epoch 291, Step:   100300, Batch Loss:     1.621802, Batch Acc: 0.592685, Tokens per Sec:     8709, Lr: 0.000028\n",
            "2023-01-20 00:32:17,504 - INFO - joeynmt.training - Epoch 291, Step:   100400, Batch Loss:     1.522358, Batch Acc: 0.591975, Tokens per Sec:    13983, Lr: 0.000028\n",
            "2023-01-20 00:32:25,536 - INFO - joeynmt.training - Epoch 291, Step:   100500, Batch Loss:     1.718056, Batch Acc: 0.586746, Tokens per Sec:    13651, Lr: 0.000028\n",
            "2023-01-20 00:32:31,665 - INFO - joeynmt.training - Epoch 291: total training loss 554.36\n",
            "2023-01-20 00:32:31,666 - INFO - joeynmt.training - EPOCH 292\n",
            "2023-01-20 00:32:33,463 - INFO - joeynmt.training - Epoch 292, Step:   100600, Batch Loss:     1.524127, Batch Acc: 0.597324, Tokens per Sec:    14226, Lr: 0.000028\n",
            "2023-01-20 00:32:41,255 - INFO - joeynmt.training - Epoch 292, Step:   100700, Batch Loss:     1.624894, Batch Acc: 0.593252, Tokens per Sec:    14210, Lr: 0.000028\n",
            "2023-01-20 00:32:49,171 - INFO - joeynmt.training - Epoch 292, Step:   100800, Batch Loss:     1.635893, Batch Acc: 0.591662, Tokens per Sec:    13851, Lr: 0.000028\n",
            "2023-01-20 00:32:56,963 - INFO - joeynmt.training - Epoch 292, Step:   100900, Batch Loss:     1.542146, Batch Acc: 0.586397, Tokens per Sec:    14099, Lr: 0.000028\n",
            "2023-01-20 00:32:58,723 - INFO - joeynmt.training - Epoch 292: total training loss 552.44\n",
            "2023-01-20 00:32:58,724 - INFO - joeynmt.training - EPOCH 293\n",
            "2023-01-20 00:33:04,951 - INFO - joeynmt.training - Epoch 293, Step:   101000, Batch Loss:     1.611767, Batch Acc: 0.594406, Tokens per Sec:    13952, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.12ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8520.02ex/s]\n",
            "2023-01-20 00:33:05,260 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=101000\n",
            "2023-01-20 00:33:05,261 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:33:09,901 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:33:09,901 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:33:09,902 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:33:09,902 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:33:09,906 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.08, loss:   2.86, ppl:  17.41, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6017[sec], evaluation: 0.0361[sec]\n",
            "2023-01-20 00:33:09,908 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:33:09,912 - INFO - joeynmt.training - \tSource:     تمام جمعیت شگفت‌زده شدند و گفتند : آیا ممکن است که او پسر داوود باشد ؟ \n",
            "2023-01-20 00:33:09,912 - INFO - joeynmt.training - \tReference:  بۆتۆن خالق بو ، داوودون اوغلو دئییلمی ؟ دئیه مات قالدێ . \n",
            "2023-01-20 00:33:09,912 - INFO - joeynmt.training - \tHypothesis: بۆتۆن ازدهام اونون هئیرته دۆشدۆ وه دئدی : داوودون اوغلودورمو ؟\n",
            "2023-01-20 00:33:09,912 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:33:09,915 - INFO - joeynmt.training - \tSource:     زیرا من خود از دیگران فرمان می‌برم و سربازانی هم زیر دست خود دارم . به یکی می‌گویم : برو ! می‌رود . به دیگری می‌گویم : بیا ! می‌آید و به غلام خود می‌گویم : این کار را انجام بده ! و او انجام می‌دهد . \n",
            "2023-01-20 00:33:09,915 - INFO - joeynmt.training - \tReference:  چۆنکی من ده تابئچیلیک آلتێندا اولان بیر آدامام ، منیم ده تابئچیلیگیمده اسگرلر وار . بیرینه گئت ! دئییرم ، او گئدیر ، دیگرینه ایسه گل ! دئییرم ، او دا گلیر . قولوما بونو ائت ! دئییرم ، او دا ائدیر . \n",
            "2023-01-20 00:33:09,915 - INFO - joeynmt.training - \tHypothesis: چۆنکی من اؤز اؤزوندن امر ائدیرم ، من ده بیرینه اوزوم . من بیرینه گئت ! دئییرم : گئت ، گئت ! بیر داها گلیرم ، او دا گلیرم ، باشقاسێ ایسه گلهنهم . او دا گلیر ، قولاغێمێ ائت ! دئییرم ، او دا اونا ائت .\n",
            "2023-01-20 00:33:09,915 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:33:09,917 - INFO - joeynmt.training - \tSource:     تا حالا دو شب است كه كاكا رستم براه شما بود . \n",
            "2023-01-20 00:33:09,917 - INFO - joeynmt.training - \tReference:  ایندی ایکی گئجه دیر کاکا رستم سیزی گؤزله بیر . \n",
            "2023-01-20 00:33:09,917 - INFO - joeynmt.training - \tHypothesis: نه قدر گئجه لریمی گونده کی اوتوزدور لاری دیر .\n",
            "2023-01-20 00:33:09,917 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:33:09,919 - INFO - joeynmt.training - \tSource:     سوگند به روشنایی روز ، \n",
            "2023-01-20 00:33:09,920 - INFO - joeynmt.training - \tReference:  آند اولسون سهره ؛ \n",
            "2023-01-20 00:33:09,920 - INFO - joeynmt.training - \tHypothesis: آند اولسون ایشیقلانما ؛\n",
            "2023-01-20 00:33:17,769 - INFO - joeynmt.training - Epoch 293, Step:   101100, Batch Loss:     1.563452, Batch Acc: 0.588534, Tokens per Sec:    13369, Lr: 0.000028\n",
            "2023-01-20 00:33:25,607 - INFO - joeynmt.training - Epoch 293, Step:   101200, Batch Loss:     1.598401, Batch Acc: 0.590621, Tokens per Sec:    14091, Lr: 0.000028\n",
            "2023-01-20 00:33:30,888 - INFO - joeynmt.training - Epoch 293: total training loss 551.61\n",
            "2023-01-20 00:33:30,888 - INFO - joeynmt.training - EPOCH 294\n",
            "2023-01-20 00:33:33,441 - INFO - joeynmt.training - Epoch 294, Step:   101300, Batch Loss:     1.698456, Batch Acc: 0.588770, Tokens per Sec:    14386, Lr: 0.000028\n",
            "2023-01-20 00:33:41,244 - INFO - joeynmt.training - Epoch 294, Step:   101400, Batch Loss:     1.621571, Batch Acc: 0.595845, Tokens per Sec:    14110, Lr: 0.000028\n",
            "2023-01-20 00:33:49,065 - INFO - joeynmt.training - Epoch 294, Step:   101500, Batch Loss:     1.566297, Batch Acc: 0.594670, Tokens per Sec:    13957, Lr: 0.000028\n",
            "2023-01-20 00:33:56,894 - INFO - joeynmt.training - Epoch 294, Step:   101600, Batch Loss:     1.618424, Batch Acc: 0.586314, Tokens per Sec:    13994, Lr: 0.000028\n",
            "2023-01-20 00:33:57,948 - INFO - joeynmt.training - Epoch 294: total training loss 552.14\n",
            "2023-01-20 00:33:57,948 - INFO - joeynmt.training - EPOCH 295\n",
            "2023-01-20 00:34:04,756 - INFO - joeynmt.training - Epoch 295, Step:   101700, Batch Loss:     1.665510, Batch Acc: 0.594294, Tokens per Sec:    14010, Lr: 0.000028\n",
            "2023-01-20 00:34:12,731 - INFO - joeynmt.training - Epoch 295, Step:   101800, Batch Loss:     1.680483, Batch Acc: 0.588171, Tokens per Sec:    13891, Lr: 0.000028\n",
            "2023-01-20 00:34:20,632 - INFO - joeynmt.training - Epoch 295, Step:   101900, Batch Loss:     1.612141, Batch Acc: 0.590127, Tokens per Sec:    13995, Lr: 0.000028\n",
            "2023-01-20 00:34:25,260 - INFO - joeynmt.training - Epoch 295: total training loss 552.38\n",
            "2023-01-20 00:34:25,261 - INFO - joeynmt.training - EPOCH 296\n",
            "2023-01-20 00:34:28,549 - INFO - joeynmt.training - Epoch 296, Step:   102000, Batch Loss:     1.623927, Batch Acc: 0.592888, Tokens per Sec:    14051, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.09ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9627.74ex/s]\n",
            "2023-01-20 00:34:28,838 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=102000\n",
            "2023-01-20 00:34:28,839 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:34:33,678 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:34:33,678 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:34:33,678 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:34:33,679 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:34:33,682 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.21, loss:   2.87, ppl:  17.68, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7986[sec], evaluation: 0.0373[sec]\n",
            "2023-01-20 00:34:33,685 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:34:33,689 - INFO - joeynmt.training - \tSource:     همچنین اگر شیطان بر ضد خود قیام کند و در سلطنتش تفرقه بیفتد ، او نمی‌تواند دوام بیاورد و پایان کارش خواهد رسید . \n",
            "2023-01-20 00:34:33,689 - INFO - joeynmt.training - \tReference:  شیطان دا اؤزونه قارشێ قالخێب اؤز اؤزونه بؤلونوبسه ، داوام گتیرمز ، یالنیز آخێرێ چاتار . \n",
            "2023-01-20 00:34:33,689 - INFO - joeynmt.training - \tHypothesis: اگر شیطان الاهیه قطل ائتمک ایستهسه ، اونو ایتیرسه ، اونا یاتان وه ایکی ایشیق گتیره بیلمز .\n",
            "2023-01-20 00:34:33,689 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:34:33,691 - INFO - joeynmt.training - \tSource:     آری ، خشنودی خدا در این بود که شکوه و ارزش این راز مقدس را بر مقدسان آشکار سازد ، چنان که در میان غیریهودیان نیز آشکار می‌شود . آن راز این است که مسیح در اتحاد با شماست ، یعنی امید دارید که در جلال او شریک شوید . \n",
            "2023-01-20 00:34:33,691 - INFO - joeynmt.training - \tReference:  آللاه ملتلر آراسێندا بو سیررین ایزهتینین نئجه زنگین اولدوغونو مۆقدسلره بیلدیرمک ایستهدی . بو سیرر سیزده اولان مصیحدر . او سیزه ایزته قوووشماق امیدینی وئریر . \n",
            "2023-01-20 00:34:33,692 - INFO - joeynmt.training - \tHypothesis: چۆنکی آللاهێن ارادهسینی رازێ سالان مۆقدسلری ایزتلندیریب . بئله کی مصیحده اولان سیررینی گؤستریب . سیزده اولان مسیحین ایزهتینین ایزتلندیرسین .\n",
            "2023-01-20 00:34:33,692 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:34:33,694 - INFO - joeynmt.training - \tSource:     به این ترتیب ، آخرین‌ها اولین خواهند بود و اولین‌ها آخرین . \n",
            "2023-01-20 00:34:33,694 - INFO - joeynmt.training - \tReference:  بئلهجه ده آخێرێنجێلار بیرینجی ، بیرینجیلر ایسه آخێرێنجێ اولاجاق . \n",
            "2023-01-20 00:34:33,694 - INFO - joeynmt.training - \tHypothesis: بیرینجیلری بیرینجی یئرده اولانلارێن هامیسینین آخێرێنجێ اولدو .\n",
            "2023-01-20 00:34:33,694 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:34:33,696 - INFO - joeynmt.training - \tSource:     اگر لایق باشند ، صلحی که آرزو کرده‌اید ، نصیبشان می‌شود ، اما اگر لایق نباشند ، صلحی که برای آنان آرزو نموده‌اید ، به شما بازمی‌گردد . \n",
            "2023-01-20 00:34:33,696 - INFO - joeynmt.training - \tReference:  اگر ائو لایقدیرسه ، امین آمانلیغینیز اونون اۆزهرینه گلسین ؛ لایق دئییلسه ، امین آمانلیغینیز اؤزونوزه قاییتسین . \n",
            "2023-01-20 00:34:33,697 - INFO - joeynmt.training - \tHypothesis: اگر سۆلح قایناغی اولانلار ، سیزه وئررسه ، اونلارێن ایزین وئریلهجک ، آمما اونلارا سۆلح اولسون .\n",
            "2023-01-20 00:34:41,689 - INFO - joeynmt.training - Epoch 296, Step:   102100, Batch Loss:     1.517127, Batch Acc: 0.593968, Tokens per Sec:    13197, Lr: 0.000028\n",
            "2023-01-20 00:34:49,616 - INFO - joeynmt.training - Epoch 296, Step:   102200, Batch Loss:     1.619003, Batch Acc: 0.589922, Tokens per Sec:    14043, Lr: 0.000028\n",
            "2023-01-20 00:34:57,461 - INFO - joeynmt.training - Epoch 296, Step:   102300, Batch Loss:     1.555317, Batch Acc: 0.593883, Tokens per Sec:    13915, Lr: 0.000028\n",
            "2023-01-20 00:34:57,755 - INFO - joeynmt.training - Epoch 296: total training loss 550.67\n",
            "2023-01-20 00:34:57,756 - INFO - joeynmt.training - EPOCH 297\n",
            "2023-01-20 00:35:05,410 - INFO - joeynmt.training - Epoch 297, Step:   102400, Batch Loss:     1.670303, Batch Acc: 0.591322, Tokens per Sec:    13996, Lr: 0.000028\n",
            "2023-01-20 00:35:13,285 - INFO - joeynmt.training - Epoch 297, Step:   102500, Batch Loss:     1.565483, Batch Acc: 0.592921, Tokens per Sec:    13852, Lr: 0.000028\n",
            "2023-01-20 00:35:21,154 - INFO - joeynmt.training - Epoch 297, Step:   102600, Batch Loss:     1.517595, Batch Acc: 0.589814, Tokens per Sec:    13990, Lr: 0.000028\n",
            "2023-01-20 00:35:25,010 - INFO - joeynmt.training - Epoch 297: total training loss 550.28\n",
            "2023-01-20 00:35:25,011 - INFO - joeynmt.training - EPOCH 298\n",
            "2023-01-20 00:35:31,121 - INFO - joeynmt.training - Epoch 298, Step:   102700, Batch Loss:     1.595149, Batch Acc: 0.600387, Tokens per Sec:     9465, Lr: 0.000028\n",
            "2023-01-20 00:35:39,889 - INFO - joeynmt.training - Epoch 298, Step:   102800, Batch Loss:     1.572528, Batch Acc: 0.591797, Tokens per Sec:    12449, Lr: 0.000028\n",
            "2023-01-20 00:35:47,796 - INFO - joeynmt.training - Epoch 298, Step:   102900, Batch Loss:     1.554813, Batch Acc: 0.590323, Tokens per Sec:    13676, Lr: 0.000028\n",
            "2023-01-20 00:35:55,251 - INFO - joeynmt.training - Epoch 298: total training loss 551.52\n",
            "2023-01-20 00:35:55,251 - INFO - joeynmt.training - EPOCH 299\n",
            "2023-01-20 00:35:55,737 - INFO - joeynmt.training - Epoch 299, Step:   103000, Batch Loss:     1.646777, Batch Acc: 0.590051, Tokens per Sec:    13432, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 125.29ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9374.04ex/s]\n",
            "2023-01-20 00:35:56,021 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=103000\n",
            "2023-01-20 00:35:56,024 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:36:00,904 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:36:00,904 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:36:00,905 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:36:00,905 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:36:00,908 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.03, loss:   2.87, ppl:  17.57, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.8300[sec], evaluation: 0.0383[sec]\n",
            "2023-01-20 00:36:00,911 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:36:00,915 - INFO - joeynmt.training - \tSource:     پطرس در مقابل به او گفت : ما همه چیز را رها کرده‌ایم و از تو پیروی می‌کنیم ؛ پس چه چیز عاید ما خواهد شد ؟ \n",
            "2023-01-20 00:36:00,915 - INFO - joeynmt.training - \tReference:  بوندان سونرا پئتئر جاواب وئرهرک اونا دئدی : باخ بیز هر شئیی قویوب سنین آردێنجا گلمیشیک ، اوهزینده نییمیز اولاجاق ؟ \n",
            "2023-01-20 00:36:00,915 - INFO - joeynmt.training - \tHypothesis: پئتئر اونا گؤره ده هامێمێزا دئدی : هر شئیی قویوب سنین آردێنجا گئدهجهییک ؟ بس اوندا نیه باخاجاق ؟\n",
            "2023-01-20 00:36:00,916 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:36:00,918 - INFO - joeynmt.training - \tSource:     خدای رحمان که بر عرش استیلا یافته است . \n",
            "2023-01-20 00:36:00,918 - INFO - joeynmt.training - \tReference:  رهمان ارشی یارادیب هؤکمو آلتێنا آلمێشدێر . \n",
            "2023-01-20 00:36:00,918 - INFO - joeynmt.training - \tHypothesis: رهمانێن ان اوجادێر .\n",
            "2023-01-20 00:36:00,918 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:36:00,921 - INFO - joeynmt.training - \tSource:     وقتی شما را به حضور مردم ، مأموران حکومتی و صاحب‌منصبان ببرند ، نگران مشوید که چگونه از خود دفاع کنید یا چه بگویید ؛ \n",
            "2023-01-20 00:36:00,921 - INFO - joeynmt.training - \tReference:  سیزی سیناقوقلارا ، باشچیلارین وه هؤکمدارلارێن قارشیسینا گتیرنده اؤزونوزو نئجه مۆدافه ائدهجهیینیز وه یاخود نه دئیهجهیینیز بارهده قایغی چکمهگین . \n",
            "2023-01-20 00:36:00,921 - INFO - joeynmt.training - \tHypothesis: سیزی اینسانلارا قارشێ مۆحافظهچیلره وه منه قارشێ چێخانلارێنا دقتله باخێن ، اؤزونه نه دئیه چؤرهیهگین . نه دئیهرک نئجه دئیین .\n",
            "2023-01-20 00:36:00,921 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:36:00,923 - INFO - joeynmt.training - \tSource:     تا پروردگارش را خواند که : من مغلوب شدم ؛ به داد من برس ! \n",
            "2023-01-20 00:36:00,924 - INFO - joeynmt.training - \tReference:   ربینه دوعا ائدیب : من مغلوب اولدوم ، بونا گؤره ده انتقام آل ! دئدی . \n",
            "2023-01-20 00:36:00,924 - INFO - joeynmt.training - \tHypothesis: ربینه دوعا ائتمک امر اولونانلارا دئیهجک .\n",
            "2023-01-20 00:36:08,883 - INFO - joeynmt.training - Epoch 299, Step:   103100, Batch Loss:     1.507997, Batch Acc: 0.595230, Tokens per Sec:    13257, Lr: 0.000028\n",
            "2023-01-20 00:36:16,778 - INFO - joeynmt.training - Epoch 299, Step:   103200, Batch Loss:     1.593031, Batch Acc: 0.593685, Tokens per Sec:    13914, Lr: 0.000028\n",
            "2023-01-20 00:36:24,568 - INFO - joeynmt.training - Epoch 299, Step:   103300, Batch Loss:     1.675935, Batch Acc: 0.589181, Tokens per Sec:    14150, Lr: 0.000028\n",
            "2023-01-20 00:36:27,763 - INFO - joeynmt.training - Epoch 299: total training loss 550.02\n",
            "2023-01-20 00:36:27,764 - INFO - joeynmt.training - EPOCH 300\n",
            "2023-01-20 00:36:32,498 - INFO - joeynmt.training - Epoch 300, Step:   103400, Batch Loss:     1.606074, Batch Acc: 0.594884, Tokens per Sec:    13867, Lr: 0.000028\n",
            "2023-01-20 00:36:40,385 - INFO - joeynmt.training - Epoch 300, Step:   103500, Batch Loss:     1.591187, Batch Acc: 0.595404, Tokens per Sec:    13977, Lr: 0.000028\n",
            "2023-01-20 00:36:48,216 - INFO - joeynmt.training - Epoch 300, Step:   103600, Batch Loss:     1.559166, Batch Acc: 0.588398, Tokens per Sec:    13916, Lr: 0.000028\n",
            "2023-01-20 00:36:55,118 - INFO - joeynmt.training - Epoch 300: total training loss 551.86\n",
            "2023-01-20 00:36:55,119 - INFO - joeynmt.training - EPOCH 301\n",
            "2023-01-20 00:36:56,167 - INFO - joeynmt.training - Epoch 301, Step:   103700, Batch Loss:     1.803870, Batch Acc: 0.593249, Tokens per Sec:    13624, Lr: 0.000028\n",
            "2023-01-20 00:37:04,068 - INFO - joeynmt.training - Epoch 301, Step:   103800, Batch Loss:     1.650385, Batch Acc: 0.593237, Tokens per Sec:    13936, Lr: 0.000028\n",
            "2023-01-20 00:37:12,035 - INFO - joeynmt.training - Epoch 301, Step:   103900, Batch Loss:     1.692143, Batch Acc: 0.594603, Tokens per Sec:    13684, Lr: 0.000028\n",
            "2023-01-20 00:37:19,913 - INFO - joeynmt.training - Epoch 301, Step:   104000, Batch Loss:     1.579891, Batch Acc: 0.591537, Tokens per Sec:    13932, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.86ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10277.28ex/s]\n",
            "2023-01-20 00:37:20,183 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=104000\n",
            "2023-01-20 00:37:20,183 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:37:24,814 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:37:24,814 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:37:24,814 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:37:24,815 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:37:24,818 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.07, loss:   2.74, ppl:  15.48, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5910[sec], evaluation: 0.0368[sec]\n",
            "2023-01-20 00:37:24,821 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:37:24,825 - INFO - joeynmt.training - \tSource:     و هیچ چیز مانع پذیرفته شدن انفاقهای آنان نشد جز اینکه به خدا و پیامبرش کفر ورزیدند ، و جز با حال‌ کسالت نماز به جا نمی‌آورند ، و جز با کراهت انفاق نمی‌کنند . \n",
            "2023-01-20 00:37:24,825 - INFO - joeynmt.training - \tReference:  اونلارێن خرجلهدیکلرینین قبول اولونماسێنا مانع اولان یالنیز آللاهێ وه اونون پیغمبرینی اینکار ائتمهلری ، نامازا تنبل تنبل گلمهلری وه ایستهمهیه ایستهمهیه خرجلهمهلریدیر . \n",
            "2023-01-20 00:37:24,825 - INFO - joeynmt.training - \tHypothesis: اونلارا وئردیگیمیز هئچ بیر شئی وئرمز ، یالنیز آللاها وه پیغمبرینه ایتاعت ائتمهدن باشقا بیر شئی ایستهمهیهیه بیلدیلر . ناماز قێلمایانلار ، ذکات وئرمزلر .\n",
            "2023-01-20 00:37:24,825 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:37:24,827 - INFO - joeynmt.training - \tSource:     و قطعا قرآن را برای پندآموزی آسان کرده‌ایم ، پس آیا پندگیرنده‌ای هست ؟ \n",
            "2023-01-20 00:37:24,827 - INFO - joeynmt.training - \tReference:  آند اولسون کی ، بیز قور آنێ ابرت آلماق اۆچۆن بئله آسانلاشدێردێق . آمما هئچ بیر ابرت آلان وارمێ \n",
            "2023-01-20 00:37:24,828 - INFO - joeynmt.training - \tHypothesis: آند اولسون کی ، بیز قور آنێ ابرت آلاندادێب ابرت آلان وارمێ\n",
            "2023-01-20 00:37:24,828 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:37:24,830 - INFO - joeynmt.training - \tSource:     و حال آنکه‌ پیش از آن ، کتاب موسی ، راهبر و مایه‌ رحمتی بود ؛ و این قرآن‌ کتابی است به زبان عربی که تصدیق‌کننده آن‌ است ، تا کسانی را که ستم کرده‌اند هشدار دهد و برای نیکوکاران مژده‌ای باشد . \n",
            "2023-01-20 00:37:24,830 - INFO - joeynmt.training - \tReference:  اوندان اول موسانێن رحبر وه مرحمت اولان کیتابی وار ایدی . بو عرب دیلینده تصدیق ائدن ، زالیملاری قورخوتماق وه یاخشی امل صاحبلرینه مۆژده وئرمک اۆچۆن اولان بیر کیتابدیر ! \n",
            "2023-01-20 00:37:24,830 - INFO - joeynmt.training - \tHypothesis: حالبوکی بوندان اول کیتابی وه مرحمتیمیزی تصدیق ائدن کیمسهلردیر . بو کیتابدا یازیلانلار ، بلکه ، ظۆلم ائدنلره مۆژده وئرنلر . حقیقتا ، یاخشی ایشلر گؤرنلر اۆچۆن مۆژده وئرنلر !\n",
            "2023-01-20 00:37:24,831 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:37:24,832 - INFO - joeynmt.training - \tSource:     آیا فرمانروایی آسمانها و زمین و آنچه میان آن دو است از آن ایشان است ؟ اگر چنین است‌ پس با چنگ زدن‌ در آن اسباب به بالا روند . \n",
            "2023-01-20 00:37:24,833 - INFO - joeynmt.training - \tReference:  یاخود گؤیلرین ، یئرین وه اونلارێن آراسێندا اولان هر شئین اختیاری اونلارێن الیندهدیر ائله ایسه قوی ایپلردن یاپیشیب قالخسێنلار ! \n",
            "2023-01-20 00:37:24,833 - INFO - joeynmt.training - \tHypothesis: مگر گؤیلرین وه یئرین هؤکمو اونون اختیاریندادیر . اونلار اونلارێن آراسێندان دؤشهیدیلر . اگر بئله اولسایدی ، اونلار اورادان چێخارماق اۆچۆن بئله مۆهورلوب .\n",
            "2023-01-20 00:37:27,477 - INFO - joeynmt.training - Epoch 301: total training loss 549.99\n",
            "2023-01-20 00:37:27,478 - INFO - joeynmt.training - EPOCH 302\n",
            "2023-01-20 00:37:32,854 - INFO - joeynmt.training - Epoch 302, Step:   104100, Batch Loss:     1.687973, Batch Acc: 0.591893, Tokens per Sec:    13796, Lr: 0.000028\n",
            "2023-01-20 00:37:40,731 - INFO - joeynmt.training - Epoch 302, Step:   104200, Batch Loss:     1.569420, Batch Acc: 0.595251, Tokens per Sec:    14003, Lr: 0.000028\n",
            "2023-01-20 00:37:48,526 - INFO - joeynmt.training - Epoch 302, Step:   104300, Batch Loss:     1.623897, Batch Acc: 0.594107, Tokens per Sec:    14101, Lr: 0.000028\n",
            "2023-01-20 00:37:54,689 - INFO - joeynmt.training - Epoch 302: total training loss 548.37\n",
            "2023-01-20 00:37:54,689 - INFO - joeynmt.training - EPOCH 303\n",
            "2023-01-20 00:37:56,387 - INFO - joeynmt.training - Epoch 303, Step:   104400, Batch Loss:     1.601722, Batch Acc: 0.599755, Tokens per Sec:    13968, Lr: 0.000028\n",
            "2023-01-20 00:38:04,186 - INFO - joeynmt.training - Epoch 303, Step:   104500, Batch Loss:     1.685956, Batch Acc: 0.595405, Tokens per Sec:    14104, Lr: 0.000028\n",
            "2023-01-20 00:38:12,033 - INFO - joeynmt.training - Epoch 303, Step:   104600, Batch Loss:     1.764793, Batch Acc: 0.593897, Tokens per Sec:    14038, Lr: 0.000028\n",
            "2023-01-20 00:38:19,822 - INFO - joeynmt.training - Epoch 303, Step:   104700, Batch Loss:     1.480599, Batch Acc: 0.592300, Tokens per Sec:    14038, Lr: 0.000028\n",
            "2023-01-20 00:38:21,728 - INFO - joeynmt.training - Epoch 303: total training loss 549.07\n",
            "2023-01-20 00:38:21,728 - INFO - joeynmt.training - EPOCH 304\n",
            "2023-01-20 00:38:27,755 - INFO - joeynmt.training - Epoch 304, Step:   104800, Batch Loss:     1.572935, Batch Acc: 0.595620, Tokens per Sec:    13899, Lr: 0.000028\n",
            "2023-01-20 00:38:35,709 - INFO - joeynmt.training - Epoch 304, Step:   104900, Batch Loss:     1.689244, Batch Acc: 0.594398, Tokens per Sec:    13754, Lr: 0.000028\n",
            "2023-01-20 00:38:43,593 - INFO - joeynmt.training - Epoch 304, Step:   105000, Batch Loss:     1.641645, Batch Acc: 0.592163, Tokens per Sec:    13873, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.26ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9840.89ex/s]\n",
            "2023-01-20 00:38:43,884 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=105000\n",
            "2023-01-20 00:38:43,885 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:38:50,257 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:38:50,267 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:38:50,267 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:38:50,268 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:38:50,273 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.24, loss:   2.89, ppl:  18.08, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 6.2954[sec], evaluation: 0.0800[sec]\n",
            "2023-01-20 00:38:50,279 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:38:50,282 - INFO - joeynmt.training - \tSource:     پس ، یوسف برخاست و شب‌هنگام با کودک و مادر او عازم مصر شد . \n",
            "2023-01-20 00:38:50,283 - INFO - joeynmt.training - \tReference:  بئلهلیکله ، یوصیف قالخدێ ، همین گئجه کؤرپه ایله آناسێنێ گؤتوروب میصیره یوللاندی . \n",
            "2023-01-20 00:38:50,283 - INFO - joeynmt.training - \tHypothesis: یوصیف قالخێب کؤرپه ایله گئجهنین آتاسێنا وه آناسێنێ گؤتوردو .\n",
            "2023-01-20 00:38:50,283 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:38:50,286 - INFO - joeynmt.training - \tSource:     آن ۲۴ پیر در مقابل آن تخت‌نشین زانو می‌زنند و او را که همیشه و تا ابد زنده است ، پرستش می‌کنند . آنگاه تاج خود را پیش آن تخت می‌اندازند و می‌گویند : \n",
            "2023-01-20 00:38:50,289 - INFO - joeynmt.training - \tReference:  ایییرمی دؤرد آغساققال تاختدا اوتورانێن اؤنونده یئره قاپانێب ابدی وار اولانا سجده ائدیر وه تاجلارێنێ تاختێن اؤنونه آتێب دئییرلر : \n",
            "2023-01-20 00:38:50,289 - INFO - joeynmt.training - \tHypothesis: ایییرمی دؤرد آغساقققال وه قادێنێن ایییرمی دؤرد آغساقال وه ابدی اولاراق عزت تاختێن اؤنونده دورارلار . اونلار اؤز تاختێنێ بئله دئییرلر :\n",
            "2023-01-20 00:38:50,289 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:38:50,292 - INFO - joeynmt.training - \tSource:     به خدا و فرستاده او بگروید و در راه خدا با مال و جانتان جهاد کنید . این گذشت و فداکاری‌ اگر بدانید ، برای شما بهتر است . \n",
            "2023-01-20 00:38:50,292 - INFO - joeynmt.training - \tReference:   آللاها وه اونون پیغمبرینه ایمان گتیرهسینیز . آللاه یولوندا مالێنێز وه جانێنێزلا ووروشاسێنێز . بیلسهنیز ، بو سیزین اۆچۆن نه قدر خئیرلیدیر ! \n",
            "2023-01-20 00:38:50,292 - INFO - joeynmt.training - \tHypothesis: آللاه اونون پیغمبرینه وه اونون پیغمبرینی اؤزونوزه اولان آللاه یولوندا مال دؤؤؤلتمهیین . بو ، بیر آز بیر شئیدیر . اگر بیلهسینیزهسینیزه ، بو سیزین اۆچۆن داها یاخشیدیر !\n",
            "2023-01-20 00:38:50,292 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:38:50,297 - INFO - joeynmt.training - \tSource:     و چون پیمانه می‌کنید ، پیمانه را تمام دهید ، و با ترازوی درست بسنجید که این بهتر و خوش فرجام‌تر است . \n",
            "2023-01-20 00:38:50,297 - INFO - joeynmt.training - \tReference:  اؤلچنده اؤلچوده دۆز اولون ، دۆزگۆن ترهزی ایله چکین . بو داها خئیرلی وه نتیجه ائ تباریله داها یاخشیدیر ! \n",
            "2023-01-20 00:38:50,297 - INFO - joeynmt.training - \tHypothesis: اهدی پوزاندا اؤلچو ایله اؤلمهنیز ده ، داها یاخشی وه ان گؤزل ترزده داها یاخشیدیر . بو ، تۆکنۆکۆدۆر !\n",
            "2023-01-20 00:38:57,134 - INFO - joeynmt.training - Epoch 304: total training loss 550.75\n",
            "2023-01-20 00:38:57,134 - INFO - joeynmt.training - EPOCH 305\n",
            "2023-01-20 00:38:59,412 - INFO - joeynmt.training - Epoch 305, Step:   105100, Batch Loss:     1.605977, Batch Acc: 0.598712, Tokens per Sec:    14114, Lr: 0.000028\n",
            "2023-01-20 00:39:07,215 - INFO - joeynmt.training - Epoch 305, Step:   105200, Batch Loss:     1.454166, Batch Acc: 0.596463, Tokens per Sec:    14039, Lr: 0.000028\n",
            "2023-01-20 00:39:15,065 - INFO - joeynmt.training - Epoch 305, Step:   105300, Batch Loss:     1.620967, Batch Acc: 0.592501, Tokens per Sec:    13955, Lr: 0.000028\n",
            "2023-01-20 00:39:22,907 - INFO - joeynmt.training - Epoch 305, Step:   105400, Batch Loss:     1.608406, Batch Acc: 0.595844, Tokens per Sec:    14151, Lr: 0.000028\n",
            "2023-01-20 00:39:24,195 - INFO - joeynmt.training - Epoch 305: total training loss 546.02\n",
            "2023-01-20 00:39:24,195 - INFO - joeynmt.training - EPOCH 306\n",
            "2023-01-20 00:39:30,799 - INFO - joeynmt.training - Epoch 306, Step:   105500, Batch Loss:     1.538719, Batch Acc: 0.598353, Tokens per Sec:    14165, Lr: 0.000028\n",
            "2023-01-20 00:39:38,601 - INFO - joeynmt.training - Epoch 306, Step:   105600, Batch Loss:     1.545613, Batch Acc: 0.591756, Tokens per Sec:    14069, Lr: 0.000028\n",
            "2023-01-20 00:39:46,359 - INFO - joeynmt.training - Epoch 306, Step:   105700, Batch Loss:     1.558026, Batch Acc: 0.598399, Tokens per Sec:    14264, Lr: 0.000028\n",
            "2023-01-20 00:39:51,126 - INFO - joeynmt.training - Epoch 306: total training loss 545.19\n",
            "2023-01-20 00:39:51,127 - INFO - joeynmt.training - EPOCH 307\n",
            "2023-01-20 00:39:54,293 - INFO - joeynmt.training - Epoch 307, Step:   105800, Batch Loss:     1.587248, Batch Acc: 0.600545, Tokens per Sec:    13909, Lr: 0.000027\n",
            "2023-01-20 00:40:02,198 - INFO - joeynmt.training - Epoch 307, Step:   105900, Batch Loss:     1.662381, Batch Acc: 0.594526, Tokens per Sec:    13890, Lr: 0.000027\n",
            "2023-01-20 00:40:10,228 - INFO - joeynmt.training - Epoch 307, Step:   106000, Batch Loss:     1.611666, Batch Acc: 0.594357, Tokens per Sec:    13643, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.85ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10417.99ex/s]\n",
            "2023-01-20 00:40:10,492 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=106000\n",
            "2023-01-20 00:40:10,493 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:40:15,241 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:40:15,242 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:40:15,242 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:40:15,243 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:40:15,246 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.35, loss:   2.83, ppl:  16.92, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7108[sec], evaluation: 0.0356[sec]\n",
            "2023-01-20 00:40:15,249 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:40:15,252 - INFO - joeynmt.training - \tSource:     خدا و رسول را فرمان برید ، باشد که مشمول رحمت قرار گیرید . \n",
            "2023-01-20 00:40:15,252 - INFO - joeynmt.training - \tReference:  آللاها وه پیغمبره ایتاعت ائدین کی ، بلکه ، باغێشلانمێش اولاسێنێز ! \n",
            "2023-01-20 00:40:15,252 - INFO - joeynmt.training - \tHypothesis: آللاه امری دۆزگۆنه ایتاعت ائدین !\n",
            "2023-01-20 00:40:15,252 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:40:15,254 - INFO - joeynmt.training - \tSource:     دیدگانشان فرو افتاده ، غبار مذلت آنان را فرو گرفته است . این است همان روزی که به ایشان وعده داده می‌شد . \n",
            "2023-01-20 00:40:15,255 - INFO - joeynmt.training - \tReference:  اونلارێن گؤزلری زلیلجهسینه یئره دیکیلهجک ، اؤزلرینی ده ذلت بۆرویهجکدیر . بو اونلارا وه د اولونموش همین قیامت گۆنۆدۆر ! \n",
            "2023-01-20 00:40:15,255 - INFO - joeynmt.training - \tHypothesis: گؤزلری زلیلجهسینه دۆشۆب اونلارێ بۆرۆدۆ . بو ، اونلارا وه د اولوندو . اونلارا وئریلهجکدیر .\n",
            "2023-01-20 00:40:15,255 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:40:15,257 - INFO - joeynmt.training - \tSource:     اگر زنی ایماندار خویشاوندانی بیوه دارد ، باید به آنان کمک کند تا سربار جماعت نباشند . به این ترتیب ، جماعت می‌تواند به بیوه‌زنانی که واقعا بی‌کس و نیازمندند ، کمک کند . \n",
            "2023-01-20 00:40:15,257 - INFO - joeynmt.training - \tReference:  ایمهانلی بیر قادێنێن دول قادێن یاخینلاری وارسا ، قوی اونلارا یاردیم ائتسین . ایمهنلیلار جمیتی بئله یۆکۆن آلتێنا گیرمهسین کی ، حقیقی دول قادێنلارا یاردیم ائده بیلسین . \n",
            "2023-01-20 00:40:15,257 - INFO - joeynmt.training - \tHypothesis: اگر ایمانسیزلێقدا ائولهننن قادێنلارا بئت آنیاداکی جمیته تام مۆدرکلیکله ایمانسیزلێق ائتسین . بئلهجه ، اونلار جمیته احتیاج یوخدور .\n",
            "2023-01-20 00:40:15,257 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:40:15,259 - INFO - joeynmt.training - \tSource:     اما مریم مجدلیه و آن مریم دیگر همچنان در آنجا در مقابل مقبره نشستند . \n",
            "2023-01-20 00:40:15,259 - INFO - joeynmt.training - \tReference:  مجدللی مریم وه او بیری مریم اورادا ، قبیرین قارشیسیندا اوتورموشدولار . \n",
            "2023-01-20 00:40:15,259 - INFO - joeynmt.training - \tHypothesis: مریم ایسه او بیری مجدللی مریم وه او بیری قبیرین قارشیسیندا اوتوردو .\n",
            "2023-01-20 00:40:23,156 - INFO - joeynmt.training - Epoch 307, Step:   106100, Batch Loss:     1.583158, Batch Acc: 0.595163, Tokens per Sec:    13420, Lr: 0.000027\n",
            "2023-01-20 00:40:23,683 - INFO - joeynmt.training - Epoch 307: total training loss 547.57\n",
            "2023-01-20 00:40:23,684 - INFO - joeynmt.training - EPOCH 308\n",
            "2023-01-20 00:40:31,021 - INFO - joeynmt.training - Epoch 308, Step:   106200, Batch Loss:     1.485127, Batch Acc: 0.599055, Tokens per Sec:    13820, Lr: 0.000027\n",
            "2023-01-20 00:40:38,883 - INFO - joeynmt.training - Epoch 308, Step:   106300, Batch Loss:     1.705518, Batch Acc: 0.591411, Tokens per Sec:    13867, Lr: 0.000027\n",
            "2023-01-20 00:40:46,768 - INFO - joeynmt.training - Epoch 308, Step:   106400, Batch Loss:     1.507727, Batch Acc: 0.594299, Tokens per Sec:    13967, Lr: 0.000027\n",
            "2023-01-20 00:40:50,957 - INFO - joeynmt.training - Epoch 308: total training loss 548.08\n",
            "2023-01-20 00:40:50,957 - INFO - joeynmt.training - EPOCH 309\n",
            "2023-01-20 00:40:54,655 - INFO - joeynmt.training - Epoch 309, Step:   106500, Batch Loss:     1.540003, Batch Acc: 0.595514, Tokens per Sec:    14001, Lr: 0.000027\n",
            "2023-01-20 00:41:02,488 - INFO - joeynmt.training - Epoch 309, Step:   106600, Batch Loss:     1.540507, Batch Acc: 0.598190, Tokens per Sec:    14055, Lr: 0.000027\n",
            "2023-01-20 00:41:10,401 - INFO - joeynmt.training - Epoch 309, Step:   106700, Batch Loss:     1.571862, Batch Acc: 0.592916, Tokens per Sec:    13829, Lr: 0.000027\n",
            "2023-01-20 00:41:18,333 - INFO - joeynmt.training - Epoch 309: total training loss 547.21\n",
            "2023-01-20 00:41:18,334 - INFO - joeynmt.training - EPOCH 310\n",
            "2023-01-20 00:41:18,414 - INFO - joeynmt.training - Epoch 310, Step:   106800, Batch Loss:     1.665374, Batch Acc: 0.576138, Tokens per Sec:    15894, Lr: 0.000027\n",
            "2023-01-20 00:41:26,400 - INFO - joeynmt.training - Epoch 310, Step:   106900, Batch Loss:     1.575627, Batch Acc: 0.596520, Tokens per Sec:    13903, Lr: 0.000027\n",
            "2023-01-20 00:41:34,265 - INFO - joeynmt.training - Epoch 310, Step:   107000, Batch Loss:     1.618581, Batch Acc: 0.596246, Tokens per Sec:    14140, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.04ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9167.99ex/s]\n",
            "2023-01-20 00:41:34,559 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=107000\n",
            "2023-01-20 00:41:34,559 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:41:39,864 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:41:39,864 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:41:39,865 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:41:39,865 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:41:39,868 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.30, loss:   2.91, ppl:  18.39, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.2635[sec], evaluation: 0.0384[sec]\n",
            "2023-01-20 00:41:39,871 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:41:39,874 - INFO - joeynmt.training - \tSource:     آن دو گفتند : پروردگارا ، ما می‌ترسیم که او آسیبی به ما برساند یا آنکه سرکشی کند . \n",
            "2023-01-20 00:41:39,875 - INFO - joeynmt.training - \tReference:  اونلار : ائی ربیمیز ! بیزه شدتلی جزا وئرمهسیندن وه یا آزغێنلاشاراق هدینی آشماسێندان قورخوروق ! دئدیلر . \n",
            "2023-01-20 00:41:39,875 - INFO - joeynmt.training - \tHypothesis: اونلار دئدیلر : ائی ربیمیز ! بیزدن قورخارێق کی ، اوندان قورخان بیر تایفایا وه یاخود باشیمیزا دۆشر .\n",
            "2023-01-20 00:41:39,875 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:41:39,877 - INFO - joeynmt.training - \tSource:     کسی که بر بام است ، پایین نیاید و به خانه‌اش داخل نشود تا چیزی از آنجا بردارد . \n",
            "2023-01-20 00:41:39,877 - INFO - joeynmt.training - \tReference:  دامدا اولان ائویندن نه ایسه گؤتورمک اۆچۆن آشاغێ ائنیب ائوینه گیرمهسین . \n",
            "2023-01-20 00:41:39,877 - INFO - joeynmt.training - \tHypothesis: هئچ کیم اۆستۆنه قاپێدان ، ائوه گیرمهین وه ائوه گیرمک اۆچۆن ایچری گیرمهیهجک .\n",
            "2023-01-20 00:41:39,877 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:41:39,879 - INFO - joeynmt.training - \tSource:     ای کسانی که ایمان آورده‌اید ، چون با گروهی برخورد می‌کنید پایداری ورزید و خدا را بسیار یاد کنید ، باشد که رستگار شوید . \n",
            "2023-01-20 00:41:39,880 - INFO - joeynmt.training - \tReference:  ائی ایمان گتیرنلر ! بیر دسته ایله اۆز اۆزه گلدیکده مۆحکم اولون وه آللاهێ چوخ یادا سالێن کی ، نجات تاپاسێنێز ! \n",
            "2023-01-20 00:41:39,880 - INFO - joeynmt.training - \tHypothesis: ائی ایمان گتیرنلر ! دسته دسته آللاهێن دؤزومهسی ایله دورمایین . آللاهێن چوخ زیکر ائدین کی ، نجات تاپاسێنێز !\n",
            "2023-01-20 00:41:39,880 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:41:39,882 - INFO - joeynmt.training - \tSource:     بلکه چنان که می‌دانید ، هرچند نخست در فیلیپی مورد آزار و اهانت قرار گرفتیم ، به کمک خدای خود شهامت یافتیم تا با وجود مخالفت‌های بسیار ، بشارت خدا را به شما برسانیم . \n",
            "2023-01-20 00:41:39,882 - INFO - joeynmt.training - \tReference:  بیلدیگینیز کیمی ، اولجه فیلیپیده ازیت چکمیش ، تحقیر اولونموشدوق . آمما بؤیوک زدیتلهیتلره باخمایاراق ، آللاهێن مۆژدسنی سیزه بیان ائتمک اۆچۆن آللاهیمیزدان جصارت آلدێق . \n",
            "2023-01-20 00:41:39,883 - INFO - joeynmt.training - \tHypothesis: آمما بیلین کی ، فیلیپی ایلک دفهدن واسطمع آلاراق ، هر بیر طقیب اولوندوق . بیز آللاهێن مۆژدسنی یایماق اۆچۆن وزیگتده سیزی سێناقدان کئچیریب .\n",
            "2023-01-20 00:41:47,864 - INFO - joeynmt.training - Epoch 310, Step:   107100, Batch Loss:     1.488169, Batch Acc: 0.593730, Tokens per Sec:    13096, Lr: 0.000027\n",
            "2023-01-20 00:41:51,314 - INFO - joeynmt.training - Epoch 310: total training loss 543.50\n",
            "2023-01-20 00:41:51,314 - INFO - joeynmt.training - EPOCH 311\n",
            "2023-01-20 00:41:55,860 - INFO - joeynmt.training - Epoch 311, Step:   107200, Batch Loss:     1.592556, Batch Acc: 0.596867, Tokens per Sec:    13931, Lr: 0.000027\n",
            "2023-01-20 00:42:03,879 - INFO - joeynmt.training - Epoch 311, Step:   107300, Batch Loss:     1.565545, Batch Acc: 0.597927, Tokens per Sec:    13812, Lr: 0.000027\n",
            "2023-01-20 00:42:13,828 - INFO - joeynmt.training - Epoch 311, Step:   107400, Batch Loss:     1.536043, Batch Acc: 0.596114, Tokens per Sec:    11045, Lr: 0.000027\n",
            "2023-01-20 00:42:21,805 - INFO - joeynmt.training - Epoch 311: total training loss 542.52\n",
            "2023-01-20 00:42:21,805 - INFO - joeynmt.training - EPOCH 312\n",
            "2023-01-20 00:42:22,837 - INFO - joeynmt.training - Epoch 312, Step:   107500, Batch Loss:     1.565030, Batch Acc: 0.601175, Tokens per Sec:    13038, Lr: 0.000027\n",
            "2023-01-20 00:42:30,880 - INFO - joeynmt.training - Epoch 312, Step:   107600, Batch Loss:     1.578408, Batch Acc: 0.596660, Tokens per Sec:    13550, Lr: 0.000027\n",
            "2023-01-20 00:42:38,988 - INFO - joeynmt.training - Epoch 312, Step:   107700, Batch Loss:     1.567339, Batch Acc: 0.597791, Tokens per Sec:    13578, Lr: 0.000027\n",
            "2023-01-20 00:42:47,095 - INFO - joeynmt.training - Epoch 312, Step:   107800, Batch Loss:     1.710441, Batch Acc: 0.596049, Tokens per Sec:    13578, Lr: 0.000027\n",
            "2023-01-20 00:42:49,821 - INFO - joeynmt.training - Epoch 312: total training loss 547.90\n",
            "2023-01-20 00:42:49,821 - INFO - joeynmt.training - EPOCH 313\n",
            "2023-01-20 00:42:55,070 - INFO - joeynmt.training - Epoch 313, Step:   107900, Batch Loss:     1.588647, Batch Acc: 0.598331, Tokens per Sec:    13746, Lr: 0.000027\n",
            "2023-01-20 00:43:02,935 - INFO - joeynmt.training - Epoch 313, Step:   108000, Batch Loss:     1.627096, Batch Acc: 0.596520, Tokens per Sec:    13952, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 126.07ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9735.03ex/s]\n",
            "2023-01-20 00:43:03,214 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=108000\n",
            "2023-01-20 00:43:03,215 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:43:08,297 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:43:08,297 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:43:08,297 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:43:08,298 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:43:08,301 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.79, loss:   2.80, ppl:  16.44, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0429[sec], evaluation: 0.0362[sec]\n",
            "2023-01-20 00:43:08,304 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:43:08,307 - INFO - joeynmt.training - \tSource:     و قطعا برای او در پیشگاه ما تقرب و فرجام نیکوست . \n",
            "2023-01-20 00:43:08,307 - INFO - joeynmt.training - \tReference:  حقیقتا ، او ، درگاهێمێزا یاخین اولاجاق وه اونون قاییدیب گلهجهگی یئر ده گؤزل اولاجاقدێر . \n",
            "2023-01-20 00:43:08,307 - INFO - joeynmt.training - \tHypothesis: شبههسیز کی ، او ، بیزیم اۆچۆن بیر یئردهدیر . عاقبتینین نه گؤزلدیر !\n",
            "2023-01-20 00:43:08,308 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:43:08,310 - INFO - joeynmt.training - \tSource:      کشتی‌ زیر نظر ما روان بود . این‌ پاداش کسی بود که مورد انکار واقع شده بود . \n",
            "2023-01-20 00:43:08,310 - INFO - joeynmt.training - \tReference:   اینکار ائدیلمیش کیمسهیه مۆکافات اولاراق وئریلن گؤزوموزون قاباغێندا اۆزۆب گئدیردی . \n",
            "2023-01-20 00:43:08,310 - INFO - joeynmt.training - \tHypothesis: گمی اونلارێن بوغو آلتێندان چایلاریدیر . بو ، هئچ کسه اینکار ائدیلنلره گؤره کافر ایدی .\n",
            "2023-01-20 00:43:08,310 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:43:08,312 - INFO - joeynmt.training - \tSource:     و او کسی است که آسمانها و زمین را به حق آفرید ، و هر گاه که می‌گوید : باش ، بی‌درنگ موجود شود ؛ سخنش راست است ؛ و روزی که در صور دمیده شود ، فرمانروایی از آن اوست ؛ داننده غیب و شهود است ؛ و اوست حکیم آگاه . \n",
            "2023-01-20 00:43:08,312 - INFO - joeynmt.training - \tReference:  گؤیلری وه یئری حاقق اولاراق یارادان اودور . اونون : اول ! دئیهجهگی گۆن درحال اولار . اونون سؤزو حاقدێر . سورون چالیناجاغی گۆن هؤکم اونوندور . غیبی وه آشکارێ بیلن ده اودور . او ، هکمت صاحبدر ، خبرداردێر ! \n",
            "2023-01-20 00:43:08,313 - INFO - joeynmt.training - \tHypothesis: گؤیلری وه یئری حاقق اولاراق یاراتدیغینیزی وه اوندان آیی اولونان کیمسهیه : اول ، حاققێ وه ساغلامدا دا اونوندور . او ، قیامت گۆنۆنده ان گؤزل روزی وئرندیر . او ، غیبی هکمت صاحبدر !\n",
            "2023-01-20 00:43:08,313 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:43:08,315 - INFO - joeynmt.training - \tSource:     و چون کسانی که به آیات ما ایمان دارند ، نزد تو آیند ، بگو : درود بر شما ، پروردگارتان رحمت را بر خود مقرر کرده که هر کس از شما به نادانی کار بدی کند و آنگاه به توبه و صلاح آید ، پس وی آمرزنده مهربان است . \n",
            "2023-01-20 00:43:08,315 - INFO - joeynmt.training - \tReference:  آیهلریمیزه ایمان گتیرنلر یانینا گلدیکده اونلارا دئ : سیزه سالام اولسون ! ربینیز اؤزو اؤزونه رحملی اولماغێ یازمیشدیر کی ، سیزلردن هر کس پیس ایش گؤرسه ، سونرا تؤؤبه ائدیب دۆزلسه . شبههسیز کی ، آللاه باغیشلایاندیر ، رحم ائدندیر ! \n",
            "2023-01-20 00:43:08,315 - INFO - joeynmt.training - \tHypothesis: آیهلریمیزی سنه ایمان گتیردیکلری زامان : ربینیزه سیزه ربینیزدن بیر مرحمت گلمیش کیمی سیزه ربینیزدن بیر مرحمت بخش ائتسهنیز ، پیس ایشدن چکینسه ، اونو تؤؤبه ائدر . بو ، تؤؤبه ائدیب باغێشلانماسینی دیله . حقیقتا ، باغیشلایاندیر ، رحم ائدندیر !\n",
            "2023-01-20 00:43:16,301 - INFO - joeynmt.training - Epoch 313, Step:   108100, Batch Loss:     1.603038, Batch Acc: 0.596105, Tokens per Sec:    13137, Lr: 0.000027\n",
            "2023-01-20 00:43:22,650 - INFO - joeynmt.training - Epoch 313: total training loss 544.99\n",
            "2023-01-20 00:43:22,650 - INFO - joeynmt.training - EPOCH 314\n",
            "2023-01-20 00:43:24,241 - INFO - joeynmt.training - Epoch 314, Step:   108200, Batch Loss:     1.484827, Batch Acc: 0.599871, Tokens per Sec:    13699, Lr: 0.000027\n",
            "2023-01-20 00:43:32,149 - INFO - joeynmt.training - Epoch 314, Step:   108300, Batch Loss:     1.544378, Batch Acc: 0.599108, Tokens per Sec:    13812, Lr: 0.000027\n",
            "2023-01-20 00:43:40,077 - INFO - joeynmt.training - Epoch 314, Step:   108400, Batch Loss:     1.631437, Batch Acc: 0.595638, Tokens per Sec:    13864, Lr: 0.000027\n",
            "2023-01-20 00:43:48,075 - INFO - joeynmt.training - Epoch 314, Step:   108500, Batch Loss:     1.638146, Batch Acc: 0.596333, Tokens per Sec:    13832, Lr: 0.000027\n",
            "2023-01-20 00:43:50,166 - INFO - joeynmt.training - Epoch 314: total training loss 544.33\n",
            "2023-01-20 00:43:50,167 - INFO - joeynmt.training - EPOCH 315\n",
            "2023-01-20 00:43:55,938 - INFO - joeynmt.training - Epoch 315, Step:   108600, Batch Loss:     1.582486, Batch Acc: 0.601356, Tokens per Sec:    13825, Lr: 0.000027\n",
            "2023-01-20 00:44:03,858 - INFO - joeynmt.training - Epoch 315, Step:   108700, Batch Loss:     1.601782, Batch Acc: 0.597103, Tokens per Sec:    13920, Lr: 0.000027\n",
            "2023-01-20 00:44:11,787 - INFO - joeynmt.training - Epoch 315, Step:   108800, Batch Loss:     1.599561, Batch Acc: 0.597657, Tokens per Sec:    13877, Lr: 0.000027\n",
            "2023-01-20 00:44:17,665 - INFO - joeynmt.training - Epoch 315: total training loss 546.13\n",
            "2023-01-20 00:44:17,665 - INFO - joeynmt.training - EPOCH 316\n",
            "2023-01-20 00:44:19,779 - INFO - joeynmt.training - Epoch 316, Step:   108900, Batch Loss:     1.580538, Batch Acc: 0.596378, Tokens per Sec:    14112, Lr: 0.000027\n",
            "2023-01-20 00:44:27,714 - INFO - joeynmt.training - Epoch 316, Step:   109000, Batch Loss:     1.591610, Batch Acc: 0.597166, Tokens per Sec:    13921, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.07ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8641.69ex/s]\n",
            "2023-01-20 00:44:28,018 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=109000\n",
            "2023-01-20 00:44:28,019 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:44:32,335 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:44:32,335 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:44:32,335 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:44:32,336 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:44:32,339 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.09, loss:   2.84, ppl:  17.13, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2777[sec], evaluation: 0.0359[sec]\n",
            "2023-01-20 00:44:32,342 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:44:32,346 - INFO - joeynmt.training - \tSource:     عیسی به او گفت : من هنوز نزد پدر بالا نرفته‌ام ، پس دیگر بر من میاویز ، بلکه نزد برادرانم برو و به آنان بگو : من نزد پدر خود و پدر شما و خدای خود و خدای شما بالا می‌روم . \n",
            "2023-01-20 00:44:32,346 - INFO - joeynmt.training - \tReference:  ایسا اونا دئدی : منه توخونما ، چۆنکی من هله آتانێن یانینا قالخمامێشام . آمما قارداشلارێمێن یانینا گئدیب اونلارا سؤیله : اؤز آتامێن سیزین آتانێزێن ، اؤز آللاهێمێن سیزین آللاهینیزین یانینا قالخێرام . \n",
            "2023-01-20 00:44:32,346 - INFO - joeynmt.training - \tHypothesis: ایسا اونا دئدی : من هله آتامێن یانینا گئدیردیم ، آرتێق آرتێق مندن قورخمادان سونرا اونلارا دئدی : قارداشلار ، من سیزین آتانێزا ، من سیزین آتانێزێم ، سیز ده آللاهێمدێر .\n",
            "2023-01-20 00:44:32,346 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:44:32,348 - INFO - joeynmt.training - \tSource:     با محبت برادرانه یکدیگر را از صمیم دل دوست داشته باشید . در حرمت گذاشتن به یکدیگر پیشقدم شوید . \n",
            "2023-01-20 00:44:32,348 - INFO - joeynmt.training - \tReference:  قارداشلێق سئوگیسی ایله بیر بیرینیزی سئوین . هؤرمت گؤسترمکده بیر بیرینیزی قاباقلایین . \n",
            "2023-01-20 00:44:32,349 - INFO - joeynmt.training - \tHypothesis: محبت محبتله بیر بیر بیرینیزی سئوهرک اۆرکدن هؤرمت ائدین .\n",
            "2023-01-20 00:44:32,349 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:44:32,351 - INFO - joeynmt.training - \tSource:     گفتم‌ : خوابت‌ پرید ! \n",
            "2023-01-20 00:44:32,351 - INFO - joeynmt.training - \tReference:  دئدیم : ائله بیل یاتمیشدین آییلدین هه ! \n",
            "2023-01-20 00:44:32,351 - INFO - joeynmt.training - \tHypothesis: دئدی : یوخودا یاتین !\n",
            "2023-01-20 00:44:32,351 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:44:32,353 - INFO - joeynmt.training - \tSource:     در واقع ، من نیز پیش از شریعت زنده بودم . اما هنگامی که آن حکم آمد ، گناه زنده شد و من مردم . \n",
            "2023-01-20 00:44:32,353 - INFO - joeynmt.training - \tReference:  بیر واخت قانون منده یوخ ایکن من ساغ ایدیم ، آمما او امر منه گلنده گۆناه دیریلدی ، \n",
            "2023-01-20 00:44:32,354 - INFO - joeynmt.training - \tHypothesis: قانون واسطهسیله آلدێغێم زامان من ده اؤلموشدوم . چۆنکی گۆناه ائتدی وه گۆناهلارێ من آلدێ .\n",
            "2023-01-20 00:44:40,224 - INFO - joeynmt.training - Epoch 316, Step:   109100, Batch Loss:     1.543052, Batch Acc: 0.595463, Tokens per Sec:    13351, Lr: 0.000027\n",
            "2023-01-20 00:44:48,140 - INFO - joeynmt.training - Epoch 316, Step:   109200, Batch Loss:     1.606722, Batch Acc: 0.597965, Tokens per Sec:    13805, Lr: 0.000027\n",
            "2023-01-20 00:44:49,749 - INFO - joeynmt.training - Epoch 316: total training loss 545.95\n",
            "2023-01-20 00:44:49,749 - INFO - joeynmt.training - EPOCH 317\n",
            "2023-01-20 00:44:56,064 - INFO - joeynmt.training - Epoch 317, Step:   109300, Batch Loss:     1.488859, Batch Acc: 0.596624, Tokens per Sec:    14057, Lr: 0.000027\n",
            "2023-01-20 00:45:03,914 - INFO - joeynmt.training - Epoch 317, Step:   109400, Batch Loss:     1.567974, Batch Acc: 0.599537, Tokens per Sec:    13908, Lr: 0.000027\n",
            "2023-01-20 00:45:11,808 - INFO - joeynmt.training - Epoch 317, Step:   109500, Batch Loss:     1.579957, Batch Acc: 0.596696, Tokens per Sec:    13991, Lr: 0.000027\n",
            "2023-01-20 00:45:17,027 - INFO - joeynmt.training - Epoch 317: total training loss 541.44\n",
            "2023-01-20 00:45:17,028 - INFO - joeynmt.training - EPOCH 318\n",
            "2023-01-20 00:45:19,826 - INFO - joeynmt.training - Epoch 318, Step:   109600, Batch Loss:     1.445600, Batch Acc: 0.598786, Tokens per Sec:    14075, Lr: 0.000027\n",
            "2023-01-20 00:45:27,750 - INFO - joeynmt.training - Epoch 318, Step:   109700, Batch Loss:     1.584419, Batch Acc: 0.597598, Tokens per Sec:    13745, Lr: 0.000027\n",
            "2023-01-20 00:45:38,207 - INFO - joeynmt.training - Epoch 318, Step:   109800, Batch Loss:     1.567264, Batch Acc: 0.598153, Tokens per Sec:    10522, Lr: 0.000027\n",
            "2023-01-20 00:45:46,449 - INFO - joeynmt.training - Epoch 318, Step:   109900, Batch Loss:     1.465236, Batch Acc: 0.597116, Tokens per Sec:    13312, Lr: 0.000027\n",
            "2023-01-20 00:45:47,388 - INFO - joeynmt.training - Epoch 318: total training loss 545.34\n",
            "2023-01-20 00:45:47,388 - INFO - joeynmt.training - EPOCH 319\n",
            "2023-01-20 00:45:54,277 - INFO - joeynmt.training - Epoch 319, Step:   110000, Batch Loss:     1.583865, Batch Acc: 0.598206, Tokens per Sec:    13836, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.70ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10304.20ex/s]\n",
            "2023-01-20 00:45:54,554 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=110000\n",
            "2023-01-20 00:45:54,554 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:45:59,039 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:45:59,040 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:45:59,040 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:45:59,041 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:45:59,044 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.53, loss:   2.80, ppl:  16.39, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4439[sec], evaluation: 0.0386[sec]\n",
            "2023-01-20 00:45:59,047 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:45:59,050 - INFO - joeynmt.training - \tSource:     زیرا اگر کسی نزد شما بیاید و در مورد عیسایی غیر از آن که ما موعظه کردیم موعظه کند ، یا اگر روحی غیر از آنچه دریافت کردید بیاورد ، یا اگر بشارتی غیر از آنچه پذیرفتید اعلام کند ، شما به‌راحتی او را تحمل می‌کنید ؛ \n",
            "2023-01-20 00:45:59,050 - INFO - joeynmt.training - \tReference:  چۆنکی اگر کیمسه یانینیزا گلیب بیزیم وز ائتدیگیمیز ایسهآدان باشقا بیر ایسنانی تبلیغ ائدیرسه ، آلدیغینیزدان فرقلی بیر روح آلیرسینیزسا وه یاخود قبول ائتدیگینیزدن فرقلی بیر مۆژدنی قبول ائدیرسینیزسه ، سیز بونا نئجه ده ممنونایتله دؤزورسونوز . \n",
            "2023-01-20 00:45:59,050 - INFO - joeynmt.training - \tHypothesis: چۆنکی کیم ایسنانین یانینا گلیب سیزه وز ائتدیگی کیمی ، روح واسطهسیله سیزه وز ائتدیگی کیمی ، یاخود سیزه وز ائتدیگینیزیگتده یاخود اونون بوشبوغازیگتده قبول ائدین .\n",
            "2023-01-20 00:45:59,051 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:45:59,052 - INFO - joeynmt.training - \tSource:     پوشاند بر آن دو شهر ، از باران گوگردی‌ آنچه را پوشاند . \n",
            "2023-01-20 00:45:59,053 - INFO - joeynmt.training - \tReference:  اونلارێ نلر ساردێ ، نلر ! \n",
            "2023-01-20 00:45:59,053 - INFO - joeynmt.training - \tHypothesis: اونلار دیبسیز شهردن ایکی دفهلرله بۆرۆدۆکۆبۆکۆلۆشدۆ .\n",
            "2023-01-20 00:45:59,053 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:45:59,055 - INFO - joeynmt.training - \tSource:     کدام یک از شما می‌تواند با نگرانی حتی لحظه‌ای به عمر خود بیفزاید ؟ \n",
            "2023-01-20 00:45:59,055 - INFO - joeynmt.training - \tReference:  هانسێ بیرینیز قایغی چکمکله اؤمرونو بیر آنلێق بئله ، اوزادا بیلر ؟ \n",
            "2023-01-20 00:45:59,055 - INFO - joeynmt.training - \tHypothesis: سیزلردن بیری اؤزلری ایله آنلایادان قورخمالییا بیلرسینیز ؟\n",
            "2023-01-20 00:45:59,055 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:45:59,058 - INFO - joeynmt.training - \tSource:     اهالی آنجا وقتی عیسی را شناختند ، عده‌ای را به تمام نواحی آن اطراف فرستادند و مردم نیز همهٔ بیماران را نزد او آوردند\n",
            "2023-01-20 00:45:59,058 - INFO - joeynmt.training - \tReference:  او یئرین آداملارێ ایسنانی تانیدیقلاری زامان اترافداکێ هر یئره خبر یایدیلار . بۆتۆن خستهلری اونون یانینا گتیردیلر . \n",
            "2023-01-20 00:45:59,058 - INFO - joeynmt.training - \tHypothesis: اونلار اورادا اولاندا ایسا بۆتۆن خالقێ تانێماق اۆچۆن بۆتۆن کندلره بؤلگهسینده وه خالقێن یانینا گتیردیلر .\n",
            "2023-01-20 00:46:06,930 - INFO - joeynmt.training - Epoch 319, Step:   110100, Batch Loss:     1.528557, Batch Acc: 0.599649, Tokens per Sec:    13482, Lr: 0.000027\n",
            "2023-01-20 00:46:14,968 - INFO - joeynmt.training - Epoch 319, Step:   110200, Batch Loss:     1.564540, Batch Acc: 0.599263, Tokens per Sec:    13712, Lr: 0.000027\n",
            "2023-01-20 00:46:19,620 - INFO - joeynmt.training - Epoch 319: total training loss 543.91\n",
            "2023-01-20 00:46:19,621 - INFO - joeynmt.training - EPOCH 320\n",
            "2023-01-20 00:46:22,858 - INFO - joeynmt.training - Epoch 320, Step:   110300, Batch Loss:     1.651745, Batch Acc: 0.597480, Tokens per Sec:    13855, Lr: 0.000027\n",
            "2023-01-20 00:46:30,805 - INFO - joeynmt.training - Epoch 320, Step:   110400, Batch Loss:     1.632426, Batch Acc: 0.598250, Tokens per Sec:    13890, Lr: 0.000027\n",
            "2023-01-20 00:46:38,604 - INFO - joeynmt.training - Epoch 320, Step:   110500, Batch Loss:     1.562837, Batch Acc: 0.601181, Tokens per Sec:    14022, Lr: 0.000027\n",
            "2023-01-20 00:46:46,534 - INFO - joeynmt.training - Epoch 320, Step:   110600, Batch Loss:     1.526844, Batch Acc: 0.595239, Tokens per Sec:    13869, Lr: 0.000027\n",
            "2023-01-20 00:46:46,979 - INFO - joeynmt.training - Epoch 320: total training loss 541.62\n",
            "2023-01-20 00:46:46,980 - INFO - joeynmt.training - EPOCH 321\n",
            "2023-01-20 00:46:54,512 - INFO - joeynmt.training - Epoch 321, Step:   110700, Batch Loss:     1.597013, Batch Acc: 0.599359, Tokens per Sec:    13801, Lr: 0.000027\n",
            "2023-01-20 00:47:02,343 - INFO - joeynmt.training - Epoch 321, Step:   110800, Batch Loss:     1.604092, Batch Acc: 0.598109, Tokens per Sec:    14058, Lr: 0.000027\n",
            "2023-01-20 00:47:10,282 - INFO - joeynmt.training - Epoch 321, Step:   110900, Batch Loss:     1.480292, Batch Acc: 0.598604, Tokens per Sec:    14023, Lr: 0.000027\n",
            "2023-01-20 00:47:14,237 - INFO - joeynmt.training - Epoch 321: total training loss 538.77\n",
            "2023-01-20 00:47:14,238 - INFO - joeynmt.training - EPOCH 322\n",
            "2023-01-20 00:47:18,223 - INFO - joeynmt.training - Epoch 322, Step:   111000, Batch Loss:     1.564147, Batch Acc: 0.599520, Tokens per Sec:    14107, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 141.95ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9784.63ex/s]\n",
            "2023-01-20 00:47:18,506 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=111000\n",
            "2023-01-20 00:47:18,506 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:47:23,657 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:47:23,658 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:47:23,658 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:47:23,659 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:47:23,662 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.45, loss:   2.85, ppl:  17.22, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1114[sec], evaluation: 0.0364[sec]\n",
            "2023-01-20 00:47:23,665 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:47:23,668 - INFO - joeynmt.training - \tSource:     با این حال ، زنان با به دنیا آوردن فرزند در امان می‌مانند ، اگر همچنان باایمان ، بامحبت ، مقدس و فهمیده باشند . \n",
            "2023-01-20 00:47:23,668 - INFO - joeynmt.training - \tReference:  آمما قادێن دوغوشلا خلاص اولاجاق ، بیر شرطله کی آغێل کاماللا اماندا ، محبتده وه مۆقدسلیکده یاشاماغا داوام ائتسین . \n",
            "2023-01-20 00:47:23,668 - INFO - joeynmt.training - \tHypothesis: آمما بو قادێنلارا اؤؤلاد قویماسا ، اوشاقلار کیمی ، کیمین ده بیرگه ، مۆقدس ، مۆقدسلره وه آغێللێ کاهینلیگه مالک اولسون .\n",
            "2023-01-20 00:47:23,668 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:47:23,670 - INFO - joeynmt.training - \tSource:     یعقوب و یوحنا ، پسران زبدی نزد عیسی آمدند و گفتند : ای استاد ، خواهش ما این است که آنچه از تو درخواست می‌کنیم ، برای ما انجام دهی . \n",
            "2023-01-20 00:47:23,670 - INFO - joeynmt.training - \tReference:  زاودایین اوغوللارێ یاقوب وه یهیا اونا یاخینلاشیب دئدیلر : مۆعلم ، ایستهییریک کی ، دیلهدیگیمیزی بیزیم اۆچۆن ائدهسن . \n",
            "2023-01-20 00:47:23,671 - INFO - joeynmt.training - \tHypothesis: یاقوب وه یهیا ، مارک آدلانان یهیانین یانینا گلیب دئدیلر : مۆعلم ، بیز سنین بیزه ائتدیگین شئیلری دیلهیریک . بو ، بیزه ایستهدیگیمیز اۆچۆن خاهش ائدیر .\n",
            "2023-01-20 00:47:23,671 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:47:23,673 - INFO - joeynmt.training - \tSource:     سران قومش گفتند : واقعا ما تو را در گمراهی آشکاری می‌بینیم . \n",
            "2023-01-20 00:47:23,673 - INFO - joeynmt.training - \tReference:  تایفاسینین باشچیلاری اونا ! بیز سنی آچێق آیدین آزمێش گؤروروک دئیه جاواب وئردیلر . \n",
            "2023-01-20 00:47:23,673 - INFO - joeynmt.training - \tHypothesis: تایفاسینین ا یانلاریندا اونا دئدیلر : بیز سنی آچێق آشکار آزمێش گؤرورسن !\n",
            "2023-01-20 00:47:23,673 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:47:23,675 - INFO - joeynmt.training - \tSource:      دو مرد برای دعا کردن به معبد رفتند ؛ یکی فریسی و دیگری خراجگیر . \n",
            "2023-01-20 00:47:23,675 - INFO - joeynmt.training - \tReference:   بیری فاریسئی ، دیگری ایسه وئرگیییغان اولان ایکی نفر دوعا ائتمک اۆچۆن مبدع گیردی . \n",
            "2023-01-20 00:47:23,675 - INFO - joeynmt.training - \tHypothesis: ایکی کیشی مبدا مبده گیردی . بیر بیر بیرگه ایکینجیسی ده باشقا بیرگه چؤرک بایرامی .\n",
            "2023-01-20 00:47:31,549 - INFO - joeynmt.training - Epoch 322, Step:   111100, Batch Loss:     1.627061, Batch Acc: 0.599951, Tokens per Sec:    13462, Lr: 0.000027\n",
            "2023-01-20 00:47:39,423 - INFO - joeynmt.training - Epoch 322, Step:   111200, Batch Loss:     1.549875, Batch Acc: 0.597608, Tokens per Sec:    14031, Lr: 0.000027\n",
            "2023-01-20 00:47:46,844 - INFO - joeynmt.training - Epoch 322: total training loss 540.57\n",
            "2023-01-20 00:47:46,845 - INFO - joeynmt.training - EPOCH 323\n",
            "2023-01-20 00:47:47,349 - INFO - joeynmt.training - Epoch 323, Step:   111300, Batch Loss:     1.610584, Batch Acc: 0.608469, Tokens per Sec:    12935, Lr: 0.000027\n",
            "2023-01-20 00:47:55,219 - INFO - joeynmt.training - Epoch 323, Step:   111400, Batch Loss:     1.516245, Batch Acc: 0.600946, Tokens per Sec:    14002, Lr: 0.000027\n",
            "2023-01-20 00:48:03,083 - INFO - joeynmt.training - Epoch 323, Step:   111500, Batch Loss:     1.549686, Batch Acc: 0.600389, Tokens per Sec:    13814, Lr: 0.000027\n",
            "2023-01-20 00:48:11,051 - INFO - joeynmt.training - Epoch 323, Step:   111600, Batch Loss:     1.540732, Batch Acc: 0.598773, Tokens per Sec:    13932, Lr: 0.000027\n",
            "2023-01-20 00:48:14,246 - INFO - joeynmt.training - Epoch 323: total training loss 540.40\n",
            "2023-01-20 00:48:14,246 - INFO - joeynmt.training - EPOCH 324\n",
            "2023-01-20 00:48:19,031 - INFO - joeynmt.training - Epoch 324, Step:   111700, Batch Loss:     1.462233, Batch Acc: 0.604293, Tokens per Sec:    13684, Lr: 0.000027\n",
            "2023-01-20 00:48:26,898 - INFO - joeynmt.training - Epoch 324, Step:   111800, Batch Loss:     1.510445, Batch Acc: 0.597607, Tokens per Sec:    14001, Lr: 0.000027\n",
            "2023-01-20 00:48:34,741 - INFO - joeynmt.training - Epoch 324, Step:   111900, Batch Loss:     1.502388, Batch Acc: 0.596788, Tokens per Sec:    13895, Lr: 0.000027\n",
            "2023-01-20 00:48:41,634 - INFO - joeynmt.training - Epoch 324: total training loss 542.72\n",
            "2023-01-20 00:48:41,634 - INFO - joeynmt.training - EPOCH 325\n",
            "2023-01-20 00:48:42,692 - INFO - joeynmt.training - Epoch 325, Step:   112000, Batch Loss:     1.706332, Batch Acc: 0.591123, Tokens per Sec:    14053, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 70.74ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10365.44ex/s]\n",
            "2023-01-20 00:48:42,979 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=112000\n",
            "2023-01-20 00:48:42,979 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:48:48,288 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:48:48,289 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:48:48,289 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:48:48,290 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:48:48,293 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.29, loss:   2.86, ppl:  17.50, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.2680[sec], evaluation: 0.0377[sec]\n",
            "2023-01-20 00:48:48,295 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:48:48,299 - INFO - joeynmt.training - \tSource:     آنگاه از او پرسیدند : پدرت کجاست ؟ عیسی پاسخ داد : شما نه مرا می‌شناسید و نه پدر مرا . اگر مرا می‌شناختید ، پدرم را نیز می‌شناختید . \n",
            "2023-01-20 00:48:48,299 - INFO - joeynmt.training - \tReference:  اوندا ایسهآدان سوروشدولار : آتان هارادادێر ؟ ایسا جاواب وئردی : سیز نه منی ، نه ده آتامێ تانییرسینیز . اگر منی تانیسایدینیز ، آتامێ دا تانییاردینیز . \n",
            "2023-01-20 00:48:48,299 - INFO - joeynmt.training - \tHypothesis: اوندان سوروشدولار : آتامێن هارادادێر ؟ ایسا جاواب وئردی : منی تانییرسینیز ، منی تانیییردینیز ، آتا دا منی تانیییردینیز .\n",
            "2023-01-20 00:48:48,299 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:48:48,301 - INFO - joeynmt.training - \tSource:     فرشته‌ای دیگر نیز از مذبح بیرون آمد که بر آتش اقتدار داشت . او با صدای بلند به آن فرشته‌ای که داسی تیز در دست داشت ، گفت : داس تیز خود را بردار و خوشه‌های انگور را از تاک زمین جمع کن ؛ زیرا انگورهایش رسیده است . \n",
            "2023-01-20 00:48:48,302 - INFO - joeynmt.training - \tReference:  اود ایشینه مسول اولان باشقا بیر ملک ده قوربانگاهدان چێخێب ایتی اوراغێ اولان ملهیه اوجا سسله ندا ائتدی : ایتتی اوراغێنێ ایشه سال وه یئر اۆزوندکی اۆزۆم تنهییندن سالخیملاری کس یێغ ، چۆنکی اۆزۆملری یئتیشیب . \n",
            "2023-01-20 00:48:48,302 - INFO - joeynmt.training - \tHypothesis: باشقا بیر ملک ده گلیب اوددان چێخدێ . او ، اوجا سسله ندا ائتدی : او ، اوجا بیر ملک الینده اوجا سسله ندا ائتدی کی ، اؤز اللری ایله قارانلێق چؤکدو . یئر اۆزۆنۆن بوغازلارێ مۆختلفه ائتمک اۆچۆن بئله یازیلیب .\n",
            "2023-01-20 00:48:48,302 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:48:48,304 - INFO - joeynmt.training - \tSource:      نه‌ بلکه اینها و پدرانشان را برخوردار کردیم تا عمرشان به درازا کشید . آیا نمی‌بینند که ما می‌آییم و زمین را از جوانب آن فرو می‌کاهیم ؟ آیا باز هم آنان پیروزند ؟ \n",
            "2023-01-20 00:48:48,304 - INFO - joeynmt.training - \tReference:  دوغروسو ، بیز بونونلا وه آتالارێنا اؤمورلری اوزانینجایا قدر گۆن گۆزران وئردیک مگر بیزیم تورپاغێنا گیریب اونو هر ترفدن اسکیلتدیگیمیزی گؤرمورلرمی ؟ بئله اولدوقدا ، غالب اولانلار اونلارمێدێر \n",
            "2023-01-20 00:48:48,305 - INFO - joeynmt.training - \tHypothesis: خئیر ، بیز اونلارێن آتالارێلارێمێزا مۆراجعت ائتمک اۆچۆن یاراتدیق . مگر بیز اونلارێ یئر اۆزۆنده گزدیرهریکمیریک ؟ اونلار یئر اۆزۆنۆن جهنهمه آتدیقلارینا گؤره بیلرلرمی ؟\n",
            "2023-01-20 00:48:48,305 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:48:48,307 - INFO - joeynmt.training - \tSource:     بدنت هم سرد است . \n",
            "2023-01-20 00:48:48,307 - INFO - joeynmt.training - \tReference:  بدنین ده سویوق دی . \n",
            "2023-01-20 00:48:48,307 - INFO - joeynmt.training - \tHypothesis: بدنیم ده دیر .\n",
            "2023-01-20 00:48:56,168 - INFO - joeynmt.training - Epoch 325, Step:   112100, Batch Loss:     1.610990, Batch Acc: 0.601880, Tokens per Sec:    13542, Lr: 0.000027\n",
            "2023-01-20 00:49:06,812 - INFO - joeynmt.training - Epoch 325, Step:   112200, Batch Loss:     1.570678, Batch Acc: 0.597762, Tokens per Sec:    10369, Lr: 0.000027\n",
            "2023-01-20 00:49:14,883 - INFO - joeynmt.training - Epoch 325, Step:   112300, Batch Loss:     1.650236, Batch Acc: 0.599889, Tokens per Sec:    13675, Lr: 0.000027\n",
            "2023-01-20 00:49:17,386 - INFO - joeynmt.training - Epoch 325: total training loss 537.27\n",
            "2023-01-20 00:49:17,386 - INFO - joeynmt.training - EPOCH 326\n",
            "2023-01-20 00:49:22,845 - INFO - joeynmt.training - Epoch 326, Step:   112400, Batch Loss:     1.547011, Batch Acc: 0.603411, Tokens per Sec:    13988, Lr: 0.000027\n",
            "2023-01-20 00:49:30,744 - INFO - joeynmt.training - Epoch 326, Step:   112500, Batch Loss:     1.516616, Batch Acc: 0.602048, Tokens per Sec:    13859, Lr: 0.000027\n",
            "2023-01-20 00:49:38,614 - INFO - joeynmt.training - Epoch 326, Step:   112600, Batch Loss:     1.618626, Batch Acc: 0.594682, Tokens per Sec:    14045, Lr: 0.000027\n",
            "2023-01-20 00:49:44,718 - INFO - joeynmt.training - Epoch 326: total training loss 539.77\n",
            "2023-01-20 00:49:44,718 - INFO - joeynmt.training - EPOCH 327\n",
            "2023-01-20 00:49:46,579 - INFO - joeynmt.training - Epoch 327, Step:   112700, Batch Loss:     1.611031, Batch Acc: 0.600040, Tokens per Sec:    13562, Lr: 0.000027\n",
            "2023-01-20 00:49:54,397 - INFO - joeynmt.training - Epoch 327, Step:   112800, Batch Loss:     1.574420, Batch Acc: 0.599528, Tokens per Sec:    14051, Lr: 0.000027\n",
            "2023-01-20 00:50:02,243 - INFO - joeynmt.training - Epoch 327, Step:   112900, Batch Loss:     1.498861, Batch Acc: 0.602652, Tokens per Sec:    13898, Lr: 0.000027\n",
            "2023-01-20 00:50:10,080 - INFO - joeynmt.training - Epoch 327, Step:   113000, Batch Loss:     1.577526, Batch Acc: 0.599618, Tokens per Sec:    14084, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.55ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10311.73ex/s]\n",
            "2023-01-20 00:50:10,345 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=113000\n",
            "2023-01-20 00:50:10,345 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:50:13,993 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:50:13,994 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:50:13,994 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:50:13,995 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:50:13,997 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.19, loss:   2.82, ppl:  16.79, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 3.6121[sec], evaluation: 0.0330[sec]\n",
            "2023-01-20 00:50:14,001 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:50:14,004 - INFO - joeynmt.training - \tSource:     کسی که بر بام است ، پایین نیاید و به خانه‌اش داخل نشود تا چیزی از آنجا بردارد . \n",
            "2023-01-20 00:50:14,004 - INFO - joeynmt.training - \tReference:  دامدا اولان ائویندن نه ایسه گؤتورمک اۆچۆن آشاغێ ائنیب ائوینه گیرمهسین . \n",
            "2023-01-20 00:50:14,004 - INFO - joeynmt.training - \tHypothesis: هئچ کیم اۆستۆنه قاپێدان ، ائوه گیریب ائوه گیرمک اۆچۆن ایچری گیرمهیهجک .\n",
            "2023-01-20 00:50:14,004 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:50:14,006 - INFO - joeynmt.training - \tSource:     دیگه هیچی . \n",
            "2023-01-20 00:50:14,006 - INFO - joeynmt.training - \tReference:  دا هئچ زاد . \n",
            "2023-01-20 00:50:14,006 - INFO - joeynmt.training - \tHypothesis: دی .\n",
            "2023-01-20 00:50:14,007 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:50:14,009 - INFO - joeynmt.training - \tSource:     و چه بسیار شهرها که از فرمان پروردگار خود و پیامبرانش سر پیچیدند و از آنها حسابی سخت کشیدیم و آنان را به عذابی بس‌ زشت عذاب کردیم . \n",
            "2023-01-20 00:50:14,009 - INFO - joeynmt.training - \tReference:  نئچه نئچه مملکت اؤز ربینین وه اونون پیغمبرینین امریندن چێخدێ . بیز ده اونلارا شدتلی جزا وئریب گؤرونمهمیش مۆدهش ازابا دۆچار ائتدیک . \n",
            "2023-01-20 00:50:14,009 - INFO - joeynmt.training - \tHypothesis: نئچه نئچه نئچه مملکتلرین ربینه بئلینده وه اونلارا اعذاب وئرمیشدیک . بیز اونلارێ شدتلی بیر ازابا دۆچار ائتدیک .\n",
            "2023-01-20 00:50:14,009 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:50:14,011 - INFO - joeynmt.training - \tSource:     سپس دست‌هایش را بر او گذاشت و همان دم ، زن راست ایستاد و شروع به تمجید خدا کرد . \n",
            "2023-01-20 00:50:14,011 - INFO - joeynmt.training - \tReference:  ایسا اللهرینی قادێنێن اۆستۆنه قویاندا او درحال دیکهلیب آللاهێ ایزتلندیرمهیه باشلادێ . \n",
            "2023-01-20 00:50:14,011 - INFO - joeynmt.training - \tHypothesis: سونرا الینی اوزادێب اونون الینه قویدو وه قادێنێن ساغێندا دوردو .\n",
            "2023-01-20 00:50:15,911 - INFO - joeynmt.training - Epoch 327: total training loss 538.96\n",
            "2023-01-20 00:50:15,912 - INFO - joeynmt.training - EPOCH 328\n",
            "2023-01-20 00:50:22,061 - INFO - joeynmt.training - Epoch 328, Step:   113100, Batch Loss:     1.560597, Batch Acc: 0.602626, Tokens per Sec:    13635, Lr: 0.000027\n",
            "2023-01-20 00:50:29,908 - INFO - joeynmt.training - Epoch 328, Step:   113200, Batch Loss:     1.492445, Batch Acc: 0.601882, Tokens per Sec:    13961, Lr: 0.000027\n",
            "2023-01-20 00:50:37,942 - INFO - joeynmt.training - Epoch 328, Step:   113300, Batch Loss:     1.601129, Batch Acc: 0.600029, Tokens per Sec:    13855, Lr: 0.000027\n",
            "2023-01-20 00:50:43,327 - INFO - joeynmt.training - Epoch 328: total training loss 539.11\n",
            "2023-01-20 00:50:43,327 - INFO - joeynmt.training - EPOCH 329\n",
            "2023-01-20 00:50:45,816 - INFO - joeynmt.training - Epoch 329, Step:   113400, Batch Loss:     1.466910, Batch Acc: 0.599833, Tokens per Sec:    13482, Lr: 0.000027\n",
            "2023-01-20 00:50:53,681 - INFO - joeynmt.training - Epoch 329, Step:   113500, Batch Loss:     1.476943, Batch Acc: 0.599835, Tokens per Sec:    13991, Lr: 0.000027\n",
            "2023-01-20 00:51:01,552 - INFO - joeynmt.training - Epoch 329, Step:   113600, Batch Loss:     1.550386, Batch Acc: 0.601559, Tokens per Sec:    13940, Lr: 0.000027\n",
            "2023-01-20 00:51:09,387 - INFO - joeynmt.training - Epoch 329, Step:   113700, Batch Loss:     1.440796, Batch Acc: 0.602500, Tokens per Sec:    14009, Lr: 0.000027\n",
            "2023-01-20 00:51:10,641 - INFO - joeynmt.training - Epoch 329: total training loss 540.38\n",
            "2023-01-20 00:51:10,641 - INFO - joeynmt.training - EPOCH 330\n",
            "2023-01-20 00:51:17,234 - INFO - joeynmt.training - Epoch 330, Step:   113800, Batch Loss:     1.405232, Batch Acc: 0.605759, Tokens per Sec:    14023, Lr: 0.000027\n",
            "2023-01-20 00:51:25,179 - INFO - joeynmt.training - Epoch 330, Step:   113900, Batch Loss:     1.616751, Batch Acc: 0.603866, Tokens per Sec:    13846, Lr: 0.000027\n",
            "2023-01-20 00:51:33,047 - INFO - joeynmt.training - Epoch 330, Step:   114000, Batch Loss:     1.587969, Batch Acc: 0.600022, Tokens per Sec:    13863, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 140.71ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9397.18ex/s]\n",
            "2023-01-20 00:51:33,336 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=114000\n",
            "2023-01-20 00:51:33,336 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:51:37,905 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:51:37,906 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:51:37,906 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:51:37,907 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:51:37,910 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.84, loss:   2.93, ppl:  18.65, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5289[sec], evaluation: 0.0384[sec]\n",
            "2023-01-20 00:51:37,913 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:51:37,916 - INFO - joeynmt.training - \tSource:     و در برابر دستور پروردگارت شکیبایی پیشه کن که تو خود در حمایت مایی و هنگامی که از خواب‌ بر می‌خیزی به نیایش پروردگارت تسبیح گوی . \n",
            "2023-01-20 00:51:37,917 - INFO - joeynmt.training - \tReference:   سن اؤز ربینین هؤکمونه صبر ائت ! شبههسیز کی ، سن بیزیم گؤزوموزون قاباغێنداسان . قالخدێقدا ربینی همد سنا ایله زیکر ائت ! \n",
            "2023-01-20 00:51:37,917 - INFO - joeynmt.training - \tHypothesis: ربینین سنه قارشێ صبر ائت . سن بیزیمله بیرلیکده صبر ائت وه یاتان بیر معؤ جۆزه گتیرن دئییلدین . او ، ربینه بیر قؤؤؤم اۆچۆن بیر یئر اۆزهرینده قالاجاغێق !\n",
            "2023-01-20 00:51:37,917 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:51:37,919 - INFO - joeynmt.training - \tSource:      روح الامین آن را بر دلت نازل کرد ، \n",
            "2023-01-20 00:51:37,919 - INFO - joeynmt.training - \tReference:  اونو جبرایل ائندیردی : \n",
            "2023-01-20 00:51:37,920 - INFO - joeynmt.training - \tHypothesis: روح اونو نازل ائتدی ،\n",
            "2023-01-20 00:51:37,920 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:51:37,921 - INFO - joeynmt.training - \tSource:     و گفت : پروردگار بزرگتر شما منم ! \n",
            "2023-01-20 00:51:37,922 - INFO - joeynmt.training - \tReference:   حقیقتا ، من سیزین ان اوجا ربینیزم ! \n",
            "2023-01-20 00:51:37,922 - INFO - joeynmt.training - \tHypothesis: دئدی : من سیزین ربینیز بؤیوکدور !\n",
            "2023-01-20 00:51:37,922 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:51:37,924 - INFO - joeynmt.training - \tSource:     موسی گفته است : یهوه خدایتان پیامبری همانند من از میان برادرانتان برای شما مبعوث خواهد کرد . شما باید به هر چه او به شما می‌گوید ، گوش دهید . \n",
            "2023-01-20 00:51:37,924 - INFO - joeynmt.training - \tReference:  موسا بئله دئمیشدی : آللاهینیز رب سیزین اۆچۆن اؤز سویداشلارینیز ایچریسیندن منیم کیمی بیر پیغمبر یئتیرهجک . سیزه نه سؤیلهیهجکسه ، اونا قولاق آسێن . \n",
            "2023-01-20 00:51:37,924 - INFO - joeynmt.training - \tHypothesis: موسا بئله دئمیشدی : سیزین آرانێزدا آللاهێن پیغمبری اولدوغو کیمی سیزین اۆچۆن بیر پیغمبرلیک ائدهجک . سیزه بؤیوک بیر پیغمبر ده وزیگتینیز وار . هر بیرینیزه قولاق آساجاقسینیز .\n",
            "2023-01-20 00:51:42,835 - INFO - joeynmt.training - Epoch 330: total training loss 537.72\n",
            "2023-01-20 00:51:42,835 - INFO - joeynmt.training - EPOCH 331\n",
            "2023-01-20 00:51:45,831 - INFO - joeynmt.training - Epoch 331, Step:   114100, Batch Loss:     1.555630, Batch Acc: 0.609005, Tokens per Sec:    14059, Lr: 0.000026\n",
            "2023-01-20 00:51:53,754 - INFO - joeynmt.training - Epoch 331, Step:   114200, Batch Loss:     1.464069, Batch Acc: 0.601013, Tokens per Sec:    14077, Lr: 0.000026\n",
            "2023-01-20 00:52:01,613 - INFO - joeynmt.training - Epoch 331, Step:   114300, Batch Loss:     1.514149, Batch Acc: 0.602616, Tokens per Sec:    14055, Lr: 0.000026\n",
            "2023-01-20 00:52:09,575 - INFO - joeynmt.training - Epoch 331, Step:   114400, Batch Loss:     1.566629, Batch Acc: 0.600525, Tokens per Sec:    13735, Lr: 0.000026\n",
            "2023-01-20 00:52:10,111 - INFO - joeynmt.training - Epoch 331: total training loss 533.94\n",
            "2023-01-20 00:52:10,111 - INFO - joeynmt.training - EPOCH 332\n",
            "2023-01-20 00:52:17,629 - INFO - joeynmt.training - Epoch 332, Step:   114500, Batch Loss:     1.567834, Batch Acc: 0.606866, Tokens per Sec:    13973, Lr: 0.000026\n",
            "2023-01-20 00:52:25,409 - INFO - joeynmt.training - Epoch 332, Step:   114600, Batch Loss:     1.553242, Batch Acc: 0.603630, Tokens per Sec:    14016, Lr: 0.000026\n",
            "2023-01-20 00:52:36,197 - INFO - joeynmt.training - Epoch 332, Step:   114700, Batch Loss:     1.596019, Batch Acc: 0.598168, Tokens per Sec:    10029, Lr: 0.000026\n",
            "2023-01-20 00:52:40,295 - INFO - joeynmt.training - Epoch 332: total training loss 539.30\n",
            "2023-01-20 00:52:40,295 - INFO - joeynmt.training - EPOCH 333\n",
            "2023-01-20 00:52:44,002 - INFO - joeynmt.training - Epoch 333, Step:   114800, Batch Loss:     1.602757, Batch Acc: 0.607135, Tokens per Sec:    13892, Lr: 0.000026\n",
            "2023-01-20 00:52:51,920 - INFO - joeynmt.training - Epoch 333, Step:   114900, Batch Loss:     1.571915, Batch Acc: 0.604864, Tokens per Sec:    13892, Lr: 0.000026\n",
            "2023-01-20 00:52:59,857 - INFO - joeynmt.training - Epoch 333, Step:   115000, Batch Loss:     1.576973, Batch Acc: 0.601564, Tokens per Sec:    13910, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.70ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10407.70ex/s]\n",
            "2023-01-20 00:53:00,123 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=115000\n",
            "2023-01-20 00:53:00,123 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:53:04,773 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:53:04,774 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:53:04,774 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:53:04,775 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:53:04,778 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.63, loss:   2.83, ppl:  16.92, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6065[sec], evaluation: 0.0406[sec]\n",
            "2023-01-20 00:53:04,780 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:53:04,784 - INFO - joeynmt.training - \tSource:     گفتند : آمین ! سپاس ، جلال ، حکمت ، شکرگزاری ، حرمت ، قدرت و توانایی همیشه و تا ابد از آن خدایمان باد ! آمین . \n",
            "2023-01-20 00:53:04,784 - INFO - joeynmt.training - \tReference:   آمین ! آللاهیمیزا ابدی اولاراق آلقێش ، عزت ، هکمت ، شۆکۆر ، شرف ، قدرت وه غۆۆت اولسون ! آمین ! \n",
            "2023-01-20 00:53:04,784 - INFO - joeynmt.training - \tHypothesis: اونلار دئدیلر : آغا ، شۆکۆر ! او ، هکمت ، مۆدرک ، هؤرمت ، هؤرمت ، هؤرمت وه ابدی اولاراق عزت ، قدرت ، ابدی اولاراق عزت اولسون ! آمین . آمین . آمین .\n",
            "2023-01-20 00:53:04,784 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:53:04,786 - INFO - joeynmt.training - \tSource:     و در صور دمیده می‌شود ، پس هر که در آسمانها و هر که در زمین است بیهوش درمی‌افتد ، مگر کسی که خدا بخواهد ؛ سپس بار دیگر در آن دمیده می‌شود و بناگاه آنان بر پای ایستاده می‌نگرند . \n",
            "2023-01-20 00:53:04,787 - INFO - joeynmt.training - \tReference:  سور چالیناجاق ، آللاهێن گؤیلرده وه یئرده اولان ایستهدیگی کیمسهلردن باشقا ، درحال هامێ یێخیلیب اؤلهجک . سونرا بیر داها چالێنان کیمی اونلار قالخێب مۆنتزر اولاجاقلار ! \n",
            "2023-01-20 00:53:04,787 - INFO - joeynmt.training - \tHypothesis: سور چالیناجاغی گۆنلرده ، گؤیلرده وه یئرده اولانلار کیمی اولمایاجاغینی آللاه ایستهسه ، اوندان باشقا هئچ کس اونون اۆستۆنه دۆشمۆشدۆ . سونرا اونلار یئنه ده ، کیمی دورموشدولار .\n",
            "2023-01-20 00:53:04,787 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:53:04,789 - INFO - joeynmt.training - \tSource:     همچنین کسی شراب تازه را در مشک کهنه نمی‌ریزد ؛ زیرا مشک می‌ترکد و شراب می‌ریزد و مشک ضایع می‌شود . شراب تازه را در مشک نو می‌ریزند و به این ترتیب ، هم شراب و هم مشک حفظ می‌شود . \n",
            "2023-01-20 00:53:04,789 - INFO - joeynmt.training - \tReference:  ائلهجه ده تزه شرابێ کؤهنه تولوقلارا دولدورمازلار . یوخسا تولوقلار پارتلایار ، شراب تؤکۆلر ، تولوقلار دا زای اولار . تزه شراب تزه تولوقلارا دولدورولار ؛ اوندا هر ایکیسی قورونار . \n",
            "2023-01-20 00:53:04,789 - INFO - joeynmt.training - \tHypothesis: هئچ کیم تزه شرابێ کؤهنه تولوقلارا دولدورماز . یوخسا شراب تولوقلاری ترک ائدیب شغارتێ شراب ایچیر . بو ، بوغازلوغا وه بوشاغازلێقلارالارالارا ، هم ده بوشاغازێلارالارالارالارالارالارالارالارالارالارا ، هم ده بو ایشلهینلره ، هم ده ویه دؤزومه ریایت ائدیر .\n",
            "2023-01-20 00:53:04,790 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:53:04,791 - INFO - joeynmt.training - \tSource:     به راستی پرهیزگاران در جایگاهی آسوده اند ، \n",
            "2023-01-20 00:53:04,792 - INFO - joeynmt.training - \tReference:  مۆتقلر ایسه قورخوسوز خطرسیز بیر یئرده \n",
            "2023-01-20 00:53:04,792 - INFO - joeynmt.training - \tHypothesis: حقیقتا ، مۆتقلرین آللاهدان قورخوب پیس امللردن چکینهلر\n",
            "2023-01-20 00:53:12,562 - INFO - joeynmt.training - Epoch 333: total training loss 535.96\n",
            "2023-01-20 00:53:12,563 - INFO - joeynmt.training - EPOCH 334\n",
            "2023-01-20 00:53:12,731 - INFO - joeynmt.training - Epoch 334, Step:   115100, Batch Loss:     1.636089, Batch Acc: 0.600261, Tokens per Sec:    13710, Lr: 0.000026\n",
            "2023-01-20 00:53:20,674 - INFO - joeynmt.training - Epoch 334, Step:   115200, Batch Loss:     1.615896, Batch Acc: 0.606256, Tokens per Sec:    13870, Lr: 0.000026\n",
            "2023-01-20 00:53:28,586 - INFO - joeynmt.training - Epoch 334, Step:   115300, Batch Loss:     1.559191, Batch Acc: 0.602856, Tokens per Sec:    13867, Lr: 0.000026\n",
            "2023-01-20 00:53:36,497 - INFO - joeynmt.training - Epoch 334, Step:   115400, Batch Loss:     1.590493, Batch Acc: 0.600249, Tokens per Sec:    13820, Lr: 0.000026\n",
            "2023-01-20 00:53:39,913 - INFO - joeynmt.training - Epoch 334: total training loss 535.16\n",
            "2023-01-20 00:53:39,914 - INFO - joeynmt.training - EPOCH 335\n",
            "2023-01-20 00:53:44,382 - INFO - joeynmt.training - Epoch 335, Step:   115500, Batch Loss:     1.566484, Batch Acc: 0.601226, Tokens per Sec:    13913, Lr: 0.000026\n",
            "2023-01-20 00:53:52,180 - INFO - joeynmt.training - Epoch 335, Step:   115600, Batch Loss:     1.582084, Batch Acc: 0.601119, Tokens per Sec:    13941, Lr: 0.000026\n",
            "2023-01-20 00:53:59,993 - INFO - joeynmt.training - Epoch 335, Step:   115700, Batch Loss:     1.560531, Batch Acc: 0.600792, Tokens per Sec:    14248, Lr: 0.000026\n",
            "2023-01-20 00:54:06,958 - INFO - joeynmt.training - Epoch 335: total training loss 536.53\n",
            "2023-01-20 00:54:06,959 - INFO - joeynmt.training - EPOCH 336\n",
            "2023-01-20 00:54:07,829 - INFO - joeynmt.training - Epoch 336, Step:   115800, Batch Loss:     1.349669, Batch Acc: 0.612830, Tokens per Sec:    13751, Lr: 0.000026\n",
            "2023-01-20 00:54:15,604 - INFO - joeynmt.training - Epoch 336, Step:   115900, Batch Loss:     1.582278, Batch Acc: 0.602630, Tokens per Sec:    14253, Lr: 0.000026\n",
            "2023-01-20 00:54:23,511 - INFO - joeynmt.training - Epoch 336, Step:   116000, Batch Loss:     1.720446, Batch Acc: 0.602609, Tokens per Sec:    13786, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 145.22ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10257.23ex/s]\n",
            "2023-01-20 00:54:23,806 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=116000\n",
            "2023-01-20 00:54:23,806 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:54:27,924 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:54:27,924 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:54:27,924 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:54:27,925 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:54:27,928 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.88, loss:   2.88, ppl:  17.83, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.0787[sec], evaluation: 0.0357[sec]\n",
            "2023-01-20 00:54:27,930 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:54:27,934 - INFO - joeynmt.training - \tSource:     و قطعا برای او در پیشگاه ما تقرب و فرجام نیکوست . \n",
            "2023-01-20 00:54:27,934 - INFO - joeynmt.training - \tReference:  حقیقتا ، او ، درگاهێمێزا یاخین اولاجاق وه اونون قاییدیب گلهجهگی یئر ده گؤزل اولاجاقدێر . \n",
            "2023-01-20 00:54:27,934 - INFO - joeynmt.training - \tHypothesis: شبههسیز کی ، او ، بیزیم اۆچۆن بیر یئردهدیر . عاقبتینین نه گؤزلدیر !\n",
            "2023-01-20 00:54:27,934 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:54:27,936 - INFO - joeynmt.training - \tSource:     پس کلام خدا همچنان گسترش می‌یافت و در اورشلیم بر شمار شاگردان همواره افزوده می‌شد و گروهی کثیر از کاهنان نیز ایمان آوردند . \n",
            "2023-01-20 00:54:27,937 - INFO - joeynmt.training - \tReference:  آللاهێن کلامێ یاییلیر ، یئروسلیمده شاگردلرین سایی خئیلی آرتێر وه بیر چوخ کاهنلر ده بو ایمانا تابع اولوردو . \n",
            "2023-01-20 00:54:27,937 - INFO - joeynmt.training - \tHypothesis: بئلهلیکله ، یئروسهلیمه گئدیردی ، یئروسلیمده دایمی شاگردلری اویغورلامێشدێ . باشچێ کاهنلر ده ایمانلی بیر تایفا ایدی .\n",
            "2023-01-20 00:54:27,937 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:54:27,939 - INFO - joeynmt.training - \tSource:     و چه بسیار نسلها که پیش از آنان هلاک کردیم . آیا کسی از آنان را می‌یابی یا صدایی از ایشان می‌شنوی ؟ \n",
            "2023-01-20 00:54:27,939 - INFO - joeynmt.training - \tReference:  بیز اونلاردان اول نئچه نئچه نسیللری محو ائتدیک . ایندی هئچ اونلاردان بیرینی گؤرۆر ، یاخود اونلاردان بیر سس سمیر ائشیدیرسنمی \n",
            "2023-01-20 00:54:27,939 - INFO - joeynmt.training - \tHypothesis: بیز اونلاردان اول نئچه نئچه نسیللری محو ائتدیک . اونلارا ائشیتمک ، یاخود سسینی ائشیتدیکمی ؟\n",
            "2023-01-20 00:54:27,939 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:54:27,941 - INFO - joeynmt.training - \tSource:     پادشاه در جواب خواهد گفت : به‌راستی به شما می‌گویم ، هر آنچه برای یکی از کوچک‌ترین برادران من کردید ، برای من کردید . \n",
            "2023-01-20 00:54:27,941 - INFO - joeynmt.training - \tReference:  پادشاه دا جاواب وئریب اونلارا دئیهجک : سیزه دوغروسونو دئییرم : سیز بو ان کیچیک قارداشلاریمدان بیرینه ائتدیگینیزی منه ائتمیش اولدونوز . \n",
            "2023-01-20 00:54:27,942 - INFO - joeynmt.training - \tHypothesis: پادشاه ایسه جاوابێندا سیزه دوغروسونو دئییرم : هر بیرینیز منیم اۆچۆن ان کیچیک بیر بیرینیزه قارشێ اۆستۆن توتون .\n",
            "2023-01-20 00:54:35,901 - INFO - joeynmt.training - Epoch 336, Step:   116100, Batch Loss:     1.541728, Batch Acc: 0.602350, Tokens per Sec:    13385, Lr: 0.000026\n",
            "2023-01-20 00:54:38,639 - INFO - joeynmt.training - Epoch 336: total training loss 534.31\n",
            "2023-01-20 00:54:38,640 - INFO - joeynmt.training - EPOCH 337\n",
            "2023-01-20 00:54:43,772 - INFO - joeynmt.training - Epoch 337, Step:   116200, Batch Loss:     1.542241, Batch Acc: 0.604582, Tokens per Sec:    14179, Lr: 0.000026\n",
            "2023-01-20 00:54:51,651 - INFO - joeynmt.training - Epoch 337, Step:   116300, Batch Loss:     1.400712, Batch Acc: 0.605980, Tokens per Sec:    13927, Lr: 0.000026\n",
            "2023-01-20 00:54:59,423 - INFO - joeynmt.training - Epoch 337, Step:   116400, Batch Loss:     1.701834, Batch Acc: 0.605198, Tokens per Sec:    14016, Lr: 0.000026\n",
            "2023-01-20 00:55:05,705 - INFO - joeynmt.training - Epoch 337: total training loss 535.12\n",
            "2023-01-20 00:55:05,706 - INFO - joeynmt.training - EPOCH 338\n",
            "2023-01-20 00:55:07,289 - INFO - joeynmt.training - Epoch 338, Step:   116500, Batch Loss:     1.679189, Batch Acc: 0.594765, Tokens per Sec:    13998, Lr: 0.000026\n",
            "2023-01-20 00:55:15,038 - INFO - joeynmt.training - Epoch 338, Step:   116600, Batch Loss:     1.490236, Batch Acc: 0.603922, Tokens per Sec:    14149, Lr: 0.000026\n",
            "2023-01-20 00:55:22,911 - INFO - joeynmt.training - Epoch 338, Step:   116700, Batch Loss:     1.599582, Batch Acc: 0.606939, Tokens per Sec:    13839, Lr: 0.000026\n",
            "2023-01-20 00:55:30,759 - INFO - joeynmt.training - Epoch 338, Step:   116800, Batch Loss:     1.542151, Batch Acc: 0.601463, Tokens per Sec:    14097, Lr: 0.000026\n",
            "2023-01-20 00:55:32,876 - INFO - joeynmt.training - Epoch 338: total training loss 535.61\n",
            "2023-01-20 00:55:32,877 - INFO - joeynmt.training - EPOCH 339\n",
            "2023-01-20 00:55:38,612 - INFO - joeynmt.training - Epoch 339, Step:   116900, Batch Loss:     1.509772, Batch Acc: 0.602845, Tokens per Sec:    14294, Lr: 0.000026\n",
            "2023-01-20 00:55:46,375 - INFO - joeynmt.training - Epoch 339, Step:   117000, Batch Loss:     1.445905, Batch Acc: 0.605496, Tokens per Sec:    14097, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 136.97ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9658.17ex/s]\n",
            "2023-01-20 00:55:46,650 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=117000\n",
            "2023-01-20 00:55:46,650 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:55:51,196 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:55:51,196 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:55:51,196 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:55:51,197 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:55:51,200 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.33, loss:   2.85, ppl:  17.22, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5045[sec], evaluation: 0.0372[sec]\n",
            "2023-01-20 00:55:51,203 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:55:51,207 - INFO - joeynmt.training - \tSource:     بیم دادن تو ، تنها کسی را سودمند است که کتاب حق را پیروی کند و از خدای‌ رحمان در نهان بترسد . چنین کسی را به آمرزش و پاداشی پر ارزش مژده ده . \n",
            "2023-01-20 00:55:51,207 - INFO - joeynmt.training - \tReference:  سن آنجاق قور آنا تابع اولوب ، رحماندان گؤرمهدن قورخان کیمسنی قورخودا بیلرسن . بئلهسینه باغیشلاناجاغی وه چوخ گؤزل بیر مۆکافاتا نائل اولاجاغێ ایله مۆژده وئر ! \n",
            "2023-01-20 00:55:51,207 - INFO - joeynmt.training - \tHypothesis: قورخانلارا حاقق حساب گۆنۆن رهمانا وه رهمانێن ایتااااااااا ، آللاهدان قورخون وه مۆکافاتێ هئچ کسه مۆژده وئر !\n",
            "2023-01-20 00:55:51,207 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:55:51,209 - INFO - joeynmt.training - \tSource:     و تو را جز رحمتی برای جهانیان نفرستادیم . \n",
            "2023-01-20 00:55:51,209 - INFO - joeynmt.training - \tReference:  سنی ده آلملره آنجاق بیر رحمت اولاراق گؤندردیک \n",
            "2023-01-20 00:55:51,209 - INFO - joeynmt.training - \tHypothesis: بیز سنی آلملرین ان بؤیوک مرحمت اولاراق گؤندردیک .\n",
            "2023-01-20 00:55:51,210 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:55:51,212 - INFO - joeynmt.training - \tSource:     شما در گذشته در طریق این دنیا گام برمی‌داشتید و از آن حاکمی که بر هوا اقتدار دارد ، پیروی می‌کردید . این هوا همان روحی است که اکنون در سرکشان عمل می‌کند . \n",
            "2023-01-20 00:55:51,212 - INFO - joeynmt.training - \tReference:  سیز بو دۆنیانین گئدیشینه وه هاوادا اولان حاکمیتین ، ینی ایندی آللاها ایتاعتسیز اینسانلاردا فعالیت گؤستهرن روحون هؤکمدارێنا اویاراق بو گۆناهلارێن ایچینده هیات سۆرۆردۆنۆز . \n",
            "2023-01-20 00:55:51,212 - INFO - joeynmt.training - \tHypothesis: سیز اوهلکی نسیللرده بو دۆنیانین حاکم اولدوغونوزو اۆستۆنده اوتوروب . بو ، روحدا نمونه گؤتوردوگونوز کیمی روحوندا قالێر .\n",
            "2023-01-20 00:55:51,213 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:55:51,215 - INFO - joeynmt.training - \tSource:     آنگاه تمام آن جمع ساکت شدند و به سخنان برنابا و پولس در مورد بسیاری نشانه‌ها و معجزات که خدا از طریق آنان در میان قوم‌ها به ظهور رسانده بود ، گوش فرادادند . \n",
            "2023-01-20 00:55:51,215 - INFO - joeynmt.training - \tReference:  اوندا بۆتۆن توپلانتێ ساکیتلشدی وه بارنابا ایله پاولا قولاق آسماغا باشلادێلار . بارنابا ایله پاول آللاهێن اونلار واسطهسیله باشقا ملتلر آراسێندا گؤستردیگی علامت وه خارقهلرین هامیسینی دانێشدێ . \n",
            "2023-01-20 00:55:51,215 - INFO - joeynmt.training - \tHypothesis: بۆتۆن خالق توپلانتێ یێغیلدی وه پاوللا بیرلیکده چوخ آدام واسطهسیله خالق آراسێندا فرقهلر گؤستردی . خالق آراسێندا فرقلنمک اۆچۆن خالقێ گؤستردی .\n",
            "2023-01-20 00:56:00,032 - INFO - joeynmt.training - Epoch 339, Step:   117100, Batch Loss:     1.524074, Batch Acc: 0.604162, Tokens per Sec:    12121, Lr: 0.000026\n",
            "2023-01-20 00:56:07,619 - INFO - joeynmt.training - Epoch 339: total training loss 530.79\n",
            "2023-01-20 00:56:07,619 - INFO - joeynmt.training - EPOCH 340\n",
            "2023-01-20 00:56:10,017 - INFO - joeynmt.training - Epoch 340, Step:   117200, Batch Loss:     1.589486, Batch Acc: 0.614828, Tokens per Sec:    13891, Lr: 0.000026\n",
            "2023-01-20 00:56:17,952 - INFO - joeynmt.training - Epoch 340, Step:   117300, Batch Loss:     1.504839, Batch Acc: 0.604733, Tokens per Sec:    13973, Lr: 0.000026\n",
            "2023-01-20 00:56:25,910 - INFO - joeynmt.training - Epoch 340, Step:   117400, Batch Loss:     1.597670, Batch Acc: 0.603040, Tokens per Sec:    13660, Lr: 0.000026\n",
            "2023-01-20 00:56:33,718 - INFO - joeynmt.training - Epoch 340, Step:   117500, Batch Loss:     1.543282, Batch Acc: 0.601600, Tokens per Sec:    14105, Lr: 0.000026\n",
            "2023-01-20 00:56:34,975 - INFO - joeynmt.training - Epoch 340: total training loss 534.56\n",
            "2023-01-20 00:56:34,976 - INFO - joeynmt.training - EPOCH 341\n",
            "2023-01-20 00:56:41,442 - INFO - joeynmt.training - Epoch 341, Step:   117600, Batch Loss:     1.643649, Batch Acc: 0.604451, Tokens per Sec:    14110, Lr: 0.000026\n",
            "2023-01-20 00:56:49,276 - INFO - joeynmt.training - Epoch 341, Step:   117700, Batch Loss:     1.611323, Batch Acc: 0.603744, Tokens per Sec:    13961, Lr: 0.000026\n",
            "2023-01-20 00:56:57,087 - INFO - joeynmt.training - Epoch 341, Step:   117800, Batch Loss:     1.574106, Batch Acc: 0.604030, Tokens per Sec:    14082, Lr: 0.000026\n",
            "2023-01-20 00:57:01,992 - INFO - joeynmt.training - Epoch 341: total training loss 535.42\n",
            "2023-01-20 00:57:01,992 - INFO - joeynmt.training - EPOCH 342\n",
            "2023-01-20 00:57:04,872 - INFO - joeynmt.training - Epoch 342, Step:   117900, Batch Loss:     1.536101, Batch Acc: 0.608605, Tokens per Sec:    14272, Lr: 0.000026\n",
            "2023-01-20 00:57:12,660 - INFO - joeynmt.training - Epoch 342, Step:   118000, Batch Loss:     1.574029, Batch Acc: 0.602367, Tokens per Sec:    14107, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 125.61ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10385.20ex/s]\n",
            "2023-01-20 00:57:12,926 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=118000\n",
            "2023-01-20 00:57:12,926 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:57:17,417 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:57:17,418 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:57:17,418 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:57:17,419 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:57:17,422 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.52, loss:   2.90, ppl:  18.26, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4532[sec], evaluation: 0.0350[sec]\n",
            "2023-01-20 00:57:17,424 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:57:17,427 - INFO - joeynmt.training - \tSource:     ای کسانی که ایمان آورده‌اید ، از خدا پروا کنید و با راستان باشید . \n",
            "2023-01-20 00:57:17,428 - INFO - joeynmt.training - \tReference:  ائی ایمان گتیرنلر ! آللاهدان قورخون وه دوغرو اولانلارلا اولون ! \n",
            "2023-01-20 00:57:17,428 - INFO - joeynmt.training - \tHypothesis: ائی ایمان گتیرنلر ! آللاهدان قورخون وه دوغرو یولا سالێن !\n",
            "2023-01-20 00:57:17,428 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:57:17,430 - INFO - joeynmt.training - \tSource:     و سرانجام‌ کار خود را کردی ، و تو از ناسپاسانی . \n",
            "2023-01-20 00:57:17,430 - INFO - joeynmt.training - \tReference:  آخێردا ائلهدیگینی ده ائلهدین . سن نانکورون بیریسن ! \n",
            "2023-01-20 00:57:17,430 - INFO - joeynmt.training - \tHypothesis: و واجینی اؤزونه سێپاپدێرێب یولدان چێخ !\n",
            "2023-01-20 00:57:17,431 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:57:17,434 - INFO - joeynmt.training - \tSource:     زن باید در سکوت و با اطاعت کامل تعلیم بگیرد . \n",
            "2023-01-20 00:57:17,434 - INFO - joeynmt.training - \tReference:  قادێن ساکیتلیک وه تام بیر تابئلیک ایچینده اؤیرنسین ؛ \n",
            "2023-01-20 00:57:17,434 - INFO - joeynmt.training - \tHypothesis: قادێنلا بیرلیکده تۆکۆلله آللاها ایتاعت ائتسین .\n",
            "2023-01-20 00:57:17,434 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:57:17,436 - INFO - joeynmt.training - \tSource:     و چون پیمانه می‌کنید ، پیمانه را تمام دهید ، و با ترازوی درست بسنجید که این بهتر و خوش فرجام‌تر است . \n",
            "2023-01-20 00:57:17,436 - INFO - joeynmt.training - \tReference:  اؤلچنده اؤلچوده دۆز اولون ، دۆزگۆن ترهزی ایله چکین . بو داها خئیرلی وه نتیجه ائ تباریله داها یاخشیدیر ! \n",
            "2023-01-20 00:57:17,436 - INFO - joeynmt.training - \tHypothesis: اهدی پوزاندا اؤلچو ایله اؤلمهنیز ده ، داها یاخشی وه ان گؤزل ترزده داها یاخشیدیر . بو ، تۆکنۆکۆ داها یاخشیدیر !\n",
            "2023-01-20 00:57:25,362 - INFO - joeynmt.training - Epoch 342, Step:   118100, Batch Loss:     1.510264, Batch Acc: 0.606729, Tokens per Sec:    13252, Lr: 0.000026\n",
            "2023-01-20 00:57:33,253 - INFO - joeynmt.training - Epoch 342, Step:   118200, Batch Loss:     1.662986, Batch Acc: 0.602547, Tokens per Sec:    13953, Lr: 0.000026\n",
            "2023-01-20 00:57:34,006 - INFO - joeynmt.training - Epoch 342: total training loss 533.29\n",
            "2023-01-20 00:57:34,006 - INFO - joeynmt.training - EPOCH 343\n",
            "2023-01-20 00:57:41,200 - INFO - joeynmt.training - Epoch 343, Step:   118300, Batch Loss:     1.509822, Batch Acc: 0.604451, Tokens per Sec:    13774, Lr: 0.000026\n",
            "2023-01-20 00:57:49,017 - INFO - joeynmt.training - Epoch 343, Step:   118400, Batch Loss:     1.569183, Batch Acc: 0.607533, Tokens per Sec:    13953, Lr: 0.000026\n",
            "2023-01-20 00:57:56,850 - INFO - joeynmt.training - Epoch 343, Step:   118500, Batch Loss:     1.507451, Batch Acc: 0.600688, Tokens per Sec:    14059, Lr: 0.000026\n",
            "2023-01-20 00:58:01,294 - INFO - joeynmt.training - Epoch 343: total training loss 536.95\n",
            "2023-01-20 00:58:01,294 - INFO - joeynmt.training - EPOCH 344\n",
            "2023-01-20 00:58:04,718 - INFO - joeynmt.training - Epoch 344, Step:   118600, Batch Loss:     1.607229, Batch Acc: 0.603008, Tokens per Sec:    13659, Lr: 0.000026\n",
            "2023-01-20 00:58:12,721 - INFO - joeynmt.training - Epoch 344, Step:   118700, Batch Loss:     1.548780, Batch Acc: 0.606417, Tokens per Sec:    13556, Lr: 0.000026\n",
            "2023-01-20 00:58:20,604 - INFO - joeynmt.training - Epoch 344, Step:   118800, Batch Loss:     1.598609, Batch Acc: 0.602764, Tokens per Sec:    13878, Lr: 0.000026\n",
            "2023-01-20 00:58:28,442 - INFO - joeynmt.training - Epoch 344, Step:   118900, Batch Loss:     1.629280, Batch Acc: 0.606045, Tokens per Sec:    14366, Lr: 0.000026\n",
            "2023-01-20 00:58:28,664 - INFO - joeynmt.training - Epoch 344: total training loss 533.02\n",
            "2023-01-20 00:58:28,664 - INFO - joeynmt.training - EPOCH 345\n",
            "2023-01-20 00:58:36,180 - INFO - joeynmt.training - Epoch 345, Step:   119000, Batch Loss:     1.546910, Batch Acc: 0.607367, Tokens per Sec:    14134, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.12ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10255.09ex/s]\n",
            "2023-01-20 00:58:36,449 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=119000\n",
            "2023-01-20 00:58:36,449 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 00:58:41,398 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 00:58:41,398 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 00:58:41,399 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 00:58:41,399 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 00:58:41,402 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.47, loss:   2.73, ppl:  15.37, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9081[sec], evaluation: 0.0379[sec]\n",
            "2023-01-20 00:58:41,405 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 00:58:41,408 - INFO - joeynmt.training - \tSource:     اگر به‌راستی خطاکارم و جرمی سزاوار مرگ مرتکب شده‌ام ، از مرگ نمی‌گریزم . اما اگر اتهاماتی که این مردان به من نسبت می‌دهند ، بی‌اساس است ، هیچ کس حق ندارد برای خشنودی آنان مرا به دستشان تسلیم کند . من از قیصر درخواست فرجام می‌کنم ! \n",
            "2023-01-20 00:58:41,409 - INFO - joeynmt.training - \tReference:  اگر حاقسێزلێق ائتمیشمسه ، اؤلوم جزاسێنا لایق بیر ایش توتموشامسا ، اؤلومدن قاچمێرام . یوخ ، اگر بونلارێن اتحاملاری اساسسێزدێرسا ، منی اونلارێن الینه تسلیم ائتمک اۆچۆن هئچ کیمین صلاحیتی یوخدور . من قئیسرین محکمهسینه مۆراجعت ائتمک ایستهییرم . \n",
            "2023-01-20 00:58:41,409 - INFO - joeynmt.training - \tHypothesis: اگر بیز گۆناهکار وه اؤلومه لایق شکیلده پیس ایش گؤرمزسه ، اؤلومدن اوزاقلاشدێم . آمما اگر بو کیشیلره اساسلانماغێمدان امتنا ائتمهسیدیلر ، منیم رازێلێغێنێن آلماق اۆچۆن منه تسلیم اولور .\n",
            "2023-01-20 00:58:41,409 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 00:58:41,411 - INFO - joeynmt.training - \tSource:     اما خدا ارزش محبت خود را این گونه به ما ثابت می‌کند که وقتی هنوز گناهکار بودیم ، مسیح جان خود را برای ما داد . \n",
            "2023-01-20 00:58:41,411 - INFO - joeynmt.training - \tReference:  لاکین آللاه بیزه اولان محبتینی بونونلا سۆبوت ائدیر کی ، بیز هله گۆناهکار اولا اولا مصیح بیزیم اوغروموزدا اؤلدو . \n",
            "2023-01-20 00:58:41,411 - INFO - joeynmt.training - \tHypothesis: آللاه بیزی مصیحه محبتله لۆتفله بیزی اؤز جانێنێ بیزیم اۆچۆن فدا ائتدی .\n",
            "2023-01-20 00:58:41,411 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 00:58:41,413 - INFO - joeynmt.training - \tSource:     سپس در همان روز است که از نعمت روی زمین‌ پرسیده خواهید شد . \n",
            "2023-01-20 00:58:41,414 - INFO - joeynmt.training - \tReference:  سونرا دا همین گۆن نئ متلر بارهسینده مۆتلق سورغو سوعال اولوناجاقسینیز ! \n",
            "2023-01-20 00:58:41,414 - INFO - joeynmt.training - \tHypothesis: او گۆن یئر اۆزۆنده نئ متلری اۆزۆندن اۆز دؤندرهجکسینیز !\n",
            "2023-01-20 00:58:41,414 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 00:58:41,416 - INFO - joeynmt.training - \tSource:     آنجا بود که‌ مؤمنان در آزمایش قرار گرفتند و سخت تکان خوردند . \n",
            "2023-01-20 00:58:41,416 - INFO - joeynmt.training - \tReference:  محض اوندا معؤ مینلر امتهاهانا چکیلمیش وه مۆحکم سارسێلمێشدێلار . \n",
            "2023-01-20 00:58:41,416 - INFO - joeynmt.training - \tHypothesis: معؤ مینلر اورادا چوخ امتهاهانا چکدیلر وه یئمهیه هئیمان اولدولار .\n",
            "2023-01-20 00:58:49,235 - INFO - joeynmt.training - Epoch 345, Step:   119100, Batch Loss:     1.557043, Batch Acc: 0.606257, Tokens per Sec:    13654, Lr: 0.000026\n",
            "2023-01-20 00:58:57,177 - INFO - joeynmt.training - Epoch 345, Step:   119200, Batch Loss:     1.523610, Batch Acc: 0.605423, Tokens per Sec:    13874, Lr: 0.000026\n",
            "2023-01-20 00:59:01,038 - INFO - joeynmt.training - Epoch 345: total training loss 531.10\n",
            "2023-01-20 00:59:01,038 - INFO - joeynmt.training - EPOCH 346\n",
            "2023-01-20 00:59:05,109 - INFO - joeynmt.training - Epoch 346, Step:   119300, Batch Loss:     1.532831, Batch Acc: 0.608841, Tokens per Sec:    13932, Lr: 0.000026\n",
            "2023-01-20 00:59:12,933 - INFO - joeynmt.training - Epoch 346, Step:   119400, Batch Loss:     1.479778, Batch Acc: 0.602805, Tokens per Sec:    13988, Lr: 0.000026\n",
            "2023-01-20 00:59:20,768 - INFO - joeynmt.training - Epoch 346, Step:   119500, Batch Loss:     1.487311, Batch Acc: 0.605428, Tokens per Sec:    14080, Lr: 0.000026\n",
            "2023-01-20 00:59:28,412 - INFO - joeynmt.training - Epoch 346: total training loss 532.04\n",
            "2023-01-20 00:59:28,412 - INFO - joeynmt.training - EPOCH 347\n",
            "2023-01-20 00:59:29,135 - INFO - joeynmt.training - Epoch 347, Step:   119600, Batch Loss:     1.422311, Batch Acc: 0.616076, Tokens per Sec:     9100, Lr: 0.000026\n",
            "2023-01-20 00:59:39,637 - INFO - joeynmt.training - Epoch 347, Step:   119700, Batch Loss:     1.519691, Batch Acc: 0.608111, Tokens per Sec:    10461, Lr: 0.000026\n",
            "2023-01-20 00:59:47,454 - INFO - joeynmt.training - Epoch 347, Step:   119800, Batch Loss:     1.460636, Batch Acc: 0.604812, Tokens per Sec:    14070, Lr: 0.000026\n",
            "2023-01-20 00:59:55,306 - INFO - joeynmt.training - Epoch 347, Step:   119900, Batch Loss:     1.538451, Batch Acc: 0.607116, Tokens per Sec:    14012, Lr: 0.000026\n",
            "2023-01-20 00:59:58,487 - INFO - joeynmt.training - Epoch 347: total training loss 531.80\n",
            "2023-01-20 00:59:58,487 - INFO - joeynmt.training - EPOCH 348\n",
            "2023-01-20 01:00:03,145 - INFO - joeynmt.training - Epoch 348, Step:   120000, Batch Loss:     1.559267, Batch Acc: 0.607902, Tokens per Sec:    14085, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.06ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8811.35ex/s]\n",
            "2023-01-20 01:00:03,442 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=120000\n",
            "2023-01-20 01:00:03,442 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:00:07,827 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:00:07,828 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:00:07,828 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:00:07,829 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:00:07,832 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.40, loss:   2.87, ppl:  17.72, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3438[sec], evaluation: 0.0385[sec]\n",
            "2023-01-20 01:00:07,834 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:00:07,838 - INFO - joeynmt.training - \tSource:     ای پدران ، به شما می‌نویسم ؛ زیرا کسی را که از آغاز بوده است ، می‌شناسید . ای مردان جوان ، به شما می‌نویسم ؛ زیرا قوی هستید و کلام خدا در شما می‌ماند و بر آن شریر غالب آمده‌اید . \n",
            "2023-01-20 01:00:07,838 - INFO - joeynmt.training - \tReference:  ائی اوشاقلار ، سیزه یازدیم ، چۆنکی آتانێ تانییرسینیز . ائی آتالار ، سیزه یازدیم ، چۆنکی باشلانغێجدان بریع وار اولانێ تانییرسینیز . ائی گنجلر ، سیزه یازدیم ، چۆنکی گۆجلۆسۆنۆز ، آللاهێن کلامێ سیزده قالێر ، شر اولانا دا غالب گلمیسینیز . \n",
            "2023-01-20 01:00:07,838 - INFO - joeynmt.training - \tHypothesis: ائی آتالار ، سیزه یازیرام ، چۆنکی باشلانغێجدان بریع وار اولانێ تانییرسینیز . ائی وار اولان ، سیزه یازیرام ، چۆنکی آللاهێن کلامێ وه زعیف وه غالب گلمیسینیز .\n",
            "2023-01-20 01:00:07,838 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:00:07,840 - INFO - joeynmt.training - \tSource:     تا همراه با تمام مقدسان بتوانید عرض ، طول ، بلندی و عمق حقیقت را به‌خوبی دریابید\n",
            "2023-01-20 01:00:07,841 - INFO - joeynmt.training - \tReference:  بئله کی بۆتۆن مۆقدسلرله بیرلیکده مسیحین محبتنینین نه قدر گئنیش ، اوزون ، اوجا وه درین اولدوغونو باشا دۆشمهیه ، \n",
            "2023-01-20 01:00:07,841 - INFO - joeynmt.training - \tHypothesis: بۆتۆن مۆقدسلره وه حقیقته گؤره اوجالدێب دوغروسو ، دوغروسو ، دنیزه ده بیلهسینیز .\n",
            "2023-01-20 01:00:07,841 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:00:07,843 - INFO - joeynmt.training - \tSource:     اما آنان به او اصرار ورزیدند که بماند و گفتند : با ما بمان ؛ زیرا چیزی به پایان روز نمانده و نزدیک غروب است . سپس با آنان به خانه رفت تا نزدشان بماند . \n",
            "2023-01-20 01:00:07,843 - INFO - joeynmt.training - \tReference:  آمما اونلار ایسهآدان تکیدله خاهش ائدیب دئدیلر : بیزیمله قال ، چۆنکی آخشام دۆشۆر وه گۆن باتماق اۆزرهدیر . او دا اونلارلا قالماق اۆچۆن ایچری گیردی . \n",
            "2023-01-20 01:00:07,843 - INFO - joeynmt.training - \tHypothesis: اونا گؤره کی او ، باشا دۆشسۆن دئیه باشچیلاری بیزیمله بیرلیکده قالدێلار . چۆنکی گۆن گۆن قالدێرێب او گۆن قالدێ .\n",
            "2023-01-20 01:00:07,844 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:00:07,846 - INFO - joeynmt.training - \tSource:     پس فرستاد تا در زندان سر یحیی را از تنش جدا کنند . \n",
            "2023-01-20 01:00:07,846 - INFO - joeynmt.training - \tReference:  بئلهلیکله ، زینداندا یهیانین بوینونو ووردوردو . \n",
            "2023-01-20 01:00:07,846 - INFO - joeynmt.training - \tHypothesis: یهیانین باشێنێ زینداندا ساخلاماق اۆچۆن یهیانین باشێییر .\n",
            "2023-01-20 01:00:15,698 - INFO - joeynmt.training - Epoch 348, Step:   120100, Batch Loss:     1.693458, Batch Acc: 0.606306, Tokens per Sec:    13425, Lr: 0.000026\n",
            "2023-01-20 01:00:23,554 - INFO - joeynmt.training - Epoch 348, Step:   120200, Batch Loss:     1.477995, Batch Acc: 0.601418, Tokens per Sec:    14059, Lr: 0.000026\n",
            "2023-01-20 01:00:30,313 - INFO - joeynmt.training - Epoch 348: total training loss 530.74\n",
            "2023-01-20 01:00:30,314 - INFO - joeynmt.training - EPOCH 349\n",
            "2023-01-20 01:00:31,524 - INFO - joeynmt.training - Epoch 349, Step:   120300, Batch Loss:     1.545445, Batch Acc: 0.602327, Tokens per Sec:    13920, Lr: 0.000026\n",
            "2023-01-20 01:00:39,443 - INFO - joeynmt.training - Epoch 349, Step:   120400, Batch Loss:     1.577787, Batch Acc: 0.611394, Tokens per Sec:    13878, Lr: 0.000026\n",
            "2023-01-20 01:00:47,264 - INFO - joeynmt.training - Epoch 349, Step:   120500, Batch Loss:     1.544261, Batch Acc: 0.606101, Tokens per Sec:    14029, Lr: 0.000026\n",
            "2023-01-20 01:00:55,037 - INFO - joeynmt.training - Epoch 349, Step:   120600, Batch Loss:     1.653111, Batch Acc: 0.605750, Tokens per Sec:    13921, Lr: 0.000026\n",
            "2023-01-20 01:00:57,575 - INFO - joeynmt.training - Epoch 349: total training loss 532.03\n",
            "2023-01-20 01:00:57,576 - INFO - joeynmt.training - EPOCH 350\n",
            "2023-01-20 01:01:02,897 - INFO - joeynmt.training - Epoch 350, Step:   120700, Batch Loss:     1.546064, Batch Acc: 0.607547, Tokens per Sec:    14130, Lr: 0.000026\n",
            "2023-01-20 01:01:10,684 - INFO - joeynmt.training - Epoch 350, Step:   120800, Batch Loss:     1.442547, Batch Acc: 0.604611, Tokens per Sec:    14089, Lr: 0.000026\n",
            "2023-01-20 01:01:18,447 - INFO - joeynmt.training - Epoch 350, Step:   120900, Batch Loss:     1.509905, Batch Acc: 0.604995, Tokens per Sec:    14057, Lr: 0.000026\n",
            "2023-01-20 01:01:24,737 - INFO - joeynmt.training - Epoch 350: total training loss 533.19\n",
            "2023-01-20 01:01:24,738 - INFO - joeynmt.training - EPOCH 351\n",
            "2023-01-20 01:01:26,411 - INFO - joeynmt.training - Epoch 351, Step:   121000, Batch Loss:     1.608583, Batch Acc: 0.608344, Tokens per Sec:    13902, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 70.31ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9324.99ex/s]\n",
            "2023-01-20 01:01:26,709 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=121000\n",
            "2023-01-20 01:01:26,710 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:01:31,497 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:01:31,497 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:01:31,497 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:01:31,498 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:01:31,501 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.17, loss:   2.92, ppl:  18.48, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7475[sec], evaluation: 0.0366[sec]\n",
            "2023-01-20 01:01:31,504 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:01:31,508 - INFO - joeynmt.training - \tSource:     آنگاه تیموتائوس را که برادر ما و خادم خداست و در مورد مسیح بشارت می‌دهد ، فرستادیم تا برای تقویت ایمانتان ، شما را استوار و دلگرم سازد\n",
            "2023-01-20 01:01:31,508 - INFO - joeynmt.training - \tReference:  وه قارداشێمێز ، مسیحین مۆژدسنی یایماقدا آللاهێن امکداشێ اولان تیموتئیی سیزی اماندا مۆحکملندیرمک وه روحلاندێرماق اۆچۆن گؤندردیک . \n",
            "2023-01-20 01:01:31,508 - INFO - joeynmt.training - \tHypothesis: بو تیموتئیی تیموتئیی وه آللاهێن خدمتینه گؤره مصیحده اولان جمیته وه ایمانینیزی مۆحکم ساییر .\n",
            "2023-01-20 01:01:31,508 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:01:31,510 - INFO - joeynmt.training - \tSource:     دعایم این است ، همین ایمان که تو و هم‌ایمانانت از آن برخوردارید ، تو را به درک تمامی برکاتی برساند که از طریق مسیح نصیبمان شده است . \n",
            "2023-01-20 01:01:31,510 - INFO - joeynmt.training - \tReference:  سن مصیحده مالک اولدوغوموز هر خئیرلی شئیی درک ائتدیکجه ایمانینی باشقالاری ایله پایلاشماقدا فعال اولماغێن اۆچۆن دوعا ائدیرم . \n",
            "2023-01-20 01:01:31,511 - INFO - joeynmt.training - \tHypothesis: ایمان واسطهسیله مۆبارضه آپارانێلان ، هامێسێ مصیحه اولان ایمان واسطهسیله مۆعزهک اولان ایمانلا واسطهسیله مۆبارضه آپاردێ .\n",
            "2023-01-20 01:01:31,511 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:01:31,513 - INFO - joeynmt.training - \tSource:     اگر عضوی دردمند شود ، تمام اعضای دیگر با آن درد خواهند کشید . اگر عضوی نیز عزت یابد ، تمام اعضای دیگر با او شادی خواهند کرد . \n",
            "2023-01-20 01:01:31,514 - INFO - joeynmt.training - \tReference:  بونا گؤره اگر بیر اۆزۆ اعذاب چکیرسه ، باشقالاری دا اونونلا بیرگه اعذاب چکیر . اۆزۆلردن بیری شرفه چاتێرسا ، باشقالاری دا اونونلا بیرگه سئوینیر . \n",
            "2023-01-20 01:01:31,514 - INFO - joeynmt.training - \tHypothesis: اگر بۆتۆن اینسانلار هامێسێ تملبهله باشقالاری ایله بیرگه آپاراجاق . اگر بۆتۆن اینسانلارلا بیرگه قالاجاقسا ، اوندا اوندا اوندا او بیری سئوینجه اویغوناجاق .\n",
            "2023-01-20 01:01:31,514 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:01:31,516 - INFO - joeynmt.training - \tSource:     اگر آن پدر را می‌خوانید که هر کس را مطابق اعمالش بی‌طرفانه قضاوت می‌کند ، پس رفتار شما مادامی که ساکن موقت هستید ، با خداترسی همراه باشد ؛ \n",
            "2023-01-20 01:01:31,516 - INFO - joeynmt.training - \tReference:  ترفکئشلیک ائتمهدن اؤز امهلینه گؤره هر کسی مۆحاکمه ائدن آللاهێ آتا دئیه چاغێردێغێنێز اۆچۆن بو غربت دۆنیاداکی واختینیزی آللاه قورخوسوندا کئچیرین . \n",
            "2023-01-20 01:01:31,516 - INFO - joeynmt.training - \tHypothesis: اگر آتالارێنێ چاغێرێرسا ، هر کسه گؤره اؤیرنکوزه گؤره اؤیرنوره . چۆنکی آللاهیمیزده یاشادیغینیز اۆچۆن داها داها یاخشیدیر .\n",
            "2023-01-20 01:01:39,450 - INFO - joeynmt.training - Epoch 351, Step:   121100, Batch Loss:     1.644474, Batch Acc: 0.608934, Tokens per Sec:    13371, Lr: 0.000026\n",
            "2023-01-20 01:01:47,238 - INFO - joeynmt.training - Epoch 351, Step:   121200, Batch Loss:     1.438446, Batch Acc: 0.605854, Tokens per Sec:    13968, Lr: 0.000026\n",
            "2023-01-20 01:01:54,997 - INFO - joeynmt.training - Epoch 351, Step:   121300, Batch Loss:     1.506347, Batch Acc: 0.605198, Tokens per Sec:    14119, Lr: 0.000026\n",
            "2023-01-20 01:01:57,086 - INFO - joeynmt.training - Epoch 351: total training loss 531.68\n",
            "2023-01-20 01:01:57,087 - INFO - joeynmt.training - EPOCH 352\n",
            "2023-01-20 01:02:02,904 - INFO - joeynmt.training - Epoch 352, Step:   121400, Batch Loss:     1.757576, Batch Acc: 0.608371, Tokens per Sec:    13882, Lr: 0.000026\n",
            "2023-01-20 01:02:10,837 - INFO - joeynmt.training - Epoch 352, Step:   121500, Batch Loss:     1.535882, Batch Acc: 0.606108, Tokens per Sec:    13829, Lr: 0.000026\n",
            "2023-01-20 01:02:18,597 - INFO - joeynmt.training - Epoch 352, Step:   121600, Batch Loss:     1.576252, Batch Acc: 0.608726, Tokens per Sec:    14392, Lr: 0.000026\n",
            "2023-01-20 01:02:24,219 - INFO - joeynmt.training - Epoch 352: total training loss 528.86\n",
            "2023-01-20 01:02:24,219 - INFO - joeynmt.training - EPOCH 353\n",
            "2023-01-20 01:02:26,538 - INFO - joeynmt.training - Epoch 353, Step:   121700, Batch Loss:     1.560507, Batch Acc: 0.610740, Tokens per Sec:    13557, Lr: 0.000026\n",
            "2023-01-20 01:02:34,387 - INFO - joeynmt.training - Epoch 353, Step:   121800, Batch Loss:     1.600595, Batch Acc: 0.608298, Tokens per Sec:    14092, Lr: 0.000026\n",
            "2023-01-20 01:02:42,218 - INFO - joeynmt.training - Epoch 353, Step:   121900, Batch Loss:     1.525822, Batch Acc: 0.607396, Tokens per Sec:    13937, Lr: 0.000026\n",
            "2023-01-20 01:02:50,055 - INFO - joeynmt.training - Epoch 353, Step:   122000, Batch Loss:     1.477969, Batch Acc: 0.605233, Tokens per Sec:    14130, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 117.49ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9895.61ex/s]\n",
            "2023-01-20 01:02:50,336 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=122000\n",
            "2023-01-20 01:02:50,336 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:02:54,343 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:02:54,343 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:02:54,344 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:02:54,344 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:02:54,348 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.22, loss:   2.89, ppl:  17.94, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 3.9690[sec], evaluation: 0.0345[sec]\n",
            "2023-01-20 01:02:54,350 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:02:54,353 - INFO - joeynmt.training - \tSource:     من به چنین شخصی فخر می‌کنم ، اما به خود فخر نخواهم کرد ، مگر در مورد ضعف‌هایم . \n",
            "2023-01-20 01:02:54,354 - INFO - joeynmt.training - \tReference:  بئله بیریسی ایله اؤیونهجهیهم ، آمما اؤزومله الاقدار اولاراق زیفلیگیمدن باشقا بیر شئیله اؤیونمهیهجهیهم . \n",
            "2023-01-20 01:02:54,354 - INFO - joeynmt.training - \tHypothesis: اؤیونمهییم ، اؤیونهمهییم ، اؤیونهمهییم .\n",
            "2023-01-20 01:02:54,354 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:02:54,356 - INFO - joeynmt.training - \tSource:     در آخر ، آن زن نیز مرد . \n",
            "2023-01-20 01:02:54,356 - INFO - joeynmt.training - \tReference:  هامیسیندان سونرا بو قادێن دا اؤلدو . \n",
            "2023-01-20 01:02:54,356 - INFO - joeynmt.training - \tHypothesis: هامیسیندا کیشی ده کیشی ده .\n",
            "2023-01-20 01:02:54,356 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:02:54,358 - INFO - joeynmt.training - \tSource:     و بسیاری از مردم آن‌ها را دیدند . \n",
            "2023-01-20 01:02:54,358 - INFO - joeynmt.training - \tReference:  اونلار ایسنانین دیریلمهسیندن سونرا قبیرلردن چێخێب مۆقدس شهره گیردیلر وه بیر چوخ اینسانا گؤروندولر . \n",
            "2023-01-20 01:02:54,358 - INFO - joeynmt.training - \tHypothesis: چوخلارێ گؤردولر .\n",
            "2023-01-20 01:02:54,359 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:02:54,361 - INFO - joeynmt.training - \tSource:      بزرگداشت‌ شنبه ، بر کسانی که در باره آن اختلاف کردند مقرر گردید ، و قطعا پروردگارت روز رستاخیز میان آنها در باره چیزی که در مورد آن اختلاف می‌کردند ، داوری خواهد کرد . \n",
            "2023-01-20 01:02:54,361 - INFO - joeynmt.training - \tReference:  شنبه گۆنۆ آنجاق اونون بارهسینده اختلافدا اولانلارا واجب ائدیلدی . حقیقتا ، ربین قیامت گۆنۆ اختلافدا اولدوقلاری مسهلر بارهسینده اونلارێن آراسێندا هؤکم وئرهجکدیر . \n",
            "2023-01-20 01:02:54,361 - INFO - joeynmt.training - \tHypothesis: او ، غیبی ، بیلنلرین اجرابت گۆنۆ اختلافدادیر . ربینین اختلافدا اولدوقلاری مسهلر بارهسینده قیامت گۆنۆ اختلافدا اولدوقلاری مسهلر بارهسینده آرالارێندا هؤکم وئرهجکدیر !\n",
            "2023-01-20 01:02:55,704 - INFO - joeynmt.training - Epoch 353: total training loss 529.87\n",
            "2023-01-20 01:02:55,704 - INFO - joeynmt.training - EPOCH 354\n",
            "2023-01-20 01:03:05,123 - INFO - joeynmt.training - Epoch 354, Step:   122100, Batch Loss:     1.574032, Batch Acc: 0.606512, Tokens per Sec:     9711, Lr: 0.000026\n",
            "2023-01-20 01:03:12,946 - INFO - joeynmt.training - Epoch 354, Step:   122200, Batch Loss:     1.456567, Batch Acc: 0.607227, Tokens per Sec:    14276, Lr: 0.000026\n",
            "2023-01-20 01:03:20,882 - INFO - joeynmt.training - Epoch 354, Step:   122300, Batch Loss:     1.639884, Batch Acc: 0.605107, Tokens per Sec:    13813, Lr: 0.000026\n",
            "2023-01-20 01:03:25,941 - INFO - joeynmt.training - Epoch 354: total training loss 527.50\n",
            "2023-01-20 01:03:25,941 - INFO - joeynmt.training - EPOCH 355\n",
            "2023-01-20 01:03:29,017 - INFO - joeynmt.training - Epoch 355, Step:   122400, Batch Loss:     1.424843, Batch Acc: 0.607932, Tokens per Sec:    13725, Lr: 0.000026\n",
            "2023-01-20 01:03:36,908 - INFO - joeynmt.training - Epoch 355, Step:   122500, Batch Loss:     1.615757, Batch Acc: 0.607522, Tokens per Sec:    14160, Lr: 0.000026\n",
            "2023-01-20 01:03:44,736 - INFO - joeynmt.training - Epoch 355, Step:   122600, Batch Loss:     1.555101, Batch Acc: 0.609779, Tokens per Sec:    14211, Lr: 0.000026\n",
            "2023-01-20 01:03:52,523 - INFO - joeynmt.training - Epoch 355, Step:   122700, Batch Loss:     1.592157, Batch Acc: 0.603030, Tokens per Sec:    13976, Lr: 0.000026\n",
            "2023-01-20 01:03:53,009 - INFO - joeynmt.training - Epoch 355: total training loss 526.65\n",
            "2023-01-20 01:03:53,010 - INFO - joeynmt.training - EPOCH 356\n",
            "2023-01-20 01:04:00,409 - INFO - joeynmt.training - Epoch 356, Step:   122800, Batch Loss:     1.614431, Batch Acc: 0.608333, Tokens per Sec:    13793, Lr: 0.000026\n",
            "2023-01-20 01:04:08,361 - INFO - joeynmt.training - Epoch 356, Step:   122900, Batch Loss:     1.492697, Batch Acc: 0.609878, Tokens per Sec:    13682, Lr: 0.000026\n",
            "2023-01-20 01:04:16,205 - INFO - joeynmt.training - Epoch 356, Step:   123000, Batch Loss:     1.554077, Batch Acc: 0.605388, Tokens per Sec:    14122, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 139.61ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9197.50ex/s]\n",
            "2023-01-20 01:04:16,497 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=123000\n",
            "2023-01-20 01:04:16,498 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:04:20,745 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:04:20,746 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:04:20,746 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:04:20,747 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:04:20,750 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.91, loss:   2.75, ppl:  15.69, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2107[sec], evaluation: 0.0348[sec]\n",
            "2023-01-20 01:04:20,753 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:04:20,756 - INFO - joeynmt.training - \tSource:     به همین‌سان ، سرور در خصوص کسانی که بشارت را اعلام می‌کنند ، فرمان داد که روزی خود را از طریق بشارت دریافت کنند . \n",
            "2023-01-20 01:04:20,756 - INFO - joeynmt.training - \tReference:  بئلهجه ده رب مۆژدنی ائلان ائدنلره مۆژدن دولانماغێ امر ائتمیشدیر . \n",
            "2023-01-20 01:04:20,756 - INFO - joeynmt.training - \tHypothesis: بئلهجه ربه امر ائتدیگی مۆژدنی یایماق اۆچۆن مۆژدنی یایماق اۆچۆن مۆژدنی یایماق اۆچۆن مۆژدنی یایماق اۆچۆن تعیین اولوندو .\n",
            "2023-01-20 01:04:20,757 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:04:20,759 - INFO - joeynmt.training - \tSource:     تمام جمعیت شگفت‌زده شدند و گفتند : آیا ممکن است که او پسر داوود باشد ؟ \n",
            "2023-01-20 01:04:20,759 - INFO - joeynmt.training - \tReference:  بۆتۆن خالق بو ، داوودون اوغلو دئییلمی ؟ دئیه مات قالدێ . \n",
            "2023-01-20 01:04:20,759 - INFO - joeynmt.training - \tHypothesis: بۆتۆن ازدهام بئله هئیرت ائدیردی : داوودون اوغلودورمو ؟\n",
            "2023-01-20 01:04:20,759 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:04:20,765 - INFO - joeynmt.training - \tSource:     عیسی گفت : ای مرد ، چه کسی مرا میان شما دو نفر ، داور یا میانجی قرار داده است ؟ \n",
            "2023-01-20 01:04:20,766 - INFO - joeynmt.training - \tReference:  ایسا اونا بئله جاواب وئردی : آی اینسان ، منی کیم اۆزهرینیزه حاکم یا وکیل قویوب ؟ \n",
            "2023-01-20 01:04:20,766 - INFO - joeynmt.training - \tHypothesis: ایسا دئدی : ائی کور آدام ، منی آرانێزدا اولان ایکی نفر وار ؟ آرانێزدا اولان وه اونلاردان ایکی نفر وار ؟\n",
            "2023-01-20 01:04:20,766 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:04:20,768 - INFO - joeynmt.training - \tSource:     آنان را به جایگاهی که آن را می‌پسندند درخواهد آورد ، و شک نیست که خداوند دانایی بردبار است . \n",
            "2023-01-20 01:04:20,769 - INFO - joeynmt.training - \tReference:   اونلارێ رازێ قالاجاقلاری بیر یئره داخل ائدهجکدیر . حقیقتا ، آللاه بیلندیر ، هلیمدیر ! \n",
            "2023-01-20 01:04:20,769 - INFO - joeynmt.training - \tHypothesis: اونلارا ائله بیر یئر اۆزرلری ده واردێر کی ، او ، شبههسیز کی ، آللاه بیلندیر !\n",
            "2023-01-20 01:04:25,108 - INFO - joeynmt.training - Epoch 356: total training loss 529.78\n",
            "2023-01-20 01:04:25,108 - INFO - joeynmt.training - EPOCH 357\n",
            "2023-01-20 01:04:28,798 - INFO - joeynmt.training - Epoch 357, Step:   123100, Batch Loss:     1.579051, Batch Acc: 0.614341, Tokens per Sec:    14042, Lr: 0.000025\n",
            "2023-01-20 01:04:36,693 - INFO - joeynmt.training - Epoch 357, Step:   123200, Batch Loss:     1.479795, Batch Acc: 0.609471, Tokens per Sec:    13948, Lr: 0.000025\n",
            "2023-01-20 01:04:44,518 - INFO - joeynmt.training - Epoch 357, Step:   123300, Batch Loss:     1.596759, Batch Acc: 0.607451, Tokens per Sec:    13945, Lr: 0.000025\n",
            "2023-01-20 01:04:52,328 - INFO - joeynmt.training - Epoch 357: total training loss 527.29\n",
            "2023-01-20 01:04:52,329 - INFO - joeynmt.training - EPOCH 358\n",
            "2023-01-20 01:04:52,406 - INFO - joeynmt.training - Epoch 358, Step:   123400, Batch Loss:     1.583216, Batch Acc: 0.608844, Tokens per Sec:    15338, Lr: 0.000025\n",
            "2023-01-20 01:05:00,248 - INFO - joeynmt.training - Epoch 358, Step:   123500, Batch Loss:     1.457469, Batch Acc: 0.607426, Tokens per Sec:    13946, Lr: 0.000025\n",
            "2023-01-20 01:05:08,105 - INFO - joeynmt.training - Epoch 358, Step:   123600, Batch Loss:     1.426559, Batch Acc: 0.606668, Tokens per Sec:    14056, Lr: 0.000025\n",
            "2023-01-20 01:05:15,893 - INFO - joeynmt.training - Epoch 358, Step:   123700, Batch Loss:     1.534682, Batch Acc: 0.608227, Tokens per Sec:    14004, Lr: 0.000025\n",
            "2023-01-20 01:05:19,454 - INFO - joeynmt.training - Epoch 358: total training loss 528.85\n",
            "2023-01-20 01:05:19,454 - INFO - joeynmt.training - EPOCH 359\n",
            "2023-01-20 01:05:23,832 - INFO - joeynmt.training - Epoch 359, Step:   123800, Batch Loss:     1.649266, Batch Acc: 0.605160, Tokens per Sec:    13875, Lr: 0.000025\n",
            "2023-01-20 01:05:31,821 - INFO - joeynmt.training - Epoch 359, Step:   123900, Batch Loss:     1.542557, Batch Acc: 0.609719, Tokens per Sec:    13721, Lr: 0.000025\n",
            "2023-01-20 01:05:39,739 - INFO - joeynmt.training - Epoch 359, Step:   124000, Batch Loss:     1.561347, Batch Acc: 0.610216, Tokens per Sec:    13967, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.25ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9737.99ex/s]\n",
            "2023-01-20 01:05:40,018 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=124000\n",
            "2023-01-20 01:05:40,018 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:05:44,891 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:05:44,891 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:05:44,892 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:05:44,892 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:05:44,895 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.82, loss:   2.88, ppl:  17.74, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.8338[sec], evaluation: 0.0362[sec]\n",
            "2023-01-20 01:05:44,898 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:05:44,902 - INFO - joeynmt.training - \tSource:     و هیچ چیز مانع پذیرفته شدن انفاقهای آنان نشد جز اینکه به خدا و پیامبرش کفر ورزیدند ، و جز با حال‌ کسالت نماز به جا نمی‌آورند ، و جز با کراهت انفاق نمی‌کنند . \n",
            "2023-01-20 01:05:44,902 - INFO - joeynmt.training - \tReference:  اونلارێن خرجلهدیکلرینین قبول اولونماسێنا مانع اولان یالنیز آللاهێ وه اونون پیغمبرینی اینکار ائتمهلری ، نامازا تنبل تنبل گلمهلری وه ایستهمهیه ایستهمهیه خرجلهمهلریدیر . \n",
            "2023-01-20 01:05:44,902 - INFO - joeynmt.training - \tHypothesis: اونلارا وئردیگیمیز هئچ بیر شئی وئرمز ، آللاها وه پیغمبرینه ایتاعت ائتمهینلر . ناماز قێلمایانلار ، ناماز قێلمایانلار ، ذکات وئرمزلر .\n",
            "2023-01-20 01:05:44,902 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:05:44,905 - INFO - joeynmt.training - \tSource:     او روزهای بسیار به این کار ادامه داد . سرانجام پولس به ستوه آمد و برگشت و خطاب به آن روح گفت : به نام عیسی مسیح به تو فرمان می‌دهم که از او بیرون آیی . روح همان دم بیرون آمد . \n",
            "2023-01-20 01:05:44,905 - INFO - joeynmt.training - \tReference:  بونو گۆنبهگۆن تکرار ائتدی . بو ، پاولو چوخ بئزدیردی . نهایت ، او چئوریلیب روحا دئدی : ایسا مسیحین آدێ ایله سنه امر ائدیرم ، بو قێزدان چێخ ! روح درحال قێزدان چێخدێ . \n",
            "2023-01-20 01:05:44,905 - INFO - joeynmt.training - \tHypothesis: بو ایشین سککیز گۆن قالدێ . سونرا پاولون جسهدینی گؤتوردو وه اونا دئدی : روحدا ، ایسا مصیح آدێنێن آدێ ایله چێخما وه امر ائدیرم کی ، روحدان چێخێب گئتسین .\n",
            "2023-01-20 01:05:44,905 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:05:44,907 - INFO - joeynmt.training - \tSource:     می‌فرماید : همان طور که نشانه‌های ما بر تو آمد و آن را به فراموشی سپردی ، امروز همان گونه فراموش می‌شوی . \n",
            "2023-01-20 01:05:44,908 - INFO - joeynmt.training - \tReference:   بویورار : ائلهدیر ، آمما سنه آیهلریمیز گلدی ، سن ایسه اونلارێ اونوتدون . بو گۆن ائلهجه ده سن اونودولاجاقسان ! \n",
            "2023-01-20 01:05:44,908 - INFO - joeynmt.training - \tHypothesis: بویورآجاقدیر : سن آیهلریمیزی سنه قایتاریلدیغین زامان بو گۆن اونوتون کیمین امانت اولوندوغونوز کیمین بو گۆنهدین !\n",
            "2023-01-20 01:05:44,908 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:05:44,910 - INFO - joeynmt.training - \tSource:     و به گواه و مورد گواهی ، \n",
            "2023-01-20 01:05:44,910 - INFO - joeynmt.training - \tReference:  آند اولسون شهادت وئرهنه وه شهادت وئریلهنه کی ، \n",
            "2023-01-20 01:05:44,910 - INFO - joeynmt.training - \tHypothesis: وه شاهدلره شهادت اولسون ،\n",
            "2023-01-20 01:05:52,028 - INFO - joeynmt.training - Epoch 359: total training loss 526.98\n",
            "2023-01-20 01:05:52,028 - INFO - joeynmt.training - EPOCH 360\n",
            "2023-01-20 01:05:52,818 - INFO - joeynmt.training - Epoch 360, Step:   124100, Batch Loss:     1.599500, Batch Acc: 0.613026, Tokens per Sec:    14341, Lr: 0.000025\n",
            "2023-01-20 01:06:00,604 - INFO - joeynmt.training - Epoch 360, Step:   124200, Batch Loss:     1.436593, Batch Acc: 0.609784, Tokens per Sec:    14254, Lr: 0.000025\n",
            "2023-01-20 01:06:08,425 - INFO - joeynmt.training - Epoch 360, Step:   124300, Batch Loss:     1.567778, Batch Acc: 0.606375, Tokens per Sec:    14011, Lr: 0.000025\n",
            "2023-01-20 01:06:16,251 - INFO - joeynmt.training - Epoch 360, Step:   124400, Batch Loss:     1.377256, Batch Acc: 0.608246, Tokens per Sec:    14028, Lr: 0.000025\n",
            "2023-01-20 01:06:19,078 - INFO - joeynmt.training - Epoch 360: total training loss 527.26\n",
            "2023-01-20 01:06:19,078 - INFO - joeynmt.training - EPOCH 361\n",
            "2023-01-20 01:06:24,064 - INFO - joeynmt.training - Epoch 361, Step:   124500, Batch Loss:     1.392938, Batch Acc: 0.615095, Tokens per Sec:    14050, Lr: 0.000025\n",
            "2023-01-20 01:06:35,080 - INFO - joeynmt.training - Epoch 361, Step:   124600, Batch Loss:     1.792647, Batch Acc: 0.606647, Tokens per Sec:     9940, Lr: 0.000025\n",
            "2023-01-20 01:06:42,998 - INFO - joeynmt.training - Epoch 361, Step:   124700, Batch Loss:     1.568551, Batch Acc: 0.608058, Tokens per Sec:    13964, Lr: 0.000025\n",
            "2023-01-20 01:06:49,334 - INFO - joeynmt.training - Epoch 361: total training loss 524.88\n",
            "2023-01-20 01:06:49,335 - INFO - joeynmt.training - EPOCH 362\n",
            "2023-01-20 01:06:50,857 - INFO - joeynmt.training - Epoch 362, Step:   124800, Batch Loss:     1.492260, Batch Acc: 0.613883, Tokens per Sec:    14107, Lr: 0.000025\n",
            "2023-01-20 01:06:58,735 - INFO - joeynmt.training - Epoch 362, Step:   124900, Batch Loss:     1.541982, Batch Acc: 0.608251, Tokens per Sec:    14094, Lr: 0.000025\n",
            "2023-01-20 01:07:06,642 - INFO - joeynmt.training - Epoch 362, Step:   125000, Batch Loss:     1.468770, Batch Acc: 0.606735, Tokens per Sec:    14015, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.35ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9012.56ex/s]\n",
            "2023-01-20 01:07:06,971 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=125000\n",
            "2023-01-20 01:07:06,971 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:07:12,255 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:07:12,255 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:07:12,256 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:07:12,257 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:07:12,260 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   5.89, loss:   2.95, ppl:  19.10, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1189[sec], evaluation: 0.1624[sec]\n",
            "2023-01-20 01:07:12,262 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:07:12,266 - INFO - joeynmt.training - \tSource:     دعایم این است ، همین ایمان که تو و هم‌ایمانانت از آن برخوردارید ، تو را به درک تمامی برکاتی برساند که از طریق مسیح نصیبمان شده است . \n",
            "2023-01-20 01:07:12,267 - INFO - joeynmt.training - \tReference:  سن مصیحده مالک اولدوغوموز هر خئیرلی شئیی درک ائتدیکجه ایمانینی باشقالاری ایله پایلاشماقدا فعال اولماغێن اۆچۆن دوعا ائدیرم . \n",
            "2023-01-20 01:07:12,267 - INFO - joeynmt.training - \tHypothesis: ایمان واسطهسیله مۆبارضه آپارانێلان ، هر وزیگتده همیشهلیک اقسانهاته مالک اولان ایمان واسطهسیله مۆعزه مالک اولدو .\n",
            "2023-01-20 01:07:12,267 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:07:12,269 - INFO - joeynmt.training - \tSource:     پس او را گرفتند ، از تاکستان بیرون انداختند و کشتند . با این وصف ، مالک تاکستان با آن باغبانان چه خواهد کرد ؟ \n",
            "2023-01-20 01:07:12,269 - INFO - joeynmt.training - \tReference:  بئلهجه اونو باغدان کنارا چێخارێب اؤلدوردولر . بئلهلیکله ، باغ صاحبی اونلارا نه ائدهجک ؟ \n",
            "2023-01-20 01:07:12,269 - INFO - joeynmt.training - \tHypothesis: اونو توتوب اؤلدوردولر . سونرا ایسه اؤلدوردولر . او ، باغبانلارلادێلار کی ، باغبانلارلا یاناشیلار نه ائدهجک ؟\n",
            "2023-01-20 01:07:12,269 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:07:12,271 - INFO - joeynmt.training - \tSource:     ای نادان ، آیا می‌خواهی بدانی که چرا ایمان ، بدون عمل بی‌فایده است ؟ \n",
            "2023-01-20 01:07:12,272 - INFO - joeynmt.training - \tReference:  ائی آغێلسێز آدام ، املسیز ایمانین پوچ اولدوغونا سۆبوت ایستهییرسنمی ؟ \n",
            "2023-01-20 01:07:12,272 - INFO - joeynmt.training - \tHypothesis: ائی آغیلسیزلێق ، نه اۆچۆن ایمانسیزلیغین حالدا نه اۆچۆن املسیزلێقدیر ؟\n",
            "2023-01-20 01:07:12,272 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:07:12,274 - INFO - joeynmt.training - \tSource:     بروید و مفهوم این گفته را درک کنید : رحمت می‌خواهم ، نه قربانی . چون آمده‌ام تا گناهکاران را فراخوانم ، نه درستکاران را . \n",
            "2023-01-20 01:07:12,274 - INFO - joeynmt.training - \tReference:  گئدین وه من قوربان دئییل ، مرحمت ایستهییرم سؤزونون مناسێنێ اؤیرهنین . چۆنکی من سالهلری دئییل ، گۆناهکارلاری چاغێرماق اۆچۆن گلمیشم . \n",
            "2023-01-20 01:07:12,274 - INFO - joeynmt.training - \tHypothesis: گئدین ، ملته ، مرحمتیمی بئله دئییلهجک : رحمی گلدیم . چۆنکی گۆناهکارلارێمه ائتمک اۆچۆن دئییل ، سالئهلیک سالهلرین دئییل .\n",
            "2023-01-20 01:07:20,057 - INFO - joeynmt.training - Epoch 362, Step:   125100, Batch Loss:     1.452099, Batch Acc: 0.609025, Tokens per Sec:    13550, Lr: 0.000025\n",
            "2023-01-20 01:07:22,036 - INFO - joeynmt.training - Epoch 362: total training loss 522.25\n",
            "2023-01-20 01:07:22,037 - INFO - joeynmt.training - EPOCH 363\n",
            "2023-01-20 01:07:28,044 - INFO - joeynmt.training - Epoch 363, Step:   125200, Batch Loss:     1.525365, Batch Acc: 0.614209, Tokens per Sec:    13884, Lr: 0.000025\n",
            "2023-01-20 01:07:35,999 - INFO - joeynmt.training - Epoch 363, Step:   125300, Batch Loss:     1.433800, Batch Acc: 0.610640, Tokens per Sec:    13797, Lr: 0.000025\n",
            "2023-01-20 01:07:43,839 - INFO - joeynmt.training - Epoch 363, Step:   125400, Batch Loss:     1.555207, Batch Acc: 0.607792, Tokens per Sec:    13932, Lr: 0.000025\n",
            "2023-01-20 01:07:49,427 - INFO - joeynmt.training - Epoch 363: total training loss 527.64\n",
            "2023-01-20 01:07:49,428 - INFO - joeynmt.training - EPOCH 364\n",
            "2023-01-20 01:07:51,717 - INFO - joeynmt.training - Epoch 364, Step:   125500, Batch Loss:     1.449426, Batch Acc: 0.610303, Tokens per Sec:    14264, Lr: 0.000025\n",
            "2023-01-20 01:07:59,533 - INFO - joeynmt.training - Epoch 364, Step:   125600, Batch Loss:     1.446748, Batch Acc: 0.609601, Tokens per Sec:    14215, Lr: 0.000025\n",
            "2023-01-20 01:08:07,365 - INFO - joeynmt.training - Epoch 364, Step:   125700, Batch Loss:     1.598202, Batch Acc: 0.607802, Tokens per Sec:    14062, Lr: 0.000025\n",
            "2023-01-20 01:08:15,173 - INFO - joeynmt.training - Epoch 364, Step:   125800, Batch Loss:     1.396557, Batch Acc: 0.609513, Tokens per Sec:    14028, Lr: 0.000025\n",
            "2023-01-20 01:08:16,428 - INFO - joeynmt.training - Epoch 364: total training loss 525.29\n",
            "2023-01-20 01:08:16,428 - INFO - joeynmt.training - EPOCH 365\n",
            "2023-01-20 01:08:23,052 - INFO - joeynmt.training - Epoch 365, Step:   125900, Batch Loss:     1.392037, Batch Acc: 0.611909, Tokens per Sec:    13920, Lr: 0.000025\n",
            "2023-01-20 01:08:31,094 - INFO - joeynmt.training - Epoch 365, Step:   126000, Batch Loss:     1.550401, Batch Acc: 0.608490, Tokens per Sec:    13725, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.46ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10350.60ex/s]\n",
            "2023-01-20 01:08:31,377 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=126000\n",
            "2023-01-20 01:08:31,377 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:08:36,007 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:08:36,008 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:08:36,008 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:08:36,009 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:08:36,011 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.90, loss:   2.87, ppl:  17.72, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5905[sec], evaluation: 0.0363[sec]\n",
            "2023-01-20 01:08:36,014 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:08:36,017 - INFO - joeynmt.training - \tSource:     تا زمانی که نور را دارید ، به نور ایمان بورزید تا پسران نور شوید . عیسی پس از گفتن این سخنان آنجا را ترک کرد و خود را از آنان پنهان ساخت . \n",
            "2023-01-20 01:08:36,017 - INFO - joeynmt.training - \tReference:  نورونوز وار ایکن نورا ایمان ائدین کی ، نور اؤؤلادلارێ اولاسێنێز . ایسا بو سؤزلری سؤیلهیندن سونرا گئدیب اونلاردان گیزلندی . \n",
            "2023-01-20 01:08:36,018 - INFO - joeynmt.training - \tHypothesis: نورونوزدا اولان ایمانین نورونوزدا اولان اوغلونون نورونو قالدێرێن . ایسا اونلارا دئدی : بو سؤزلری سؤیلهدیکدن سونرا اونلارێ ترک ائتدی .\n",
            "2023-01-20 01:08:36,018 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:08:36,020 - INFO - joeynmt.training - \tSource:     و اوست آن کس که بادها را نویدی پیشاپیش رحمت خویش باران‌ فرستاد و از آسمان ، آبی پاک فرود آوردیم ، \n",
            "2023-01-20 01:08:36,020 - INFO - joeynmt.training - \tReference:  کۆلکلری رحمتی اؤنونده مۆژدهچی اولاراق گؤندهرن ده اودور . بیز گؤیدن ترتمیز سو ائندیردیک کی ، \n",
            "2023-01-20 01:08:36,020 - INFO - joeynmt.training - \tHypothesis: اؤز مرحمتی ایله اولجهدن سیزه اؤز مرحمتندن اول اؤز مرحمتندن اۆستۆن توتاجاغێق .\n",
            "2023-01-20 01:08:36,020 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:08:36,022 - INFO - joeynmt.training - \tSource:     آیا ندیده‌اند که خدا چگونه آفرینش را آغاز می‌کند سپس آن را باز می‌گرداند ؟ در حقیقت ، این کار بر خدا آسان است . \n",
            "2023-01-20 01:08:36,023 - INFO - joeynmt.training - \tReference:  مگر اونلار آللاهێن مخلوقاتێ اولجه نئجه یاراتدیغینی ، سونرا دا اونو یئنیدن دیریلدهجهیینی بیلمیرلرمی ؟ حقیقتا ، بو ، آللاه اۆچۆن آساندێر ! \n",
            "2023-01-20 01:08:36,023 - INFO - joeynmt.training - \tHypothesis: مگر آللاهێن یارادیلیشی نئجه یارادیلیشی گؤرمورسنمی ؟ سونرا آللاه بو ایشی گؤرۆر . بو ، آللاهێن چوخ آساندێر .\n",
            "2023-01-20 01:08:36,023 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:08:36,025 - INFO - joeynmt.training - \tSource:     یا چون عذاب را ببیند ، بگوید : کاش مرا برگشتی بود تا از نیکوکاران می‌شدم . \n",
            "2023-01-20 01:08:36,025 - INFO - joeynmt.training - \tReference:  وه یا ازابێ گؤردوگو زامان : کاش بیر دفه ده قاییدا بیلهیدیم ؛ یاخشی ایشلر گؤرنلردن اولاردێم ! دئمهسین ! \n",
            "2023-01-20 01:08:36,025 - INFO - joeynmt.training - \tHypothesis: یا اعذاب گؤردۆکده دئ : من یاخشی ایش گؤرمکدن اولارام ! دئیهجکدیر .\n",
            "2023-01-20 01:08:43,919 - INFO - joeynmt.training - Epoch 365, Step:   126100, Batch Loss:     1.541531, Batch Acc: 0.610406, Tokens per Sec:    13340, Lr: 0.000025\n",
            "2023-01-20 01:08:48,824 - INFO - joeynmt.training - Epoch 365: total training loss 525.54\n",
            "2023-01-20 01:08:48,825 - INFO - joeynmt.training - EPOCH 366\n",
            "2023-01-20 01:08:51,867 - INFO - joeynmt.training - Epoch 366, Step:   126200, Batch Loss:     1.565261, Batch Acc: 0.604800, Tokens per Sec:    14097, Lr: 0.000025\n",
            "2023-01-20 01:08:59,722 - INFO - joeynmt.training - Epoch 366, Step:   126300, Batch Loss:     1.520934, Batch Acc: 0.611909, Tokens per Sec:    13853, Lr: 0.000025\n",
            "2023-01-20 01:09:07,579 - INFO - joeynmt.training - Epoch 366, Step:   126400, Batch Loss:     1.538795, Batch Acc: 0.609101, Tokens per Sec:    14077, Lr: 0.000025\n",
            "2023-01-20 01:09:15,456 - INFO - joeynmt.training - Epoch 366, Step:   126500, Batch Loss:     1.431727, Batch Acc: 0.608829, Tokens per Sec:    14034, Lr: 0.000025\n",
            "2023-01-20 01:09:15,984 - INFO - joeynmt.training - Epoch 366: total training loss 522.71\n",
            "2023-01-20 01:09:15,984 - INFO - joeynmt.training - EPOCH 367\n",
            "2023-01-20 01:09:23,356 - INFO - joeynmt.training - Epoch 367, Step:   126600, Batch Loss:     1.541091, Batch Acc: 0.609854, Tokens per Sec:    14130, Lr: 0.000025\n",
            "2023-01-20 01:09:31,305 - INFO - joeynmt.training - Epoch 367, Step:   126700, Batch Loss:     1.497718, Batch Acc: 0.612437, Tokens per Sec:    13795, Lr: 0.000025\n",
            "2023-01-20 01:09:39,136 - INFO - joeynmt.training - Epoch 367, Step:   126800, Batch Loss:     1.498020, Batch Acc: 0.610682, Tokens per Sec:    14047, Lr: 0.000025\n",
            "2023-01-20 01:09:43,185 - INFO - joeynmt.training - Epoch 367: total training loss 523.65\n",
            "2023-01-20 01:09:43,185 - INFO - joeynmt.training - EPOCH 368\n",
            "2023-01-20 01:09:47,095 - INFO - joeynmt.training - Epoch 368, Step:   126900, Batch Loss:     1.510801, Batch Acc: 0.610917, Tokens per Sec:    13922, Lr: 0.000025\n",
            "2023-01-20 01:09:54,954 - INFO - joeynmt.training - Epoch 368, Step:   127000, Batch Loss:     1.525856, Batch Acc: 0.613704, Tokens per Sec:    14197, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.84ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9235.43ex/s]\n",
            "2023-01-20 01:09:55,262 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=127000\n",
            "2023-01-20 01:09:55,262 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:10:01,133 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:10:01,137 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:10:01,137 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:10:01,138 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:10:01,144 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.59, loss:   2.82, ppl:  16.79, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.7971[sec], evaluation: 0.0723[sec]\n",
            "2023-01-20 01:10:01,150 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:10:01,154 - INFO - joeynmt.training - \tSource:     کدام یک از شما می‌تواند مرا به گناه محکوم کند ؟ پس اگر من حقیقت را می‌گویم ، چرا سخن مرا باور نمی‌کنید ؟ \n",
            "2023-01-20 01:10:01,154 - INFO - joeynmt.training - \tReference:  سیزدن کیم سۆبوت ائدر کی ، من بیر گۆناه ائتمیشم ؟ من حقیقتی سؤیلهییرهمسه ، نیه منه اینانمیرسینیز ؟ \n",
            "2023-01-20 01:10:01,154 - INFO - joeynmt.training - \tHypothesis: سیزلردن هانسێ منی محکوم ائده بیلرسینیزسه ، من نه اۆچۆن حقیقتی سؤیلهییرم ؟ بس منیمله دانێشمێرسێنێز ؟\n",
            "2023-01-20 01:10:01,156 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:10:01,159 - INFO - joeynmt.training - \tSource:     و می‌گویند : به خدا و پیامبر او گرویدیم و اطاعت کردیم . آنگاه دسته‌ای از ایشان پس از این اقرار روی برمی‌گردانند ، و آنان مؤمن نیستند . \n",
            "2023-01-20 01:10:01,159 - INFO - joeynmt.training - \tReference:   آللاها ، پیغمبره ایمان گتیردیک ، ایتاعت ائتدیک ! دئیر ، بوندان سونرا ایسه اونلاردان بیر دسته اۆز چئویرر . بئلهلری معؤ مین دئییللر . \n",
            "2023-01-20 01:10:01,159 - INFO - joeynmt.training - \tHypothesis: اونلار : آللاها وه پیغمبره ایتاعت ائتدیک . بیز اونا ایتاعت ائتدیک . سونرا اونلارا بیر دسته اۆز چئویریب گئتدیلر . اونلار معؤ مین اولارلار .\n",
            "2023-01-20 01:10:01,159 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:10:01,162 - INFO - joeynmt.training - \tSource:     كه بد جوش خورده بود و گوشت سرخ از لای شیارهای صورتش برق میزد \n",
            "2023-01-20 01:10:01,165 - INFO - joeynmt.training - \tReference:  کی واختیندا پیس بیتیشیب أتی نین یاریقی گؤزه ویریردی\n",
            "2023-01-20 01:10:01,165 - INFO - joeynmt.training - \tHypothesis: کی حاجینین اتینی و قاتلاشیب و گؤزونون آلتی گونلری قیرمیزی قاتلادی\n",
            "2023-01-20 01:10:01,165 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:10:01,168 - INFO - joeynmt.training - \tSource:     پس از آن چنین افزود : پادشاهی خدا را می‌توان این گونه تشبیه کرد ؛ مردی بذر بر زمین می‌پاشد ، \n",
            "2023-01-20 01:10:01,168 - INFO - joeynmt.training - \tReference:  سونرا ایسا دئدی : آللاهێن پادشاهلێغێ تورپاغا توخوم سپن اکینچیه بنزهییر . \n",
            "2023-01-20 01:10:01,168 - INFO - joeynmt.training - \tHypothesis: بوندان سونرا ایسا بئله دئییر : آللاهێن پادشاهلیغینی ایرس اولاراق آلاجاق . یئر اۆزۆنده بیر آدامێن اۆزۆنه قارالێغا چکیر .\n",
            "2023-01-20 01:10:10,514 - INFO - joeynmt.training - Epoch 368, Step:   127100, Batch Loss:     1.632097, Batch Acc: 0.607956, Tokens per Sec:    11302, Lr: 0.000025\n",
            "2023-01-20 01:10:18,041 - INFO - joeynmt.training - Epoch 368: total training loss 523.26\n",
            "2023-01-20 01:10:18,041 - INFO - joeynmt.training - EPOCH 369\n",
            "2023-01-20 01:10:18,362 - INFO - joeynmt.training - Epoch 369, Step:   127200, Batch Loss:     1.495073, Batch Acc: 0.622289, Tokens per Sec:    13521, Lr: 0.000025\n",
            "2023-01-20 01:10:26,286 - INFO - joeynmt.training - Epoch 369, Step:   127300, Batch Loss:     1.483727, Batch Acc: 0.612729, Tokens per Sec:    13958, Lr: 0.000025\n",
            "2023-01-20 01:10:34,286 - INFO - joeynmt.training - Epoch 369, Step:   127400, Batch Loss:     1.544509, Batch Acc: 0.611537, Tokens per Sec:    13767, Lr: 0.000025\n",
            "2023-01-20 01:10:42,099 - INFO - joeynmt.training - Epoch 369, Step:   127500, Batch Loss:     1.550072, Batch Acc: 0.608777, Tokens per Sec:    13940, Lr: 0.000025\n",
            "2023-01-20 01:10:45,416 - INFO - joeynmt.training - Epoch 369: total training loss 524.55\n",
            "2023-01-20 01:10:45,417 - INFO - joeynmt.training - EPOCH 370\n",
            "2023-01-20 01:10:49,995 - INFO - joeynmt.training - Epoch 370, Step:   127600, Batch Loss:     1.449071, Batch Acc: 0.612995, Tokens per Sec:    13922, Lr: 0.000025\n",
            "2023-01-20 01:10:57,799 - INFO - joeynmt.training - Epoch 370, Step:   127700, Batch Loss:     1.619954, Batch Acc: 0.609183, Tokens per Sec:    14072, Lr: 0.000025\n",
            "2023-01-20 01:11:05,596 - INFO - joeynmt.training - Epoch 370, Step:   127800, Batch Loss:     1.407997, Batch Acc: 0.609227, Tokens per Sec:    14167, Lr: 0.000025\n",
            "2023-01-20 01:11:12,558 - INFO - joeynmt.training - Epoch 370: total training loss 522.92\n",
            "2023-01-20 01:11:12,558 - INFO - joeynmt.training - EPOCH 371\n",
            "2023-01-20 01:11:13,595 - INFO - joeynmt.training - Epoch 371, Step:   127900, Batch Loss:     1.490545, Batch Acc: 0.613531, Tokens per Sec:    14088, Lr: 0.000025\n",
            "2023-01-20 01:11:21,423 - INFO - joeynmt.training - Epoch 371, Step:   128000, Batch Loss:     1.546337, Batch Acc: 0.612058, Tokens per Sec:    14001, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 136.63ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8554.66ex/s]\n",
            "2023-01-20 01:11:21,724 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=128000\n",
            "2023-01-20 01:11:21,724 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:11:26,361 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:11:26,362 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:11:26,362 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:11:26,363 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:11:26,366 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.65, loss:   2.89, ppl:  18.01, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5986[sec], evaluation: 0.0355[sec]\n",
            "2023-01-20 01:11:26,368 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:11:26,371 - INFO - joeynmt.training - \tSource:     و در برابر دستور پروردگارت شکیبایی پیشه کن که تو خود در حمایت مایی و هنگامی که از خواب‌ بر می‌خیزی به نیایش پروردگارت تسبیح گوی . \n",
            "2023-01-20 01:11:26,372 - INFO - joeynmt.training - \tReference:   سن اؤز ربینین هؤکمونه صبر ائت ! شبههسیز کی ، سن بیزیم گؤزوموزون قاباغێنداسان . قالخدێقدا ربینی همد سنا ایله زیکر ائت ! \n",
            "2023-01-20 01:11:26,372 - INFO - joeynmt.training - \tHypothesis: ربینین سنه قارشێ صبر ائت . سن بیزیمله بیرلیکده صبر ائت وه هئچ بیرینین آردێنجا گئت . او ، ربینه بیر اؤتورمایان بیر مۆصبت ای بیر سؤز سؤیله !\n",
            "2023-01-20 01:11:26,372 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:11:26,374 - INFO - joeynmt.training - \tSource:     از احکام‌ دین ، آنچه را که به نوح در باره آن سفارش کرد ، برای شما تشریع کرد و آنچه را به تو وحی کردیم و آنچه را که در باره آن به ابراهیم و موسی و عیسی سفارش نمودیم که : دین را برپا دارید و در آن تفرقه‌اندازی مکنید . بر مشرکان آنچه که ایشان را به سوی آن فرا می‌خوانی ، گران می‌آید . خدا هر که را بخواهد ، به سوی خود برمی‌گزیند ، و هر که را که از در توبه درآید ، به سوی خود راه می‌نماید . \n",
            "2023-01-20 01:11:26,374 - INFO - joeynmt.training - \tReference:   آللاه : دینی دوغرو دۆرۆست توتون ، اوندا آیریلیغا دۆشمهیین ! دئیه نوحا تؤؤسیه ائتدیگینی ، سنه وحی بویوردوغونو ، ابراهیمه ، موسایا ، وه ایسهآیا تؤؤسیه ائتدیگینی دینده سیزین اۆچۆن ده قانونی ائتدی . سنین ده وط ائتدیگین مۆشرکلره آغێر گلدی . آللاه ایستهدیگی کیمسنی اؤزونه سئچر وه تؤؤبه ائدیب اونا طرف قاییدان کیمسنی ده دوغرو یولا یؤنلدر ! \n",
            "2023-01-20 01:11:26,374 - INFO - joeynmt.training - \tHypothesis: امری ایله سیزه اولان امری ایله سنه اویدورانێ ، سنه وحی ائتدیگیمیز امللر بارهسینده ، دئیه وحی ائتدیگیمیزه وحی ائتدیگیمیزه وحی ائتدیگیمیز امللر بارهسینده بئله بویورموشدور . اونلارا بئله بیر دینار گتیردیکده : آللاهێن دینینی ، دوغرو یولا یؤنلتدیگی شئیی ده وط ائت !\n",
            "2023-01-20 01:11:26,375 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:11:26,376 - INFO - joeynmt.training - \tSource:     هنگامی که به مادرت آنچه را که باید وحی می‌شد وحی کردیم : \n",
            "2023-01-20 01:11:26,377 - INFO - joeynmt.training - \tReference:  آنانا وحی اولوناجاق شئیی وحی ائتدیگیمیز زامان . \n",
            "2023-01-20 01:11:26,377 - INFO - joeynmt.training - \tHypothesis: او زامان کی ، آنا وحی اولوندو :\n",
            "2023-01-20 01:11:26,377 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:11:26,379 - INFO - joeynmt.training - \tSource:     شاگردان درست همان طور که عیسی گفته بود ، پاسخ دادند و آنان نیز اجازه دادند که آن را ببرند . \n",
            "2023-01-20 01:11:26,379 - INFO - joeynmt.training - \tReference:  شاگردلر ایسنانین دئدیکلرینی اونلارا تکرارلایاندا او آداملار شاگردلره امکان وئردی . \n",
            "2023-01-20 01:11:26,379 - INFO - joeynmt.training - \tHypothesis: شاگردلر ایسنانین دئدیگی کیمی اونلارا جاواب وئردیلر وه اونلارا ایزین وئریلدیگینی اونلارا آپاردێلار .\n",
            "2023-01-20 01:11:34,252 - INFO - joeynmt.training - Epoch 371, Step:   128100, Batch Loss:     1.572430, Batch Acc: 0.611148, Tokens per Sec:    13360, Lr: 0.000025\n",
            "2023-01-20 01:11:42,156 - INFO - joeynmt.training - Epoch 371, Step:   128200, Batch Loss:     1.679906, Batch Acc: 0.607687, Tokens per Sec:    13911, Lr: 0.000025\n",
            "2023-01-20 01:11:44,787 - INFO - joeynmt.training - Epoch 371: total training loss 523.81\n",
            "2023-01-20 01:11:44,788 - INFO - joeynmt.training - EPOCH 372\n",
            "2023-01-20 01:11:50,089 - INFO - joeynmt.training - Epoch 372, Step:   128300, Batch Loss:     1.592768, Batch Acc: 0.613067, Tokens per Sec:    13950, Lr: 0.000025\n",
            "2023-01-20 01:11:57,992 - INFO - joeynmt.training - Epoch 372, Step:   128400, Batch Loss:     1.519039, Batch Acc: 0.609201, Tokens per Sec:    13749, Lr: 0.000025\n",
            "2023-01-20 01:12:05,824 - INFO - joeynmt.training - Epoch 372, Step:   128500, Batch Loss:     1.604475, Batch Acc: 0.609741, Tokens per Sec:    13908, Lr: 0.000025\n",
            "2023-01-20 01:12:12,239 - INFO - joeynmt.training - Epoch 372: total training loss 525.18\n",
            "2023-01-20 01:12:12,239 - INFO - joeynmt.training - EPOCH 373\n",
            "2023-01-20 01:12:13,819 - INFO - joeynmt.training - Epoch 373, Step:   128600, Batch Loss:     1.381790, Batch Acc: 0.610636, Tokens per Sec:    14080, Lr: 0.000025\n",
            "2023-01-20 01:12:21,714 - INFO - joeynmt.training - Epoch 373, Step:   128700, Batch Loss:     1.456906, Batch Acc: 0.611596, Tokens per Sec:    13953, Lr: 0.000025\n",
            "2023-01-20 01:12:29,731 - INFO - joeynmt.training - Epoch 373, Step:   128800, Batch Loss:     1.584431, Batch Acc: 0.614897, Tokens per Sec:    13618, Lr: 0.000025\n",
            "2023-01-20 01:12:37,643 - INFO - joeynmt.training - Epoch 373, Step:   128900, Batch Loss:     1.512179, Batch Acc: 0.608227, Tokens per Sec:    13829, Lr: 0.000025\n",
            "2023-01-20 01:12:39,774 - INFO - joeynmt.training - Epoch 373: total training loss 523.53\n",
            "2023-01-20 01:12:39,774 - INFO - joeynmt.training - EPOCH 374\n",
            "2023-01-20 01:12:45,642 - INFO - joeynmt.training - Epoch 374, Step:   129000, Batch Loss:     1.511473, Batch Acc: 0.615109, Tokens per Sec:    13954, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.47ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9699.36ex/s]\n",
            "2023-01-20 01:12:45,928 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=129000\n",
            "2023-01-20 01:12:45,928 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:12:50,737 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:12:50,738 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:12:50,738 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:12:50,739 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:12:50,742 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.33, loss:   2.87, ppl:  17.71, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7647[sec], evaluation: 0.0410[sec]\n",
            "2023-01-20 01:12:50,744 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:12:50,748 - INFO - joeynmt.training - \tSource:     ای کسانی که ایمان آورده‌اید ، از غیر خودتان ، دوست و همراز مگیرید . آنان‌ از هیچ نابکاری در حق شما کوتاهی نمی‌ورزند . آرزو دارند که در رنج بیفتید . دشمنی از لحن و سخنشان آشکار است ؛ و آنچه سینه‌هایشان نهان می‌دارد ، بزرگتر است . در حقیقت ، ما نشانه‌ها ی دشمنی آنان‌ را برای شما بیان کردیم ، اگر تعقل کنید . \n",
            "2023-01-20 01:12:50,748 - INFO - joeynmt.training - \tReference:  ائی ایمان گتیرنلر ! اؤزوندن باشقاسێنێ اؤزونوزه سیرداش توتمایین . اونلار سیزین بارهنیزده فیتنه فصاد تؤرتمکدن ال چکمزلر ، سیزین ازییته دۆشمهیینیزی ایستهییرلر . حقیقتا ، اونلارێن سیزه قارشێ اولان اداوتی آغێزلارێندان چێخان سؤزلردن آشکار اولور . آمما اۆرکلرینده گیزلتدیکلری ایسه داها بؤیوکدور . اگر دۆشۆنۆب درک ائدیرسینیزسه ، آیهلری آرتێق سیزه اضاح ائتدیک . \n",
            "2023-01-20 01:12:50,748 - INFO - joeynmt.training - \tHypothesis: ائی ایمان گتیرنلر ! سیزه دوستلوق ، باشقالارینین دوستلوق ائتمزسینیز . اونلارێ اؤزلرینه ظۆلم ائدنلره ، حاقسێزلێق ائتمهینلره ، بوش یئره ظۆلم ائدنلره ، آشکار دۆشمهنینیزدیر . اگر اونلارا ائتدیکلرینیزدن داها بؤیوکدور . بیز اونلاردان اؤترو اؤترو اؤترو اؤترو اؤترو اؤترو اؤترو اؤترو اؤترو گۆناهلاری دئییلیک !\n",
            "2023-01-20 01:12:50,748 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:12:50,750 - INFO - joeynmt.training - \tSource:     و چون کار زشتی کنند ، می‌گویند : پدران خود را بر آن یافتیم و خدا ما را بدان فرمان داده است . بگو : قطعا خدا به کار زشت فرمان نمی‌دهد ، آیا چیزی را که نمی‌دانید به خدا نسبت می‌دهید ؟ \n",
            "2023-01-20 01:12:50,750 - INFO - joeynmt.training - \tReference:   ادبسیزلیک ائتدیکلری زامان : آتالاریمیزی بئله گؤردۆک . بونو بیزه آللاه امر ائتمیشدیر ، دئییرلر . دئ : آللاه ادبسیزلیک امر ائتمز . آللاها قارشێ بیلمهدیگینیز شئییمی دئییرسینیز ؟ \n",
            "2023-01-20 01:12:50,750 - INFO - joeynmt.training - \tHypothesis: اونلار بیر ایش گؤرکده : آتالاریمیزین گئتدیک . دئ : آللاه بیزه امر ائتدیگینی بیلهر . دئ : آللاه هئچ بیر شئیه هئچ بیر شئی گؤرمهییب ؟ آللاه هئچ بیر شئیی احتوا ائتمیشدیر !\n",
            "2023-01-20 01:12:50,751 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:12:50,753 - INFO - joeynmt.training - \tSource:     و ثمود و قوم لوط و اصحاب ایکه نیز به تکذیب پرداختند آنها دسته‌های مخالف بودند . \n",
            "2023-01-20 01:12:50,753 - INFO - joeynmt.training - \tReference:  ائلهجه ده سمود ، لوت قؤومو ، ایکه اهلی بو فرقهلرین\n",
            "2023-01-20 01:12:50,753 - INFO - joeynmt.training - \tHypothesis: سمود قؤومونه وه لوت قؤومونون باشێنا گلنلر ده تکذیب ائتمیشدیلر .\n",
            "2023-01-20 01:12:50,753 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:12:50,755 - INFO - joeynmt.training - \tSource:     پس در مورد این امور چه می‌توان گفت ؟ اگر خدا با ماست ، چه کسی می‌تواند بر ضد ما باشد ؟ \n",
            "2023-01-20 01:12:50,755 - INFO - joeynmt.training - \tReference:  بس بونلار بارهده نه دئیه بیلهریک ؟ اگر آللاه بیزیملهدیرسه ، کیم بیزه قارشێ دورا بیلر ؟ \n",
            "2023-01-20 01:12:50,756 - INFO - joeynmt.training - \tHypothesis: بس بو شئیلری نه ایستهییر ؟ اگر آللاه بیزه قارشێدێرسا ، نه جۆرهجۆره کیمه قارشێ دانێشا بیلر ؟\n",
            "2023-01-20 01:12:58,574 - INFO - joeynmt.training - Epoch 374, Step:   129100, Batch Loss:     1.426031, Batch Acc: 0.612028, Tokens per Sec:    13408, Lr: 0.000025\n",
            "2023-01-20 01:13:06,416 - INFO - joeynmt.training - Epoch 374, Step:   129200, Batch Loss:     1.564140, Batch Acc: 0.608513, Tokens per Sec:    14014, Lr: 0.000025\n",
            "2023-01-20 01:13:12,118 - INFO - joeynmt.training - Epoch 374: total training loss 521.59\n",
            "2023-01-20 01:13:12,118 - INFO - joeynmt.training - EPOCH 375\n",
            "2023-01-20 01:13:14,459 - INFO - joeynmt.training - Epoch 375, Step:   129300, Batch Loss:     1.472736, Batch Acc: 0.616187, Tokens per Sec:    13639, Lr: 0.000025\n",
            "2023-01-20 01:13:22,401 - INFO - joeynmt.training - Epoch 375, Step:   129400, Batch Loss:     1.491563, Batch Acc: 0.612584, Tokens per Sec:    13849, Lr: 0.000025\n",
            "2023-01-20 01:13:31,361 - INFO - joeynmt.training - Epoch 375, Step:   129500, Batch Loss:     1.575074, Batch Acc: 0.610708, Tokens per Sec:    12495, Lr: 0.000025\n",
            "2023-01-20 01:13:41,585 - INFO - joeynmt.training - Epoch 375, Step:   129600, Batch Loss:     1.622804, Batch Acc: 0.611092, Tokens per Sec:    10787, Lr: 0.000025\n",
            "2023-01-20 01:13:42,813 - INFO - joeynmt.training - Epoch 375: total training loss 519.40\n",
            "2023-01-20 01:13:42,813 - INFO - joeynmt.training - EPOCH 376\n",
            "2023-01-20 01:13:49,824 - INFO - joeynmt.training - Epoch 376, Step:   129700, Batch Loss:     1.474266, Batch Acc: 0.612234, Tokens per Sec:    13295, Lr: 0.000025\n",
            "2023-01-20 01:13:57,917 - INFO - joeynmt.training - Epoch 376, Step:   129800, Batch Loss:     1.468884, Batch Acc: 0.612063, Tokens per Sec:    13536, Lr: 0.000025\n",
            "2023-01-20 01:14:05,934 - INFO - joeynmt.training - Epoch 376, Step:   129900, Batch Loss:     1.408763, Batch Acc: 0.612082, Tokens per Sec:    13550, Lr: 0.000025\n",
            "2023-01-20 01:14:10,961 - INFO - joeynmt.training - Epoch 376: total training loss 523.08\n",
            "2023-01-20 01:14:10,962 - INFO - joeynmt.training - EPOCH 377\n",
            "2023-01-20 01:14:13,995 - INFO - joeynmt.training - Epoch 377, Step:   130000, Batch Loss:     1.545656, Batch Acc: 0.615753, Tokens per Sec:    13829, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.23ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9572.40ex/s]\n",
            "2023-01-20 01:14:14,276 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=130000\n",
            "2023-01-20 01:14:14,277 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:14:19,057 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:14:19,058 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:14:19,058 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:14:19,059 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:14:19,061 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   5.95, loss:   2.95, ppl:  19.07, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7423[sec], evaluation: 0.0352[sec]\n",
            "2023-01-20 01:14:19,064 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:14:19,068 - INFO - joeynmt.training - \tSource:      همین شخص ، با مزدی که بابت عمل شریرانهٔ خود گرفت ، مزرعه‌ای خرید و با سر در همان مزرعه سقوط کرد و شکمش پاره شد و تمام امعا و احشایش بیرون ریخت . \n",
            "2023-01-20 01:14:19,068 - INFO - joeynmt.training - \tReference:  بو آدام ائتدیگی حاقسێزلێغێن حاققێنا بیر تارلا ساتێن آلدێ ، سونرا دا اورادان کللهمایاللاق آشێب قارنێ یێرتیلدی وه باغێرساقلارێ تؤکۆلدۆ . \n",
            "2023-01-20 01:14:19,068 - INFO - joeynmt.training - \tHypothesis: بو آدام اؤز امهلینین شرفی ایله پیس آداملا ، اخلاقسیزلێقلا ، صاحیفه وه بۆتۆن خالاتیالیغا وه بۆتۆن یارانانی دایا اوتوزلوقلا اؤنونه تؤکۆلدو .\n",
            "2023-01-20 01:14:19,068 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:14:19,070 - INFO - joeynmt.training - \tSource:     این اتمام حجت‌ بدان سبب است که پروردگار تو هیچ گاه شهرها را به ستم نابوده نکرده ، در حالی که مردم آن غافل باشند . \n",
            "2023-01-20 01:14:19,070 - INFO - joeynmt.training - \tReference:  بو ، اونا گؤرهدیر کی ، ربین مملهکتلرین اهالیسینی ، اونلار قافل اولا اولا ظۆلمله محو ائتمهیی اؤزونه روا بیلمز ! \n",
            "2023-01-20 01:14:19,070 - INFO - joeynmt.training - \tHypothesis: بو سنین ربینی مجللیتدیر کی ، او شهرلرین هئچ بیرینی محو ائتمهسین . او ، اینسانلارا ظۆلم ائتمهدی .\n",
            "2023-01-20 01:14:19,071 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:14:19,072 - INFO - joeynmt.training - \tSource:     اگر شما کسی را ببخشید ، من نیز او را می‌بخشم . در واقع ، اگر خطایی را بخشیده‌ام به خاطر شما و در حضور مسیح بوده است\n",
            "2023-01-20 01:14:19,073 - INFO - joeynmt.training - \tReference:  کیمی باغیشلاسانیز ، من ده باغیشلایارام . اگر من بیر شئی باغیشلامیشامسا ، مسیحین هۆزوروندا سیزین خئیرینیز اۆچۆن باغێشلامێشام کی ، \n",
            "2023-01-20 01:14:19,073 - INFO - joeynmt.training - \tHypothesis: کیم سیزی باغێشلاسانێز ، من ده باغیشلاییرسا ، مصیحه اولان لۆتفلره گؤره باغیشلایار .\n",
            "2023-01-20 01:14:19,073 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:14:19,075 - INFO - joeynmt.training - \tSource:     و اگر آن را بر برخی از غیر عرب زبانان نازل می‌کردیم ، \n",
            "2023-01-20 01:14:19,075 - INFO - joeynmt.training - \tReference:  اگر اونو عرب اولمایانلاردان بیرینه نازل ائتسیدیک ؛ \n",
            "2023-01-20 01:14:19,075 - INFO - joeynmt.training - \tHypothesis: اگر اونو آنجاق دیلدن توتموشوقسا ، دیللرده دانێشێرێق .\n",
            "2023-01-20 01:14:27,160 - INFO - joeynmt.training - Epoch 377, Step:   130100, Batch Loss:     1.677339, Batch Acc: 0.611819, Tokens per Sec:    13032, Lr: 0.000025\n",
            "2023-01-20 01:14:35,191 - INFO - joeynmt.training - Epoch 377, Step:   130200, Batch Loss:     1.553732, Batch Acc: 0.610898, Tokens per Sec:    13658, Lr: 0.000025\n",
            "2023-01-20 01:14:43,232 - INFO - joeynmt.training - Epoch 377, Step:   130300, Batch Loss:     1.499836, Batch Acc: 0.611821, Tokens per Sec:    13771, Lr: 0.000025\n",
            "2023-01-20 01:14:43,934 - INFO - joeynmt.training - Epoch 377: total training loss 522.30\n",
            "2023-01-20 01:14:43,934 - INFO - joeynmt.training - EPOCH 378\n",
            "2023-01-20 01:14:51,310 - INFO - joeynmt.training - Epoch 378, Step:   130400, Batch Loss:     1.558364, Batch Acc: 0.613185, Tokens per Sec:    13703, Lr: 0.000025\n",
            "2023-01-20 01:14:59,273 - INFO - joeynmt.training - Epoch 378, Step:   130500, Batch Loss:     1.519946, Batch Acc: 0.614426, Tokens per Sec:    13701, Lr: 0.000025\n",
            "2023-01-20 01:15:07,349 - INFO - joeynmt.training - Epoch 378, Step:   130600, Batch Loss:     1.489463, Batch Acc: 0.610778, Tokens per Sec:    13602, Lr: 0.000025\n",
            "2023-01-20 01:15:11,795 - INFO - joeynmt.training - Epoch 378: total training loss 521.89\n",
            "2023-01-20 01:15:11,796 - INFO - joeynmt.training - EPOCH 379\n",
            "2023-01-20 01:15:15,509 - INFO - joeynmt.training - Epoch 379, Step:   130700, Batch Loss:     1.491149, Batch Acc: 0.619072, Tokens per Sec:    13863, Lr: 0.000025\n",
            "2023-01-20 01:15:23,443 - INFO - joeynmt.training - Epoch 379, Step:   130800, Batch Loss:     1.375175, Batch Acc: 0.612786, Tokens per Sec:    13857, Lr: 0.000025\n",
            "2023-01-20 01:15:31,513 - INFO - joeynmt.training - Epoch 379, Step:   130900, Batch Loss:     1.457915, Batch Acc: 0.613050, Tokens per Sec:    13659, Lr: 0.000025\n",
            "2023-01-20 01:15:39,531 - INFO - joeynmt.training - Epoch 379, Step:   131000, Batch Loss:     1.498716, Batch Acc: 0.607856, Tokens per Sec:    13565, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.12ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9397.26ex/s]\n",
            "2023-01-20 01:15:39,829 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=131000\n",
            "2023-01-20 01:15:39,829 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:15:44,282 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:15:44,283 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:15:44,283 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:15:44,284 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:15:44,287 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.98, loss:   2.87, ppl:  17.67, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4149[sec], evaluation: 0.0351[sec]\n",
            "2023-01-20 01:15:44,289 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:15:44,293 - INFO - joeynmt.training - \tSource:     این قرآن‌ ابلاغی برای مردم است تا به وسیله آن هدایت شوند و بدان بیم یابند و بدانند که او معبودی یگانه است ، و تا صاحبان خرد پند گیرند . \n",
            "2023-01-20 01:15:44,293 - INFO - joeynmt.training - \tReference:  بو اینسانلار اۆچۆن ائله بیر موعزدیر کی ، اونونلا هم قورخسونلار ، هم ده آللاهێن تک بیر تانرێ اولدوغونو بیلسینلر ، هم ده آغێل صاحبلری دۆشۆنۆب ابرت آلسێنلار ! \n",
            "2023-01-20 01:15:44,293 - INFO - joeynmt.training - \tHypothesis: بو قور آنێ اینسانلار اۆچۆن هدایتدیر . اونونلا بیرلیکده دوغرو یولا یؤنلدرلر کی ، بیلهسینیز وه اونون تانرێیا اوزان بیر قور آندێرلار وه آغێل صاحبلری ایله اوز اولارلار .\n",
            "2023-01-20 01:15:44,293 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:15:44,295 - INFO - joeynmt.training - \tSource:     پطرس در مقابل به او گفت : ما همه چیز را رها کرده‌ایم و از تو پیروی می‌کنیم ؛ پس چه چیز عاید ما خواهد شد ؟ \n",
            "2023-01-20 01:15:44,295 - INFO - joeynmt.training - \tReference:  بوندان سونرا پئتئر جاواب وئرهرک اونا دئدی : باخ بیز هر شئیی قویوب سنین آردێنجا گلمیشیک ، اوهزینده نییمیز اولاجاق ؟ \n",
            "2023-01-20 01:15:44,295 - INFO - joeynmt.training - \tHypothesis: پئتئر اونا گؤره ده هامێمێزا دئدی : قوی سنین هر شئیی قویوب گلیریک . بس اوندا نیه گلهجهییک ؟\n",
            "2023-01-20 01:15:44,296 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:15:44,297 - INFO - joeynmt.training - \tSource:     و لی‌ کسانی که مرتکب گناهان شدند ، آنگاه توبه کردند و ایمان آوردند ، قطعا پروردگار تو پس از آن آمرزنده مهربان خواهد بود . \n",
            "2023-01-20 01:15:44,298 - INFO - joeynmt.training - \tReference:  پیس امللر ائتدیکدن سونرا تؤؤبه ائدیب ایمان گتیرنلره گلدیکده ایسه ، ربین تؤؤبهدن سونرا باغیشلایاندیر ، رحم ائدندیر ! \n",
            "2023-01-20 01:15:44,298 - INFO - joeynmt.training - \tHypothesis: گۆناهلارێنێن جزاسێقان وه گۆناهلارێ ایمان گتیرمیش وه سونرا دا ربیندن سونرا ایمان گتیریب باغیشلایاندیر ، رحم ائدندیر !\n",
            "2023-01-20 01:15:44,298 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:15:44,300 - INFO - joeynmt.training - \tSource:     و لی‌ ما همگی به حال آماده‌باش درآمده‌ایم . \n",
            "2023-01-20 01:15:44,300 - INFO - joeynmt.training - \tReference:  بیز ایسه قۆۆتلی بیر جاماآتێق ! \n",
            "2023-01-20 01:15:44,300 - INFO - joeynmt.training - \tHypothesis: لاکین هامێمێز هامیسینی حاضرلاییق .\n",
            "2023-01-20 01:15:44,304 - INFO - joeynmt.training - Epoch 379: total training loss 521.25\n",
            "2023-01-20 01:15:44,304 - INFO - joeynmt.training - EPOCH 380\n",
            "2023-01-20 01:15:52,382 - INFO - joeynmt.training - Epoch 380, Step:   131100, Batch Loss:     1.499567, Batch Acc: 0.611030, Tokens per Sec:    13624, Lr: 0.000025\n",
            "2023-01-20 01:16:00,353 - INFO - joeynmt.training - Epoch 380, Step:   131200, Batch Loss:     1.459585, Batch Acc: 0.613729, Tokens per Sec:    13661, Lr: 0.000025\n",
            "2023-01-20 01:16:08,394 - INFO - joeynmt.training - Epoch 380, Step:   131300, Batch Loss:     1.516849, Batch Acc: 0.611856, Tokens per Sec:    13743, Lr: 0.000025\n",
            "2023-01-20 01:16:12,196 - INFO - joeynmt.training - Epoch 380: total training loss 520.74\n",
            "2023-01-20 01:16:12,197 - INFO - joeynmt.training - EPOCH 381\n",
            "2023-01-20 01:16:16,508 - INFO - joeynmt.training - Epoch 381, Step:   131400, Batch Loss:     1.530827, Batch Acc: 0.617245, Tokens per Sec:    14003, Lr: 0.000025\n",
            "2023-01-20 01:16:24,470 - INFO - joeynmt.training - Epoch 381, Step:   131500, Batch Loss:     1.593766, Batch Acc: 0.612713, Tokens per Sec:    13719, Lr: 0.000025\n",
            "2023-01-20 01:16:32,590 - INFO - joeynmt.training - Epoch 381, Step:   131600, Batch Loss:     1.513194, Batch Acc: 0.612620, Tokens per Sec:    13560, Lr: 0.000025\n",
            "2023-01-20 01:16:39,981 - INFO - joeynmt.training - Epoch 381: total training loss 519.61\n",
            "2023-01-20 01:16:39,982 - INFO - joeynmt.training - EPOCH 382\n",
            "2023-01-20 01:16:40,632 - INFO - joeynmt.training - Epoch 382, Step:   131700, Batch Loss:     1.495199, Batch Acc: 0.621967, Tokens per Sec:    13131, Lr: 0.000025\n",
            "2023-01-20 01:16:48,615 - INFO - joeynmt.training - Epoch 382, Step:   131800, Batch Loss:     1.368249, Batch Acc: 0.614589, Tokens per Sec:    13874, Lr: 0.000025\n",
            "2023-01-20 01:16:56,567 - INFO - joeynmt.training - Epoch 382, Step:   131900, Batch Loss:     1.633024, Batch Acc: 0.610444, Tokens per Sec:    13675, Lr: 0.000025\n",
            "2023-01-20 01:17:07,342 - INFO - joeynmt.training - Epoch 382, Step:   132000, Batch Loss:     1.494659, Batch Acc: 0.614468, Tokens per Sec:    10225, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 115.51ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8793.39ex/s]\n",
            "2023-01-20 01:17:07,640 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=132000\n",
            "2023-01-20 01:17:07,641 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:17:11,978 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:17:11,978 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:17:11,978 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:17:11,979 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:17:11,982 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.62, loss:   2.82, ppl:  16.75, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2988[sec], evaluation: 0.0357[sec]\n",
            "2023-01-20 01:17:11,985 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:17:11,988 - INFO - joeynmt.training - \tSource:     و می‌گفتند : آن پادشاه یهودیان که متولد شده است ، کجاست ؟ زیرا ما ستارهٔ او را زمانی که در مشرق بودیم ، دیدیم و آمده‌ایم تا در برابر او سر تعظیم فرود آوریم . \n",
            "2023-01-20 01:17:11,988 - INFO - joeynmt.training - \tReference:  دئدیلر : یهودیلرین آنادان اولموش پادشاهێ هارادادێر ؟ شرقده اونون اولدوزونو گؤردۆک وه اونا سجده قێلماغا گلدیک . \n",
            "2023-01-20 01:17:11,988 - INFO - joeynmt.training - \tHypothesis: یهودیلر دئییردی : یهودیلرین پادشاهێدێر . هارا چێراق یاندیریب ، چۆنکی بیز اونو گؤرنده بیر زامانێ اونون باشێنا گتیرک . او ، گؤروب حۆضوروموزا گتیرهجهییک .\n",
            "2023-01-20 01:17:11,989 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:17:11,991 - INFO - joeynmt.training - \tSource:     اما سران کاهنان مردم را تحریک کردند که باراباس را در عوض ، برایشان آزاد کند . \n",
            "2023-01-20 01:17:11,991 - INFO - joeynmt.training - \tReference:  آنجاق باشچێ کاهنلر خالقێ قێزێشدێردێلار کی ، ایسنانین یوخ ، بارابانێن آزاد اولماسێنێ خاهش ائتسینلر . \n",
            "2023-01-20 01:17:11,991 - INFO - joeynmt.training - \tHypothesis: باشچێ کاهنلری آلێب باراباشی ایله بۆرۆدۆلر ، آزاد ائدهجکلر .\n",
            "2023-01-20 01:17:11,991 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:17:11,993 - INFO - joeynmt.training - \tSource:     گویی که آنها یاقوت و مرجانند . \n",
            "2023-01-20 01:17:11,993 - INFO - joeynmt.training - \tReference:  اونلار ، سانکی یاقوت وه مرجاندێرلار . \n",
            "2023-01-20 01:17:11,993 - INFO - joeynmt.training - \tHypothesis: یاخود اونلار ساوال ، سانکیلر ،\n",
            "2023-01-20 01:17:11,994 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:17:11,995 - INFO - joeynmt.training - \tSource:     او گفت : واقعا چگونه بدانم ، اگر کسی مرا راهنمایی نکند ؟ پس ، از فیلیپس خواهش کرد که سوار ارابه شود و کنار او بنشیند . \n",
            "2023-01-20 01:17:11,996 - INFO - joeynmt.training - \tReference:  هرمآغاسی اونا دئدی : اگر منه یول گؤستهرن یوخدورسا ، بونو نئجه باشا دۆشمک اولار ؟ سونرا فیلیپدن خاهش ائتدی کی ، آرابایا مینیب اونون یانیندا اوتورسون . \n",
            "2023-01-20 01:17:11,996 - INFO - joeynmt.training - \tHypothesis: او دئدی : اگر منی نئجه چاغێرسام ، نئجه ده پیس یولا سالا بیلر ؟ فیلیپ منی اؤلدورمهیه چالێشێب داشقالاق ائدیلسین .\n",
            "2023-01-20 01:17:15,095 - INFO - joeynmt.training - Epoch 382: total training loss 519.83\n",
            "2023-01-20 01:17:15,095 - INFO - joeynmt.training - EPOCH 383\n",
            "2023-01-20 01:17:20,033 - INFO - joeynmt.training - Epoch 383, Step:   132100, Batch Loss:     1.478259, Batch Acc: 0.614251, Tokens per Sec:    13883, Lr: 0.000025\n",
            "2023-01-20 01:17:28,003 - INFO - joeynmt.training - Epoch 383, Step:   132200, Batch Loss:     1.495093, Batch Acc: 0.615911, Tokens per Sec:    13875, Lr: 0.000025\n",
            "2023-01-20 01:17:36,158 - INFO - joeynmt.training - Epoch 383, Step:   132300, Batch Loss:     1.526912, Batch Acc: 0.613745, Tokens per Sec:    13370, Lr: 0.000025\n",
            "2023-01-20 01:17:42,939 - INFO - joeynmt.training - Epoch 383: total training loss 519.59\n",
            "2023-01-20 01:17:42,940 - INFO - joeynmt.training - EPOCH 384\n",
            "2023-01-20 01:17:44,267 - INFO - joeynmt.training - Epoch 384, Step:   132400, Batch Loss:     1.497536, Batch Acc: 0.612837, Tokens per Sec:    13498, Lr: 0.000025\n",
            "2023-01-20 01:17:52,352 - INFO - joeynmt.training - Epoch 384, Step:   132500, Batch Loss:     1.565750, Batch Acc: 0.614001, Tokens per Sec:    13508, Lr: 0.000025\n",
            "2023-01-20 01:18:00,274 - INFO - joeynmt.training - Epoch 384, Step:   132600, Batch Loss:     1.724159, Batch Acc: 0.611513, Tokens per Sec:    13882, Lr: 0.000025\n",
            "2023-01-20 01:18:08,200 - INFO - joeynmt.training - Epoch 384, Step:   132700, Batch Loss:     1.511176, Batch Acc: 0.612598, Tokens per Sec:    13816, Lr: 0.000025\n",
            "2023-01-20 01:18:10,752 - INFO - joeynmt.training - Epoch 384: total training loss 520.79\n",
            "2023-01-20 01:18:10,753 - INFO - joeynmt.training - EPOCH 385\n",
            "2023-01-20 01:18:16,300 - INFO - joeynmt.training - Epoch 385, Step:   132800, Batch Loss:     1.403589, Batch Acc: 0.613435, Tokens per Sec:    13728, Lr: 0.000025\n",
            "2023-01-20 01:18:24,261 - INFO - joeynmt.training - Epoch 385, Step:   132900, Batch Loss:     1.549227, Batch Acc: 0.616622, Tokens per Sec:    13763, Lr: 0.000025\n",
            "2023-01-20 01:18:32,297 - INFO - joeynmt.training - Epoch 385, Step:   133000, Batch Loss:     1.526054, Batch Acc: 0.612945, Tokens per Sec:    13494, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.81ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9993.05ex/s]\n",
            "2023-01-20 01:18:32,569 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=133000\n",
            "2023-01-20 01:18:32,569 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:18:36,986 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:18:36,987 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:18:36,987 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:18:36,988 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:18:36,991 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.34, loss:   2.80, ppl:  16.43, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3766[sec], evaluation: 0.0372[sec]\n",
            "2023-01-20 01:18:36,993 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:18:36,997 - INFO - joeynmt.training - \tSource:     و یاد کن‌ هنگامی را که به فرشتگان گفتیم : برای آدم سجده کنید . پس ، جز ابلیس که سر باز زد همه‌ سجده کردند . \n",
            "2023-01-20 01:18:36,997 - INFO - joeynmt.training - \tReference:  بیر زامان ملکلره : آدمه سجده ائدین ! دئیه بویورموشدوق . ایبلیسدن باشقا هامێسێ سجده ائتدی . او ، بویون قاچێرتدێ . \n",
            "2023-01-20 01:18:36,997 - INFO - joeynmt.training - \tHypothesis: خاطرلا کی ، بیر زامان ملکلره : آدمه سجده ائدین ! دئمیشدیک . ایبلیسدن باشقا سجده ائتدی . ایبلیس حالدا ، هامێسێ سجده ائتدی .\n",
            "2023-01-20 01:18:36,997 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:18:36,999 - INFO - joeynmt.training - \tSource:     مردم به شما خواهند گفت : آنجا را ببین ! یا اینجا را ببین ! بیرون مروید یا آنان را دنبال مکنید . \n",
            "2023-01-20 01:18:37,000 - INFO - joeynmt.training - \tReference:  آداملار سیزه باخ ، او اورادادێر یاخود باخ ، بورادادێر دئیهجک . گئدیب اونلارێن آرخاسێنجا دۆشمهیین ! \n",
            "2023-01-20 01:18:37,000 - INFO - joeynmt.training - \tHypothesis: جاماآت سیزی ده گؤرهجکسینیز ، یاخود اورادا گؤرمهیین ! اونلار دا دا گؤرمهیین . یوخسا اونلارێ چاغێرمایین !\n",
            "2023-01-20 01:18:37,000 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:18:37,002 - INFO - joeynmt.training - \tSource:     فرعون گفت : آیا پیش از آنکه به شما رخصت دهم ، به او ایمان آوردید ؟ قطعا این نیرنگی است که در شهر به راه انداخته‌اید تا مردمش را از آن بیرون کنید . پس به زودی خواهید دانست . \n",
            "2023-01-20 01:18:37,002 - INFO - joeynmt.training - \tReference:  فیر اون دئدی : من سیزه ایزین وئرمهدن اول سیز اونا ایمان گتیردینیز ؟ بو ، شبههسیز کی ، اهالیسینی چێخارتماق مقصدله شهرده قوردوغونوز بیر هییلهدیر . بیلهجکسینیز ! \n",
            "2023-01-20 01:18:37,002 - INFO - joeynmt.training - \tHypothesis: فیر اوندان اول سیزه ایمان گتیرمهدینیز ، او ، شبههسیز کی ، اونون هییله قور آنێ تقیبه سالێب بیلهسینیزمی ؟ حقیقتا ، بونا گؤره ده او ، مۆتلق بیلهجکلر !\n",
            "2023-01-20 01:18:37,002 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:18:37,004 - INFO - joeynmt.training - \tSource:     سوگند به این شهر ، \n",
            "2023-01-20 01:18:37,004 - INFO - joeynmt.training - \tReference:  آند ایچیرم بو شهره \n",
            "2023-01-20 01:18:37,004 - INFO - joeynmt.training - \tHypothesis: آند اولسون بو شهره ؛\n",
            "2023-01-20 01:18:43,321 - INFO - joeynmt.training - Epoch 385: total training loss 521.06\n",
            "2023-01-20 01:18:43,322 - INFO - joeynmt.training - EPOCH 386\n",
            "2023-01-20 01:18:45,155 - INFO - joeynmt.training - Epoch 386, Step:   133100, Batch Loss:     1.549596, Batch Acc: 0.616839, Tokens per Sec:    13215, Lr: 0.000025\n",
            "2023-01-20 01:18:53,245 - INFO - joeynmt.training - Epoch 386, Step:   133200, Batch Loss:     1.427307, Batch Acc: 0.615740, Tokens per Sec:    13543, Lr: 0.000025\n",
            "2023-01-20 01:19:01,312 - INFO - joeynmt.training - Epoch 386, Step:   133300, Batch Loss:     1.470771, Batch Acc: 0.612864, Tokens per Sec:    13613, Lr: 0.000024\n",
            "2023-01-20 01:19:09,413 - INFO - joeynmt.training - Epoch 386, Step:   133400, Batch Loss:     1.514122, Batch Acc: 0.612298, Tokens per Sec:    13584, Lr: 0.000024\n",
            "2023-01-20 01:19:11,396 - INFO - joeynmt.training - Epoch 386: total training loss 518.42\n",
            "2023-01-20 01:19:11,396 - INFO - joeynmt.training - EPOCH 387\n",
            "2023-01-20 01:19:17,484 - INFO - joeynmt.training - Epoch 387, Step:   133500, Batch Loss:     1.443218, Batch Acc: 0.614031, Tokens per Sec:    13668, Lr: 0.000024\n",
            "2023-01-20 01:19:25,413 - INFO - joeynmt.training - Epoch 387, Step:   133600, Batch Loss:     1.482972, Batch Acc: 0.615468, Tokens per Sec:    14007, Lr: 0.000024\n",
            "2023-01-20 01:19:33,438 - INFO - joeynmt.training - Epoch 387, Step:   133700, Batch Loss:     1.327558, Batch Acc: 0.618899, Tokens per Sec:    13806, Lr: 0.000024\n",
            "2023-01-20 01:19:38,975 - INFO - joeynmt.training - Epoch 387: total training loss 517.65\n",
            "2023-01-20 01:19:38,975 - INFO - joeynmt.training - EPOCH 388\n",
            "2023-01-20 01:19:41,409 - INFO - joeynmt.training - Epoch 388, Step:   133800, Batch Loss:     1.548026, Batch Acc: 0.613174, Tokens per Sec:    13581, Lr: 0.000024\n",
            "2023-01-20 01:19:49,407 - INFO - joeynmt.training - Epoch 388, Step:   133900, Batch Loss:     1.533511, Batch Acc: 0.617233, Tokens per Sec:    13728, Lr: 0.000024\n",
            "2023-01-20 01:19:57,428 - INFO - joeynmt.training - Epoch 388, Step:   134000, Batch Loss:     1.485038, Batch Acc: 0.613784, Tokens per Sec:    13621, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 127.43ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10494.52ex/s]\n",
            "2023-01-20 01:19:57,709 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=134000\n",
            "2023-01-20 01:19:57,716 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:20:02,718 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:20:02,720 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:20:02,720 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:20:02,721 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:20:02,729 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.86, loss:   2.86, ppl:  17.45, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9272[sec], evaluation: 0.0732[sec]\n",
            "2023-01-20 01:20:02,735 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:20:02,741 - INFO - joeynmt.training - \tSource:     آیا ندیدی خدا چگونه مثل زده : سخنی پاک که مانند درختی پاک است که ریشه‌اش استوار و شاخه‌اش در آسمان است ؟ \n",
            "2023-01-20 01:20:02,741 - INFO - joeynmt.training - \tReference:  مگر آللاهێن نئجه بیر مسل چکدیگینی گؤرمورسنمی ؟ خوش بیر سؤز کؤکو یئرده مۆحکم اولوب بوداقلارێ گؤیه اوجالان گؤزل بیر آغاج کیمیدیر . \n",
            "2023-01-20 01:20:02,741 - INFO - joeynmt.training - \tHypothesis: مگر آللاه اونلارا بئله بیر مسل چکدی : تمیز بیر قایدابل کیمدیر ؟ گؤیده ده کؤکی قورولموشدور ؟\n",
            "2023-01-20 01:20:02,741 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:20:02,744 - INFO - joeynmt.training - \tSource:     عیسی پسر مریم گفت : بار الها ، پروردگارا ، از آسمان ، خوانی بر ما فرو فرست تا عیدی برای اول و آخر ما باشد و نشانه‌ای از جانب تو . و ما را روزی ده که تو بهترین روزی‌دهندگانی . \n",
            "2023-01-20 01:20:02,746 - INFO - joeynmt.training - \tReference:  مریم اوغلو ایسا دئدی : یا آللاه ، ائی بیزیم ربیمیز ! بیزه گؤیدن بیر صفره نازل ائت کی ، او بیزیم هم بیرینجیمیز ، هم ده آخێرێنجێمێز اۆچۆن بیر بایرام وه سندن بیر معؤ جۆزه اولسون . بیزه روزی وئر کی ، سن روزی وئرنلرین ان یاخشیسیسان ! \n",
            "2023-01-20 01:20:02,746 - INFO - joeynmt.training - \tHypothesis: ایسا اونا دئدی : ائی مریم ! سن گؤیه یئردن سیزه گؤیدن یاغما ! نه قدر مۆحافظهچیلره ، نه ده اۆچ گۆن ، هم ده سنه قورتول وئر ! سن ، حقیقتا ، سن حاکمسن !\n",
            "2023-01-20 01:20:02,746 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:20:02,749 - INFO - joeynmt.training - \tSource:     همچنین هیرودیس و سربازانش به وی بی‌حرمتی کردند و هیرودیس برای تمسخر او ، ردایی فاخر بر او پوشاند و سپس او را نزد پیلاتس بازگرداند . \n",
            "2023-01-20 01:20:02,749 - INFO - joeynmt.training - \tReference:  هیرود اؤز اسگرلری ایله اونو تحقیر ائدیب اله سالدێ . اونون اینینه بیر گؤزل پالتار گئییندیریب پیلاتین یانینا قایتاردی . \n",
            "2023-01-20 01:20:02,752 - INFO - joeynmt.training - \tHypothesis: هیرود وه ائفئسده هیرودیانیا وه شاعل سارسێتێ هیرودوو اۆستهلیک اۆچۆن پالتار گئییندیردیلر . سونرا پیلات اونو قاییدیب پیلاتا قاییدیب پیلات اونو پیلاتا قایێتدی .\n",
            "2023-01-20 01:20:02,752 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:20:02,754 - INFO - joeynmt.training - \tSource:     زیرا می‌شد آن را به مبلغی زیاد فروخت و پولش را به فقیران داد . \n",
            "2023-01-20 01:20:02,755 - INFO - joeynmt.training - \tReference:  آخێ بو اتیری باها قییمته ساتێب پولونو یوخسوللارا وئرمک اولاردێ . \n",
            "2023-01-20 01:20:02,755 - INFO - joeynmt.training - \tHypothesis: چۆنکی اونو ایتیریب پولا ساتێلدێ ، یوخسوللارا وئردی .\n",
            "2023-01-20 01:20:13,446 - INFO - joeynmt.training - Epoch 388, Step:   134100, Batch Loss:     1.458591, Batch Acc: 0.613489, Tokens per Sec:    10058, Lr: 0.000024\n",
            "2023-01-20 01:20:14,724 - INFO - joeynmt.training - Epoch 388: total training loss 518.75\n",
            "2023-01-20 01:20:14,725 - INFO - joeynmt.training - EPOCH 389\n",
            "2023-01-20 01:20:21,447 - INFO - joeynmt.training - Epoch 389, Step:   134200, Batch Loss:     1.542891, Batch Acc: 0.619794, Tokens per Sec:    13717, Lr: 0.000024\n",
            "2023-01-20 01:20:29,552 - INFO - joeynmt.training - Epoch 389, Step:   134300, Batch Loss:     1.392894, Batch Acc: 0.611427, Tokens per Sec:    13596, Lr: 0.000024\n",
            "2023-01-20 01:20:37,772 - INFO - joeynmt.training - Epoch 389, Step:   134400, Batch Loss:     1.422189, Batch Acc: 0.612035, Tokens per Sec:    13397, Lr: 0.000024\n",
            "2023-01-20 01:20:42,680 - INFO - joeynmt.training - Epoch 389: total training loss 516.37\n",
            "2023-01-20 01:20:42,680 - INFO - joeynmt.training - EPOCH 390\n",
            "2023-01-20 01:20:45,825 - INFO - joeynmt.training - Epoch 390, Step:   134500, Batch Loss:     1.450083, Batch Acc: 0.617078, Tokens per Sec:    13542, Lr: 0.000024\n",
            "2023-01-20 01:20:54,041 - INFO - joeynmt.training - Epoch 390, Step:   134600, Batch Loss:     1.463430, Batch Acc: 0.618938, Tokens per Sec:    13515, Lr: 0.000024\n",
            "2023-01-20 01:21:02,095 - INFO - joeynmt.training - Epoch 390, Step:   134700, Batch Loss:     1.574579, Batch Acc: 0.613022, Tokens per Sec:    13659, Lr: 0.000024\n",
            "2023-01-20 01:21:10,157 - INFO - joeynmt.training - Epoch 390, Step:   134800, Batch Loss:     1.659963, Batch Acc: 0.611644, Tokens per Sec:    13596, Lr: 0.000024\n",
            "2023-01-20 01:21:10,692 - INFO - joeynmt.training - Epoch 390: total training loss 516.48\n",
            "2023-01-20 01:21:10,693 - INFO - joeynmt.training - EPOCH 391\n",
            "2023-01-20 01:21:18,275 - INFO - joeynmt.training - Epoch 391, Step:   134900, Batch Loss:     1.682032, Batch Acc: 0.617682, Tokens per Sec:    13774, Lr: 0.000024\n",
            "2023-01-20 01:21:26,225 - INFO - joeynmt.training - Epoch 391, Step:   135000, Batch Loss:     1.474825, Batch Acc: 0.616861, Tokens per Sec:    13876, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 115.92ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9130.75ex/s]\n",
            "2023-01-20 01:21:26,529 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=135000\n",
            "2023-01-20 01:21:26,529 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:21:31,293 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:21:31,293 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:21:31,293 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:21:31,294 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:21:31,297 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.50, loss:   2.88, ppl:  17.82, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7238[sec], evaluation: 0.0366[sec]\n",
            "2023-01-20 01:21:31,300 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:21:31,303 - INFO - joeynmt.training - \tSource:     و آن کس که بر آزادکردن بنده‌ دسترسی ندارد ، باید پیش از تماس با زن خود دو ماه پیاپی روزه بدارد ؛ و هر که نتواند ، باید شصت بینوا را خوراک بدهد . این حکم‌ برای آن است که به خدا و فرستاده او ایمان بیاورید ، و این است حدود خدا . و کافران را عذابی پردرد خواهد بود . \n",
            "2023-01-20 01:21:31,304 - INFO - joeynmt.training - \tReference:  کیم تاپماسا ، قادێنێ ایله یاخینلێق ائتمزدن اول ایکی آی سراسر اوروج توتمالێ ، بونا دا گۆجۆ چاتماسا ، آلتمێش یوخسولو یئدیردیب دویدورمالیدیر . بو سیزین آللاه وه اونون پیغمبرینه ایمان گتیرمنیز اۆچۆندۆر . بونلار آللاهێن هدلریدیر . کافرلری شدتلی بیر اعذاب گؤزلهییر ! \n",
            "2023-01-20 01:21:31,304 - INFO - joeynmt.training - \tHypothesis: آزادلێغێن ! دوغمایان بیر بنده ایله دول قادێنێن اؤنونهده ایکی گۆندن اول فیتنه فصاد تؤرتمهیه وه آییرد ائتمهین . بو ، آللاهێن امری یئرینه یئتیرمکله شدتلی اعذاب گؤزلهییر . آللاه ایسه شدتلی بیر اعذاب گؤزلهییر !\n",
            "2023-01-20 01:21:31,304 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:21:31,306 - INFO - joeynmt.training - \tSource:     یا در کاری ابرام ورزیده‌اند ؟ ما نیز ابرام می‌ورزیم . \n",
            "2023-01-20 01:21:31,306 - INFO - joeynmt.training - \tReference:  یوخسا اونلار دۆزگۆن ایش گؤرموشدۆلر ائله ایسه بیز ده دۆزگۆن ایش گؤروروک ! \n",
            "2023-01-20 01:21:31,306 - INFO - joeynmt.training - \tHypothesis: یوخسا بولودلارا بیر ایشلهییب بولودا دالانێب بولودا دالغالانێرێق .\n",
            "2023-01-20 01:21:31,306 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:21:31,308 - INFO - joeynmt.training - \tSource:     اف بر شما و بر آنچه غیر از خدا می‌پرستید . مگر نمی‌اندیشید ؟ \n",
            "2023-01-20 01:21:31,309 - INFO - joeynmt.training - \tReference:  تفو سیزه ده ، آللاهدان باشقا ابادت ائتدیگینیز بۆتلره ده ! اجابا ، آنلامێرسێنێز ؟ \n",
            "2023-01-20 01:21:31,309 - INFO - joeynmt.training - \tHypothesis: سیزین اۆستۆنۆزه وه ابادت ائتدیگینیز بۆتلردن باشقا ! مگر دۆشۆنۆنز !\n",
            "2023-01-20 01:21:31,309 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:21:31,311 - INFO - joeynmt.training - \tSource:     كه صداي آمبولانس بلند شد . \n",
            "2023-01-20 01:21:31,311 - INFO - joeynmt.training - \tReference:  کی آمبولانسین سسی اوجالدی . \n",
            "2023-01-20 01:21:31,311 - INFO - joeynmt.training - \tHypothesis: کی ایستک لر ووران دی .\n",
            "2023-01-20 01:21:39,376 - INFO - joeynmt.training - Epoch 391, Step:   135100, Batch Loss:     1.511960, Batch Acc: 0.612428, Tokens per Sec:    12971, Lr: 0.000024\n",
            "2023-01-20 01:21:43,534 - INFO - joeynmt.training - Epoch 391: total training loss 517.35\n",
            "2023-01-20 01:21:43,534 - INFO - joeynmt.training - EPOCH 392\n",
            "2023-01-20 01:21:47,393 - INFO - joeynmt.training - Epoch 392, Step:   135200, Batch Loss:     1.565564, Batch Acc: 0.620760, Tokens per Sec:    13510, Lr: 0.000024\n",
            "2023-01-20 01:21:55,464 - INFO - joeynmt.training - Epoch 392, Step:   135300, Batch Loss:     1.493629, Batch Acc: 0.617973, Tokens per Sec:    13643, Lr: 0.000024\n",
            "2023-01-20 01:22:03,510 - INFO - joeynmt.training - Epoch 392, Step:   135400, Batch Loss:     1.498481, Batch Acc: 0.614272, Tokens per Sec:    13683, Lr: 0.000024\n",
            "2023-01-20 01:22:11,357 - INFO - joeynmt.training - Epoch 392: total training loss 517.42\n",
            "2023-01-20 01:22:11,357 - INFO - joeynmt.training - EPOCH 393\n",
            "2023-01-20 01:22:11,522 - INFO - joeynmt.training - Epoch 393, Step:   135500, Batch Loss:     1.484352, Batch Acc: 0.612477, Tokens per Sec:    13351, Lr: 0.000024\n",
            "2023-01-20 01:22:19,513 - INFO - joeynmt.training - Epoch 393, Step:   135600, Batch Loss:     1.533447, Batch Acc: 0.615847, Tokens per Sec:    13837, Lr: 0.000024\n",
            "2023-01-20 01:22:27,518 - INFO - joeynmt.training - Epoch 393, Step:   135700, Batch Loss:     1.552025, Batch Acc: 0.620305, Tokens per Sec:    13695, Lr: 0.000024\n",
            "2023-01-20 01:22:35,517 - INFO - joeynmt.training - Epoch 393, Step:   135800, Batch Loss:     1.504004, Batch Acc: 0.613425, Tokens per Sec:    13523, Lr: 0.000024\n",
            "2023-01-20 01:22:39,108 - INFO - joeynmt.training - Epoch 393: total training loss 517.46\n",
            "2023-01-20 01:22:39,108 - INFO - joeynmt.training - EPOCH 394\n",
            "2023-01-20 01:22:43,586 - INFO - joeynmt.training - Epoch 394, Step:   135900, Batch Loss:     1.512844, Batch Acc: 0.619816, Tokens per Sec:    13782, Lr: 0.000024\n",
            "2023-01-20 01:22:51,628 - INFO - joeynmt.training - Epoch 394, Step:   136000, Batch Loss:     1.458603, Batch Acc: 0.616654, Tokens per Sec:    13721, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.73ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10193.32ex/s]\n",
            "2023-01-20 01:22:51,923 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=136000\n",
            "2023-01-20 01:22:51,923 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:22:56,974 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:22:56,974 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:22:56,975 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:22:56,975 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:22:56,978 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.34, loss:   3.01, ppl:  20.36, acc:   0.40, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0114[sec], evaluation: 0.0362[sec]\n",
            "2023-01-20 01:22:56,981 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:22:56,985 - INFO - joeynmt.training - \tSource:     شاید دلیل این که انیسیموس مدتی کوتاه از تو جدا شد ، واقعا این بوده است که او را برای همیشه بازیابی ، \n",
            "2023-01-20 01:22:56,985 - INFO - joeynmt.training - \tReference:  اونایسیمین بیر مۆدت سندن آیریلماسی بلکه ده بوندان اؤترو اولدو کی ، سن اونو همیشهلیک ، \n",
            "2023-01-20 01:22:56,985 - INFO - joeynmt.training - \tHypothesis: بو سبدن سنه بیر مۆدتع مندن قورخدوم ، بو اونو مۆحکملندیرمک اۆچۆن اونو یئنه ده همیشه همیشهلیکدیر .\n",
            "2023-01-20 01:22:56,985 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:22:56,987 - INFO - joeynmt.training - \tSource:     از آن زمان عیسی موعظه را آغاز کرد . او می‌گفت : توبه کنید ؛ زیرا پادشاهی آسمان‌ها نزدیک شده است . \n",
            "2023-01-20 01:22:56,987 - INFO - joeynmt.training - \tReference:  او واختدان ایسا وز ائدیب بئله دئمهیه باشلادێ : تؤؤبه ائدین ! چۆنکی سماوی پادشاهلێق یاخینلاشیب . \n",
            "2023-01-20 01:22:56,987 - INFO - joeynmt.training - \tHypothesis: او واخت ایسا باشلایاندان سونرا اونو قبول ائدین ، چۆنکی تؤؤبه ائدیب دئدی : سماوی پادشاهلێق یاخینلیغیندا یاخیندیر .\n",
            "2023-01-20 01:22:56,988 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:22:56,989 - INFO - joeynmt.training - \tSource:     همان طور که ما شکل انسان خاکی را به خود گرفته‌ایم ، در آینده شکل او را نیز که از آسمان آمد ، به خود خواهیم گرفت . \n",
            "2023-01-20 01:22:56,990 - INFO - joeynmt.training - \tReference:  بیز تورپاقدان یارانانین سورهتینی نئجه گزدیریکسه ، سماوی اولانێن سورهتینی ده ائله گزدیرهجهییک . \n",
            "2023-01-20 01:22:56,990 - INFO - joeynmt.training - \tHypothesis: ائلهجه ده اینسان چێراقدان چادێرێلێب اونو کسب گؤیدن ائنمیشیک .\n",
            "2023-01-20 01:22:56,990 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:22:56,992 - INFO - joeynmt.training - \tSource:     و قطعا پدیدار شدن نخستین خود را شناختید ؛ پس چرا سر عبرت گرفتن ندارید ؟ \n",
            "2023-01-20 01:22:56,992 - INFO - joeynmt.training - \tReference:  آند اولسون کی ، سیز ایلک یارادیلیش بیلیرسینیز . ائله ایسه هئچ دۆشۆنمۆرسۆنۆز \n",
            "2023-01-20 01:22:56,992 - INFO - joeynmt.training - \tHypothesis: آند اولسون کی ، سیز ایلک دفهلرله ایلک دفه آنلایاراق نیه ابرت آلمانێزێ !\n",
            "2023-01-20 01:23:05,040 - INFO - joeynmt.training - Epoch 394, Step:   136100, Batch Loss:     1.463296, Batch Acc: 0.614116, Tokens per Sec:    13179, Lr: 0.000024\n",
            "2023-01-20 01:23:12,388 - INFO - joeynmt.training - Epoch 394: total training loss 516.66\n",
            "2023-01-20 01:23:12,388 - INFO - joeynmt.training - EPOCH 395\n",
            "2023-01-20 01:23:13,201 - INFO - joeynmt.training - Epoch 395, Step:   136200, Batch Loss:     1.581809, Batch Acc: 0.622467, Tokens per Sec:    13435, Lr: 0.000024\n",
            "2023-01-20 01:23:21,134 - INFO - joeynmt.training - Epoch 395, Step:   136300, Batch Loss:     1.443323, Batch Acc: 0.616194, Tokens per Sec:    13795, Lr: 0.000024\n",
            "2023-01-20 01:23:29,144 - INFO - joeynmt.training - Epoch 395, Step:   136400, Batch Loss:     1.387626, Batch Acc: 0.615807, Tokens per Sec:    13772, Lr: 0.000024\n",
            "2023-01-20 01:23:39,721 - INFO - joeynmt.training - Epoch 395, Step:   136500, Batch Loss:     1.546489, Batch Acc: 0.617044, Tokens per Sec:    10443, Lr: 0.000024\n",
            "2023-01-20 01:23:43,024 - INFO - joeynmt.training - Epoch 395: total training loss 516.82\n",
            "2023-01-20 01:23:43,025 - INFO - joeynmt.training - EPOCH 396\n",
            "2023-01-20 01:23:48,061 - INFO - joeynmt.training - Epoch 396, Step:   136600, Batch Loss:     1.424398, Batch Acc: 0.616931, Tokens per Sec:    13967, Lr: 0.000024\n",
            "2023-01-20 01:23:56,034 - INFO - joeynmt.training - Epoch 396, Step:   136700, Batch Loss:     1.491721, Batch Acc: 0.615067, Tokens per Sec:    13911, Lr: 0.000024\n",
            "2023-01-20 01:24:03,960 - INFO - joeynmt.training - Epoch 396, Step:   136800, Batch Loss:     1.678247, Batch Acc: 0.618195, Tokens per Sec:    13896, Lr: 0.000024\n",
            "2023-01-20 01:24:10,442 - INFO - joeynmt.training - Epoch 396: total training loss 514.16\n",
            "2023-01-20 01:24:10,442 - INFO - joeynmt.training - EPOCH 397\n",
            "2023-01-20 01:24:11,958 - INFO - joeynmt.training - Epoch 397, Step:   136900, Batch Loss:     1.445212, Batch Acc: 0.628374, Tokens per Sec:    13717, Lr: 0.000024\n",
            "2023-01-20 01:24:19,800 - INFO - joeynmt.training - Epoch 397, Step:   137000, Batch Loss:     1.437182, Batch Acc: 0.614141, Tokens per Sec:    13994, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.23ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10511.42ex/s]\n",
            "2023-01-20 01:24:20,063 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=137000\n",
            "2023-01-20 01:24:20,063 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:24:25,016 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:24:25,017 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:24:25,017 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:24:25,018 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:24:25,021 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.06, loss:   2.89, ppl:  17.98, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9138[sec], evaluation: 0.0366[sec]\n",
            "2023-01-20 01:24:25,024 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:24:25,027 - INFO - joeynmt.training - \tSource:     و فورا در کنیسه‌ها به موعظهٔ این پیام پرداخت که عیسی همان پسر خداست . \n",
            "2023-01-20 01:24:25,028 - INFO - joeynmt.training - \tReference:  درحال سیناقاوقلاردا ایسنانین آللاهێن اوغلو اولدوغونو وز ائتمهیه باشلادێ . \n",
            "2023-01-20 01:24:25,028 - INFO - joeynmt.training - \tHypothesis: اورادان سیناقوقلاردا وزیگتده وز ائدیردی کی ، ایسا آللاهێن اوغلودور .\n",
            "2023-01-20 01:24:25,028 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:24:25,030 - INFO - joeynmt.training - \tSource:     دیر میرسی . \n",
            "2023-01-20 01:24:25,030 - INFO - joeynmt.training - \tReference:  بئواختا قالیرسان . \n",
            "2023-01-20 01:24:25,030 - INFO - joeynmt.training - \tHypothesis: یورسان .\n",
            "2023-01-20 01:24:25,030 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:24:25,032 - INFO - joeynmt.training - \tSource:     بگو : پروردگارم به دادگری فرمان داده است ، و اینکه‌ در هر مسجدی روی خود را مستقیم به سوی قبله‌ کنید ، و در حالی که دین خود را برای او خالص گردانیده‌اید وی را بخوانید ، همان گونه که شما را پدید آورد به سوی او برمی‌گردید . \n",
            "2023-01-20 01:24:25,033 - INFO - joeynmt.training - \tReference:  دئ : ربیم عدالتی امر ائتدی . هر سجده اۆزۆنۆزۆ توتون . دینی یالنیز اونا مخصوص ائدهرک ابادت ائدین . سیزی یاراتدیغی کیمی ، یئنه اونون هۆزورونا قاییداجاقسینیز ! \n",
            "2023-01-20 01:24:25,033 - INFO - joeynmt.training - \tHypothesis: دئ : ربیمه ایتاعت ائدین ، امر ائتدیگینه گؤره اؤزوموزه تابع اولون . هر هانسێ بیر دینده اۆزرینیزه طرف اۆز توتون ، سیزه دینینی اۆز توتوب ابادت ائدین . سیز اونو ، سیزین ده اونون دینینه قوووشدورمو ؟\n",
            "2023-01-20 01:24:25,033 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:24:25,035 - INFO - joeynmt.training - \tSource:     و در برابر دستور پروردگارت شکیبایی پیشه کن که تو خود در حمایت مایی و هنگامی که از خواب‌ بر می‌خیزی به نیایش پروردگارت تسبیح گوی . \n",
            "2023-01-20 01:24:25,035 - INFO - joeynmt.training - \tReference:   سن اؤز ربینین هؤکمونه صبر ائت ! شبههسیز کی ، سن بیزیم گؤزوموزون قاباغێنداسان . قالخدێقدا ربینی همد سنا ایله زیکر ائت ! \n",
            "2023-01-20 01:24:25,035 - INFO - joeynmt.training - \tHypothesis: ربینین سنه قارشێ بیر معؤ مین ایزنیله ، اؤز اؤزوندن اول . چۆنکی ربینه بیر اؤتوروندن بیر معؤ جۆزه گتیرن گؤرکسن !\n",
            "2023-01-20 01:24:32,942 - INFO - joeynmt.training - Epoch 397, Step:   137100, Batch Loss:     1.504054, Batch Acc: 0.621373, Tokens per Sec:    13381, Lr: 0.000024\n",
            "2023-01-20 01:24:40,877 - INFO - joeynmt.training - Epoch 397, Step:   137200, Batch Loss:     1.577207, Batch Acc: 0.613185, Tokens per Sec:    13930, Lr: 0.000024\n",
            "2023-01-20 01:24:42,971 - INFO - joeynmt.training - Epoch 397: total training loss 513.55\n",
            "2023-01-20 01:24:42,971 - INFO - joeynmt.training - EPOCH 398\n",
            "2023-01-20 01:24:48,764 - INFO - joeynmt.training - Epoch 398, Step:   137300, Batch Loss:     1.406676, Batch Acc: 0.614453, Tokens per Sec:    14157, Lr: 0.000024\n",
            "2023-01-20 01:24:56,676 - INFO - joeynmt.training - Epoch 398, Step:   137400, Batch Loss:     1.575840, Batch Acc: 0.616554, Tokens per Sec:    13907, Lr: 0.000024\n",
            "2023-01-20 01:25:04,504 - INFO - joeynmt.training - Epoch 398, Step:   137500, Batch Loss:     1.492108, Batch Acc: 0.618640, Tokens per Sec:    13957, Lr: 0.000024\n",
            "2023-01-20 01:25:10,123 - INFO - joeynmt.training - Epoch 398: total training loss 514.39\n",
            "2023-01-20 01:25:10,124 - INFO - joeynmt.training - EPOCH 399\n",
            "2023-01-20 01:25:12,446 - INFO - joeynmt.training - Epoch 399, Step:   137600, Batch Loss:     1.529563, Batch Acc: 0.623811, Tokens per Sec:    13995, Lr: 0.000024\n",
            "2023-01-20 01:25:20,326 - INFO - joeynmt.training - Epoch 399, Step:   137700, Batch Loss:     1.500666, Batch Acc: 0.616419, Tokens per Sec:    13916, Lr: 0.000024\n",
            "2023-01-20 01:25:28,289 - INFO - joeynmt.training - Epoch 399, Step:   137800, Batch Loss:     1.577898, Batch Acc: 0.616505, Tokens per Sec:    13688, Lr: 0.000024\n",
            "2023-01-20 01:25:36,331 - INFO - joeynmt.training - Epoch 399, Step:   137900, Batch Loss:     1.516370, Batch Acc: 0.617664, Tokens per Sec:    13703, Lr: 0.000024\n",
            "2023-01-20 01:25:37,699 - INFO - joeynmt.training - Epoch 399: total training loss 514.52\n",
            "2023-01-20 01:25:37,699 - INFO - joeynmt.training - EPOCH 400\n",
            "2023-01-20 01:25:44,246 - INFO - joeynmt.training - Epoch 400, Step:   138000, Batch Loss:     1.470852, Batch Acc: 0.620868, Tokens per Sec:    14116, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.69ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9791.58ex/s]\n",
            "2023-01-20 01:25:44,537 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=138000\n",
            "2023-01-20 01:25:44,538 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:25:49,267 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:25:49,268 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:25:49,268 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:25:49,269 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:25:49,272 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.68, loss:   2.87, ppl:  17.65, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6883[sec], evaluation: 0.0375[sec]\n",
            "2023-01-20 01:25:49,277 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:25:49,280 - INFO - joeynmt.training - \tSource:     از طایفهٔ یهودا ۱۲ ۰۰۰ نفر مهر شدند ؛ از طایفهٔ رئوبین ۱۲ ۰۰۰ نفر ؛ از طایفهٔ جاد ۱۲ ۰۰۰ نفر ؛ \n",
            "2023-01-20 01:25:49,280 - INFO - joeynmt.training - \tReference:  یهودا قبیلهسیندن 12 ، 000 مۆهورلنمیش نفر ؛ رووئن قبیلهسیندن 12 ، 000 نفر ؛ قاد قبیلهسیندن 12 ، 000 نفر ؛ \n",
            "2023-01-20 01:25:49,280 - INFO - joeynmt.training - \tHypothesis: قبیلهسیندن 12 ، 000 نفر ؛ قبیلهسیندن 12 ، 000 نفر ؛ مئللهسیندن 12 ، 000 نفر ؛\n",
            "2023-01-20 01:25:49,280 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:25:49,283 - INFO - joeynmt.training - \tSource:     پس آن اژدهای بزرگ به پایین افکنده شد ؛ یعنی همان مار کهن که ابلیس و شیطان خوانده می‌شود و تمام ساکنان زمین را گمراه می‌کند . او به همراه فرشتگانش به زمین افکنده شد . \n",
            "2023-01-20 01:25:49,283 - INFO - joeynmt.training - \tReference:  او بؤیوک اژداها ابلیس وه شیطان دئییلن ، بۆتۆن دۆنیانی آلدادان قدیم ایلان یئر اۆزۆنه آتێلدێ ، ملکلری ده اونونلا بیرگه آتێلدێ . \n",
            "2023-01-20 01:25:49,283 - INFO - joeynmt.training - \tHypothesis: بوندان سونرا او بؤیوک اژداها آتێلدێ . شیطانین بۆتۆن ابلیسین تایلارینین هامیسینا آتێلدێ وه یئر اۆزۆنۆن بۆتۆن یئر اۆزۆنۆن اودلالتدێ .\n",
            "2023-01-20 01:25:49,283 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:25:49,285 - INFO - joeynmt.training - \tSource:     با این حال ، این عیب را در تو می‌بینم که محبت نخستین خود را از دست داده‌ای . \n",
            "2023-01-20 01:25:49,285 - INFO - joeynmt.training - \tReference:  آمما سندن بیر شکایتیم وار : اوهلکی محبتندن ال چکمیسن . \n",
            "2023-01-20 01:25:49,285 - INFO - joeynmt.training - \tHypothesis: آمما سن محبتی سنین محبتین الینی گؤستریرم کی ، اؤز الینی ایله اولجهدن وئردی .\n",
            "2023-01-20 01:25:49,285 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:25:49,288 - INFO - joeynmt.training - \tSource:     اما زمانی که گالیو ، استاندار اخائیه بود ، یهودیان همگی بر ضد پولس برخاستند و او را به مقابل مسند داوری کشیدند . \n",
            "2023-01-20 01:25:49,288 - INFO - joeynmt.training - \tReference:  آخایا والصی قاللیو اولان زامان یهودیلر بیرلهشرک پاولا حۆجوم ائتدیلر وه اونو والینین هؤکم کۆرسۆسۆ قارشیسینا چێخارتدێلار . \n",
            "2023-01-20 01:25:49,288 - INFO - joeynmt.training - \tHypothesis: آمما یهودیلرین والی بیر یهودی باشچیلاری هامیسینی مۆحاکمه ائتدی وه پاولون الئیهینه قوودو .\n",
            "2023-01-20 01:25:57,325 - INFO - joeynmt.training - Epoch 400, Step:   138100, Batch Loss:     1.545076, Batch Acc: 0.618472, Tokens per Sec:    13093, Lr: 0.000024\n",
            "2023-01-20 01:26:05,200 - INFO - joeynmt.training - Epoch 400, Step:   138200, Batch Loss:     1.399387, Batch Acc: 0.614802, Tokens per Sec:    13960, Lr: 0.000024\n",
            "2023-01-20 01:26:10,216 - INFO - joeynmt.training - Epoch 400: total training loss 514.22\n",
            "2023-01-20 01:26:10,217 - INFO - joeynmt.training - EPOCH 401\n",
            "2023-01-20 01:26:13,159 - INFO - joeynmt.training - Epoch 401, Step:   138300, Batch Loss:     1.457746, Batch Acc: 0.622388, Tokens per Sec:    13876, Lr: 0.000024\n",
            "2023-01-20 01:26:21,056 - INFO - joeynmt.training - Epoch 401, Step:   138400, Batch Loss:     1.496700, Batch Acc: 0.619443, Tokens per Sec:    13881, Lr: 0.000024\n",
            "2023-01-20 01:26:29,037 - INFO - joeynmt.training - Epoch 401, Step:   138500, Batch Loss:     1.515214, Batch Acc: 0.612692, Tokens per Sec:    13750, Lr: 0.000024\n",
            "2023-01-20 01:26:37,036 - INFO - joeynmt.training - Epoch 401, Step:   138600, Batch Loss:     1.410878, Batch Acc: 0.619348, Tokens per Sec:    13797, Lr: 0.000024\n",
            "2023-01-20 01:26:37,776 - INFO - joeynmt.training - Epoch 401: total training loss 514.62\n",
            "2023-01-20 01:26:37,776 - INFO - joeynmt.training - EPOCH 402\n",
            "2023-01-20 01:26:45,017 - INFO - joeynmt.training - Epoch 402, Step:   138700, Batch Loss:     1.423482, Batch Acc: 0.619387, Tokens per Sec:    14051, Lr: 0.000024\n",
            "2023-01-20 01:26:52,946 - INFO - joeynmt.training - Epoch 402, Step:   138800, Batch Loss:     1.427259, Batch Acc: 0.621761, Tokens per Sec:    14088, Lr: 0.000024\n",
            "2023-01-20 01:27:00,917 - INFO - joeynmt.training - Epoch 402, Step:   138900, Batch Loss:     1.490035, Batch Acc: 0.617315, Tokens per Sec:    13752, Lr: 0.000024\n",
            "2023-01-20 01:27:05,087 - INFO - joeynmt.training - Epoch 402: total training loss 510.99\n",
            "2023-01-20 01:27:05,087 - INFO - joeynmt.training - EPOCH 403\n",
            "2023-01-20 01:27:10,494 - INFO - joeynmt.training - Epoch 403, Step:   139000, Batch Loss:     1.459143, Batch Acc: 0.616977, Tokens per Sec:     9409, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 69.30ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 5147.78ex/s]\n",
            "2023-01-20 01:27:11,054 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=139000\n",
            "2023-01-20 01:27:11,054 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:27:17,040 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:27:17,040 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:27:17,040 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:27:17,041 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:27:17,044 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.80, loss:   2.91, ppl:  18.39, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.9451[sec], evaluation: 0.0371[sec]\n",
            "2023-01-20 01:27:17,051 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:27:17,054 - INFO - joeynmt.training - \tSource:     بشارتگر و هشداردهنده است . و لی‌ بیشتر آنان رویگردان شدند ، در نتیجه چیزی را نمی‌شنوند . \n",
            "2023-01-20 01:27:17,054 - INFO - joeynmt.training - \tReference:   هم مۆژده وئرندیر وه هم ده قورخودان . اونلارێن اکسریتیتی اۆز دؤندهریب دینلهمز . \n",
            "2023-01-20 01:27:17,054 - INFO - joeynmt.training - \tHypothesis: مۆژده وئرن وه قورخودان بیر پیغمبردیر . لاکین اونلارێن اکسریتیتیتی ائشیدیب ائشیتمزلر . اونلار ائشیتمزلر .\n",
            "2023-01-20 01:27:17,054 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:27:17,057 - INFO - joeynmt.training - \tSource:     باید بگردم‌ . \n",
            "2023-01-20 01:27:17,058 - INFO - joeynmt.training - \tReference:  گرک آختارام . \n",
            "2023-01-20 01:27:17,058 - INFO - joeynmt.training - \tHypothesis: گینه .\n",
            "2023-01-20 01:27:17,058 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:27:17,061 - INFO - joeynmt.training - \tSource:     بگو : اگر انس و جن گرد آیند تا نظیر این قرآن را بیاورند ، مانند آن را نخواهند آورد ، هر چند برخی از آنها پشتیبان برخی دیگر باشند . \n",
            "2023-01-20 01:27:17,061 - INFO - joeynmt.training - \tReference:   دئ : اگر اینسانلار وه جینلر بیر یئره یێغیشیب بو قور آنا بنزر بیر شئی گتیرمک اۆچۆن بیر بیرینه کؤمک ائتسهلر ، یئنه ده اونا بنزرینی گتیره بیلمزلر . \n",
            "2023-01-20 01:27:17,061 - INFO - joeynmt.training - \tHypothesis: دئ : اگر بو اینسانلار بیر دسته ملکته گتیرسه ، اونو داها بیر قدر گتیرسهلر ، اونو داها بیر نئچه نئچه نئچهلر واردێر .\n",
            "2023-01-20 01:27:17,061 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:27:17,063 - INFO - joeynmt.training - \tSource:     آنگاه به او گفتند : پس به ما بگو تو کیستی تا بتوانیم برای آنان که ما را فرستاده‌اند ، جوابی ببریم ؛ در مورد خود چه می‌گویی ؟ \n",
            "2023-01-20 01:27:17,063 - INFO - joeynmt.training - \tReference:  سونرا اونا دئدیلر : بس سن کیمسن ؟ بیزی گؤندهرنلره نه جاواب وئرک ؟ اؤزون بارهده نه دئییرسن ؟ \n",
            "2023-01-20 01:27:17,064 - INFO - joeynmt.training - \tHypothesis: اوندا ایسهآیا دئدیلر : بس سن کیمسن کی ، بیز سنین اۆچۆن پیغمبرلیک ائتسینلر . نه اۆچۆن گؤندهریلنلره جاواب وئرهجهییک ؟\n",
            "2023-01-20 01:27:24,962 - INFO - joeynmt.training - Epoch 403, Step:   139100, Batch Loss:     1.539093, Batch Acc: 0.619018, Tokens per Sec:    12876, Lr: 0.000024\n",
            "2023-01-20 01:27:32,914 - INFO - joeynmt.training - Epoch 403, Step:   139200, Batch Loss:     1.579786, Batch Acc: 0.618735, Tokens per Sec:    13833, Lr: 0.000024\n",
            "2023-01-20 01:27:40,876 - INFO - joeynmt.training - Epoch 403, Step:   139300, Batch Loss:     1.457718, Batch Acc: 0.616886, Tokens per Sec:    13814, Lr: 0.000024\n",
            "2023-01-20 01:27:40,912 - INFO - joeynmt.training - Epoch 403: total training loss 514.28\n",
            "2023-01-20 01:27:40,912 - INFO - joeynmt.training - EPOCH 404\n",
            "2023-01-20 01:27:48,818 - INFO - joeynmt.training - Epoch 404, Step:   139400, Batch Loss:     1.415119, Batch Acc: 0.617502, Tokens per Sec:    14012, Lr: 0.000024\n",
            "2023-01-20 01:27:56,788 - INFO - joeynmt.training - Epoch 404, Step:   139500, Batch Loss:     1.535050, Batch Acc: 0.622590, Tokens per Sec:    13762, Lr: 0.000024\n",
            "2023-01-20 01:28:04,719 - INFO - joeynmt.training - Epoch 404, Step:   139600, Batch Loss:     1.676211, Batch Acc: 0.615073, Tokens per Sec:    13933, Lr: 0.000024\n",
            "2023-01-20 01:28:08,252 - INFO - joeynmt.training - Epoch 404: total training loss 510.62\n",
            "2023-01-20 01:28:08,253 - INFO - joeynmt.training - EPOCH 405\n",
            "2023-01-20 01:28:12,738 - INFO - joeynmt.training - Epoch 405, Step:   139700, Batch Loss:     1.406409, Batch Acc: 0.622666, Tokens per Sec:    13759, Lr: 0.000024\n",
            "2023-01-20 01:28:20,697 - INFO - joeynmt.training - Epoch 405, Step:   139800, Batch Loss:     1.527436, Batch Acc: 0.621872, Tokens per Sec:    13848, Lr: 0.000024\n",
            "2023-01-20 01:28:28,761 - INFO - joeynmt.training - Epoch 405, Step:   139900, Batch Loss:     1.581725, Batch Acc: 0.614834, Tokens per Sec:    13662, Lr: 0.000024\n",
            "2023-01-20 01:28:35,928 - INFO - joeynmt.training - Epoch 405: total training loss 512.08\n",
            "2023-01-20 01:28:35,928 - INFO - joeynmt.training - EPOCH 406\n",
            "2023-01-20 01:28:36,820 - INFO - joeynmt.training - Epoch 406, Step:   140000, Batch Loss:     1.483888, Batch Acc: 0.613950, Tokens per Sec:    13604, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.02ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9154.30ex/s]\n",
            "2023-01-20 01:28:37,110 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=140000\n",
            "2023-01-20 01:28:37,110 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:28:41,658 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:28:41,658 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:28:41,659 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:28:41,659 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:28:41,662 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.57, loss:   2.85, ppl:  17.25, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5109[sec], evaluation: 0.0343[sec]\n",
            "2023-01-20 01:28:41,665 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:28:41,668 - INFO - joeynmt.training - \tSource:     این کاهنان به خدمت مقدس مشغولند که مظهر و سایهٔ چیزهای آسمانی است ؛ موسی نیز هنگامی که می‌خواست خیمه را بسازد به او این حکم ال هی داده شد : دقت کن و همه چیز را مطابق نمونه‌ای بساز که در کوه به تو نشان داده شد . \n",
            "2023-01-20 01:28:41,668 - INFO - joeynmt.training - \tReference:  اونلار گؤیلردکیلرین تصویری وه کؤلگهسی اولان یئرده ابادت ائدیر ، نئجه کی موسا ابادت چادێرێنێ قورماق اۆزره اولاندا آللاه اونو خبردار ائتمیشدی : داغدا سنه گؤستریلن نمونهیه گؤره بونلارێن هامیسینی احتیاتلا دۆزلت . \n",
            "2023-01-20 01:28:41,669 - INFO - joeynmt.training - \tHypothesis: بو شئیلری باشچێ کاهنلره خدمتچیلیک ائتمک اۆچۆن گؤیدن گلن کیمی ، بو شئیلری یئرینه یئتیرنلره قاداغان ائتدی . او ، هر شئین بوغا گؤتوردو وه درحال سنه دانێشدێ .\n",
            "2023-01-20 01:28:41,669 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:28:41,671 - INFO - joeynmt.training - \tSource:     اگر کسی بگوید : خدا را دوست دارم ، اما از برادر خود نفرت داشته باشد دروغگوست ؛ زیرا هر که برادر خود را که دیده است دوست نداشته باشد ، نمی‌تواند خدایی را که ندیده است دوست داشته باشد . \n",
            "2023-01-20 01:28:41,671 - INFO - joeynmt.training - \tReference:  کیم من آللاهێ سئویرم دئییر ، آمما اؤز قارداشێنا نفرت ائدیرسه ، یالانچیدیر . آخێ گؤردوگو قارداشێنێ سئومهین گؤرمهدیگی آللاهێ سئوه بیلمز . \n",
            "2023-01-20 01:28:41,671 - INFO - joeynmt.training - \tHypothesis: کیم دئ : بیر کس قارداشێنێ سئویرسه ، آمما سئودیگین آدام اولدوغونا گؤره قارداشێنێ سئومهین آداما گؤرمهین ، چۆنکی هئچ بیر سئومهینسه ، سئومهین .\n",
            "2023-01-20 01:28:41,671 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:28:41,673 - INFO - joeynmt.training - \tSource:     ای افعی‌زادگان ! چگونه می‌توانید سخن نیکو بگویید ، در حالی که خود شریرید ؟ زیرا زبان از آنچه دل از آن پر است ، سخن می‌گوید . \n",
            "2023-01-20 01:28:41,673 - INFO - joeynmt.training - \tReference:  ائی گۆرزهلر نسلی ، سیز پیس اولدوغونوز حالدا نئجه یاخشی شئیلر سؤیلهیه بیلرسینیز ؟ چۆنکی اۆرک دولولوغوندان آغێز دانێشار . \n",
            "2023-01-20 01:28:41,674 - INFO - joeynmt.training - \tHypothesis: ائی زئوولو ! سیزه نه سؤیلهییرسینیز ؟ چۆنکی دیللرده دانێشێرسێنێز ؟ چۆنکی اۆرهیینده دانێشان ، چۆنکی اۆرهیین دانێشان شئیلردن دانێشێر .\n",
            "2023-01-20 01:28:41,674 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:28:41,676 - INFO - joeynmt.training - \tSource:     پس خواستند به او نیرنگی زنند ؛ و لی‌ ما آنان را پست گردانیدیم . \n",
            "2023-01-20 01:28:41,676 - INFO - joeynmt.training - \tReference:  اونلار اونون اۆچۆن بئله بیر هییه قورماق ایستهدیلر ، بیز ایسه اونلارێ چوخ صفیل بیر وزیته سالدێق \n",
            "2023-01-20 01:28:41,676 - INFO - joeynmt.training - \tHypothesis: بونا گؤره ده اونو هییله قورماق اۆچۆن بیر هییله قوردولار . لاکین بیز اونلارێ بیر هییلهلره گؤره ده بیز ده اونو تۆکنمزدیک .\n",
            "2023-01-20 01:28:49,617 - INFO - joeynmt.training - Epoch 406, Step:   140100, Batch Loss:     1.546410, Batch Acc: 0.618456, Tokens per Sec:    13290, Lr: 0.000024\n",
            "2023-01-20 01:28:57,601 - INFO - joeynmt.training - Epoch 406, Step:   140200, Batch Loss:     1.549235, Batch Acc: 0.618391, Tokens per Sec:    13961, Lr: 0.000024\n",
            "2023-01-20 01:29:05,599 - INFO - joeynmt.training - Epoch 406, Step:   140300, Batch Loss:     1.629150, Batch Acc: 0.614404, Tokens per Sec:    13672, Lr: 0.000024\n",
            "2023-01-20 01:29:08,403 - INFO - joeynmt.training - Epoch 406: total training loss 513.15\n",
            "2023-01-20 01:29:08,403 - INFO - joeynmt.training - EPOCH 407\n",
            "2023-01-20 01:29:13,638 - INFO - joeynmt.training - Epoch 407, Step:   140400, Batch Loss:     1.639063, Batch Acc: 0.616490, Tokens per Sec:    13425, Lr: 0.000024\n",
            "2023-01-20 01:29:21,669 - INFO - joeynmt.training - Epoch 407, Step:   140500, Batch Loss:     1.465103, Batch Acc: 0.619662, Tokens per Sec:    13730, Lr: 0.000024\n",
            "2023-01-20 01:29:29,606 - INFO - joeynmt.training - Epoch 407, Step:   140600, Batch Loss:     1.377216, Batch Acc: 0.622472, Tokens per Sec:    13846, Lr: 0.000024\n",
            "2023-01-20 01:29:36,108 - INFO - joeynmt.training - Epoch 407: total training loss 513.96\n",
            "2023-01-20 01:29:36,108 - INFO - joeynmt.training - EPOCH 408\n",
            "2023-01-20 01:29:37,530 - INFO - joeynmt.training - Epoch 408, Step:   140700, Batch Loss:     1.430680, Batch Acc: 0.623989, Tokens per Sec:    13661, Lr: 0.000024\n",
            "2023-01-20 01:29:45,515 - INFO - joeynmt.training - Epoch 408, Step:   140800, Batch Loss:     1.506404, Batch Acc: 0.616796, Tokens per Sec:    13779, Lr: 0.000024\n",
            "2023-01-20 01:29:53,508 - INFO - joeynmt.training - Epoch 408, Step:   140900, Batch Loss:     1.515980, Batch Acc: 0.619538, Tokens per Sec:    13954, Lr: 0.000024\n",
            "2023-01-20 01:30:01,434 - INFO - joeynmt.training - Epoch 408, Step:   141000, Batch Loss:     1.473273, Batch Acc: 0.616771, Tokens per Sec:    14041, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 140.75ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10319.81ex/s]\n",
            "2023-01-20 01:30:01,697 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=141000\n",
            "2023-01-20 01:30:01,698 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:30:06,464 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:30:06,464 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:30:06,464 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:30:06,465 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:30:06,468 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.52, loss:   2.88, ppl:  17.90, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7214[sec], evaluation: 0.0371[sec]\n",
            "2023-01-20 01:30:06,471 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:30:06,474 - INFO - joeynmt.training - \tSource:     جزای آنها این است ، چرا که آیات ما را انکار کردند و گفتند : آیا وقتی ما استخوان و خاک شدیم باز در آفرینشی جدید برانگیخته خواهیم شد ؟ \n",
            "2023-01-20 01:30:06,474 - INFO - joeynmt.training - \tReference:  بو ، آیهلریمیزی اینکار ائتدیکلرینه وه : سۆر سۆمۆک ، چۆرۆگۆب اووخالانمێش تورپاق اولدوغوموز حالدا ، بیز دیریلدیلیب یئنی بیر مخلوقمو اولاجاغێق ؟ دئدیکلرینه گؤره اونلارێن جزاسێدێر . \n",
            "2023-01-20 01:30:06,474 - INFO - joeynmt.training - \tHypothesis: بو اونلارێن آیهلریدیر . اونلار آنجاق : بیز آیهلریمیزی اینکار ائدیب ، سۆر سۆمۆک اولدوقدان سونرا دیریلدیلهجک بیر یئره جهدی اولاجاغێق ؟ دئیه جاواب وئردیلر .\n",
            "2023-01-20 01:30:06,475 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:30:06,477 - INFO - joeynmt.training - \tSource:     که در خصوص مسیح به کار برد ، یعنی زمانی که او را از میان مردگان برخیزاند و در جایگاه‌های آسمانی بر دست راست خود نشاند ؛ \n",
            "2023-01-20 01:30:06,477 - INFO - joeynmt.training - \tReference:  آللاه بو قدرتله مصیحده ائله فعالیت گؤستردی کی ، اونو اؤلولر آراسێندان دیریلدیب سمادا اؤز ساغێندا اوتورتدو\n",
            "2023-01-20 01:30:06,477 - INFO - joeynmt.training - \tHypothesis: مصیح اؤلولر آراسێندان دیریلدیگینه گؤره دیریلنلره هم اؤلولر آراسێندان دیریلنلره ، هم ده گؤیلرده اولانێن ساغێندا ،\n",
            "2023-01-20 01:30:06,477 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:30:06,479 - INFO - joeynmt.training - \tSource:     و در حقیقت ، ما به موسی نه نشانه آشکار دادیم . پس ، از فرزندان اسرائیل بپرس آنگاه که نزد آنان آمد ، و فرعون به او گفت : ای موسی‌ ، من جدا تو را افسون‌شده می‌پندارم . \n",
            "2023-01-20 01:30:06,479 - INFO - joeynmt.training - \tReference:  بیز موسایا دوققوز آشکار معؤ جۆزه وئردیک . اسراعل اوغوللاریندان سوروش : اونلارێن یانینا گلدیکده ، فیر اون اونا : یا موسا ! منه ائله گلیر کی ، سن اووسونلانمێشان ، دئمیشدی . \n",
            "2023-01-20 01:30:06,479 - INFO - joeynmt.training - \tHypothesis: بیز موسایا آچێق آشکار معؤ جۆزهلر وئردیک . اسراعلدن سونرا اسراعل اوغوللارێنا گلدی . او ایسه : ائی فیر اون ، فیر اون وه یاتمیشم ! دئدی .\n",
            "2023-01-20 01:30:06,480 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:30:06,482 - INFO - joeynmt.training - \tSource:     فرزندان او را با بلای مرگبار خواهم کشت . به این ترتیب ، همهٔ جماعت‌ها خواهند دانست ، من آن کسی هستم که عمیق‌ترین افکار و دل‌ها را می‌کاود . من به هر یک از شما مطابق اعمالتان عوض خواهم داد . \n",
            "2023-01-20 01:30:06,482 - INFO - joeynmt.training - \tReference:  اونون اؤؤلادلارێنێ آزارا سالێب اؤلدورهجهیم . بۆتۆن جمیتلر ده بیلهجک کی ، اینسانین داخلینی وه اۆرهیینی آراشدێران منم وه هر بیرینیزه امللرینیزه گؤره اوز وئرهجهیهم . \n",
            "2023-01-20 01:30:06,482 - INFO - joeynmt.training - \tHypothesis: اونون نسلینی اؤلدورهجک . من ده بۆتۆن جمیتلره بئلهجهیم . اونلارا هامێسێ منم ، سیزین کیمین اۆزۆلهرینه یئنیدن خاهش ائدیرم .\n",
            "2023-01-20 01:30:08,606 - INFO - joeynmt.training - Epoch 408: total training loss 510.26\n",
            "2023-01-20 01:30:08,607 - INFO - joeynmt.training - EPOCH 409\n",
            "2023-01-20 01:30:14,455 - INFO - joeynmt.training - Epoch 409, Step:   141100, Batch Loss:     1.488810, Batch Acc: 0.622281, Tokens per Sec:    13934, Lr: 0.000024\n",
            "2023-01-20 01:30:22,351 - INFO - joeynmt.training - Epoch 409, Step:   141200, Batch Loss:     1.549876, Batch Acc: 0.618028, Tokens per Sec:    13926, Lr: 0.000024\n",
            "2023-01-20 01:30:30,338 - INFO - joeynmt.training - Epoch 409, Step:   141300, Batch Loss:     1.516327, Batch Acc: 0.618280, Tokens per Sec:    13860, Lr: 0.000024\n",
            "2023-01-20 01:30:36,001 - INFO - joeynmt.training - Epoch 409: total training loss 511.06\n",
            "2023-01-20 01:30:36,001 - INFO - joeynmt.training - EPOCH 410\n",
            "2023-01-20 01:30:38,292 - INFO - joeynmt.training - Epoch 410, Step:   141400, Batch Loss:     1.338822, Batch Acc: 0.628973, Tokens per Sec:    13657, Lr: 0.000024\n",
            "2023-01-20 01:30:49,285 - INFO - joeynmt.training - Epoch 410, Step:   141500, Batch Loss:     1.534712, Batch Acc: 0.618426, Tokens per Sec:    10083, Lr: 0.000024\n",
            "2023-01-20 01:30:57,207 - INFO - joeynmt.training - Epoch 410, Step:   141600, Batch Loss:     1.530022, Batch Acc: 0.619643, Tokens per Sec:    14026, Lr: 0.000024\n",
            "2023-01-20 01:31:05,166 - INFO - joeynmt.training - Epoch 410, Step:   141700, Batch Loss:     1.528191, Batch Acc: 0.618742, Tokens per Sec:    14015, Lr: 0.000024\n",
            "2023-01-20 01:31:06,280 - INFO - joeynmt.training - Epoch 410: total training loss 507.04\n",
            "2023-01-20 01:31:06,281 - INFO - joeynmt.training - EPOCH 411\n",
            "2023-01-20 01:31:13,114 - INFO - joeynmt.training - Epoch 411, Step:   141800, Batch Loss:     1.436235, Batch Acc: 0.621695, Tokens per Sec:    13965, Lr: 0.000024\n",
            "2023-01-20 01:31:21,009 - INFO - joeynmt.training - Epoch 411, Step:   141900, Batch Loss:     1.548126, Batch Acc: 0.618465, Tokens per Sec:    13877, Lr: 0.000024\n",
            "2023-01-20 01:31:28,936 - INFO - joeynmt.training - Epoch 411, Step:   142000, Batch Loss:     1.522616, Batch Acc: 0.618334, Tokens per Sec:    13911, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.17ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10050.01ex/s]\n",
            "2023-01-20 01:31:29,209 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=142000\n",
            "2023-01-20 01:31:29,210 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:31:33,938 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:31:33,939 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:31:33,939 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:31:33,940 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:31:33,943 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.73, loss:   2.89, ppl:  17.91, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6897[sec], evaluation: 0.0362[sec]\n",
            "2023-01-20 01:31:33,946 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:31:33,949 - INFO - joeynmt.training - \tSource:     ای کسانی که ایمان آورده‌اید ، چون زنان با ایمان مهاجر ، نزد شما آیند آنان را بیازمایید . خدا به ایمان آنان داناتر است . پس اگر آنان را باایمان تشخیص دادید ، دیگر ایشان را به سوی کافران بازنگردانید : نه آن زنان بر ایشان حلالند و نه آن مردان‌ بر این زنان حلال . و هر چه خرج این زنان‌ کرده‌اند به شوهران‌ آنها بدهید ، و بر شما گناهی نیست که در صورتی که مهرشان را به آنان بدهید با ایشان ازدواج کنید ، و به پیوندهای قبلی کافران متمسک نشوید و پایبند نباشید و آنچه را شما برای زنان مرتد و فراری خود که به کفار پناهنده شده‌اند خرج کرده‌اید ، از کافران‌ مطالبه کنید ، و آنها هم باید آنچه را خرج کرده‌اند از شما مطالبه کنند . این حکم خداست که‌ میان شما داوری می‌کند ، و خدا دانای حکیم است . \n",
            "2023-01-20 01:31:33,949 - INFO - joeynmt.training - \tReference:  ائی ایمان گتیرنلر ! معؤ مین قادێنلار سیزین یانینیزا مۆحاجر کیمی گلدیکلری زامان اونلارێ امتهاهانا چکین . آللاه اونلارێن ایمانینی چوخ گؤزل بیلیر . اگر بونلارێن معؤ مین اولدوقلارینی بیلسهنیز ، آرتێق اونلارێ کافرلرین یانینا قایتارمایین . نه بونلار اونلارا ، نه ده اونلار بونلارا هالالدێر . اونلارێن خرجلهدیکلرینی اؤزلرینه قایتاریب وئرین . بونلارێن مئهرلرینی اؤزلرینه وئردیگینیز تقدیرده اونلارلا ائولنمهیینیزدن سیزه هئچ بیر گۆناه گلمز . کافر قادێنلارێ اؤز کبینینیز آلتێندا ساخلامایین . وئردیگینیز مئهری ایستهگین . صرف ائتدیکلری مئهری ایستهسینلر . آللاهێن هؤکمو بودور . او سیزین آرانێزدا هؤکم ائدر . آللاه بیلندیر ، هکمت صاحبدر ! \n",
            "2023-01-20 01:31:33,949 - INFO - joeynmt.training - \tHypothesis: ائی ایمان گتیرنلر ! اونلارلا بیرلیکده مۆشرکلره ، اونلاردان داها یاخشی بیلیرسینیز . اگر ایمانینیزی داها یاخشی بیلیرسینیزسه ، اونلاردان داها یاخشی بیلیرسینیز . اونلار اؤزلری ایله ووروشون ، اونلارا دوستلارێنا دا بیر حاقسێزلێق ائدنلرها داها بؤیوکدور . اونلاردان دفع ائده بیلین کی ، بو قادێنلارێ آلدادێر .\n",
            "2023-01-20 01:31:33,950 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:31:33,952 - INFO - joeynmt.training - \tSource:     اما او این را در مورد روحی گفت که به‌زودی به کسانی داده می‌شد که به او ایمان می‌آوردند ؛ زیرا تا آن زمان روح عطا نشده بود ، چون عیسی هنوز جلال نیافته بود . \n",
            "2023-01-20 01:31:33,952 - INFO - joeynmt.training - \tReference:  بونو اونا ایمان ائدنلرین آلاجاقلاری روح بارهده سؤیلهدی . روح ایسه هله یوخ ایدی ، چۆنکی ایسا هله ایزتلنمهمیشدی . \n",
            "2023-01-20 01:31:33,952 - INFO - joeynmt.training - \tHypothesis: لاکین ایسا حاققێندا سؤزو سؤیلهدیگی روحا وئریلن شخصه وئریلنلره وئریلن روحون احتشامینی آلمێشدێ . چۆنکی ایسا هله ایزتلنمیشدی .\n",
            "2023-01-20 01:31:33,952 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:31:33,954 - INFO - joeynmt.training - \tSource:     ای کسانی که ایمان آورده‌اید ، چون با گروهی برخورد می‌کنید پایداری ورزید و خدا را بسیار یاد کنید ، باشد که رستگار شوید . \n",
            "2023-01-20 01:31:33,954 - INFO - joeynmt.training - \tReference:  ائی ایمان گتیرنلر ! بیر دسته ایله اۆز اۆزه گلدیکده مۆحکم اولون وه آللاهێ چوخ یادا سالێن کی ، نجات تاپاسێنێز ! \n",
            "2023-01-20 01:31:33,955 - INFO - joeynmt.training - \tHypothesis: ائی ایمان گتیرنلر ! دسته دسته آللاهێن دؤزومه دورون ، دؤزومو ده دوعا ائدین . نجات تاپاسێنێز !\n",
            "2023-01-20 01:31:33,955 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:31:33,957 - INFO - joeynmt.training - \tSource:     و خدا شما را به سرای سلامت فرا می‌خواند ، و هر که را بخواهد به راه راست هدایت می‌کند . \n",
            "2023-01-20 01:31:33,957 - INFO - joeynmt.training - \tReference:  آللاه امین آمانلێق یوردونا چاغێرێر وه ایستهدیگینی دوغرو یولا سالێر ! \n",
            "2023-01-20 01:31:33,957 - INFO - joeynmt.training - \tHypothesis: آللاه سیزه سالام گؤندهریر . او ، باتلدیر ، ایستهدیگینی ده دوغرو یولا یؤنلدر .\n",
            "2023-01-20 01:31:38,563 - INFO - joeynmt.training - Epoch 411: total training loss 508.27\n",
            "2023-01-20 01:31:38,564 - INFO - joeynmt.training - EPOCH 412\n",
            "2023-01-20 01:31:41,926 - INFO - joeynmt.training - Epoch 412, Step:   142100, Batch Loss:     1.455441, Batch Acc: 0.620690, Tokens per Sec:    13933, Lr: 0.000024\n",
            "2023-01-20 01:31:49,762 - INFO - joeynmt.training - Epoch 412, Step:   142200, Batch Loss:     1.502597, Batch Acc: 0.621608, Tokens per Sec:    13868, Lr: 0.000024\n",
            "2023-01-20 01:31:57,690 - INFO - joeynmt.training - Epoch 412, Step:   142300, Batch Loss:     1.495339, Batch Acc: 0.619334, Tokens per Sec:    13840, Lr: 0.000024\n",
            "2023-01-20 01:32:05,681 - INFO - joeynmt.training - Epoch 412, Step:   142400, Batch Loss:     1.392489, Batch Acc: 0.617247, Tokens per Sec:    13829, Lr: 0.000024\n",
            "2023-01-20 01:32:06,028 - INFO - joeynmt.training - Epoch 412: total training loss 510.65\n",
            "2023-01-20 01:32:06,028 - INFO - joeynmt.training - EPOCH 413\n",
            "2023-01-20 01:32:13,552 - INFO - joeynmt.training - Epoch 413, Step:   142500, Batch Loss:     1.423585, Batch Acc: 0.621544, Tokens per Sec:    14306, Lr: 0.000024\n",
            "2023-01-20 01:32:21,399 - INFO - joeynmt.training - Epoch 413, Step:   142600, Batch Loss:     1.466005, Batch Acc: 0.618536, Tokens per Sec:    14079, Lr: 0.000024\n",
            "2023-01-20 01:32:29,270 - INFO - joeynmt.training - Epoch 413, Step:   142700, Batch Loss:     1.481530, Batch Acc: 0.619399, Tokens per Sec:    13816, Lr: 0.000024\n",
            "2023-01-20 01:32:33,175 - INFO - joeynmt.training - Epoch 413: total training loss 510.39\n",
            "2023-01-20 01:32:33,176 - INFO - joeynmt.training - EPOCH 414\n",
            "2023-01-20 01:32:37,193 - INFO - joeynmt.training - Epoch 414, Step:   142800, Batch Loss:     1.480434, Batch Acc: 0.624259, Tokens per Sec:    13991, Lr: 0.000024\n",
            "2023-01-20 01:32:45,149 - INFO - joeynmt.training - Epoch 414, Step:   142900, Batch Loss:     1.461666, Batch Acc: 0.619901, Tokens per Sec:    13797, Lr: 0.000024\n",
            "2023-01-20 01:32:53,021 - INFO - joeynmt.training - Epoch 414, Step:   143000, Batch Loss:     1.320007, Batch Acc: 0.622599, Tokens per Sec:    13821, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 118.15ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9430.92ex/s]\n",
            "2023-01-20 01:32:53,305 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=143000\n",
            "2023-01-20 01:32:53,306 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:32:57,591 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:32:57,592 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:32:57,592 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:32:57,593 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:32:57,595 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.50, loss:   2.99, ppl:  19.87, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2460[sec], evaluation: 0.0364[sec]\n",
            "2023-01-20 01:32:57,598 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:32:57,602 - INFO - joeynmt.training - \tSource:     اما چون بر من فاش شد که توطئه‌ای علیه او در کار است ، فورا او را نزد شما فرستادم و به متهم‌کنندگانش حکم کردم تا شکایتی را که علیه او دارند ، به شما بگویند . \n",
            "2023-01-20 01:32:57,602 - INFO - joeynmt.training - \tReference:  منه چاتدێرێلاندا کی بو کیشیگه قارشێ بیر سوعی قصد قورولوب ، درحال اونو سنین یانینا گؤندردیم . اونو اتهام ائدنلرین ده شکایتلرینی سنه بیلدیرمهلرینی امر ائتدیم . \n",
            "2023-01-20 01:32:57,602 - INFO - joeynmt.training - \tHypothesis: آمما سنین فئشهیه گؤندهریلن بیر ایش گؤرسم ، اونو یانینیزا گؤندردیم . اونو امر ائتمک اۆچۆن یانینا گؤندهرهجهیم کی ، اونون امرینی یئرینه یئتیرسینلر .\n",
            "2023-01-20 01:32:57,602 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:32:57,604 - INFO - joeynmt.training - \tSource:     پس ، از آنجا که به این امر اطمینان دارم ، می‌دانم که برای پیشرفت و شادی شما در ایمان ، در جسم خواهم ماند و با همهٔ شما به سر خواهم برد . \n",
            "2023-01-20 01:32:57,604 - INFO - joeynmt.training - \tReference:  بونا امین اولاراق بیلیرم : ساغ قالاجاغام وه هامینیزلا بیرلیکده قالماقدا داوام ائدهجهیم کی ، اماندا هم ایرهلیلهیهسینیز ، هم ده سئوینهسینیز . \n",
            "2023-01-20 01:32:57,605 - INFO - joeynmt.training - \tHypothesis: بونا گؤره ده بیلیرم کی ، منده قالدێغێم حالدا ایمانینیزا گؤره سئوینهسینیز وه محبتده قالاجاغام .\n",
            "2023-01-20 01:32:57,605 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:32:57,607 - INFO - joeynmt.training - \tSource:     پس چه بگوییم ؟ اسرائیل به آنچه مشتاقانه در پی آن بود ، دست نیافت ، بلکه برگزیدگان به آن دست یافتند . سایرین سختدل گردیدند ، \n",
            "2023-01-20 01:32:57,607 - INFO - joeynmt.training - \tReference:  اوندا نه دئیه بیلهریک ؟ اسراعل آختاردێغێنێ الده ائتمهدی ، سئچیلمیشلر الده ائتدیلر ؛ دیگرلری ایسه اینادکار اولدو . \n",
            "2023-01-20 01:32:57,607 - INFO - joeynmt.training - \tHypothesis: بس اوندا نه دئیه بیلهریک ؟ اسراعل اوغوللارێن الیندهدیر . لاکین اللری تاپمادێلار ، سئچیلمیشلر .\n",
            "2023-01-20 01:32:57,607 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:32:57,609 - INFO - joeynmt.training - \tSource:     پس آن اژدهای بزرگ به پایین افکنده شد ؛ یعنی همان مار کهن که ابلیس و شیطان خوانده می‌شود و تمام ساکنان زمین را گمراه می‌کند . او به همراه فرشتگانش به زمین افکنده شد . \n",
            "2023-01-20 01:32:57,609 - INFO - joeynmt.training - \tReference:  او بؤیوک اژداها ابلیس وه شیطان دئییلن ، بۆتۆن دۆنیانی آلدادان قدیم ایلان یئر اۆزۆنه آتێلدێ ، ملکلری ده اونونلا بیرگه آتێلدێ . \n",
            "2023-01-20 01:32:57,609 - INFO - joeynmt.training - \tHypothesis: بوندان سونرا او بؤیوک اژداها آتێلدێ . شیطانین بۆتۆن ابلیسی وه یئر اۆزۆنۆن بۆتۆن ابلیسین آردێنجا گئدیردی . یئر اۆزۆنۆن بۆتۆن ملکلری دهشه آتێلدێ .\n",
            "2023-01-20 01:33:05,180 - INFO - joeynmt.training - Epoch 414: total training loss 511.87\n",
            "2023-01-20 01:33:05,180 - INFO - joeynmt.training - EPOCH 415\n",
            "2023-01-20 01:33:05,501 - INFO - joeynmt.training - Epoch 415, Step:   143100, Batch Loss:     1.399829, Batch Acc: 0.633052, Tokens per Sec:    14078, Lr: 0.000024\n",
            "2023-01-20 01:33:13,345 - INFO - joeynmt.training - Epoch 415, Step:   143200, Batch Loss:     1.380481, Batch Acc: 0.623567, Tokens per Sec:    14012, Lr: 0.000024\n",
            "2023-01-20 01:33:21,112 - INFO - joeynmt.training - Epoch 415, Step:   143300, Batch Loss:     1.493675, Batch Acc: 0.618762, Tokens per Sec:    14240, Lr: 0.000024\n",
            "2023-01-20 01:33:28,999 - INFO - joeynmt.training - Epoch 415, Step:   143400, Batch Loss:     1.517609, Batch Acc: 0.618459, Tokens per Sec:    13980, Lr: 0.000024\n",
            "2023-01-20 01:33:32,255 - INFO - joeynmt.training - Epoch 415: total training loss 507.50\n",
            "2023-01-20 01:33:32,255 - INFO - joeynmt.training - EPOCH 416\n",
            "2023-01-20 01:33:36,856 - INFO - joeynmt.training - Epoch 416, Step:   143500, Batch Loss:     1.439998, Batch Acc: 0.618400, Tokens per Sec:    13990, Lr: 0.000024\n",
            "2023-01-20 01:33:44,768 - INFO - joeynmt.training - Epoch 416, Step:   143600, Batch Loss:     1.502955, Batch Acc: 0.621666, Tokens per Sec:    13953, Lr: 0.000024\n",
            "2023-01-20 01:33:52,568 - INFO - joeynmt.training - Epoch 416, Step:   143700, Batch Loss:     1.481292, Batch Acc: 0.619779, Tokens per Sec:    13990, Lr: 0.000024\n",
            "2023-01-20 01:33:59,469 - INFO - joeynmt.training - Epoch 416: total training loss 509.77\n",
            "2023-01-20 01:33:59,470 - INFO - joeynmt.training - EPOCH 417\n",
            "2023-01-20 01:34:00,517 - INFO - joeynmt.training - Epoch 417, Step:   143800, Batch Loss:     1.520788, Batch Acc: 0.619276, Tokens per Sec:    13561, Lr: 0.000024\n",
            "2023-01-20 01:34:08,301 - INFO - joeynmt.training - Epoch 417, Step:   143900, Batch Loss:     1.456114, Batch Acc: 0.621559, Tokens per Sec:    14055, Lr: 0.000024\n",
            "2023-01-20 01:34:16,132 - INFO - joeynmt.training - Epoch 417, Step:   144000, Batch Loss:     1.545043, Batch Acc: 0.621212, Tokens per Sec:    13922, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.83ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10412.66ex/s]\n",
            "2023-01-20 01:34:16,399 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=144000\n",
            "2023-01-20 01:34:16,399 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:34:20,597 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:34:20,597 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:34:20,598 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:34:20,599 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:34:20,608 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.89, loss:   2.85, ppl:  17.34, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.1129[sec], evaluation: 0.0809[sec]\n",
            "2023-01-20 01:34:20,611 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:34:20,617 - INFO - joeynmt.training - \tSource:     محبت به این معنی است که مطابق احکام او گام برداریم . چنان که از آغاز شنیده‌اید ، آن حکم این است که همچنان در محبت گام بردارید ؛ \n",
            "2023-01-20 01:34:20,617 - INFO - joeynmt.training - \tReference:  محبت آتانێن امرلرینه گؤره هیات سۆرمهییمیزی طلب ائدیر . اولدن بریع ائشیتدیگینیز کیمی امر بودور ؛ گرک بو امره گؤره هیات سۆرهسینیز . \n",
            "2023-01-20 01:34:20,617 - INFO - joeynmt.training - \tHypothesis: سئودیگینیز محبتین تبیعی یئرینه یئتیرین . بوندان ائشیتدیگینیز محبتی وه بئله هیات سۆرن حدسیز محبتی یئرینه یئتیرین .\n",
            "2023-01-20 01:34:20,618 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:34:20,620 - INFO - joeynmt.training - \tSource:     به همین گونه ، پسر انسان نیامد تا به او خدمت شود ، بلکه آمد تا خدمت کند و جان خود را همچون بهای رهایی در عوض بسیاری بدهد . \n",
            "2023-01-20 01:34:20,623 - INFO - joeynmt.training - \tReference:  بئلهجه بشر اوغلو گلمهدی کی ، اونا خدمت ائتسینلر ، گلدی کی ، اؤزو خدمت ائتسین وه چوخلارێنێ ساتێن آلماق اۆچۆن اؤز جانێنێ فدیه وئرسین . \n",
            "2023-01-20 01:34:20,623 - INFO - joeynmt.training - \tHypothesis: ائلهجه ده اوغولا قوللوق ائتمک اۆچۆن گلدی کی ، اونا خدمت ائتسین وه اؤز جانێنێ ساتێن آلسێن .\n",
            "2023-01-20 01:34:20,623 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:34:20,626 - INFO - joeynmt.training - \tSource:     با ایمان بود که موسی مصر را بدون ترس از خشم پادشاه ترک کرد ؛ زیرا ثابت‌قدم بود ، چنان که گویی آن نادیده را می‌دید . \n",
            "2023-01-20 01:34:20,626 - INFO - joeynmt.training - \tReference:  ایمان واسطهسیله پادشاهێن قزهبیندن قورخماییب میصیردن چێخدێ ، چۆنکی او ، گؤزه گؤرونمهینی سانکی گؤرموش کیمی مۆحکم دایاندی . \n",
            "2023-01-20 01:34:20,626 - INFO - joeynmt.training - \tHypothesis: موسا قورخودان قورخانلار پادشاهدان قورخاراق خبردارلێق ائتدی . چۆنکی اونون غزبیگتدهدهییرسن کی ، ناراهاتلێق ائدن کیمی گؤرورسن .\n",
            "2023-01-20 01:34:20,626 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:34:20,629 - INFO - joeynmt.training - \tSource:     و در دو طرف روز اول و آخر آن‌ و نخستین ساعات شب نماز را برپا دار ، زیرا خوبیها بدیها را از میان می‌برد . این برای پندگیرندگان ، پندی است . \n",
            "2023-01-20 01:34:20,630 - INFO - joeynmt.training - \tReference:   نامازێ گۆندۆزۆن ایکی باشێندا وه گئجهنین به زی ساعاتلارێندا قێل . حقیقتا ، یاخشی امللر پیس ایشلهری یویوب آپارار . بو ، یادا سالانلارا اؤیود نصیحتدیر . \n",
            "2023-01-20 01:34:20,630 - INFO - joeynmt.training - \tHypothesis: هر ایکیسی ایکی گۆندن سونرا ، نامازێنێن وه ایلکایتکارسینی یاخشی بیلیر . چۆنکی پیس آداملاردان قورورلار . بو ، اؤیود نصیحت قبول ائدنلر اۆچۆن ابرت واردێر !\n",
            "2023-01-20 01:34:31,146 - INFO - joeynmt.training - Epoch 417, Step:   144100, Batch Loss:     1.496985, Batch Acc: 0.621538, Tokens per Sec:    10307, Lr: 0.000024\n",
            "2023-01-20 01:34:33,764 - INFO - joeynmt.training - Epoch 417: total training loss 509.59\n",
            "2023-01-20 01:34:33,764 - INFO - joeynmt.training - EPOCH 418\n",
            "2023-01-20 01:34:39,000 - INFO - joeynmt.training - Epoch 418, Step:   144200, Batch Loss:     1.564330, Batch Acc: 0.621642, Tokens per Sec:    13898, Lr: 0.000024\n",
            "2023-01-20 01:34:46,925 - INFO - joeynmt.training - Epoch 418, Step:   144300, Batch Loss:     1.497506, Batch Acc: 0.623064, Tokens per Sec:    13947, Lr: 0.000024\n",
            "2023-01-20 01:34:54,765 - INFO - joeynmt.training - Epoch 418, Step:   144400, Batch Loss:     1.513723, Batch Acc: 0.617609, Tokens per Sec:    14055, Lr: 0.000024\n",
            "2023-01-20 01:35:00,924 - INFO - joeynmt.training - Epoch 418: total training loss 509.99\n",
            "2023-01-20 01:35:00,924 - INFO - joeynmt.training - EPOCH 419\n",
            "2023-01-20 01:35:02,622 - INFO - joeynmt.training - Epoch 419, Step:   144500, Batch Loss:     1.440580, Batch Acc: 0.615813, Tokens per Sec:    13119, Lr: 0.000024\n",
            "2023-01-20 01:35:10,520 - INFO - joeynmt.training - Epoch 419, Step:   144600, Batch Loss:     1.489917, Batch Acc: 0.622440, Tokens per Sec:    14019, Lr: 0.000024\n",
            "2023-01-20 01:35:18,384 - INFO - joeynmt.training - Epoch 419, Step:   144700, Batch Loss:     1.575509, Batch Acc: 0.623143, Tokens per Sec:    13900, Lr: 0.000024\n",
            "2023-01-20 01:35:26,277 - INFO - joeynmt.training - Epoch 419, Step:   144800, Batch Loss:     1.512837, Batch Acc: 0.620181, Tokens per Sec:    13794, Lr: 0.000024\n",
            "2023-01-20 01:35:28,385 - INFO - joeynmt.training - Epoch 419: total training loss 510.05\n",
            "2023-01-20 01:35:28,386 - INFO - joeynmt.training - EPOCH 420\n",
            "2023-01-20 01:35:34,210 - INFO - joeynmt.training - Epoch 420, Step:   144900, Batch Loss:     1.428493, Batch Acc: 0.624328, Tokens per Sec:    14027, Lr: 0.000023\n",
            "2023-01-20 01:35:42,091 - INFO - joeynmt.training - Epoch 420, Step:   145000, Batch Loss:     1.331464, Batch Acc: 0.620948, Tokens per Sec:    13869, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.75ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10118.91ex/s]\n",
            "2023-01-20 01:35:42,376 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=145000\n",
            "2023-01-20 01:35:42,376 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:35:46,325 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:35:46,325 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:35:46,325 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:35:46,326 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:35:46,329 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.88, loss:   2.83, ppl:  16.95, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 3.9126[sec], evaluation: 0.0330[sec]\n",
            "2023-01-20 01:35:46,331 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:35:46,335 - INFO - joeynmt.training - \tSource:     هر یک از شما باید بداند که برای قدوسیت و نیکنامی ، چگونه بر بدن خود تسلط داشته باشد\n",
            "2023-01-20 01:35:46,335 - INFO - joeynmt.training - \tReference:  هر بیرینیز آللاهێ تانیمآیان بۆتپرستلر کیمی شهوت وه احترآسلا دئییل ، مۆقدسلیک وه هؤرمتله آرواد آلماغێ اؤیرهنین ، \n",
            "2023-01-20 01:35:46,335 - INFO - joeynmt.training - \tHypothesis: هر بیرینیز اؤز بدهنیندن یاخشیی تلکۆکهیهسی ایله بۆرۆدۆ ، بدن بدنده اولدوغو حالدا اؤزونه تلکۆل ائتسین .\n",
            "2023-01-20 01:35:46,335 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:35:46,337 - INFO - joeynmt.training - \tSource:     و از دادن‌ زکات و وسایل و مایحتاج خانه‌ خودداری می‌ورزند . \n",
            "2023-01-20 01:35:46,337 - INFO - joeynmt.training - \tReference:  وه ذکات وئرمهیی قاداغان ائدرلر . \n",
            "2023-01-20 01:35:46,337 - INFO - joeynmt.training - \tHypothesis: اؤز اترافێندان وه زۆمرهییمیزدن ائویمیزدن یئتر .\n",
            "2023-01-20 01:35:46,338 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:35:46,339 - INFO - joeynmt.training - \tSource:     سلام مرا به تمام کسانی که هدایت را در میان شما بر عهده دارند و به تمام مقدسان برسانید . برادران در ایتالیا به شما سلام می‌رسانند . \n",
            "2023-01-20 01:35:46,340 - INFO - joeynmt.training - \tReference:  بۆتۆن رحبرلرینیزی وه مۆقدسلرین هامیسینی سالاملایین . ایتالییادان اولانلار سیزه سالام گؤندهریرلر . \n",
            "2023-01-20 01:35:46,340 - INFO - joeynmt.training - \tHypothesis: سیزی بۆتۆن ایمانلیلارلا بیرلیکده رحبرلره سالاملایین . بۆتۆن باجێ قارداشلارا سالام گؤندهریر .\n",
            "2023-01-20 01:35:46,340 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:35:46,342 - INFO - joeynmt.training - \tSource:     آنگاه می‌رود و هفت روح شریرتر از خود را می‌آورد و با آنان به آن خانه داخل گشته و در آن ساکن می‌شوند . در نتیجه ، عاقبت آن شخص از ابتدایش بدتر می‌شود . \n",
            "2023-01-20 01:35:46,342 - INFO - joeynmt.training - \tReference:  بو واخت او گئدیب اؤزوندن داها بئتر یئددی باشقا روحو گؤتورر . اونلار دا اورایا گیریب مسکونلاشار . اوندا بو آدامێن آخێرێ اوهلکیندن داها پیس اولار . \n",
            "2023-01-20 01:35:46,342 - INFO - joeynmt.training - \tHypothesis: اوندا یئددی روحو وه داخل اولان یئددی روحو ایله آپارێر . اونلار دا ایچری گیریب ائوه گیریب ائوه گیریب گئدیرلر . سونرا بو آدامێ داها پیس اولار .\n",
            "2023-01-20 01:35:54,121 - INFO - joeynmt.training - Epoch 420, Step:   145100, Batch Loss:     1.571154, Batch Acc: 0.620930, Tokens per Sec:    13632, Lr: 0.000023\n",
            "2023-01-20 01:35:59,751 - INFO - joeynmt.training - Epoch 420: total training loss 508.91\n",
            "2023-01-20 01:35:59,752 - INFO - joeynmt.training - EPOCH 421\n",
            "2023-01-20 01:36:01,950 - INFO - joeynmt.training - Epoch 421, Step:   145200, Batch Loss:     1.457697, Batch Acc: 0.624650, Tokens per Sec:    14322, Lr: 0.000023\n",
            "2023-01-20 01:36:09,902 - INFO - joeynmt.training - Epoch 421, Step:   145300, Batch Loss:     1.456594, Batch Acc: 0.622587, Tokens per Sec:    13906, Lr: 0.000023\n",
            "2023-01-20 01:36:17,692 - INFO - joeynmt.training - Epoch 421, Step:   145400, Batch Loss:     1.434789, Batch Acc: 0.619866, Tokens per Sec:    14235, Lr: 0.000023\n",
            "2023-01-20 01:36:25,647 - INFO - joeynmt.training - Epoch 421, Step:   145500, Batch Loss:     1.489645, Batch Acc: 0.625169, Tokens per Sec:    13793, Lr: 0.000023\n",
            "2023-01-20 01:36:26,966 - INFO - joeynmt.training - Epoch 421: total training loss 505.40\n",
            "2023-01-20 01:36:26,967 - INFO - joeynmt.training - EPOCH 422\n",
            "2023-01-20 01:36:33,609 - INFO - joeynmt.training - Epoch 422, Step:   145600, Batch Loss:     1.339833, Batch Acc: 0.620109, Tokens per Sec:    14003, Lr: 0.000023\n",
            "2023-01-20 01:36:41,499 - INFO - joeynmt.training - Epoch 422, Step:   145700, Batch Loss:     1.386961, Batch Acc: 0.623385, Tokens per Sec:    13983, Lr: 0.000023\n",
            "2023-01-20 01:36:49,376 - INFO - joeynmt.training - Epoch 422, Step:   145800, Batch Loss:     1.342238, Batch Acc: 0.622190, Tokens per Sec:    14011, Lr: 0.000023\n",
            "2023-01-20 01:36:54,246 - INFO - joeynmt.training - Epoch 422: total training loss 507.27\n",
            "2023-01-20 01:36:54,247 - INFO - joeynmt.training - EPOCH 423\n",
            "2023-01-20 01:36:57,308 - INFO - joeynmt.training - Epoch 423, Step:   145900, Batch Loss:     1.383714, Batch Acc: 0.625728, Tokens per Sec:    14080, Lr: 0.000023\n",
            "2023-01-20 01:37:05,090 - INFO - joeynmt.training - Epoch 423, Step:   146000, Batch Loss:     1.446644, Batch Acc: 0.620977, Tokens per Sec:    13971, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 139.39ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10348.86ex/s]\n",
            "2023-01-20 01:37:05,356 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=146000\n",
            "2023-01-20 01:37:05,357 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:37:10,534 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:37:10,535 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:37:10,535 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:37:10,536 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:37:10,540 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.95, loss:   2.86, ppl:  17.38, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1378[sec], evaluation: 0.0370[sec]\n",
            "2023-01-20 01:37:10,542 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:37:10,546 - INFO - joeynmt.training - \tSource:     از سر سفرهٔ شام برخاست و ردای خود را به کناری گذاشت . دستمالی نیز گرفت و به کمر بست . \n",
            "2023-01-20 01:37:10,546 - INFO - joeynmt.training - \tReference:  او ، سفرهدن قالخێب اۆست پالتارێنێ بیر یانا قویدو وه بیر دسمال گؤتوروب بئلینه باغلادێ . \n",
            "2023-01-20 01:37:10,546 - INFO - joeynmt.training - \tHypothesis: صفرهیه اوتوراندا سئوینهرک اؤز پالتارێنێ بیر طرفه چکدی . الینه بیر آزالتدی وه قاپێتا اوزاناغیناندان بیر یول آچدێ .\n",
            "2023-01-20 01:37:10,546 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:37:10,548 - INFO - joeynmt.training - \tSource:     و باغهای در هم پیچیده و انبوه . \n",
            "2023-01-20 01:37:10,548 - INFO - joeynmt.training - \tReference:  ائلهجه ده سارماشان باغلار . \n",
            "2023-01-20 01:37:10,549 - INFO - joeynmt.training - \tHypothesis: سارسێلمادان اۆزۆم باغلارێ ؛\n",
            "2023-01-20 01:37:10,549 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:37:10,551 - INFO - joeynmt.training - \tSource:     در واقع ، اگر یهوه آن روزها را کوتاه نمی‌کرد ، هیچ کس نجات نمی‌یافت . اما به خاطر برگزیدگانی که خود برگزیده ، آن روزها را کوتاه کرده است . \n",
            "2023-01-20 01:37:10,551 - INFO - joeynmt.training - \tReference:  اگر رب او گۆنلری قێسالتماسایدی ، هئچ بیر اینسان خلاص اولا بیلمزدی . آمما رب سئچیلمیشلره اؤز سئچدیگی اینسانلارا گؤره او گۆنلری قێسالدێب . \n",
            "2023-01-20 01:37:10,551 - INFO - joeynmt.training - \tHypothesis: قوی ربین گۆندهده قێسالمێرسا ، هئچ کس خلاص ائتمهینلردن اۆستۆنه قویمایان اینسان سئچدیگی سئچدیگی اۆچۆن سئچدیگی گۆنه گؤره اونو سئچدی .\n",
            "2023-01-20 01:37:10,551 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:37:10,553 - INFO - joeynmt.training - \tSource:     آنها بودند که کفر ورزیدند و شما را از مسجد الحرام بازداشتند و نگذاشتند قربانی شما که بازداشته شده بود به محلش برسد ، و اگر در مکه‌ مردان و زنان با ایمانی نبودند که ممکن بود بی‌آنکه آنان را بشناسید ، ندانسته پایمالشان کنید و تاوانشان بر شما بماند فرمان حمله به مکه می‌دادیم‌ تا خدا هر که را بخواهد در جوار رحمت خویش درآورد . اگر کافر و مؤمن‌ از هم متمایز می‌شدند ، قطعا کافران را به عذاب دردناکی معذب می‌داشتیم . \n",
            "2023-01-20 01:37:10,554 - INFO - joeynmt.training - \tReference:  کۆفر ائدنلر ده ، سیزین مسجیدالحراما داخل اولماغێنێزا مانئچیلیک تؤرهدنلر ده ، قوربانلێقلاری اؤز یئرینه گئدیب چاتماغا قویمایانلار دا محض اونلاردێر . اگر تانیمادیغینیز معؤ مین کیشی وه قادێنلارێ بیلمهدن آیاق آلتێنا آلێب ازمک وه بوندان دولایی سیزه گۆناه گلمک احتمالێ اولماسایدی . آمما آللاه ایستهدیگینی اؤز مرحمتینه قوووشدورسون دئیه . اگر اونلار بیر بیریندن سئچیلیب آیریلمیش اولسایدیلار ، بیز اونلاردان کافر اولانلارێ شدتلی بیر ازابا دۆچار ائدردیک ! \n",
            "2023-01-20 01:37:10,554 - INFO - joeynmt.training - \tHypothesis: اونلار کافر اولوب مسجیدالحراما طرف چئویریب مجلسلری سیزی حالالماسێنلار . سیزه قوربانگاهێندان ، زیناکارلێق ائدن ، ایمان گتیرنلره دوست توتولموش ، اگر اونلارلا بیرلیکده ایمان گتیرمهمیش ، کۆفره سۆرۆکلهنننننجه ، کۆفرۆ اۆمیدلرینی اۆز دؤندرسۆکۆشدۆک . اگر اۆز چئویرسۆشدۆک ، ایمان گتیرسۆنلری محو ائدیب\n",
            "2023-01-20 01:37:18,388 - INFO - joeynmt.training - Epoch 423, Step:   146100, Batch Loss:     1.529460, Batch Acc: 0.619004, Tokens per Sec:    13316, Lr: 0.000023\n",
            "2023-01-20 01:37:26,226 - INFO - joeynmt.training - Epoch 423, Step:   146200, Batch Loss:     1.537987, Batch Acc: 0.621391, Tokens per Sec:    14195, Lr: 0.000023\n",
            "2023-01-20 01:37:26,915 - INFO - joeynmt.training - Epoch 423: total training loss 510.62\n",
            "2023-01-20 01:37:26,916 - INFO - joeynmt.training - EPOCH 424\n",
            "2023-01-20 01:37:34,132 - INFO - joeynmt.training - Epoch 424, Step:   146300, Batch Loss:     1.467614, Batch Acc: 0.626133, Tokens per Sec:    14123, Lr: 0.000023\n",
            "2023-01-20 01:37:41,919 - INFO - joeynmt.training - Epoch 424, Step:   146400, Batch Loss:     1.403457, Batch Acc: 0.621098, Tokens per Sec:    14017, Lr: 0.000023\n",
            "2023-01-20 01:37:49,793 - INFO - joeynmt.training - Epoch 424, Step:   146500, Batch Loss:     1.491843, Batch Acc: 0.623140, Tokens per Sec:    13966, Lr: 0.000023\n",
            "2023-01-20 01:37:54,029 - INFO - joeynmt.training - Epoch 424: total training loss 507.03\n",
            "2023-01-20 01:37:54,029 - INFO - joeynmt.training - EPOCH 425\n",
            "2023-01-20 01:37:57,610 - INFO - joeynmt.training - Epoch 425, Step:   146600, Batch Loss:     1.384752, Batch Acc: 0.623375, Tokens per Sec:    14135, Lr: 0.000023\n",
            "2023-01-20 01:38:08,116 - INFO - joeynmt.training - Epoch 425, Step:   146700, Batch Loss:     1.383617, Batch Acc: 0.623321, Tokens per Sec:    10412, Lr: 0.000023\n",
            "2023-01-20 01:38:16,265 - INFO - joeynmt.training - Epoch 425, Step:   146800, Batch Loss:     1.289091, Batch Acc: 0.625377, Tokens per Sec:    13392, Lr: 0.000023\n",
            "2023-01-20 01:38:24,055 - INFO - joeynmt.training - Epoch 425, Step:   146900, Batch Loss:     1.566952, Batch Acc: 0.619610, Tokens per Sec:    14242, Lr: 0.000023\n",
            "2023-01-20 01:38:24,086 - INFO - joeynmt.training - Epoch 425: total training loss 507.30\n",
            "2023-01-20 01:38:24,086 - INFO - joeynmt.training - EPOCH 426\n",
            "2023-01-20 01:38:31,960 - INFO - joeynmt.training - Epoch 426, Step:   147000, Batch Loss:     1.504247, Batch Acc: 0.626540, Tokens per Sec:    13848, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 142.86ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9297.64ex/s]\n",
            "2023-01-20 01:38:32,264 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=147000\n",
            "2023-01-20 01:38:32,265 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:38:36,652 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:38:36,653 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:38:36,653 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:38:36,654 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:38:36,657 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.90, loss:   2.83, ppl:  16.87, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3495[sec], evaluation: 0.0352[sec]\n",
            "2023-01-20 01:38:36,659 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:38:36,663 - INFO - joeynmt.training - \tSource:     و برخی‌شان رو به برخی کنند و از هم پرسند ، \n",
            "2023-01-20 01:38:36,663 - INFO - joeynmt.training - \tReference:  اونلار بیر بیریندن حال اهوال توتماغا باشلایاجاقلار . \n",
            "2023-01-20 01:38:36,663 - INFO - joeynmt.training - \tHypothesis: اونلارلا بیرگه قالخێب بۆتپرستلر ده بۆتلره قوشولوب\n",
            "2023-01-20 01:38:36,663 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:38:36,665 - INFO - joeynmt.training - \tSource:     و حال آنکه‌ پیش از آن ، کتاب موسی ، راهبر و مایه‌ رحمتی بود ؛ و این قرآن‌ کتابی است به زبان عربی که تصدیق‌کننده آن‌ است ، تا کسانی را که ستم کرده‌اند هشدار دهد و برای نیکوکاران مژده‌ای باشد . \n",
            "2023-01-20 01:38:36,665 - INFO - joeynmt.training - \tReference:  اوندان اول موسانێن رحبر وه مرحمت اولان کیتابی وار ایدی . بو عرب دیلینده تصدیق ائدن ، زالیملاری قورخوتماق وه یاخشی امل صاحبلرینه مۆژده وئرمک اۆچۆن اولان بیر کیتابدیر ! \n",
            "2023-01-20 01:38:36,665 - INFO - joeynmt.training - \tHypothesis: بیز موساندان اول بیر مرحمت اولاراق ، موسانێن کیتابی وه مرحمتی تصدیق ائدن بیر کیتابدیر . او ، معؤ مینلره اؤزلری قورخوتماق اۆچۆن نازل ائتمیشدیر . یاخشی ایشلر گؤرنلر اۆچۆن مۆژدهلیدیر .\n",
            "2023-01-20 01:38:36,666 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:38:36,667 - INFO - joeynmt.training - \tSource:     همانان که در شهرها سر به طغیان برداشتند ، \n",
            "2023-01-20 01:38:36,668 - INFO - joeynmt.training - \tReference:  او کسلر کی ، مملهکتلرده تۆغیان ائدیر ، \n",
            "2023-01-20 01:38:36,668 - INFO - joeynmt.training - \tHypothesis: او شهرلرده پارچالانانانلار دا گؤتوروب پارچالاناندێرێلاندێرێلمێشدێ .\n",
            "2023-01-20 01:38:36,668 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:38:36,670 - INFO - joeynmt.training - \tSource:      به‌راستی آن غلام امین و دانا کیست که اربابش او را بر خادمان خانهٔ خود گماشت تا به‌موقع خوراک آنان را بدهد ؟ \n",
            "2023-01-20 01:38:36,670 - INFO - joeynmt.training - \tReference:  واختلێ واختێندا اونلارا یئمک وئرمک اۆچۆن آغاسێنێن اؤز نؤکرلری اۆزهرینه قویدوغو صادق وه آغێللێ قول کیمدیر ؟ \n",
            "2023-01-20 01:38:36,670 - INFO - joeynmt.training - \tHypothesis: آند اولسون کی ، او قوللارێنا صادق قوللوق ائتسین کی ، اؤز قوللارێنا قوللوق ائتمک اۆچۆن اؤز قوللارێنێن اۆستۆنۆ اؤرتسون ؟\n",
            "2023-01-20 01:38:44,629 - INFO - joeynmt.training - Epoch 426, Step:   147100, Batch Loss:     1.388999, Batch Acc: 0.620418, Tokens per Sec:    13411, Lr: 0.000023\n",
            "2023-01-20 01:38:52,498 - INFO - joeynmt.training - Epoch 426, Step:   147200, Batch Loss:     1.526588, Batch Acc: 0.623113, Tokens per Sec:    13803, Lr: 0.000023\n",
            "2023-01-20 01:38:56,140 - INFO - joeynmt.training - Epoch 426: total training loss 506.68\n",
            "2023-01-20 01:38:56,140 - INFO - joeynmt.training - EPOCH 427\n",
            "2023-01-20 01:39:00,404 - INFO - joeynmt.training - Epoch 427, Step:   147300, Batch Loss:     1.448008, Batch Acc: 0.626308, Tokens per Sec:    13946, Lr: 0.000023\n",
            "2023-01-20 01:39:08,295 - INFO - joeynmt.training - Epoch 427, Step:   147400, Batch Loss:     1.452626, Batch Acc: 0.624542, Tokens per Sec:    13835, Lr: 0.000023\n",
            "2023-01-20 01:39:16,140 - INFO - joeynmt.training - Epoch 427, Step:   147500, Batch Loss:     1.323954, Batch Acc: 0.619069, Tokens per Sec:    13905, Lr: 0.000023\n",
            "2023-01-20 01:39:23,595 - INFO - joeynmt.training - Epoch 427: total training loss 509.31\n",
            "2023-01-20 01:39:23,595 - INFO - joeynmt.training - EPOCH 428\n",
            "2023-01-20 01:39:24,084 - INFO - joeynmt.training - Epoch 428, Step:   147600, Batch Loss:     1.315641, Batch Acc: 0.640610, Tokens per Sec:    13163, Lr: 0.000023\n",
            "2023-01-20 01:39:31,930 - INFO - joeynmt.training - Epoch 428, Step:   147700, Batch Loss:     1.504753, Batch Acc: 0.625811, Tokens per Sec:    13999, Lr: 0.000023\n",
            "2023-01-20 01:39:39,792 - INFO - joeynmt.training - Epoch 428, Step:   147800, Batch Loss:     1.507566, Batch Acc: 0.621289, Tokens per Sec:    14035, Lr: 0.000023\n",
            "2023-01-20 01:39:47,638 - INFO - joeynmt.training - Epoch 428, Step:   147900, Batch Loss:     1.492563, Batch Acc: 0.622079, Tokens per Sec:    13821, Lr: 0.000023\n",
            "2023-01-20 01:39:50,862 - INFO - joeynmt.training - Epoch 428: total training loss 508.47\n",
            "2023-01-20 01:39:50,863 - INFO - joeynmt.training - EPOCH 429\n",
            "2023-01-20 01:39:55,462 - INFO - joeynmt.training - Epoch 429, Step:   148000, Batch Loss:     1.465490, Batch Acc: 0.627240, Tokens per Sec:    13895, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 140.24ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10194.93ex/s]\n",
            "2023-01-20 01:39:55,729 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=148000\n",
            "2023-01-20 01:39:55,729 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:40:00,402 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:40:00,402 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:40:00,402 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:40:00,403 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:40:00,406 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.84, loss:   2.82, ppl:  16.80, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6341[sec], evaluation: 0.0356[sec]\n",
            "2023-01-20 01:40:00,409 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:40:00,412 - INFO - joeynmt.training - \tSource:     پس فرستاد تا در زندان سر یحیی را از تنش جدا کنند . \n",
            "2023-01-20 01:40:00,412 - INFO - joeynmt.training - \tReference:  بئلهلیکله ، زینداندا یهیانین بوینونو ووردوردو . \n",
            "2023-01-20 01:40:00,412 - INFO - joeynmt.training - \tHypothesis: یهیانین باشێنێ زینداندا ساخلاییر .\n",
            "2023-01-20 01:40:00,413 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:40:00,415 - INFO - joeynmt.training - \tSource:      شما می‌دانید که دو روز دیگر عید پسح است و پسر انسان به دست دشمنان تحویل داده خواهد شد تا بر تیر اعدام شود . \n",
            "2023-01-20 01:40:00,415 - INFO - joeynmt.training - \tReference:   ایکتی گۆن سونرا پاسخا بایرامی اولاجاغێنێ بیلیرسینیز . بشر اوغلو چارمێخا چکیلمک اۆچۆن تسلیم ائدیلهجک . \n",
            "2023-01-20 01:40:00,415 - INFO - joeynmt.training - \tHypothesis: بلی ، سیز ایکی گۆندن سونرا بشر اوغلونا قارشێ دۆشمندیر . بشر اوغلو چارمێخا چکیلمک اۆچۆن بشر اوغلونا تسلیم ائدیلهجک .\n",
            "2023-01-20 01:40:00,415 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:40:00,417 - INFO - joeynmt.training - \tSource:     هیچ چیز نمی‌تواند از بیرون به انسان وارد شود و او را نجس کند ، بلکه آنچه از انسان بیرون می‌آید ، او را نجس می‌سازد . \n",
            "2023-01-20 01:40:00,417 - INFO - joeynmt.training - \tReference:  کناردان اینسانین داخلینه گیرن هئچ بیر شئی اینسانی موردار ائده بیلمز . آمما اینسانی موردار ائدن شئیلر داخلیندن چێخانلاردێر . \n",
            "2023-01-20 01:40:00,417 - INFO - joeynmt.training - \tHypothesis: هئچ بیر اینسان چێخا بیلمز . اینسانین داخلیندن چێخماییب ، اکسینه ، اینسانین ایتیرسه ، اونو چێخارماغا چالێشێر .\n",
            "2023-01-20 01:40:00,418 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:40:00,420 - INFO - joeynmt.training - \tSource:      چرا شاگردان تو سنت گذشتگان را زیر پا می‌گذارند ؟ مثلا ، پیش از خوردن غذا دست‌هایشان را آب نمی‌کشند ! \n",
            "2023-01-20 01:40:00,420 - INFO - joeynmt.training - \tReference:   نه اۆچۆن شاگردلرین آغساققاللاردان قالان آدت انهنهنی پوزور وه چؤرک یئدیکلری زامان اللهرینی یومورلار ؟ \n",
            "2023-01-20 01:40:00,420 - INFO - joeynmt.training - \tHypothesis: بس اوندا شاگردلر سنین شاگردلرینین پیسلیگینه گؤرهسن ؟ اونلار اؤزلری ایله یئمهیه اجازهلری یوخدور .\n",
            "2023-01-20 01:40:08,339 - INFO - joeynmt.training - Epoch 429, Step:   148100, Batch Loss:     1.462910, Batch Acc: 0.622789, Tokens per Sec:    13564, Lr: 0.000023\n",
            "2023-01-20 01:40:16,170 - INFO - joeynmt.training - Epoch 429, Step:   148200, Batch Loss:     1.485662, Batch Acc: 0.623290, Tokens per Sec:    14139, Lr: 0.000023\n",
            "2023-01-20 01:40:22,935 - INFO - joeynmt.training - Epoch 429: total training loss 504.46\n",
            "2023-01-20 01:40:22,936 - INFO - joeynmt.training - EPOCH 430\n",
            "2023-01-20 01:40:24,045 - INFO - joeynmt.training - Epoch 430, Step:   148300, Batch Loss:     1.411863, Batch Acc: 0.621486, Tokens per Sec:    13992, Lr: 0.000023\n",
            "2023-01-20 01:40:31,984 - INFO - joeynmt.training - Epoch 430, Step:   148400, Batch Loss:     1.477177, Batch Acc: 0.624079, Tokens per Sec:    13998, Lr: 0.000023\n",
            "2023-01-20 01:40:39,953 - INFO - joeynmt.training - Epoch 430, Step:   148500, Batch Loss:     1.568735, Batch Acc: 0.625876, Tokens per Sec:    13849, Lr: 0.000023\n",
            "2023-01-20 01:40:48,031 - INFO - joeynmt.training - Epoch 430, Step:   148600, Batch Loss:     1.523645, Batch Acc: 0.622770, Tokens per Sec:    13771, Lr: 0.000023\n",
            "2023-01-20 01:40:50,358 - INFO - joeynmt.training - Epoch 430: total training loss 500.30\n",
            "2023-01-20 01:40:50,359 - INFO - joeynmt.training - EPOCH 431\n",
            "2023-01-20 01:40:55,935 - INFO - joeynmt.training - Epoch 431, Step:   148700, Batch Loss:     1.602129, Batch Acc: 0.623980, Tokens per Sec:    14096, Lr: 0.000023\n",
            "2023-01-20 01:41:03,746 - INFO - joeynmt.training - Epoch 431, Step:   148800, Batch Loss:     1.536144, Batch Acc: 0.620951, Tokens per Sec:    14097, Lr: 0.000023\n",
            "2023-01-20 01:41:11,643 - INFO - joeynmt.training - Epoch 431, Step:   148900, Batch Loss:     1.379240, Batch Acc: 0.627108, Tokens per Sec:    13885, Lr: 0.000023\n",
            "2023-01-20 01:41:17,386 - INFO - joeynmt.training - Epoch 431: total training loss 503.42\n",
            "2023-01-20 01:41:17,387 - INFO - joeynmt.training - EPOCH 432\n",
            "2023-01-20 01:41:19,499 - INFO - joeynmt.training - Epoch 432, Step:   149000, Batch Loss:     1.354749, Batch Acc: 0.630970, Tokens per Sec:    14492, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 128.14ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9601.85ex/s]\n",
            "2023-01-20 01:41:19,780 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=149000\n",
            "2023-01-20 01:41:19,781 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:41:24,434 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:41:24,434 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:41:24,435 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:41:24,435 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:41:24,438 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.02, loss:   2.95, ppl:  19.04, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6131[sec], evaluation: 0.0375[sec]\n",
            "2023-01-20 01:41:24,459 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:41:24,462 - INFO - joeynmt.training - \tSource:      خوشا به حال آنان که دلی پاک دارند ؛ زیرا خدا را خواهند دید . \n",
            "2023-01-20 01:41:24,462 - INFO - joeynmt.training - \tReference:  نه بختیاردیر اۆرهیی تمیز اولانلار ! چۆنکی اونلار آللاهێ گؤرهجک . \n",
            "2023-01-20 01:41:24,462 - INFO - joeynmt.training - \tHypothesis: نه بختیاردیر اونلار آللاهێن اۆرکلرینی گؤرهجکسن ، چۆنکی آللاه گؤرهجک .\n",
            "2023-01-20 01:41:24,462 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:41:24,464 - INFO - joeynmt.training - \tSource:     ما تو را بحق فرستادیم ، تا بشارتگر و بیم‌دهنده باشی ، و لی‌ درباره دوزخیان ، از تو پرسشی نخواهد شد . \n",
            "2023-01-20 01:41:24,465 - INFO - joeynmt.training - \tReference:   بیز سنی حاقق ایله مۆژده وئرمهیه وه قورخوتماغا گوندردیک . جهنم اهلی بارهسینده ایسه سن سورغو سوعالا توتولمایاجاقسان . \n",
            "2023-01-20 01:41:24,465 - INFO - joeynmt.training - \tHypothesis: بیز سنی مۆژده وئرن وه مۆژده وئرنلر . سن جهنمده کافرلرین قاپیلاری جهنمی یالنیز جهنهد گؤستهرنسن !\n",
            "2023-01-20 01:41:24,465 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:41:24,467 - INFO - joeynmt.training - \tSource:     سرانجام ، پس از تمسخر کردن او ، ردا را از تنش درآوردند و لباسش را بر تنش کردند و او را از آنجا بردند تا به تیر میخکوب کنند . \n",
            "2023-01-20 01:41:24,467 - INFO - joeynmt.training - \tReference:  اونو اله سالاندان سونرا خالاتێ اینیندن چێخارێب اؤز پالتارێنێ گئییندیردیلر . سونرا اونو چارمێخا چکمهیه آپاردێلار . \n",
            "2023-01-20 01:41:24,467 - INFO - joeynmt.training - \tHypothesis: بئلهلیکله ، بیرگه تۆند قێرمێزێ پالتار گئیینیب چارمێخا چکدیلر وه چارمێخا چکدیلر .\n",
            "2023-01-20 01:41:24,467 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:41:24,469 - INFO - joeynmt.training - \tSource:     اما تولد عیسی مسیح به این صورت بود : زمانی که مریم ، مادر عیسی ، نامزد یوسف بود ، پیش از آن که به خانهٔ شوهر برود ، معلوم شد که به وسیلهٔ روح‌القدس باردار شده است . \n",
            "2023-01-20 01:41:24,470 - INFO - joeynmt.training - \tReference:  ایسا مسیحین دوغولماسێ بئله اولدو . آناسێ مریم یوصفه نیشانلانمیشدی . آمما بیرلیکده اولمالاریندان اول مریمین مۆقدس روحدان حامله اولدوغو آشکار اولدو . \n",
            "2023-01-20 01:41:24,470 - INFO - joeynmt.training - \tHypothesis: آمما سن ایسا مصیحه اولان بیر زامان اوندان اول مریمین آدێ ایله اون ائولهننیب ائولهرینه گئدنرکن مۆقدس روحون سئلینهسینه دۆشۆب بو قادێنێ داشێ اولان یوصیف آدت انهنه بنزهییر .\n",
            "2023-01-20 01:41:32,319 - INFO - joeynmt.training - Epoch 432, Step:   149100, Batch Loss:     1.398950, Batch Acc: 0.625254, Tokens per Sec:    13398, Lr: 0.000023\n",
            "2023-01-20 01:41:40,168 - INFO - joeynmt.training - Epoch 432, Step:   149200, Batch Loss:     1.403282, Batch Acc: 0.624383, Tokens per Sec:    13897, Lr: 0.000023\n",
            "2023-01-20 01:41:51,103 - INFO - joeynmt.training - Epoch 432, Step:   149300, Batch Loss:     1.554501, Batch Acc: 0.622489, Tokens per Sec:    10186, Lr: 0.000023\n",
            "2023-01-20 01:41:52,540 - INFO - joeynmt.training - Epoch 432: total training loss 503.46\n",
            "2023-01-20 01:41:52,540 - INFO - joeynmt.training - EPOCH 433\n",
            "2023-01-20 01:41:59,099 - INFO - joeynmt.training - Epoch 433, Step:   149400, Batch Loss:     1.504909, Batch Acc: 0.625847, Tokens per Sec:    13883, Lr: 0.000023\n",
            "2023-01-20 01:42:06,981 - INFO - joeynmt.training - Epoch 433, Step:   149500, Batch Loss:     1.358366, Batch Acc: 0.624935, Tokens per Sec:    13859, Lr: 0.000023\n",
            "2023-01-20 01:42:14,867 - INFO - joeynmt.training - Epoch 433, Step:   149600, Batch Loss:     1.529064, Batch Acc: 0.624007, Tokens per Sec:    14022, Lr: 0.000023\n",
            "2023-01-20 01:42:19,851 - INFO - joeynmt.training - Epoch 433: total training loss 504.06\n",
            "2023-01-20 01:42:19,851 - INFO - joeynmt.training - EPOCH 434\n",
            "2023-01-20 01:42:22,827 - INFO - joeynmt.training - Epoch 434, Step:   149700, Batch Loss:     1.419353, Batch Acc: 0.626826, Tokens per Sec:    13946, Lr: 0.000023\n",
            "2023-01-20 01:42:30,697 - INFO - joeynmt.training - Epoch 434, Step:   149800, Batch Loss:     1.491244, Batch Acc: 0.622001, Tokens per Sec:    14039, Lr: 0.000023\n",
            "2023-01-20 01:42:38,554 - INFO - joeynmt.training - Epoch 434, Step:   149900, Batch Loss:     1.432038, Batch Acc: 0.625639, Tokens per Sec:    13959, Lr: 0.000023\n",
            "2023-01-20 01:42:46,470 - INFO - joeynmt.training - Epoch 434, Step:   150000, Batch Loss:     1.564969, Batch Acc: 0.623724, Tokens per Sec:    13742, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.72ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10469.66ex/s]\n",
            "2023-01-20 01:42:46,752 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=150000\n",
            "2023-01-20 01:42:46,752 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:42:51,904 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:42:51,904 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:42:51,904 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:42:51,905 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:42:51,908 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.75, loss:   2.90, ppl:  18.11, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1119[sec], evaluation: 0.0370[sec]\n",
            "2023-01-20 01:42:51,911 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:42:51,914 - INFO - joeynmt.training - \tSource:     پس همگی ، سوستنیس ، مسئول کنیسه را گرفتند و او را در مقابل مسند داوری زدند . اما گالیو به این امور کاملا بی‌اعتنا بود . \n",
            "2023-01-20 01:42:51,914 - INFO - joeynmt.training - \tReference:  هامێ سیناقوق رعیسی سوستئنی توتوب هؤکم کۆرسۆسۆنۆن قارشیسیندا دؤیدو . قاللیو ایسه بو حادثهلره احمیت وئرمهدی . \n",
            "2023-01-20 01:42:51,914 - INFO - joeynmt.training - \tHypothesis: بوندان سونرا اونلارێن سیناقوقونا گتیردیلر وه ایسنانی هؤکمرانێ مۆحاکمه ائتمهیه باشلادێلار . آمما بو آدامێ مۆحاکمه ائتمهیه باشلادێ . آمما بو شئیلری تاماملاناراق اؤزونو تاماملاندیر ،\n",
            "2023-01-20 01:42:51,915 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:42:51,917 - INFO - joeynmt.training - \tSource:     هش‌دار که آنچه در آسمانها و زمین است از آن خداست . به یقین آنچه را که بر آنید می‌داند ، و روزی که به سوی او بازگردانیده می‌شوند آنان را از حقیقت‌ آنچه انجام داده‌اند خبر می‌دهد ، و خدا به هر چیزی داناست . \n",
            "2023-01-20 01:42:51,917 - INFO - joeynmt.training - \tReference:  بیلین وه آگاه اولون کی ، گؤیلرده وه یئرده نه وارسا ، آللاهێندێر . او ، حقیقتا ، سیزین نه امل صاحبی اولدوغونوزو بیلیر . آللاه اونون هۆزورونا قایتاریلاجاقلاری گۆن اونلارا نه ائتدیکلرینی بیلدیرهجکدیر . آللاه هر شئیی بیلندیر ! \n",
            "2023-01-20 01:42:51,917 - INFO - joeynmt.training - \tHypothesis: گؤیلرده وه یئرده نه وارسا ، آللاها مخصوصدور . او ، غالب گلین ! او ، یئنه ده او ، یئنه ده بیر روزی وئرر . حقیقتا ، آللاه هر شئیی بیلندیر !\n",
            "2023-01-20 01:42:51,917 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:42:51,919 - INFO - joeynmt.training - \tSource:      او اهل رامه ، یکی از شهرهای یهودیان بود و انتظار پادشاهی خدا را می‌کشید . \n",
            "2023-01-20 01:42:51,919 - INFO - joeynmt.training - \tReference:  او ، عالی شورانێن قرارێ وه امهلی ایله رازێ اولمامێشدێ . بو آدام یهودئیانین آریمآتئیا شهریندن ایدی وه او دا آللاهێن پادشاهلیغینی گؤزلهییردی . \n",
            "2023-01-20 01:42:51,919 - INFO - joeynmt.training - \tHypothesis: او ، یهودیلرین بیر شهریندن بیرینده ، آللاهێن پادشاهلیغینین گؤزلهدیگی ، آللاهێن پادشاهلیغینی گؤزلهیهرک ایسنانی گؤزلهییردی .\n",
            "2023-01-20 01:42:51,920 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:42:51,922 - INFO - joeynmt.training - \tSource:     در آن دو باغ‌ از هر میوه‌ای دو گونه است . \n",
            "2023-01-20 01:42:51,922 - INFO - joeynmt.training - \tReference:  اورادا هر مئیوهدن ایکی نؤؤ واردێر . \n",
            "2023-01-20 01:42:51,922 - INFO - joeynmt.training - \tHypothesis: اورادا ایکی جنت واردێر .\n",
            "2023-01-20 01:42:52,670 - INFO - joeynmt.training - Epoch 434: total training loss 504.91\n",
            "2023-01-20 01:42:52,671 - INFO - joeynmt.training - EPOCH 435\n",
            "2023-01-20 01:42:59,745 - INFO - joeynmt.training - Epoch 435, Step:   150100, Batch Loss:     1.360798, Batch Acc: 0.625171, Tokens per Sec:    14071, Lr: 0.000023\n",
            "2023-01-20 01:43:07,589 - INFO - joeynmt.training - Epoch 435, Step:   150200, Batch Loss:     1.490312, Batch Acc: 0.625005, Tokens per Sec:    13875, Lr: 0.000023\n",
            "2023-01-20 01:43:15,537 - INFO - joeynmt.training - Epoch 435, Step:   150300, Batch Loss:     1.389125, Batch Acc: 0.624044, Tokens per Sec:    13773, Lr: 0.000023\n",
            "2023-01-20 01:43:20,046 - INFO - joeynmt.training - Epoch 435: total training loss 506.14\n",
            "2023-01-20 01:43:20,046 - INFO - joeynmt.training - EPOCH 436\n",
            "2023-01-20 01:43:23,569 - INFO - joeynmt.training - Epoch 436, Step:   150400, Batch Loss:     1.394263, Batch Acc: 0.624675, Tokens per Sec:    13761, Lr: 0.000023\n",
            "2023-01-20 01:43:31,619 - INFO - joeynmt.training - Epoch 436, Step:   150500, Batch Loss:     1.494194, Batch Acc: 0.624421, Tokens per Sec:    13716, Lr: 0.000023\n",
            "2023-01-20 01:43:39,588 - INFO - joeynmt.training - Epoch 436, Step:   150600, Batch Loss:     1.577851, Batch Acc: 0.624370, Tokens per Sec:    13849, Lr: 0.000023\n",
            "2023-01-20 01:43:47,656 - INFO - joeynmt.training - Epoch 436, Step:   150700, Batch Loss:     1.552901, Batch Acc: 0.625897, Tokens per Sec:    13598, Lr: 0.000023\n",
            "2023-01-20 01:43:47,752 - INFO - joeynmt.training - Epoch 436: total training loss 501.88\n",
            "2023-01-20 01:43:47,752 - INFO - joeynmt.training - EPOCH 437\n",
            "2023-01-20 01:43:55,615 - INFO - joeynmt.training - Epoch 437, Step:   150800, Batch Loss:     1.412046, Batch Acc: 0.625670, Tokens per Sec:    14039, Lr: 0.000023\n",
            "2023-01-20 01:44:03,598 - INFO - joeynmt.training - Epoch 437, Step:   150900, Batch Loss:     1.490683, Batch Acc: 0.624120, Tokens per Sec:    13678, Lr: 0.000023\n",
            "2023-01-20 01:44:11,587 - INFO - joeynmt.training - Epoch 437, Step:   151000, Batch Loss:     1.511662, Batch Acc: 0.626385, Tokens per Sec:    13757, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 128.01ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9727.70ex/s]\n",
            "2023-01-20 01:44:11,874 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=151000\n",
            "2023-01-20 01:44:11,875 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:44:16,329 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:44:16,330 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:44:16,330 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:44:16,331 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:44:16,334 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.87, loss:   2.75, ppl:  15.68, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4167[sec], evaluation: 0.0354[sec]\n",
            "2023-01-20 01:44:16,336 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:44:16,340 - INFO - joeynmt.training - \tSource:     اگر عضوی دردمند شود ، تمام اعضای دیگر با آن درد خواهند کشید . اگر عضوی نیز عزت یابد ، تمام اعضای دیگر با او شادی خواهند کرد . \n",
            "2023-01-20 01:44:16,340 - INFO - joeynmt.training - \tReference:  بونا گؤره اگر بیر اۆزۆ اعذاب چکیرسه ، باشقالاری دا اونونلا بیرگه اعذاب چکیر . اۆزۆلردن بیری شرفه چاتێرسا ، باشقالاری دا اونونلا بیرگه سئوینیر . \n",
            "2023-01-20 01:44:16,340 - INFO - joeynmt.training - \tHypothesis: اگر بۆتۆن اینسانلارین هامێسێ اؤیرن اولاجاقسا ، باشقالارینین هامێسێ محو اولاجاق . اگر بیر بیرینیزین آردێنجا گئدهجک .\n",
            "2023-01-20 01:44:16,340 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:44:16,343 - INFO - joeynmt.training - \tSource:     این زن آنچه در توان داشت ، انجام داد و مرا با ریختن این روغن معطر بر بدن من ، پیشاپیش برای تدفین آماده کرد . \n",
            "2023-01-20 01:44:16,344 - INFO - joeynmt.training - \tReference:  قادێن باجاردێغێنێ ائتدی . منیم بدهنیمه دفن اۆچۆن اولجهدن یاغ چکدی . \n",
            "2023-01-20 01:44:16,344 - INFO - joeynmt.training - \tHypothesis: قادێن منیم بدهنیم تمهین تؤکۆب بو قادێنا چکمکله منی دفن ائدیلمک اۆچۆن بو بدن اۆچۆن حاضرلامێشدێر .\n",
            "2023-01-20 01:44:16,344 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:44:16,346 - INFO - joeynmt.training - \tSource:     فرستاده‌ای از جانب خدا که بر آنان‌ صحیفه‌هایی پاک را تلاوت کند ، \n",
            "2023-01-20 01:44:16,346 - INFO - joeynmt.training - \tReference:   پاک صحیفهلری اونلارا اوخویان ، آللاه ترهفیندن گؤندهریلمیش پیغمبردیر . \n",
            "2023-01-20 01:44:16,346 - INFO - joeynmt.training - \tHypothesis: آللاه ترهفیندن بیر پیغمبر واسطهسیله تمیزلهننن بیر پیغمبر گؤندهرهنی\n",
            "2023-01-20 01:44:16,346 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:44:16,348 - INFO - joeynmt.training - \tSource:     آنگاه تمام آن جمع ساکت شدند و به سخنان برنابا و پولس در مورد بسیاری نشانه‌ها و معجزات که خدا از طریق آنان در میان قوم‌ها به ظهور رسانده بود ، گوش فرادادند . \n",
            "2023-01-20 01:44:16,349 - INFO - joeynmt.training - \tReference:  اوندا بۆتۆن توپلانتێ ساکیتلشدی وه بارنابا ایله پاولا قولاق آسماغا باشلادێلار . بارنابا ایله پاول آللاهێن اونلار واسطهسیله باشقا ملتلر آراسێندا گؤستردیگی علامت وه خارقهلرین هامیسینی دانێشدێ . \n",
            "2023-01-20 01:44:16,349 - INFO - joeynmt.training - \tHypothesis: بۆتۆن خالق توپلاشێب پاوللا بارنابا وه خالق آراسێندا فرقهلره وه خارقهلر گؤسترمک اۆچۆن خالق آراسێندا فرقلنیردی .\n",
            "2023-01-20 01:44:19,965 - INFO - joeynmt.training - Epoch 437: total training loss 502.01\n",
            "2023-01-20 01:44:19,965 - INFO - joeynmt.training - EPOCH 438\n",
            "2023-01-20 01:44:24,316 - INFO - joeynmt.training - Epoch 438, Step:   151100, Batch Loss:     1.312746, Batch Acc: 0.629293, Tokens per Sec:    13827, Lr: 0.000023\n",
            "2023-01-20 01:44:32,359 - INFO - joeynmt.training - Epoch 438, Step:   151200, Batch Loss:     1.428548, Batch Acc: 0.624406, Tokens per Sec:    13681, Lr: 0.000023\n",
            "2023-01-20 01:44:40,407 - INFO - joeynmt.training - Epoch 438, Step:   151300, Batch Loss:     1.453721, Batch Acc: 0.620326, Tokens per Sec:    13602, Lr: 0.000023\n",
            "2023-01-20 01:44:47,880 - INFO - joeynmt.training - Epoch 438: total training loss 504.73\n",
            "2023-01-20 01:44:47,881 - INFO - joeynmt.training - EPOCH 439\n",
            "2023-01-20 01:44:48,642 - INFO - joeynmt.training - Epoch 439, Step:   151400, Batch Loss:     1.461132, Batch Acc: 0.624292, Tokens per Sec:    12768, Lr: 0.000023\n",
            "2023-01-20 01:44:56,695 - INFO - joeynmt.training - Epoch 439, Step:   151500, Batch Loss:     1.475848, Batch Acc: 0.625147, Tokens per Sec:    13713, Lr: 0.000023\n",
            "2023-01-20 01:45:04,725 - INFO - joeynmt.training - Epoch 439, Step:   151600, Batch Loss:     1.430388, Batch Acc: 0.622659, Tokens per Sec:    13697, Lr: 0.000023\n",
            "2023-01-20 01:45:12,715 - INFO - joeynmt.training - Epoch 439, Step:   151700, Batch Loss:     1.476058, Batch Acc: 0.625290, Tokens per Sec:    13742, Lr: 0.000023\n",
            "2023-01-20 01:45:15,716 - INFO - joeynmt.training - Epoch 439: total training loss 503.75\n",
            "2023-01-20 01:45:15,716 - INFO - joeynmt.training - EPOCH 440\n",
            "2023-01-20 01:45:20,704 - INFO - joeynmt.training - Epoch 440, Step:   151800, Batch Loss:     1.400795, Batch Acc: 0.625643, Tokens per Sec:    13962, Lr: 0.000023\n",
            "2023-01-20 01:45:31,607 - INFO - joeynmt.training - Epoch 440, Step:   151900, Batch Loss:     1.469607, Batch Acc: 0.630155, Tokens per Sec:    10079, Lr: 0.000023\n",
            "2023-01-20 01:45:39,590 - INFO - joeynmt.training - Epoch 440, Step:   152000, Batch Loss:     1.371982, Batch Acc: 0.627312, Tokens per Sec:    13637, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.54ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10238.07ex/s]\n",
            "2023-01-20 01:45:39,867 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=152000\n",
            "2023-01-20 01:45:39,867 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:45:44,193 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:45:44,194 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:45:44,194 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:45:44,195 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:45:44,198 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.16, loss:   2.93, ppl:  18.69, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2880[sec], evaluation: 0.0352[sec]\n",
            "2023-01-20 01:45:44,201 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:45:44,203 - INFO - joeynmt.training - \tSource:     چرا از شما چیزی نپذیرفته‌ام ؟ آیا به این دلیل است که شما را دوست ندارم ؟ خدا می‌داند که دوستتان دارم . \n",
            "2023-01-20 01:45:44,204 - INFO - joeynmt.training - \tReference:  نیه ؟ سیزی سئومهدیگیمه گؤرهمی ؟ آللاه بیلیر کی ، سئویرم . \n",
            "2023-01-20 01:45:44,204 - INFO - joeynmt.training - \tHypothesis: نیه باخمادێمێ ؟ مگر سیزه دوست دئییلمی ؟ آللاهدان باشقا دوستلارێمادێغێنێز حالدا من سیزین دوستێنێزێ سئومهییم .\n",
            "2023-01-20 01:45:44,204 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:45:44,206 - INFO - joeynmt.training - \tSource:      و ای موسی ، چه چیز تو را دور از قوم خودت ، به شتاب واداشته است ؟ \n",
            "2023-01-20 01:45:44,206 - INFO - joeynmt.training - \tReference:   یا موسا ! سنی اؤز جاماآتێندان آییریب تلسدیرن نه ایدی ؟ \n",
            "2023-01-20 01:45:44,206 - INFO - joeynmt.training - \tHypothesis: یا موسا ! سنین جاماآتێنێن اوزاقلاشدێن ؟\n",
            "2023-01-20 01:45:44,206 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:45:44,208 - INFO - joeynmt.training - \tSource:     و کسانی که به نشانه‌های پروردگارشان ایمان می‌آورند ، \n",
            "2023-01-20 01:45:44,208 - INFO - joeynmt.training - \tReference:  ربینین آیهلرینه اینانلار ؛ \n",
            "2023-01-20 01:45:44,209 - INFO - joeynmt.training - \tHypothesis: ایمان گتیرنلرین ربینه اینانیرلار .\n",
            "2023-01-20 01:45:44,209 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:45:44,211 - INFO - joeynmt.training - \tSource:     برای هر یک از شما که خواهد به راه راست رود . \n",
            "2023-01-20 01:45:44,211 - INFO - joeynmt.training - \tReference:  ائلهجه ده سیزدن دوغرو دۆز اولماق ایستهینلر اۆچۆن . \n",
            "2023-01-20 01:45:44,211 - INFO - joeynmt.training - \tHypothesis: هر بیرینیزدن بیری اؤز یولونا گئدهجهیینه صببه گؤره یولدان چێخار .\n",
            "2023-01-20 01:45:50,843 - INFO - joeynmt.training - Epoch 440: total training loss 502.45\n",
            "2023-01-20 01:45:50,843 - INFO - joeynmt.training - EPOCH 441\n",
            "2023-01-20 01:45:52,248 - INFO - joeynmt.training - Epoch 441, Step:   152100, Batch Loss:     1.349730, Batch Acc: 0.625960, Tokens per Sec:    13446, Lr: 0.000023\n",
            "2023-01-20 01:46:00,205 - INFO - joeynmt.training - Epoch 441, Step:   152200, Batch Loss:     1.536367, Batch Acc: 0.624577, Tokens per Sec:    13680, Lr: 0.000023\n",
            "2023-01-20 01:46:08,197 - INFO - joeynmt.training - Epoch 441, Step:   152300, Batch Loss:     1.519318, Batch Acc: 0.631039, Tokens per Sec:    13821, Lr: 0.000023\n",
            "2023-01-20 01:46:16,261 - INFO - joeynmt.training - Epoch 441, Step:   152400, Batch Loss:     1.490281, Batch Acc: 0.623504, Tokens per Sec:    13668, Lr: 0.000023\n",
            "2023-01-20 01:46:18,564 - INFO - joeynmt.training - Epoch 441: total training loss 503.21\n",
            "2023-01-20 01:46:18,564 - INFO - joeynmt.training - EPOCH 442\n",
            "2023-01-20 01:46:24,172 - INFO - joeynmt.training - Epoch 442, Step:   152500, Batch Loss:     1.463689, Batch Acc: 0.626856, Tokens per Sec:    13755, Lr: 0.000023\n",
            "2023-01-20 01:46:32,111 - INFO - joeynmt.training - Epoch 442, Step:   152600, Batch Loss:     1.450067, Batch Acc: 0.626655, Tokens per Sec:    13827, Lr: 0.000023\n",
            "2023-01-20 01:46:40,054 - INFO - joeynmt.training - Epoch 442, Step:   152700, Batch Loss:     1.452370, Batch Acc: 0.627200, Tokens per Sec:    13748, Lr: 0.000023\n",
            "2023-01-20 01:46:46,201 - INFO - joeynmt.training - Epoch 442: total training loss 505.58\n",
            "2023-01-20 01:46:46,202 - INFO - joeynmt.training - EPOCH 443\n",
            "2023-01-20 01:46:48,079 - INFO - joeynmt.training - Epoch 443, Step:   152800, Batch Loss:     1.510527, Batch Acc: 0.628192, Tokens per Sec:    13629, Lr: 0.000023\n",
            "2023-01-20 01:46:55,978 - INFO - joeynmt.training - Epoch 443, Step:   152900, Batch Loss:     1.497732, Batch Acc: 0.627839, Tokens per Sec:    14027, Lr: 0.000023\n",
            "2023-01-20 01:47:03,859 - INFO - joeynmt.training - Epoch 443, Step:   153000, Batch Loss:     1.491139, Batch Acc: 0.627496, Tokens per Sec:    13740, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.36ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10181.98ex/s]\n",
            "2023-01-20 01:47:04,132 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=153000\n",
            "2023-01-20 01:47:04,132 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:47:09,005 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:47:09,005 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:47:09,006 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:47:09,007 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:47:09,010 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.40, loss:   2.97, ppl:  19.46, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.8221[sec], evaluation: 0.0482[sec]\n",
            "2023-01-20 01:47:09,012 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:47:09,016 - INFO - joeynmt.training - \tSource:     بله‌تان فقط بله و نه‌تان فقط نه باشد ؛ چون سخنی بیش از این ، از آن شریر است . \n",
            "2023-01-20 01:47:09,016 - INFO - joeynmt.training - \tReference:  آنجاق سؤزونوزده بلی نیز بلی ، خئیر اینیز خئیر اولسون . بوندان قالانێ شر اولانداندێر . \n",
            "2023-01-20 01:47:09,016 - INFO - joeynmt.training - \tHypothesis: یالنیز بیر یالنیز یالنیز بیر دئییل ، یالنیز بوندان اول بیر سؤز سؤیلهییر .\n",
            "2023-01-20 01:47:09,016 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:47:09,019 - INFO - joeynmt.training - \tSource:     فرمود : به زودی بازویت را به وسیله‌ برادرت نیرومند خواهیم کرد و برای شما هر دو ، تسلطی قرار خواهیم داد که با وجود آیات ما ، به شما دست نخواهند یافت شما و هر که شما را پیروی کند چیره خواهید بود . \n",
            "2023-01-20 01:47:09,019 - INFO - joeynmt.training - \tReference:   بویوردو : سنین آرخانێ قارداشێنلا مۆحکملندیرجهجک ، ایکینیزه ده دلیل وئرهجهییک . اونلار آیهلریمیز سایهسینده سیزه هئچ بیر پیسلیک ائده بیلمهیهجکلر . سیز ده ، سیزه تابع اولانلار دا مۆتلق غالب گلهجکسینیز ! \n",
            "2023-01-20 01:47:09,020 - INFO - joeynmt.training - \tHypothesis: بئله بویوردو : بیز قارداشێنلا بیرلیکده دیریلدهجک وه هر ایکی ایکی قارداش گتیرهجهییک . هر ایکیسی سیزین اۆچۆن بیر زهمت گؤستهرن بیر نئ مت کیمی سیزینله بیرلیکده اویاجاق وه هره اؤز الئیه کئچهجهییک . سیزه اویاقلا بیرلیکده تاپمێش اولارسینیز !\n",
            "2023-01-20 01:47:09,020 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:47:09,022 - INFO - joeynmt.training - \tSource:     و یهود گفتند : عزیر ، پسر خداست . و نصاری گفتند : مسیح ، پسر خداست . این سخنی است باطل‌ که به زبان می‌آورند ، و به گفتار کسانی که پیش از این کافر شده‌اند شباهت دارد . خدا آنان را بکشد ؛ چگونه از حق‌ بازگردانده می‌شوند ؟ \n",
            "2023-01-20 01:47:09,022 - INFO - joeynmt.training - \tReference:  یهودیلر : اۆزئیر آللاهێن اوغلودور ، خاچپرستلر ده : مصیح آللاهێن اوغلودور ، دئدیلر . اونلارێن آغزێندا گزن بو سؤزلر داها اؤنجه کۆفر ائدنلرین سؤزلرینه بنزهییر . آللاه اونلارێ اؤلدورسون ! نئجه ده دؤندهریلیرلر ! \n",
            "2023-01-20 01:47:09,022 - INFO - joeynmt.training - \tHypothesis: رب ایسه دئییردی : قزبلندیر . آللاه اؤز اوغلونا مۆراجعت ائتمیشدی . اونلار دا : بو گئجهنین دیلی اینانیر ، اؤز دونوزو آللاهدێر . اونلار دا گئجه گۆندۆزۆ نئجه دؤندهریلیرلر ؟ آللاه اونلار دوغرو یولا یؤنلتمیشلر .\n",
            "2023-01-20 01:47:09,022 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:47:09,024 - INFO - joeynmt.training - \tSource:     فیلیپس به او گفت : سرور ، پدر را به ما نشان بده و این برای ما کافی است . \n",
            "2023-01-20 01:47:09,024 - INFO - joeynmt.training - \tReference:  فیلیپ اونا دئدی : یا رب ، آتانێ بیزه گؤستر ، بو بیزیم اۆچۆن کفایتدیر . \n",
            "2023-01-20 01:47:09,024 - INFO - joeynmt.training - \tHypothesis: فیلیپ اونا دئدی : یا رب ، بیزه گؤستر ، بو بیزیم اۆچۆن آتامێزا گؤستر .\n",
            "2023-01-20 01:47:17,006 - INFO - joeynmt.training - Epoch 443, Step:   153100, Batch Loss:     1.601920, Batch Acc: 0.621831, Tokens per Sec:    13137, Lr: 0.000023\n",
            "2023-01-20 01:47:18,971 - INFO - joeynmt.training - Epoch 443: total training loss 503.44\n",
            "2023-01-20 01:47:18,972 - INFO - joeynmt.training - EPOCH 444\n",
            "2023-01-20 01:47:25,001 - INFO - joeynmt.training - Epoch 444, Step:   153200, Batch Loss:     1.441738, Batch Acc: 0.626353, Tokens per Sec:    13792, Lr: 0.000023\n",
            "2023-01-20 01:47:32,963 - INFO - joeynmt.training - Epoch 444, Step:   153300, Batch Loss:     1.454499, Batch Acc: 0.627363, Tokens per Sec:    13663, Lr: 0.000023\n",
            "2023-01-20 01:47:40,869 - INFO - joeynmt.training - Epoch 444, Step:   153400, Batch Loss:     1.395611, Batch Acc: 0.625640, Tokens per Sec:    14027, Lr: 0.000023\n",
            "2023-01-20 01:47:46,476 - INFO - joeynmt.training - Epoch 444: total training loss 502.18\n",
            "2023-01-20 01:47:46,477 - INFO - joeynmt.training - EPOCH 445\n",
            "2023-01-20 01:47:48,887 - INFO - joeynmt.training - Epoch 445, Step:   153500, Batch Loss:     1.455486, Batch Acc: 0.622053, Tokens per Sec:    13819, Lr: 0.000023\n",
            "2023-01-20 01:47:56,879 - INFO - joeynmt.training - Epoch 445, Step:   153600, Batch Loss:     1.456823, Batch Acc: 0.627179, Tokens per Sec:    13945, Lr: 0.000023\n",
            "2023-01-20 01:48:04,799 - INFO - joeynmt.training - Epoch 445, Step:   153700, Batch Loss:     1.281537, Batch Acc: 0.628922, Tokens per Sec:    13881, Lr: 0.000023\n",
            "2023-01-20 01:48:12,717 - INFO - joeynmt.training - Epoch 445, Step:   153800, Batch Loss:     1.468489, Batch Acc: 0.622692, Tokens per Sec:    13902, Lr: 0.000023\n",
            "2023-01-20 01:48:13,829 - INFO - joeynmt.training - Epoch 445: total training loss 498.85\n",
            "2023-01-20 01:48:13,829 - INFO - joeynmt.training - EPOCH 446\n",
            "2023-01-20 01:48:20,745 - INFO - joeynmt.training - Epoch 446, Step:   153900, Batch Loss:     1.297800, Batch Acc: 0.630237, Tokens per Sec:    13683, Lr: 0.000023\n",
            "2023-01-20 01:48:28,694 - INFO - joeynmt.training - Epoch 446, Step:   154000, Batch Loss:     1.386654, Batch Acc: 0.624840, Tokens per Sec:    13968, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 66.49ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 7981.29ex/s]\n",
            "2023-01-20 01:48:29,029 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=154000\n",
            "2023-01-20 01:48:29,029 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:48:34,194 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:48:34,195 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:48:34,195 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:48:34,196 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:48:34,199 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.80, loss:   2.90, ppl:  18.19, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1235[sec], evaluation: 0.0377[sec]\n",
            "2023-01-20 01:48:34,202 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:48:34,205 - INFO - joeynmt.training - \tSource:      وقتی کسی تو را به جشن عروسی دعوت می‌کند ، در صدر مجلس منشین . شاید شخصی مهم‌تر از تو را نیز دعوت کرده باشد . \n",
            "2023-01-20 01:48:34,206 - INFO - joeynmt.training - \tReference:   بیر نفر سنی تویا دوت ائدیرسه ، یوخاری باشدا اوتورما . بلکه ائو یییهسینین سندن ده هؤرمتلی بیر قوناغێ وار . \n",
            "2023-01-20 01:48:34,206 - INFO - joeynmt.training - \tHypothesis: کیم جیت دوت اولونانلارێن دوت اولوندوغونو گؤردوگون کیمی ، منه ده سئوینج وئریر . سنین ده وطنداشێ وط ائت .\n",
            "2023-01-20 01:48:34,206 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:48:34,208 - INFO - joeynmt.training - \tSource:     و برای هر یک از این دو گروه‌ ، از آنچه انجام داده‌اند ، در جزا مراتبی خواهد بود ، و پروردگارت از آنچه می‌کنند غافل نیست . \n",
            "2023-01-20 01:48:34,208 - INFO - joeynmt.training - \tReference:  هر کس اۆچۆن ائتدیگی امللره گؤره درهجهلر واردێر . ربین اونلارێن نه ائتدیکلریندن قافل دئییلدیر ! \n",
            "2023-01-20 01:48:34,209 - INFO - joeynmt.training - \tHypothesis: هر ایکیسی ده اونلاردان بیری اؤز درگاهێندان اولان ایکیسیندن باشقا بیر شئیه گؤره منی ربینه عاید ائدهجک .\n",
            "2023-01-20 01:48:34,209 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:48:34,211 - INFO - joeynmt.training - \tSource:     مادر عیسی در کنار شاگردی که عیسی دوستش می‌داشت ، ایستاده بود . وقتی عیسی آنان را دید به مادرش گفت : ای مادر ، او از این پس پسر توست ! \n",
            "2023-01-20 01:48:34,211 - INFO - joeynmt.training - \tReference:  ایسا آناسێنێ وه اونون یانیندا اؤز سئویملی شاگردینین دایاندیغینی گؤردۆکده آناسێنا دئدی : آنا ، بو سنین اوغلوندور ! \n",
            "2023-01-20 01:48:34,211 - INFO - joeynmt.training - \tHypothesis: ایسنانین شاگردی ایله بیرلیکده سئودیگی کیمی ایسا دایانیب دئدی : یا آناسێ ، آتان آناسێندان سونرا سنه دئدی : یا ، بو ، اوغلوندێر !\n",
            "2023-01-20 01:48:34,211 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:48:34,213 - INFO - joeynmt.training - \tSource:     و روز قیامت همه آنها تنها ، به سوی او خواهند آمد . \n",
            "2023-01-20 01:48:34,213 - INFO - joeynmt.training - \tReference:  اونلارێن هامێسێ قیامت گۆنۆ اونون هۆزورونا گلهجک . \n",
            "2023-01-20 01:48:34,213 - INFO - joeynmt.training - \tHypothesis: قیامت گۆنۆ اونلارێن هامیسینین آخێرێ آنجاق اونا طرف گلهجکلر !\n",
            "2023-01-20 01:48:42,086 - INFO - joeynmt.training - Epoch 446, Step:   154100, Batch Loss:     1.512282, Batch Acc: 0.622899, Tokens per Sec:    13334, Lr: 0.000023\n",
            "2023-01-20 01:48:46,857 - INFO - joeynmt.training - Epoch 446: total training loss 500.18\n",
            "2023-01-20 01:48:46,857 - INFO - joeynmt.training - EPOCH 447\n",
            "2023-01-20 01:48:50,214 - INFO - joeynmt.training - Epoch 447, Step:   154200, Batch Loss:     1.441637, Batch Acc: 0.626511, Tokens per Sec:    13208, Lr: 0.000023\n",
            "2023-01-20 01:48:58,138 - INFO - joeynmt.training - Epoch 447, Step:   154300, Batch Loss:     1.435509, Batch Acc: 0.627917, Tokens per Sec:    13899, Lr: 0.000023\n",
            "2023-01-20 01:49:08,303 - INFO - joeynmt.training - Epoch 447, Step:   154400, Batch Loss:     1.503698, Batch Acc: 0.624794, Tokens per Sec:    10940, Lr: 0.000023\n",
            "2023-01-20 01:49:17,010 - INFO - joeynmt.training - Epoch 447, Step:   154500, Batch Loss:     1.412038, Batch Acc: 0.625913, Tokens per Sec:    12651, Lr: 0.000023\n",
            "2023-01-20 01:49:17,364 - INFO - joeynmt.training - Epoch 447: total training loss 500.31\n",
            "2023-01-20 01:49:17,365 - INFO - joeynmt.training - EPOCH 448\n",
            "2023-01-20 01:49:24,979 - INFO - joeynmt.training - Epoch 448, Step:   154600, Batch Loss:     1.402889, Batch Acc: 0.628712, Tokens per Sec:    13731, Lr: 0.000023\n",
            "2023-01-20 01:49:32,917 - INFO - joeynmt.training - Epoch 448, Step:   154700, Batch Loss:     1.355558, Batch Acc: 0.627821, Tokens per Sec:    14046, Lr: 0.000023\n",
            "2023-01-20 01:49:40,787 - INFO - joeynmt.training - Epoch 448, Step:   154800, Batch Loss:     1.442886, Batch Acc: 0.625984, Tokens per Sec:    13941, Lr: 0.000023\n",
            "2023-01-20 01:49:44,753 - INFO - joeynmt.training - Epoch 448: total training loss 498.73\n",
            "2023-01-20 01:49:44,753 - INFO - joeynmt.training - EPOCH 449\n",
            "2023-01-20 01:49:48,848 - INFO - joeynmt.training - Epoch 449, Step:   154900, Batch Loss:     1.402050, Batch Acc: 0.629208, Tokens per Sec:    13704, Lr: 0.000023\n",
            "2023-01-20 01:49:56,999 - INFO - joeynmt.training - Epoch 449, Step:   155000, Batch Loss:     1.555919, Batch Acc: 0.625739, Tokens per Sec:    13538, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.14ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9120.62ex/s]\n",
            "2023-01-20 01:49:57,295 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=155000\n",
            "2023-01-20 01:49:57,296 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:50:01,640 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:50:01,640 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:50:01,640 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:50:01,641 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:50:01,644 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.28, loss:   3.01, ppl:  20.34, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3052[sec], evaluation: 0.0359[sec]\n",
            "2023-01-20 01:50:01,647 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:50:01,650 - INFO - joeynmt.training - \tSource:     كه بد جوش خورده بود و گوشت سرخ از لای شیارهای صورتش برق میزد \n",
            "2023-01-20 01:50:01,651 - INFO - joeynmt.training - \tReference:  کی واختیندا پیس بیتیشیب أتی نین یاریقی گؤزه ویریردی\n",
            "2023-01-20 01:50:01,651 - INFO - joeynmt.training - \tHypothesis: کی حاجینین آلتی و لری چاخیریندان کی آلتیندا اوزه سی یانیراغی لارین بیره سی گولدی\n",
            "2023-01-20 01:50:01,651 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:50:01,653 - INFO - joeynmt.training - \tSource:     و او را به عنوان‌ پیامبری به سوی بنی اسرائیل می‌فرستد ، که او به آنان می‌گوید : در حقیقت ، من از جانب پروردگارتان برایتان معجزه‌ای آورده‌ام : من از گل برای شما چیزی‌ به شکل پرنده می‌سازم ، آنگاه در آن می‌دمم ، پس به اذن خدا پرنده‌ای می شود ؛ و به اذن خدا نابینای مادرزاد و پیس را بهبود می‌بخشم ؛ و مردگان را زنده می‌گردانم ؛ و شما را از آنچه می‌خورید و در خانه هایتان ذخیره می‌کنید ، خبر می‌دهم ؛ مسلما در این معجزات‌ ، برای شما اگر مؤمن باشید عبرت است . \n",
            "2023-01-20 01:50:01,654 - INFO - joeynmt.training - \tReference:  وه اونو اسراعل اؤؤلادێنا پیغمبر گؤندهرهجک . من ، حقیقتا ، ربینیزدن سیزه معؤ جۆزه گتیرمیشم . سیزین اۆچۆن پالچێقدان قوشا بنزر بیر سورت دۆزلهدیب اونا اۆفۆرهم ، او دا آللاهێن ایزنیله قوش اولار . آنادانگلمه کورلارێ ، جۆجام خستهلیگینه توتولانلارێ ساغالدار وه آللاهێن ایزنیله اؤلولهری دیریلدرم . من ائولرینیزده یئدیگینیز وه یێغیب ساخلادێغێنێز شئیلری ده سیزه خبر وئرهرم . اگر معؤ مینسینیزسه ، بوندا بیر دلیل واردێر . \n",
            "2023-01-20 01:50:01,654 - INFO - joeynmt.training - \tHypothesis: اونو وه اسراعل اوغوللارێنا بیر پیغمبر گؤندریب بئله دئییر : ائی اسراعل اوغوللارێ ! من ده سیزین اۆچۆن ربینیزدن بیر شئی گتیرمیشم . من ده سیزین اۆچۆن بیر شئی مجلسم ، بیر شئیه یئتیشرمهسه ، اونو اؤلدورولن وه سیزه بیر پیسلیک ائدن بیر شئی وئررم .\n",
            "2023-01-20 01:50:01,654 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:50:01,656 - INFO - joeynmt.training - \tSource:     و دلهایشان را استوار گردانیدیم آنگاه که به قصد مخالفت با شرک‌ برخاستند و گفتند : پروردگار ما پروردگار آسمانها و زمین است . جز او هرگز معبودی را نخواهیم خواند ، که در این صورت قطعا ناصواب گفته‌ایم . \n",
            "2023-01-20 01:50:01,656 - INFO - joeynmt.training - \tReference:  اونلار دوروب : ربیمیز گؤیلرین وه یئرین ربیدیر . بیز اوندان باشقا هئچ بیر تانرێیا ابادت ائتمهیهجهییک . اکس تقدیرده ، دانێشماقدا هدی آشمێش اولارێق ! دئدیکلری زامان اونلارێن اۆرکلرینه غۆۆت وئرمیشدیک . \n",
            "2023-01-20 01:50:01,657 - INFO - joeynmt.training - \tHypothesis: بیز اونلارێن قلبلرینی مۆحکملندیردیک . اونلار : ائی ربیمیز ! بیزه شریک قوشدولار . بیز ، گؤیلری وه یئرین ربی اولان آللاها ابادت ائتمهیک ! دئدیلر . بیز هئچ بیر تانرێ یوخدور . حقیقتا ، سؤزسۆز کی ، او ، کۆفرلرده هئچ بیر تانرێ یوخدور !\n",
            "2023-01-20 01:50:01,657 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:50:01,659 - INFO - joeynmt.training - \tSource:     میان من و آنان فیصله ده ، و من و هر کس از مؤمنان را که با من است نجات بخش . \n",
            "2023-01-20 01:50:01,659 - INFO - joeynmt.training - \tReference:  آرتێق منیمله اونلار آراسێندا سن هؤکم وئر ، منی وه منیمله بیرلیکده اولان معؤ مینلری قورتار ! \n",
            "2023-01-20 01:50:01,659 - INFO - joeynmt.training - \tHypothesis: اونلارێن آراسێندا وه فێرێمدان ، منیم معؤ مینلردن وه معؤ مینلردن اولان کیمسهیه تؤسیه ائدن اودور !\n",
            "2023-01-20 01:50:09,720 - INFO - joeynmt.training - Epoch 449, Step:   155100, Batch Loss:     1.493553, Batch Acc: 0.624074, Tokens per Sec:    13123, Lr: 0.000023\n",
            "2023-01-20 01:50:17,252 - INFO - joeynmt.training - Epoch 449: total training loss 500.51\n",
            "2023-01-20 01:50:17,253 - INFO - joeynmt.training - EPOCH 450\n",
            "2023-01-20 01:50:17,742 - INFO - joeynmt.training - Epoch 450, Step:   155200, Batch Loss:     1.371195, Batch Acc: 0.642364, Tokens per Sec:    13640, Lr: 0.000023\n",
            "2023-01-20 01:50:25,609 - INFO - joeynmt.training - Epoch 450, Step:   155300, Batch Loss:     1.405924, Batch Acc: 0.629114, Tokens per Sec:    14042, Lr: 0.000023\n",
            "2023-01-20 01:50:33,658 - INFO - joeynmt.training - Epoch 450, Step:   155400, Batch Loss:     1.451953, Batch Acc: 0.627368, Tokens per Sec:    13640, Lr: 0.000023\n",
            "2023-01-20 01:50:41,540 - INFO - joeynmt.training - Epoch 450, Step:   155500, Batch Loss:     1.478859, Batch Acc: 0.627405, Tokens per Sec:    13940, Lr: 0.000023\n",
            "2023-01-20 01:50:44,728 - INFO - joeynmt.training - Epoch 450: total training loss 500.10\n",
            "2023-01-20 01:50:44,729 - INFO - joeynmt.training - EPOCH 451\n",
            "2023-01-20 01:50:49,536 - INFO - joeynmt.training - Epoch 451, Step:   155600, Batch Loss:     1.512152, Batch Acc: 0.627420, Tokens per Sec:    13734, Lr: 0.000023\n",
            "2023-01-20 01:50:57,541 - INFO - joeynmt.training - Epoch 451, Step:   155700, Batch Loss:     1.580461, Batch Acc: 0.627144, Tokens per Sec:    13901, Lr: 0.000023\n",
            "2023-01-20 01:51:05,452 - INFO - joeynmt.training - Epoch 451, Step:   155800, Batch Loss:     1.474091, Batch Acc: 0.627785, Tokens per Sec:    13821, Lr: 0.000023\n",
            "2023-01-20 01:51:12,243 - INFO - joeynmt.training - Epoch 451: total training loss 500.72\n",
            "2023-01-20 01:51:12,244 - INFO - joeynmt.training - EPOCH 452\n",
            "2023-01-20 01:51:13,358 - INFO - joeynmt.training - Epoch 452, Step:   155900, Batch Loss:     1.360719, Batch Acc: 0.633744, Tokens per Sec:    13495, Lr: 0.000023\n",
            "2023-01-20 01:51:21,212 - INFO - joeynmt.training - Epoch 452, Step:   156000, Batch Loss:     1.390397, Batch Acc: 0.630373, Tokens per Sec:    13993, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 128.98ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8704.23ex/s]\n",
            "2023-01-20 01:51:21,509 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=156000\n",
            "2023-01-20 01:51:21,509 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:51:25,931 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:51:25,932 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:51:25,932 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:51:25,933 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:51:25,936 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.33, loss:   2.95, ppl:  19.05, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3839[sec], evaluation: 0.0355[sec]\n",
            "2023-01-20 01:51:25,938 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:51:25,941 - INFO - joeynmt.training - \tSource:     کسانی که کتاب خدا و آنچه را که فرستادگان خود را بدان گسیل داشته‌ایم تکذیب کرده‌اند ، به زودی خواهند دانست ؛ \n",
            "2023-01-20 01:51:25,942 - INFO - joeynmt.training - \tReference:  کیتابی وه پیغمبرلریمزی گؤندردیکلریمیزی یالان حساب ائدنلر مۆتلق بیلهجکلر ! \n",
            "2023-01-20 01:51:25,942 - INFO - joeynmt.training - \tHypothesis: کیتابدا اولان پیغمبرلری تکذیب ائدنلری تکذیب ائدنلری ده بیله بیله بیله بیله بیلهجکلر .\n",
            "2023-01-20 01:51:25,942 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:51:25,944 - INFO - joeynmt.training - \tSource:     او بارها از عیسی استدعا کرد که آن‌ها را از آن ناحیه بیرون نکند . \n",
            "2023-01-20 01:51:25,944 - INFO - joeynmt.training - \tReference:  روحلارێ بو دیاردان قووماسێن دئیه او ، ایسهآیا چوخ یالواردی . \n",
            "2023-01-20 01:51:25,944 - INFO - joeynmt.training - \tHypothesis: ایسا اوندان خبر آلماییب کی ، بوندان کؤهنهیه دۆشمهسین .\n",
            "2023-01-20 01:51:25,944 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:51:25,946 - INFO - joeynmt.training - \tSource:     در چنان روزی پروردگارشان به حال‌ ایشان نیک آگاه است ؟ \n",
            "2023-01-20 01:51:25,947 - INFO - joeynmt.training - \tReference:  همین گۆن ربی اونلارێ خبردار ائدهجکدیر ! \n",
            "2023-01-20 01:51:25,947 - INFO - joeynmt.training - \tHypothesis: او گۆن اونلار ربینین هۆزوروندا دا بیلیرلر .\n",
            "2023-01-20 01:51:25,947 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:51:25,949 - INFO - joeynmt.training - \tSource:     بارها او را با زنجیر و پابندهای آهنی بسته بودند ، اما هر بار زنجیرها را پاره می‌کرد و پابندها را می‌شکست و هیچ کس قدرت مهار کردن او را نداشت . \n",
            "2023-01-20 01:51:25,949 - INFO - joeynmt.training - \tReference:  نئچه دفه اونا بوخوو ، زنجیر وورموشدولار ، لاکین او ، زنجیرلری قێرمێش ، بوخوولارێ پارچالامێشدێ . هئچ کسین اونو ساکت ائتمهیه گۆجۆ چاتمێردێ . \n",
            "2023-01-20 01:51:25,949 - INFO - joeynmt.training - \tHypothesis: اونو زنجیرلر باغلاییب قانامێ باغلاییب اونو بیر باتێرا آتێردێلار . هر کس اونو آنایا آتدێ وه هئچ کس اونو کسه شۆکۆر ائده بیلر .\n",
            "2023-01-20 01:51:33,959 - INFO - joeynmt.training - Epoch 452, Step:   156100, Batch Loss:     1.477758, Batch Acc: 0.628406, Tokens per Sec:    13234, Lr: 0.000023\n",
            "2023-01-20 01:51:41,754 - INFO - joeynmt.training - Epoch 452, Step:   156200, Batch Loss:     1.461973, Batch Acc: 0.625872, Tokens per Sec:    14201, Lr: 0.000023\n",
            "2023-01-20 01:51:44,281 - INFO - joeynmt.training - Epoch 452: total training loss 498.96\n",
            "2023-01-20 01:51:44,281 - INFO - joeynmt.training - EPOCH 453\n",
            "2023-01-20 01:51:49,721 - INFO - joeynmt.training - Epoch 453, Step:   156300, Batch Loss:     1.493779, Batch Acc: 0.627467, Tokens per Sec:    13943, Lr: 0.000023\n",
            "2023-01-20 01:51:57,724 - INFO - joeynmt.training - Epoch 453, Step:   156400, Batch Loss:     1.403023, Batch Acc: 0.630260, Tokens per Sec:    13580, Lr: 0.000023\n",
            "2023-01-20 01:52:05,701 - INFO - joeynmt.training - Epoch 453, Step:   156500, Batch Loss:     1.505057, Batch Acc: 0.626310, Tokens per Sec:    13899, Lr: 0.000023\n",
            "2023-01-20 01:52:11,773 - INFO - joeynmt.training - Epoch 453: total training loss 499.30\n",
            "2023-01-20 01:52:11,773 - INFO - joeynmt.training - EPOCH 454\n",
            "2023-01-20 01:52:13,636 - INFO - joeynmt.training - Epoch 454, Step:   156600, Batch Loss:     1.444911, Batch Acc: 0.628317, Tokens per Sec:    14187, Lr: 0.000023\n",
            "2023-01-20 01:52:21,553 - INFO - joeynmt.training - Epoch 454, Step:   156700, Batch Loss:     1.393654, Batch Acc: 0.629379, Tokens per Sec:    14111, Lr: 0.000023\n",
            "2023-01-20 01:52:29,496 - INFO - joeynmt.training - Epoch 454, Step:   156800, Batch Loss:     1.435658, Batch Acc: 0.631239, Tokens per Sec:    13644, Lr: 0.000023\n",
            "2023-01-20 01:52:37,419 - INFO - joeynmt.training - Epoch 454, Step:   156900, Batch Loss:     1.430946, Batch Acc: 0.626099, Tokens per Sec:    13776, Lr: 0.000023\n",
            "2023-01-20 01:52:39,185 - INFO - joeynmt.training - Epoch 454: total training loss 498.40\n",
            "2023-01-20 01:52:39,185 - INFO - joeynmt.training - EPOCH 455\n",
            "2023-01-20 01:52:45,349 - INFO - joeynmt.training - Epoch 455, Step:   157000, Batch Loss:     1.583879, Batch Acc: 0.629317, Tokens per Sec:    13889, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 128.48ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8482.01ex/s]\n",
            "2023-01-20 01:52:45,659 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=157000\n",
            "2023-01-20 01:52:45,659 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:52:52,793 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:52:52,793 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:52:52,793 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:52:52,795 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:52:52,803 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   6.56, loss:   2.91, ppl:  18.37, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 7.0634[sec], evaluation: 0.0677[sec]\n",
            "2023-01-20 01:52:52,807 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:52:52,811 - INFO - joeynmt.training - \tSource:     و بیشتر مردم هر چند آرزومند باشی ایمان‌آورنده نیستند . \n",
            "2023-01-20 01:52:52,811 - INFO - joeynmt.training - \tReference:  سن تشنه اولسان دا ، اینسانلارین اکسریتیتی ایمان گتیرن دئییلدیر ! \n",
            "2023-01-20 01:52:52,811 - INFO - joeynmt.training - \tHypothesis: اونلار چوخ هر هانسێ بیر نئچه آنلامایانلار داها ایمان گتیرمزلر .\n",
            "2023-01-20 01:52:52,811 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:52:52,814 - INFO - joeynmt.training - \tSource:     ای ابراهیم ! از این چون و چرا روی برتاب ، که فرمان پروردگارت آمده و برای آنان عذابی که بی‌بازگشت است خواهد آمد . \n",
            "2023-01-20 01:52:52,815 - INFO - joeynmt.training - \tReference:   ائی ابراهیم ! بوندان ال چک ، ربینین امری آرتێق گلمیشدیر . اونلارا مۆتلق قارشیسیآلینماز بیر اعذاب گلهجکدیر . \n",
            "2023-01-20 01:52:52,815 - INFO - joeynmt.training - \tHypothesis: ائی ابراهیم ! بو نه اۆچۆن درگاهێندا اۆز دؤندر ! سنین امریندن گلیب چاتدێقدا اونلارا بیر اعذاب گلمیشدیر !\n",
            "2023-01-20 01:52:52,815 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:52:52,818 - INFO - joeynmt.training - \tSource:      ای برادران ، من می‌توانم آزادانه در مورد جدمان ، داوود با شما صحبت کنم که درگذشت ، دفن شد و مقبره‌اش تا امروز در میان ماست ؛ \n",
            "2023-01-20 01:52:52,818 - INFO - joeynmt.training - \tReference:  قارداشلار ، سیزه جصارتله دئمهلییم کی ، دوغرودان دا ، اولو بابامێز داوود اؤلوب وه دفن اولونوب . اونون قبری بو گۆنه قدر بورادادێر . \n",
            "2023-01-20 01:52:52,818 - INFO - joeynmt.training - \tHypothesis: ائی قارداشلار ، من آزادلێغێنێزێملا بارماق اۆچۆن داوودون بارێشدێرێما یولونو آزێب بو گۆنه قدر آغلادێم .\n",
            "2023-01-20 01:52:52,818 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:52:52,821 - INFO - joeynmt.training - \tSource:     زیرا نوشته شده است ، او در مورد تو به فرشتگان خود فرمان خواهد داد تا تو را حفظ کنند \n",
            "2023-01-20 01:52:52,821 - INFO - joeynmt.training - \tReference:  آخێ آللاه سنه گؤره ملکلرنه امر ائدر کی ، سنی قوروسونلار ، \n",
            "2023-01-20 01:52:52,821 - INFO - joeynmt.training - \tHypothesis: چۆنکی اونون ملکلرین امرینه اساسلاناجاق کی ، سنی امر ائدهجک .\n",
            "2023-01-20 01:53:01,048 - INFO - joeynmt.training - Epoch 455, Step:   157100, Batch Loss:     1.560083, Batch Acc: 0.629427, Tokens per Sec:    12746, Lr: 0.000023\n",
            "2023-01-20 01:53:08,950 - INFO - joeynmt.training - Epoch 455, Step:   157200, Batch Loss:     1.451734, Batch Acc: 0.628968, Tokens per Sec:    13920, Lr: 0.000023\n",
            "2023-01-20 01:53:14,393 - INFO - joeynmt.training - Epoch 455: total training loss 499.03\n",
            "2023-01-20 01:53:14,393 - INFO - joeynmt.training - EPOCH 456\n",
            "2023-01-20 01:53:16,912 - INFO - joeynmt.training - Epoch 456, Step:   157300, Batch Loss:     1.576674, Batch Acc: 0.628837, Tokens per Sec:    13815, Lr: 0.000023\n",
            "2023-01-20 01:53:24,839 - INFO - joeynmt.training - Epoch 456, Step:   157400, Batch Loss:     1.328985, Batch Acc: 0.628998, Tokens per Sec:    13980, Lr: 0.000023\n",
            "2023-01-20 01:53:32,684 - INFO - joeynmt.training - Epoch 456, Step:   157500, Batch Loss:     1.453122, Batch Acc: 0.628007, Tokens per Sec:    14027, Lr: 0.000023\n",
            "2023-01-20 01:53:40,437 - INFO - joeynmt.training - Epoch 456, Step:   157600, Batch Loss:     1.430133, Batch Acc: 0.626586, Tokens per Sec:    14058, Lr: 0.000023\n",
            "2023-01-20 01:53:41,548 - INFO - joeynmt.training - Epoch 456: total training loss 499.04\n",
            "2023-01-20 01:53:41,548 - INFO - joeynmt.training - EPOCH 457\n",
            "2023-01-20 01:53:48,305 - INFO - joeynmt.training - Epoch 457, Step:   157700, Batch Loss:     1.277854, Batch Acc: 0.629237, Tokens per Sec:    13999, Lr: 0.000023\n",
            "2023-01-20 01:53:56,308 - INFO - joeynmt.training - Epoch 457, Step:   157800, Batch Loss:     1.514798, Batch Acc: 0.627495, Tokens per Sec:    13689, Lr: 0.000023\n",
            "2023-01-20 01:54:04,130 - INFO - joeynmt.training - Epoch 457, Step:   157900, Batch Loss:     1.531455, Batch Acc: 0.627896, Tokens per Sec:    14049, Lr: 0.000023\n",
            "2023-01-20 01:54:08,753 - INFO - joeynmt.training - Epoch 457: total training loss 497.82\n",
            "2023-01-20 01:54:08,754 - INFO - joeynmt.training - EPOCH 458\n",
            "2023-01-20 01:54:11,964 - INFO - joeynmt.training - Epoch 458, Step:   158000, Batch Loss:     1.422076, Batch Acc: 0.632589, Tokens per Sec:    14288, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.36ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10443.29ex/s]\n",
            "2023-01-20 01:54:12,251 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=158000\n",
            "2023-01-20 01:54:12,251 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:54:16,384 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:54:16,384 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:54:16,384 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:54:16,385 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:54:16,388 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.36, loss:   2.99, ppl:  19.91, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.0960[sec], evaluation: 0.0343[sec]\n",
            "2023-01-20 01:54:16,391 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:54:16,394 - INFO - joeynmt.training - \tSource:     و قطعا برای او در پیشگاه ما تقرب و فرجام نیکوست . \n",
            "2023-01-20 01:54:16,394 - INFO - joeynmt.training - \tReference:  حقیقتا ، او ، درگاهێمێزا یاخین اولاجاق وه اونون قاییدیب گلهجهگی یئر ده گؤزل اولاجاقدێر . \n",
            "2023-01-20 01:54:16,395 - INFO - joeynmt.training - \tHypothesis: شبههسیز کی ، او ، بیزیم اۆچۆن بیر یئرده اولان آییرد ائتمهسی !\n",
            "2023-01-20 01:54:16,395 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:54:16,397 - INFO - joeynmt.training - \tSource:     بلکه اینان و پدرانشان را برخوداری دادم تا حقیقت و فرستاده‌ای آشکار به سویشان آمد . \n",
            "2023-01-20 01:54:16,397 - INFO - joeynmt.training - \tReference:  خئیر ، من اونلارا دا ، آتالارێنا دا اؤزلرینه حاقق وه بیان ائدن بیر پیغمبر گلهنهدک گۆن گۆزران وئردیم ! \n",
            "2023-01-20 01:54:16,397 - INFO - joeynmt.training - \tHypothesis: خئیر ، اونلارێن آتالارێمێزا دا گؤندریب آچێق آشکار بیر پیغمبر گلمیشدی .\n",
            "2023-01-20 01:54:16,397 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:54:16,399 - INFO - joeynmt.training - \tSource:     بعد فكر كردم احتمالا جنازه را فقط مادرم دیده و این نمی بایست زیاد مهم باشد . \n",
            "2023-01-20 01:54:16,399 - INFO - joeynmt.training - \tReference:  سونرا فیکریمه گلدی کی جنازه می تکجه آنام گؤرور سه چوخدا اؤنمی یوخ دی . \n",
            "2023-01-20 01:54:16,400 - INFO - joeynmt.training - \tHypothesis: سونرا کاکارستم بو آنایاغی چکه رک آنایارام .\n",
            "2023-01-20 01:54:16,400 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:54:16,402 - INFO - joeynmt.training - \tSource:     و چون بر ایشان فرو خوانده می‌شود ، می‌گویند : بدان ایمان آوردیم که آن درست است و از طرف پروردگار ماست ؛ ما پیش از آن هم‌ از تسلیم‌شوندگان بودیم . \n",
            "2023-01-20 01:54:16,402 - INFO - joeynmt.training - \tReference:  اونلارا اوخوندوغو زامان : بیز اونا ایناندیق . دوغرودان دا ، ربیمیزدن حاقدێر . بیز اوندان اول ده مۆسلمان ایدیک ! دئییرلر . \n",
            "2023-01-20 01:54:16,402 - INFO - joeynmt.training - \tHypothesis: اونلارا اوخوندوغو زامان : بیز اونا ایمان گتیردیک ، دئیه جاواب وئرمیشدیک . اوندان اوهلکی ربیندن اول گؤندهریلنلر . بیز ده اوندان اول ربیمیزدن اول گؤندهریلمیشیک ! دئییرلر .\n",
            "2023-01-20 01:54:24,211 - INFO - joeynmt.training - Epoch 458, Step:   158100, Batch Loss:     1.472584, Batch Acc: 0.633372, Tokens per Sec:    13388, Lr: 0.000022\n",
            "2023-01-20 01:54:32,023 - INFO - joeynmt.training - Epoch 458, Step:   158200, Batch Loss:     1.434548, Batch Acc: 0.629015, Tokens per Sec:    14118, Lr: 0.000022\n",
            "2023-01-20 01:54:39,830 - INFO - joeynmt.training - Epoch 458, Step:   158300, Batch Loss:     1.434959, Batch Acc: 0.625162, Tokens per Sec:    14167, Lr: 0.000022\n",
            "2023-01-20 01:54:40,184 - INFO - joeynmt.training - Epoch 458: total training loss 497.55\n",
            "2023-01-20 01:54:40,184 - INFO - joeynmt.training - EPOCH 459\n",
            "2023-01-20 01:54:47,643 - INFO - joeynmt.training - Epoch 459, Step:   158400, Batch Loss:     1.544060, Batch Acc: 0.628811, Tokens per Sec:    14214, Lr: 0.000022\n",
            "2023-01-20 01:54:55,518 - INFO - joeynmt.training - Epoch 459, Step:   158500, Batch Loss:     1.446494, Batch Acc: 0.628217, Tokens per Sec:    13936, Lr: 0.000022\n",
            "2023-01-20 01:55:03,308 - INFO - joeynmt.training - Epoch 459, Step:   158600, Batch Loss:     1.467225, Batch Acc: 0.625489, Tokens per Sec:    14019, Lr: 0.000022\n",
            "2023-01-20 01:55:07,279 - INFO - joeynmt.training - Epoch 459: total training loss 498.09\n",
            "2023-01-20 01:55:07,279 - INFO - joeynmt.training - EPOCH 460\n",
            "2023-01-20 01:55:11,219 - INFO - joeynmt.training - Epoch 460, Step:   158700, Batch Loss:     1.490488, Batch Acc: 0.625057, Tokens per Sec:    13949, Lr: 0.000022\n",
            "2023-01-20 01:55:19,079 - INFO - joeynmt.training - Epoch 460, Step:   158800, Batch Loss:     1.441265, Batch Acc: 0.629482, Tokens per Sec:    14023, Lr: 0.000022\n",
            "2023-01-20 01:55:26,963 - INFO - joeynmt.training - Epoch 460, Step:   158900, Batch Loss:     1.485314, Batch Acc: 0.629883, Tokens per Sec:    13885, Lr: 0.000022\n",
            "2023-01-20 01:55:34,636 - INFO - joeynmt.training - Epoch 460: total training loss 498.47\n",
            "2023-01-20 01:55:34,637 - INFO - joeynmt.training - EPOCH 461\n",
            "2023-01-20 01:55:34,987 - INFO - joeynmt.training - Epoch 461, Step:   159000, Batch Loss:     1.328180, Batch Acc: 0.625526, Tokens per Sec:    12215, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 117.19ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10448.49ex/s]\n",
            "2023-01-20 01:55:35,260 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=159000\n",
            "2023-01-20 01:55:35,260 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:55:40,064 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:55:40,065 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:55:40,065 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:55:40,066 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:55:40,069 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.12, loss:   2.92, ppl:  18.46, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7660[sec], evaluation: 0.0357[sec]\n",
            "2023-01-20 01:55:40,071 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:55:40,075 - INFO - joeynmt.training - \tSource:     به علاوه ، دنیا و خواسته‌های آن گذراست ، اما کسی که خواست خدا را به جا می‌آورد ، تا ابد باقی خواهد ماند . \n",
            "2023-01-20 01:55:40,075 - INFO - joeynmt.training - \tReference:  دۆنیا وه اونداکێ احترآسلار کئچیب گئدیر ، آمما آللاهێن ارادهسینه امل ائدن ابدی قالێر . \n",
            "2023-01-20 01:55:40,076 - INFO - joeynmt.training - \tHypothesis: دۆنیا ایستهینلر دۆنیانین ایستهدی . لاکین او ، ارادهسینی یئرینه یئتیرمک اۆچۆن داها ابدی قالاجاق .\n",
            "2023-01-20 01:55:40,076 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:55:40,078 - INFO - joeynmt.training - \tSource:     اما آن زن پیش عیسی آمد ، در مقابل او به زانو افتاد و گفت : سرورم ، به من کمک کن ! \n",
            "2023-01-20 01:55:40,078 - INFO - joeynmt.training - \tReference:  قادێن ایسه یاخینلاشدی وه اونا سجده قێلێب دئدی : یا رب ، منه امداد ائت . \n",
            "2023-01-20 01:55:40,078 - INFO - joeynmt.training - \tHypothesis: قادێن ایسنانین یانینا گلدی وه قادێنا دئدی : یا رب ، منه کؤمک ائت ، منه کؤمک ائت !\n",
            "2023-01-20 01:55:40,078 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:55:40,080 - INFO - joeynmt.training - \tSource:     مردمی بسیار نزد او آمدند و گفتند : یحیی یک معجزه هم به ظهور نرساند ، اما هر چه در مورد این مرد گفت ، درست بود . \n",
            "2023-01-20 01:55:40,080 - INFO - joeynmt.training - \tReference:  چوخلو آدام اونون یانینا گلیب دئدی : یهیا هئچ بیر علامت گؤسترمهسه ده ، اونون بو آدام بارهده دئدیگی بۆتۆن سؤزلر دوغرو چێخدێ . \n",
            "2023-01-20 01:55:40,081 - INFO - joeynmt.training - \tHypothesis: خالق اونون یانینا گلیب دئدیلر : بیر آدام دا اویغون گؤستریب هامێ ایله گلدیگینی بیلمیردی . بو آدام نه دئدیگینی نه نه قدر یاخشی ایدی .\n",
            "2023-01-20 01:55:40,081 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:55:40,083 - INFO - joeynmt.training - \tSource:     آنگاه آن مردان با صدای بلند فریاد کشیدند و دست‌های خود را روی گوش‌هایشان گذاشتند و همگی با هم به طرف او هجوم آوردند . \n",
            "2023-01-20 01:55:40,083 - INFO - joeynmt.training - \tReference:  اونلار قولاقلارێنێ توتوب هارای قوپاردێلار وه هامێسێ بیرلیکده استئفانێن اۆستۆنه حۆجوم چکدیلر . \n",
            "2023-01-20 01:55:40,083 - INFO - joeynmt.training - \tHypothesis: او زامان اونلار اوجا سسله ندا ائدهرک اؤز پالتارلارێنێن اۆستۆنه قوپدولار وه هامێسێ اونا طرف گلیردی .\n",
            "2023-01-20 01:55:47,863 - INFO - joeynmt.training - Epoch 461, Step:   159100, Batch Loss:     1.443746, Batch Acc: 0.631724, Tokens per Sec:    13602, Lr: 0.000022\n",
            "2023-01-20 01:55:55,794 - INFO - joeynmt.training - Epoch 461, Step:   159200, Batch Loss:     1.396437, Batch Acc: 0.625203, Tokens per Sec:    13722, Lr: 0.000022\n",
            "2023-01-20 01:56:03,592 - INFO - joeynmt.training - Epoch 461, Step:   159300, Batch Loss:     1.317430, Batch Acc: 0.629180, Tokens per Sec:    14267, Lr: 0.000022\n",
            "2023-01-20 01:56:06,891 - INFO - joeynmt.training - Epoch 461: total training loss 498.09\n",
            "2023-01-20 01:56:06,891 - INFO - joeynmt.training - EPOCH 462\n",
            "2023-01-20 01:56:11,466 - INFO - joeynmt.training - Epoch 462, Step:   159400, Batch Loss:     1.418008, Batch Acc: 0.629453, Tokens per Sec:    13894, Lr: 0.000022\n",
            "2023-01-20 01:56:19,210 - INFO - joeynmt.training - Epoch 462, Step:   159500, Batch Loss:     1.409575, Batch Acc: 0.630508, Tokens per Sec:    14155, Lr: 0.000022\n",
            "2023-01-20 01:56:27,025 - INFO - joeynmt.training - Epoch 462, Step:   159600, Batch Loss:     1.349990, Batch Acc: 0.629097, Tokens per Sec:    14175, Lr: 0.000022\n",
            "2023-01-20 01:56:35,299 - INFO - joeynmt.training - Epoch 462: total training loss 497.58\n",
            "2023-01-20 01:56:35,303 - INFO - joeynmt.training - EPOCH 463\n",
            "2023-01-20 01:56:36,852 - INFO - joeynmt.training - Epoch 463, Step:   159700, Batch Loss:     1.347004, Batch Acc: 0.644194, Tokens per Sec:     8524, Lr: 0.000022\n",
            "2023-01-20 01:56:45,504 - INFO - joeynmt.training - Epoch 463, Step:   159800, Batch Loss:     1.398352, Batch Acc: 0.631314, Tokens per Sec:    12770, Lr: 0.000022\n",
            "2023-01-20 01:56:53,337 - INFO - joeynmt.training - Epoch 463, Step:   159900, Batch Loss:     1.349288, Batch Acc: 0.629447, Tokens per Sec:    13992, Lr: 0.000022\n",
            "2023-01-20 01:57:01,294 - INFO - joeynmt.training - Epoch 463, Step:   160000, Batch Loss:     1.308985, Batch Acc: 0.628208, Tokens per Sec:    13959, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 136.93ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9642.06ex/s]\n",
            "2023-01-20 01:57:01,582 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=160000\n",
            "2023-01-20 01:57:01,582 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:57:06,685 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:57:06,685 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:57:06,685 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:57:06,686 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:57:06,689 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.00, loss:   2.87, ppl:  17.60, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9230[sec], evaluation: 0.1768[sec]\n",
            "2023-01-20 01:57:06,692 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:57:06,697 - INFO - joeynmt.training - \tSource:     کسی که پدر یا مادر خود را بیش از من دوست داشته باشد ، لایق من نیست و کسی که پسر یا دختر خود را بیش از من دوست داشته باشد ، لایق من نیست . \n",
            "2023-01-20 01:57:06,697 - INFO - joeynmt.training - \tReference:  آتاسێنێ یا آناسێنێ مندن آرتێق سئون کس منه لایق دئییل . اوغلونو یا قێزێنێ مندن آرتێق سئون ده منه لایق دئییل . \n",
            "2023-01-20 01:57:06,697 - INFO - joeynmt.training - \tHypothesis: آتا آناسێنا یاخود منیم سئویملی اوغلومدور ، مندن وه قێزێمادان ، قێزێمێن اؤزوندن وه یا سئودیگیم کیمی سئودیگیم کیمی مندن داها لایق دئییل .\n",
            "2023-01-20 01:57:06,697 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:57:06,700 - INFO - joeynmt.training - \tSource:     پس از این واقعه ، بیماران دیگر آن جزیره نیز نزد او می‌آمدند و شفا می‌یافتند . \n",
            "2023-01-20 01:57:06,700 - INFO - joeynmt.training - \tReference:  بو حادسا سونرا آداداقێ قالان خستهلر ده گلیب شفا تاپدێ . \n",
            "2023-01-20 01:57:06,701 - INFO - joeynmt.training - \tHypothesis: بوندان سونرا لازار خستهلری اونون یانینا گلیردی وه شفا تاپدێلار .\n",
            "2023-01-20 01:57:06,701 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:57:06,703 - INFO - joeynmt.training - \tSource:     بگو : خدا به آنچه درنگ کردند داناتر است . نهان آسمانها و زمین به او اختصاص دارد . وه ! چه بینا و شنواست . برای آنان یاوری جز او نیست و هیچ کس را در فرمانروایی خود شریک نمی‌گیرد . \n",
            "2023-01-20 01:57:06,703 - INFO - joeynmt.training - \tReference:   دئ : اونلارێن نه قدر قالدیقلارینی آللاه داها یاخشی بیلیر . گؤیلرین وه یئرین غیبینی بیلمک آنجاق اونا مخصوصدور . او ، هر شئیی نئجه گؤزل گؤرۆر ، نئجه ده یاخشی ائشیدیر ! اونلارێن آللاهدان باشقا هئچ بیر حامثی یوخدور . او ، هئچ کسی اؤز هؤکمونه شریک ائتمز ! \n",
            "2023-01-20 01:57:06,704 - INFO - joeynmt.training - \tHypothesis: دئ : آللاه گؤیلرده وه یئرده نه وارسا ، نه گؤیلری ، نه ده آللاهێ تقدیس ائدیب . او ، ائشیدندیر ، هئچ کسه هئچ بیر کؤمکچی یوخدور . او ، هئچ کسه هئچ بیر کؤمک ائده بیلمز !\n",
            "2023-01-20 01:57:06,704 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:57:06,706 - INFO - joeynmt.training - \tSource:      چرا شاگردان تو سنت گذشتگان را زیر پا می‌گذارند ؟ مثلا ، پیش از خوردن غذا دست‌هایشان را آب نمی‌کشند ! \n",
            "2023-01-20 01:57:06,707 - INFO - joeynmt.training - \tReference:   نه اۆچۆن شاگردلرین آغساققاللاردان قالان آدت انهنهنی پوزور وه چؤرک یئدیکلری زامان اللهرینی یومورلار ؟ \n",
            "2023-01-20 01:57:06,707 - INFO - joeynmt.training - \tHypothesis: بس اوندا سندن اوللر ده پیس یولا یوردون ؟ اونلار اؤزلری ایله یئمهیه اجازهلری یوخدور .\n",
            "2023-01-20 01:57:09,315 - INFO - joeynmt.training - Epoch 463: total training loss 494.33\n",
            "2023-01-20 01:57:09,315 - INFO - joeynmt.training - EPOCH 464\n",
            "2023-01-20 01:57:14,687 - INFO - joeynmt.training - Epoch 464, Step:   160100, Batch Loss:     1.358733, Batch Acc: 0.629401, Tokens per Sec:    13696, Lr: 0.000022\n",
            "2023-01-20 01:57:22,540 - INFO - joeynmt.training - Epoch 464, Step:   160200, Batch Loss:     1.466048, Batch Acc: 0.628926, Tokens per Sec:    14014, Lr: 0.000022\n",
            "2023-01-20 01:57:30,433 - INFO - joeynmt.training - Epoch 464, Step:   160300, Batch Loss:     1.450438, Batch Acc: 0.628233, Tokens per Sec:    13936, Lr: 0.000022\n",
            "2023-01-20 01:57:36,579 - INFO - joeynmt.training - Epoch 464: total training loss 497.30\n",
            "2023-01-20 01:57:36,579 - INFO - joeynmt.training - EPOCH 465\n",
            "2023-01-20 01:57:38,305 - INFO - joeynmt.training - Epoch 465, Step:   160400, Batch Loss:     1.493665, Batch Acc: 0.629932, Tokens per Sec:    13909, Lr: 0.000022\n",
            "2023-01-20 01:57:46,084 - INFO - joeynmt.training - Epoch 465, Step:   160500, Batch Loss:     1.534989, Batch Acc: 0.630552, Tokens per Sec:    14146, Lr: 0.000022\n",
            "2023-01-20 01:57:53,830 - INFO - joeynmt.training - Epoch 465, Step:   160600, Batch Loss:     1.385027, Batch Acc: 0.632236, Tokens per Sec:    14164, Lr: 0.000022\n",
            "2023-01-20 01:58:01,811 - INFO - joeynmt.training - Epoch 465, Step:   160700, Batch Loss:     1.379135, Batch Acc: 0.628078, Tokens per Sec:    13764, Lr: 0.000022\n",
            "2023-01-20 01:58:03,746 - INFO - joeynmt.training - Epoch 465: total training loss 496.80\n",
            "2023-01-20 01:58:03,747 - INFO - joeynmt.training - EPOCH 466\n",
            "2023-01-20 01:58:09,754 - INFO - joeynmt.training - Epoch 466, Step:   160800, Batch Loss:     1.426198, Batch Acc: 0.635288, Tokens per Sec:    13930, Lr: 0.000022\n",
            "2023-01-20 01:58:17,670 - INFO - joeynmt.training - Epoch 466, Step:   160900, Batch Loss:     1.372074, Batch Acc: 0.630043, Tokens per Sec:    13844, Lr: 0.000022\n",
            "2023-01-20 01:58:25,443 - INFO - joeynmt.training - Epoch 466, Step:   161000, Batch Loss:     1.432329, Batch Acc: 0.631000, Tokens per Sec:    14006, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.37ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9044.15ex/s]\n",
            "2023-01-20 01:58:25,735 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=161000\n",
            "2023-01-20 01:58:25,736 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:58:30,223 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:58:30,223 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:58:30,223 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:58:30,224 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:58:30,227 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.09, loss:   2.90, ppl:  18.26, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4481[sec], evaluation: 0.0364[sec]\n",
            "2023-01-20 01:58:30,230 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:58:30,234 - INFO - joeynmt.training - \tSource:     به این طریق تشخیص می‌دهید که گفته‌ای الهام‌شده از خداست : هر گفتهٔ الهام‌شده که تصدیق کند عیسی مسیح به صورت انسان آمد ، از جانب خداست . \n",
            "2023-01-20 01:58:30,234 - INFO - joeynmt.training - \tReference:  آللاهێن روحونو بوندان تانییا بیلرسینیز : ایسا مسیحین جسما گلدیگینی اقرار ائدن هر روح آللاهداندێر ، \n",
            "2023-01-20 01:58:30,234 - INFO - joeynmt.training - \tHypothesis: بو شئیلر آللاهێن اؤنونده اسعادا اولان سؤزلره گؤره سؤیلهینهرک دئمیشدی : هر شئی ایسا مصیحدن ابارت ائدهجهگی سؤزلره گؤره اقرار ائتدی .\n",
            "2023-01-20 01:58:30,234 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:58:30,236 - INFO - joeynmt.training - \tSource:     همانان که از ایشان پیمان گرفتی ولی هر بار پیمان خود را می‌شکنند و از خدا پروا نمی‌دارند . \n",
            "2023-01-20 01:58:30,236 - INFO - joeynmt.training - \tReference:  اونلار اهد باغلادێغێن کیمسهلردیر کی ، سونرا هر دفه اهدلرینی پوزار وه آللاهدان دا قورخمازلار . \n",
            "2023-01-20 01:58:30,236 - INFO - joeynmt.training - \tHypothesis: او کسلر کی ، اهد آلدیقلاری زامان هر هانسێ بیر اهدی پوزمایانلار . آللاه اونلاردان بیر اهد بارهسینده مۆباحصه ائدر .\n",
            "2023-01-20 01:58:30,237 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:58:30,238 - INFO - joeynmt.training - \tSource:     و یاران راست ؛ یاران راست کدامند ؟ \n",
            "2023-01-20 01:58:30,239 - INFO - joeynmt.training - \tReference:  ساغ طرف صاحبلری ! کیمدیر او ساغ طرف صاحبلری ؟ \n",
            "2023-01-20 01:58:30,239 - INFO - joeynmt.training - \tHypothesis: ساغ طرف صاحبلری . کیمدیر او طرف صاحبلری ؟\n",
            "2023-01-20 01:58:30,239 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:58:30,241 - INFO - joeynmt.training - \tSource:     و خدا از آسمان آبی فرود آورد و با آن زمین را پس از پژمردنش زنده گردانید ، قطعا در این امر برای مردمی که شنوایی دارند نشانه‌ای است . \n",
            "2023-01-20 01:58:30,241 - INFO - joeynmt.training - \tReference:  آللاه گؤیدن بیر یاغیش ائندیرر ، اونونلا یئر اۆزۆنۆ اؤلدوکدن سونرا دیریلدر . قولاق آسانلار اۆچۆن بوندا بیر ابرت واردێر . \n",
            "2023-01-20 01:58:30,241 - INFO - joeynmt.training - \tHypothesis: آللاه گؤیدن یاغمور ائندیردی . سونرا دا اونونلا مۆژدهلهیه چێخارتمایان ، شبههسیز کی ، بوندا آغێل صاحبلری اۆچۆن ابرتلر واردێر !\n",
            "2023-01-20 01:58:35,887 - INFO - joeynmt.training - Epoch 466: total training loss 497.99\n",
            "2023-01-20 01:58:35,888 - INFO - joeynmt.training - EPOCH 467\n",
            "2023-01-20 01:58:38,174 - INFO - joeynmt.training - Epoch 467, Step:   161100, Batch Loss:     1.463397, Batch Acc: 0.631882, Tokens per Sec:    14043, Lr: 0.000022\n",
            "2023-01-20 01:58:45,910 - INFO - joeynmt.training - Epoch 467, Step:   161200, Batch Loss:     1.428685, Batch Acc: 0.630901, Tokens per Sec:    14168, Lr: 0.000022\n",
            "2023-01-20 01:58:53,680 - INFO - joeynmt.training - Epoch 467, Step:   161300, Batch Loss:     1.509928, Batch Acc: 0.628799, Tokens per Sec:    14152, Lr: 0.000022\n",
            "2023-01-20 01:59:01,556 - INFO - joeynmt.training - Epoch 467, Step:   161400, Batch Loss:     1.362910, Batch Acc: 0.630345, Tokens per Sec:    13819, Lr: 0.000022\n",
            "2023-01-20 01:59:03,031 - INFO - joeynmt.training - Epoch 467: total training loss 497.34\n",
            "2023-01-20 01:59:03,032 - INFO - joeynmt.training - EPOCH 468\n",
            "2023-01-20 01:59:09,475 - INFO - joeynmt.training - Epoch 468, Step:   161500, Batch Loss:     1.402491, Batch Acc: 0.632214, Tokens per Sec:    13883, Lr: 0.000022\n",
            "2023-01-20 01:59:17,249 - INFO - joeynmt.training - Epoch 468, Step:   161600, Batch Loss:     1.480160, Batch Acc: 0.627368, Tokens per Sec:    14141, Lr: 0.000022\n",
            "2023-01-20 01:59:25,138 - INFO - joeynmt.training - Epoch 468, Step:   161700, Batch Loss:     1.429912, Batch Acc: 0.631678, Tokens per Sec:    14232, Lr: 0.000022\n",
            "2023-01-20 01:59:30,161 - INFO - joeynmt.training - Epoch 468: total training loss 494.43\n",
            "2023-01-20 01:59:30,162 - INFO - joeynmt.training - EPOCH 469\n",
            "2023-01-20 01:59:33,100 - INFO - joeynmt.training - Epoch 469, Step:   161800, Batch Loss:     1.393818, Batch Acc: 0.639043, Tokens per Sec:    13941, Lr: 0.000022\n",
            "2023-01-20 01:59:40,973 - INFO - joeynmt.training - Epoch 469, Step:   161900, Batch Loss:     1.416025, Batch Acc: 0.629770, Tokens per Sec:    14020, Lr: 0.000022\n",
            "2023-01-20 01:59:48,790 - INFO - joeynmt.training - Epoch 469, Step:   162000, Batch Loss:     1.328126, Batch Acc: 0.630642, Tokens per Sec:    14088, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 123.78ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9895.09ex/s]\n",
            "2023-01-20 01:59:49,066 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=162000\n",
            "2023-01-20 01:59:49,067 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 01:59:53,492 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 01:59:53,493 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 01:59:53,493 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 01:59:53,494 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 01:59:53,497 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.13, loss:   2.88, ppl:  17.90, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3869[sec], evaluation: 0.0356[sec]\n",
            "2023-01-20 01:59:53,499 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 01:59:53,502 - INFO - joeynmt.training - \tSource:      مشرکان‌ ، به جای او ، جز بتهای مادینه را به دعا نمی‌خوانند ، و جز شیطان سرکش را نمی‌خوانند . \n",
            "2023-01-20 01:59:53,503 - INFO - joeynmt.training - \tReference:   آللاهێ قویوب یالنیز قادێن بۆتلره تاپێنێر وه یالنیز آسی شیطانا ابادت ائدیرلر . \n",
            "2023-01-20 01:59:53,503 - INFO - joeynmt.training - \tHypothesis: اوندان غیری ابادت ائتدیکلرینه یالنیز اونا عایددیر . اونلار شیطانین ابادتینی ابادت ائتمزلر .\n",
            "2023-01-20 01:59:53,503 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 01:59:53,505 - INFO - joeynmt.training - \tSource:     زیرا قدرت خدا هر آنچه را که نیاز داریم تا بر طبق وقفمان به او زندگی کنیم ، به ما ارزانی داشته است . این از طریق شناخت دقیق او امکان‌پذیر شده است که ما را به وسیلهٔ جلال و نیکویی خود فراخوانده است . \n",
            "2023-01-20 01:59:53,505 - INFO - joeynmt.training - \tReference:  بیزی اؤز ایزهتی وه عالیجنابلیغینا چاغێرانێن الاهی گۆجۆ اونو تانیمآغیمیز واسطهسیله یاشامآغیمیز وه مؤمین اولماغێمێز اۆچۆن لازێم گلن هر شئیی بیزه بخش ائتدی . \n",
            "2023-01-20 01:59:53,505 - INFO - joeynmt.training - \tHypothesis: چۆنکی بیز آللاهێن قدرتی ایله تعیین اولونموش اۆچۆن اۆچۆن تعیین اولوندوق کی ، او بیزی احتیاجیمیزدان تانێماق اۆچۆن هر جۆرهجۆر ، او هم ده بیزیم ایزتلندیری .\n",
            "2023-01-20 01:59:53,505 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 01:59:53,508 - INFO - joeynmt.training - \tSource:     پس زن سامری به او گفت : چطور تو با این که یهودی هستی ، از من که زنی سامری هستم ، آب می‌خواهی ؟ \n",
            "2023-01-20 01:59:53,508 - INFO - joeynmt.training - \tReference:  ساماریالی قادێن اونا دئدی : سن بیر یهودیسن ، منسه ساماریالی بیر قادێن . نئجه سن مندن سو ایستهیه بیلرسن ؟ چۆنکی یهودیلر ساماریالیلارلا اۆنسیت ائتمیر . \n",
            "2023-01-20 01:59:53,508 - INFO - joeynmt.training - \tHypothesis: قادێن اونا دئدی : سن بو یهودییام کی ، سندن ، بو قادێنێن یانیما گلهجهیهم ؟\n",
            "2023-01-20 01:59:53,508 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 01:59:53,510 - INFO - joeynmt.training - \tSource:     پس همان گونه که پیامبران نستوه ، صبر کردند ، صبر کن ، و برای آنان شتابزدگی به خرج مده . روزی که آنچه را وعده داده می‌شوند بنگرند ، گویی که آنان جز ساعتی از روز را در دنیا نمانده‌اند ؛ این‌ ابلاغی است . پس آیا جز مردم نافرمان هلاکت خواهند یافت ؟ \n",
            "2023-01-20 01:59:53,511 - INFO - joeynmt.training - \tReference:   پیغمبرلردن ازم صاحبلری اولانلارێن صبر ائتدیگی کیمی ، سن ده صبر ائت . تئز گلمهسینی ایستهمه . اونلار وه د اولوندوقلارینی گؤرهجکلری گۆن گۆندۆزۆن آنجاق بیر ساعاتێ قدر قالدیقلارینی ساناجاقلار . بیر خبردارلێقدێر . فاصق بیر قؤؤمدن باشقاسێ دا هئچ محو ائدیلرمی \n",
            "2023-01-20 01:59:53,511 - INFO - joeynmt.training - \tHypothesis: پیغمبرلری صبر ائتدیکلرینه گؤره صبر ائت . صبر ائت . اونلار غضب گلمهسینی ایستهییرلر . بیر گۆندن باشقا بیر ساعاتا قسمینی وه د اولوندوقلاری مۆدتدن باشقا بیر ساعاتا سالدێلار . مگر اونلار آنجاق دۆنیادا اعذاب وئرنلردیرلرمی ؟\n",
            "2023-01-20 02:00:01,462 - INFO - joeynmt.training - Epoch 469, Step:   162100, Batch Loss:     1.508391, Batch Acc: 0.628165, Tokens per Sec:    13179, Lr: 0.000022\n",
            "2023-01-20 02:00:02,185 - INFO - joeynmt.training - Epoch 469: total training loss 496.26\n",
            "2023-01-20 02:00:02,185 - INFO - joeynmt.training - EPOCH 470\n",
            "2023-01-20 02:00:09,290 - INFO - joeynmt.training - Epoch 470, Step:   162200, Batch Loss:     1.396090, Batch Acc: 0.633012, Tokens per Sec:    14091, Lr: 0.000022\n",
            "2023-01-20 02:00:17,589 - INFO - joeynmt.training - Epoch 470, Step:   162300, Batch Loss:     1.412332, Batch Acc: 0.636002, Tokens per Sec:    13014, Lr: 0.000022\n",
            "2023-01-20 02:00:27,849 - INFO - joeynmt.training - Epoch 470, Step:   162400, Batch Loss:     1.474218, Batch Acc: 0.627746, Tokens per Sec:    10688, Lr: 0.000022\n",
            "2023-01-20 02:00:32,278 - INFO - joeynmt.training - Epoch 470: total training loss 496.58\n",
            "2023-01-20 02:00:32,279 - INFO - joeynmt.training - EPOCH 471\n",
            "2023-01-20 02:00:35,725 - INFO - joeynmt.training - Epoch 471, Step:   162500, Batch Loss:     1.463820, Batch Acc: 0.635374, Tokens per Sec:    14095, Lr: 0.000022\n",
            "2023-01-20 02:00:43,524 - INFO - joeynmt.training - Epoch 471, Step:   162600, Batch Loss:     1.458829, Batch Acc: 0.631864, Tokens per Sec:    14197, Lr: 0.000022\n",
            "2023-01-20 02:00:51,413 - INFO - joeynmt.training - Epoch 471, Step:   162700, Batch Loss:     1.480164, Batch Acc: 0.630187, Tokens per Sec:    13986, Lr: 0.000022\n",
            "2023-01-20 02:00:59,397 - INFO - joeynmt.training - Epoch 471, Step:   162800, Batch Loss:     1.322888, Batch Acc: 0.627525, Tokens per Sec:    13762, Lr: 0.000022\n",
            "2023-01-20 02:00:59,472 - INFO - joeynmt.training - Epoch 471: total training loss 494.11\n",
            "2023-01-20 02:00:59,472 - INFO - joeynmt.training - EPOCH 472\n",
            "2023-01-20 02:01:07,199 - INFO - joeynmt.training - Epoch 472, Step:   162900, Batch Loss:     1.363013, Batch Acc: 0.631047, Tokens per Sec:    14013, Lr: 0.000022\n",
            "2023-01-20 02:01:15,056 - INFO - joeynmt.training - Epoch 472, Step:   163000, Batch Loss:     1.470709, Batch Acc: 0.629678, Tokens per Sec:    14125, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.97ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10539.85ex/s]\n",
            "2023-01-20 02:01:15,319 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=163000\n",
            "2023-01-20 02:01:15,319 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:01:20,294 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:01:20,295 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:01:20,295 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:01:20,296 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:01:20,299 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.64, loss:   2.96, ppl:  19.36, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9358[sec], evaluation: 0.0369[sec]\n",
            "2023-01-20 02:01:20,301 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 02:01:20,305 - INFO - joeynmt.training - \tSource:     در آنجا مردی را دید به نام اینیاس که برای هشت سال مفلوج و در بستر بود . \n",
            "2023-01-20 02:01:20,305 - INFO - joeynmt.training - \tReference:  اورادا ائنئیا آدلێ بیر نفره باش چکدی . ائنئیا افلیج اولموشدو وه سککیز ایل ایدی کی ، یاتاقدا ایدی . \n",
            "2023-01-20 02:01:20,305 - INFO - joeynmt.training - \tHypothesis: اورادا بو آدلێ بیر آدام اونو الاه سالاغی ایل بو آداما نقل ائتدی .\n",
            "2023-01-20 02:01:20,305 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 02:01:20,307 - INFO - joeynmt.training - \tSource:     در واقع ، شما نیز فراخوانده شده‌اید تا این راه را دنبال کنید ؛ زیرا حتی مسیح برای شما رنج کشید و سرمشقی برای شما قرار داد تا به‌دقت در جای پای او گام بردارید . \n",
            "2023-01-20 02:01:20,308 - INFO - joeynmt.training - \tReference:  سیز محض بونون اۆچۆن چاغێرێلدێنێز . چۆنکی مصیح ده سیزین اۆچۆن اعذاب چکدی وه سیزه نمونه اولدو کی ، سیز ده اونون ایزی ایله گئدهسینیز . \n",
            "2023-01-20 02:01:20,308 - INFO - joeynmt.training - \tHypothesis: چاغێرێش آلدێغێنێز دا سیزها مصیحی یولوناوزا قدر اعذاب چکیرسینیز . چۆنکی او ، ایشیقلیگه چاتاندا یئر اۆزۆنده اوغرادێجدان آرتێق آلدێ .\n",
            "2023-01-20 02:01:20,308 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 02:01:20,310 - INFO - joeynmt.training - \tSource:     عیسی از سنگدلی‌شان عمیقا اندوهگین شد و با خشم به آنان نظر افکند . سپس به آن مرد گفت : دستت را دراز کن . او دستش را دراز کرد و دست او شفا یافت . \n",
            "2023-01-20 02:01:20,310 - INFO - joeynmt.training - \tReference:  ایسا اترافینداکیلارا غزبله باخدێ وه اونلارێن اینادکار اۆرکلی اولدوقلارینا گؤره کدرلهنیب او آداما دئدی : الینی اوزات ! او ، الینی اوزاتدێ وه الی اوهلکی هالێنا قایێتدی . \n",
            "2023-01-20 02:01:20,310 - INFO - joeynmt.training - \tHypothesis: ایسا داشقالاقدان داشێجانێب هدتلندی . اونلارا دئدی : ساغێندا اؤلدو . او ، الینی اوزاتدێ وه الینی اوزادێب ساغالتدێ .\n",
            "2023-01-20 02:01:20,311 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 02:01:20,313 - INFO - joeynmt.training - \tSource:     به همین ترتیب نیز ، وقتی شما وقوع این چیزها را می‌بینید ، بدانید که پادشاهی خدا نزدیک است . \n",
            "2023-01-20 02:01:20,313 - INFO - joeynmt.training - \tReference:  ائلهجه سیز ده بو شئیلرین باش وئردیگینی گؤرنده بیلین کی ، آللاهێن پادشاهلێغێ یاخینلاشیب . \n",
            "2023-01-20 02:01:20,313 - INFO - joeynmt.training - \tHypothesis: ائلهجه ده بو شئیلرین باش وئردیگینی بیلین کی ، سیز ده ایندی آللاهێن پادشاهلێغێ یاخیندیر .\n",
            "2023-01-20 02:01:28,102 - INFO - joeynmt.training - Epoch 472, Step:   163100, Batch Loss:     1.416665, Batch Acc: 0.630564, Tokens per Sec:    13607, Lr: 0.000022\n",
            "2023-01-20 02:01:31,852 - INFO - joeynmt.training - Epoch 472: total training loss 495.55\n",
            "2023-01-20 02:01:31,852 - INFO - joeynmt.training - EPOCH 473\n",
            "2023-01-20 02:01:36,008 - INFO - joeynmt.training - Epoch 473, Step:   163200, Batch Loss:     1.291369, Batch Acc: 0.632250, Tokens per Sec:    13749, Lr: 0.000022\n",
            "2023-01-20 02:01:43,826 - INFO - joeynmt.training - Epoch 473, Step:   163300, Batch Loss:     1.311197, Batch Acc: 0.630664, Tokens per Sec:    14302, Lr: 0.000022\n",
            "2023-01-20 02:01:51,677 - INFO - joeynmt.training - Epoch 473, Step:   163400, Batch Loss:     1.396567, Batch Acc: 0.630895, Tokens per Sec:    14009, Lr: 0.000022\n",
            "2023-01-20 02:01:58,976 - INFO - joeynmt.training - Epoch 473: total training loss 492.58\n",
            "2023-01-20 02:01:58,976 - INFO - joeynmt.training - EPOCH 474\n",
            "2023-01-20 02:01:59,700 - INFO - joeynmt.training - Epoch 474, Step:   163500, Batch Loss:     1.442586, Batch Acc: 0.632500, Tokens per Sec:    12005, Lr: 0.000022\n",
            "2023-01-20 02:02:07,605 - INFO - joeynmt.training - Epoch 474, Step:   163600, Batch Loss:     1.530203, Batch Acc: 0.630362, Tokens per Sec:    14070, Lr: 0.000022\n",
            "2023-01-20 02:02:15,487 - INFO - joeynmt.training - Epoch 474, Step:   163700, Batch Loss:     1.362283, Batch Acc: 0.634936, Tokens per Sec:    13885, Lr: 0.000022\n",
            "2023-01-20 02:02:23,339 - INFO - joeynmt.training - Epoch 474, Step:   163800, Batch Loss:     1.408877, Batch Acc: 0.630619, Tokens per Sec:    14007, Lr: 0.000022\n",
            "2023-01-20 02:02:26,224 - INFO - joeynmt.training - Epoch 474: total training loss 493.41\n",
            "2023-01-20 02:02:26,224 - INFO - joeynmt.training - EPOCH 475\n",
            "2023-01-20 02:02:31,263 - INFO - joeynmt.training - Epoch 475, Step:   163900, Batch Loss:     1.384253, Batch Acc: 0.635108, Tokens per Sec:    13984, Lr: 0.000022\n",
            "2023-01-20 02:02:39,075 - INFO - joeynmt.training - Epoch 475, Step:   164000, Batch Loss:     1.303124, Batch Acc: 0.628329, Tokens per Sec:    14018, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.19ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9330.66ex/s]\n",
            "2023-01-20 02:02:39,360 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=164000\n",
            "2023-01-20 02:02:39,361 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:02:43,133 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:02:43,133 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:02:43,133 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:02:43,134 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:02:43,137 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.19, loss:   2.85, ppl:  17.27, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 3.7352[sec], evaluation: 0.0348[sec]\n",
            "2023-01-20 02:02:43,140 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 02:02:43,144 - INFO - joeynmt.training - \tSource:     تو قلكی كه‌ برات‌ گذاشتم‌ چند تایی‌ سكه‌ بود . \n",
            "2023-01-20 02:02:43,144 - INFO - joeynmt.training - \tReference:  سنه قالان دخیل ده گرک بیر نئچه سیککه اولاردی . \n",
            "2023-01-20 02:02:43,144 - INFO - joeynmt.training - \tHypothesis: قییمت ألینده نئچه نئچه له سی سیاغی آلمیشدی .\n",
            "2023-01-20 02:02:43,144 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 02:02:43,146 - INFO - joeynmt.training - \tSource:      همان‌ کسانی که از پروردگارشان در نهان می‌ترسند و از قیامت هراسناکند . \n",
            "2023-01-20 02:02:43,147 - INFO - joeynmt.training - \tReference:  او کسلردن اؤترو کی ، اونلار اؤز ربیندن اونو گؤرمهدن قورخار وه قیامتدن لرزهیه گلرلر . \n",
            "2023-01-20 02:02:43,147 - INFO - joeynmt.training - \tHypothesis: او کسلر کی ، او کسلردن قورخانلار وه قیامت گۆنۆنه اینانیرلار .\n",
            "2023-01-20 02:02:43,147 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 02:02:43,149 - INFO - joeynmt.training - \tSource:     شما آنچه را که نمی‌شناسید می‌پرستید ، ما آنچه را که می‌شناسیم می‌پرستیم ؛ زیرا نجات از یهودیان آغاز می‌شود . \n",
            "2023-01-20 02:02:43,149 - INFO - joeynmt.training - \tReference:  سیز بیلمهدیگینیزه ابادت ائدیرسینیز ، بیزسه بیلدیگیمیزه ابادت ائدیریک ، چۆنکی خلاص یهودیلردن گلیر . \n",
            "2023-01-20 02:02:43,149 - INFO - joeynmt.training - \tHypothesis: سیز تانیدیغیمیزی تانیمادینیز ، چۆنکی بیزدن باشلانغێجێ تانیمایاجاغیق . چۆنکی یهودیلرین خلاص اولدوغونو بیلیرسینیز .\n",
            "2023-01-20 02:02:43,150 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 02:02:43,152 - INFO - joeynmt.training - \tSource:     نازل شدن این کتاب که هیچ جای‌ شک در آن نیست از طرف پروردگار جهانهاست . \n",
            "2023-01-20 02:02:43,152 - INFO - joeynmt.training - \tReference:  کیتابین آلملرین ربی ترهفیندن نازل ائدیلمهسینده هئچ بیر شک شبههه یوخدور ! \n",
            "2023-01-20 02:02:43,152 - INFO - joeynmt.training - \tHypothesis: کیتابدا هئچ بیر شک شبههه یوخدور کی ، او ، ربینین ایزنی اولمادان بیر یئره توپلایاجاغیدیر !\n",
            "2023-01-20 02:02:51,013 - INFO - joeynmt.training - Epoch 475, Step:   164100, Batch Loss:     1.495489, Batch Acc: 0.631973, Tokens per Sec:    13498, Lr: 0.000022\n",
            "2023-01-20 02:02:57,436 - INFO - joeynmt.training - Epoch 475: total training loss 493.08\n",
            "2023-01-20 02:02:57,436 - INFO - joeynmt.training - EPOCH 476\n",
            "2023-01-20 02:02:58,857 - INFO - joeynmt.training - Epoch 476, Step:   164200, Batch Loss:     1.407859, Batch Acc: 0.634811, Tokens per Sec:    14220, Lr: 0.000022\n",
            "2023-01-20 02:03:06,767 - INFO - joeynmt.training - Epoch 476, Step:   164300, Batch Loss:     1.429267, Batch Acc: 0.633176, Tokens per Sec:    13978, Lr: 0.000022\n",
            "2023-01-20 02:03:14,610 - INFO - joeynmt.training - Epoch 476, Step:   164400, Batch Loss:     1.470260, Batch Acc: 0.632412, Tokens per Sec:    14124, Lr: 0.000022\n",
            "2023-01-20 02:03:22,387 - INFO - joeynmt.training - Epoch 476, Step:   164500, Batch Loss:     1.547381, Batch Acc: 0.625936, Tokens per Sec:    14080, Lr: 0.000022\n",
            "2023-01-20 02:03:24,556 - INFO - joeynmt.training - Epoch 476: total training loss 493.92\n",
            "2023-01-20 02:03:24,556 - INFO - joeynmt.training - EPOCH 477\n",
            "2023-01-20 02:03:30,286 - INFO - joeynmt.training - Epoch 477, Step:   164600, Batch Loss:     1.578847, Batch Acc: 0.632942, Tokens per Sec:    13975, Lr: 0.000022\n",
            "2023-01-20 02:03:38,207 - INFO - joeynmt.training - Epoch 477, Step:   164700, Batch Loss:     1.460850, Batch Acc: 0.633012, Tokens per Sec:    13790, Lr: 0.000022\n",
            "2023-01-20 02:03:46,086 - INFO - joeynmt.training - Epoch 477, Step:   164800, Batch Loss:     1.387055, Batch Acc: 0.632777, Tokens per Sec:    14112, Lr: 0.000022\n",
            "2023-01-20 02:03:51,748 - INFO - joeynmt.training - Epoch 477: total training loss 492.15\n",
            "2023-01-20 02:03:51,749 - INFO - joeynmt.training - EPOCH 478\n",
            "2023-01-20 02:03:53,929 - INFO - joeynmt.training - Epoch 478, Step:   164900, Batch Loss:     1.554350, Batch Acc: 0.632541, Tokens per Sec:    14279, Lr: 0.000022\n",
            "2023-01-20 02:04:01,808 - INFO - joeynmt.training - Epoch 478, Step:   165000, Batch Loss:     1.403240, Batch Acc: 0.633786, Tokens per Sec:    13941, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 66.85ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9411.56ex/s]\n",
            "2023-01-20 02:04:02,105 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=165000\n",
            "2023-01-20 02:04:02,105 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:04:08,123 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:04:08,127 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:04:08,129 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:04:08,130 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:04:08,136 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.98, loss:   2.94, ppl:  18.83, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.9458[sec], evaluation: 0.0711[sec]\n",
            "2023-01-20 02:04:08,142 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 02:04:08,146 - INFO - joeynmt.training - \tSource:     زیرا کدام بزرگ‌تر است ، آن که بر سر سفره می‌نشیند یا آن که خدمت می‌کند ؟ آیا آن که بر سر سفره نشسته است ، بزرگ‌تر نیست ؟ اما من در میان شما همچون خدمتگزار هستم . \n",
            "2023-01-20 02:04:08,146 - INFO - joeynmt.training - \tReference:  آخێ کیم داها بؤیوکدور ، سفرهده اوتوران ، یوخسا خدمت ائدن ؟ سفرهده اوتوران دئییلمی ؟ منسه سیزین آرانێزدا خدمتچی کیمیگم . \n",
            "2023-01-20 02:04:08,147 - INFO - joeynmt.training - \tHypothesis: چۆنکی بؤیوک هانسێ داها بؤیوکدور ؟ یاخود صفرهیه اوتوراندا دئمک ایستهییرسنمی ؟ کور آدامدا اوتوراندا منیم اۆچۆن صفرهیه اوتورموشدو .\n",
            "2023-01-20 02:04:08,147 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 02:04:08,150 - INFO - joeynmt.training - \tSource:     ای ابراهیم ! از این چون و چرا روی برتاب ، که فرمان پروردگارت آمده و برای آنان عذابی که بی‌بازگشت است خواهد آمد . \n",
            "2023-01-20 02:04:08,153 - INFO - joeynmt.training - \tReference:   ائی ابراهیم ! بوندان ال چک ، ربینین امری آرتێق گلمیشدیر . اونلارا مۆتلق قارشیسیآلینماز بیر اعذاب گلهجکدیر . \n",
            "2023-01-20 02:04:08,153 - INFO - joeynmt.training - \tHypothesis: ائی ابراهیم ! بو نه اۆچۆن درگاهێندا اۆز دؤندر ! سنین امریندن اونلارا بیر اعذاب گلمیشدیر .\n",
            "2023-01-20 02:04:08,153 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 02:04:08,156 - INFO - joeynmt.training - \tSource:     پروردگارا ، من یکی از فرزندانم را در دره‌ای بی‌کشت ، نزد خانه محترم تو ، سکونت دادم . پروردگارا ، تا نماز را به پا دارند ، پس دلهای برخی از مردم را به سوی آنان گرایش ده و آنان را از محصولات مورد نیازشان‌ روزی ده ، باشد که سپاسگزاری کنند . \n",
            "2023-01-20 02:04:08,157 - INFO - joeynmt.training - \tReference:  ائی ربیمیز ! من اهلی ایالیمدان به زیسینی سنین بیطالحرامینین یاخینلیغیندا ، اکین بیتمز بیر واده ساکین ائتدیم . ائی ربیمیز ! اونلار ناماز قێلسێنلار دئیه بئله ائتدیم . ائله ائت کی ، اینسانلارین بیر قسمینین قلبلری اونلارا مئیل ائتسین . اونلارا مئیوهلریندن روزی وئر کی ، شۆکۆر ائده بیلسینلر ! \n",
            "2023-01-20 02:04:08,157 - INFO - joeynmt.training - \tHypothesis: ائی ربیم ! مندن بیر اؤؤلادێم ! سندن داها پیس یولا سالمێشام . ائی ربیمیز ! سن اینسانلاری اؤز ربینه دوعا ائت وه اینسانلاردان اۆز دؤندرمک اۆچۆن محصول وئر . شۆکۆر ائتمهلری اۆچۆن کی ، شۆکۆر ائتمهسینلر !\n",
            "2023-01-20 02:04:08,158 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 02:04:08,161 - INFO - joeynmt.training - \tSource:     از آن زمان عیسی موعظه را آغاز کرد . او می‌گفت : توبه کنید ؛ زیرا پادشاهی آسمان‌ها نزدیک شده است . \n",
            "2023-01-20 02:04:08,162 - INFO - joeynmt.training - \tReference:  او واختدان ایسا وز ائدیب بئله دئمهیه باشلادێ : تؤؤبه ائدین ! چۆنکی سماوی پادشاهلێق یاخینلاشیب . \n",
            "2023-01-20 02:04:08,162 - INFO - joeynmt.training - \tHypothesis: او واخت ایسا مۆژدسنی وز ائدیردی . چۆنکی تؤؤبه ائدیب دئدی : سماوی پادشاهلێغێ یاخین اولدو !\n",
            "2023-01-20 02:04:17,420 - INFO - joeynmt.training - Epoch 478, Step:   165100, Batch Loss:     1.404823, Batch Acc: 0.630443, Tokens per Sec:    11425, Lr: 0.000022\n",
            "2023-01-20 02:04:25,220 - INFO - joeynmt.training - Epoch 478, Step:   165200, Batch Loss:     1.414523, Batch Acc: 0.630351, Tokens per Sec:    14101, Lr: 0.000022\n",
            "2023-01-20 02:04:26,676 - INFO - joeynmt.training - Epoch 478: total training loss 493.62\n",
            "2023-01-20 02:04:26,677 - INFO - joeynmt.training - EPOCH 479\n",
            "2023-01-20 02:04:33,144 - INFO - joeynmt.training - Epoch 479, Step:   165300, Batch Loss:     1.480526, Batch Acc: 0.634684, Tokens per Sec:    14162, Lr: 0.000022\n",
            "2023-01-20 02:04:41,025 - INFO - joeynmt.training - Epoch 479, Step:   165400, Batch Loss:     1.453336, Batch Acc: 0.632721, Tokens per Sec:    13922, Lr: 0.000022\n",
            "2023-01-20 02:04:48,814 - INFO - joeynmt.training - Epoch 479, Step:   165500, Batch Loss:     1.435004, Batch Acc: 0.629977, Tokens per Sec:    14226, Lr: 0.000022\n",
            "2023-01-20 02:04:53,682 - INFO - joeynmt.training - Epoch 479: total training loss 490.91\n",
            "2023-01-20 02:04:53,683 - INFO - joeynmt.training - EPOCH 480\n",
            "2023-01-20 02:04:56,654 - INFO - joeynmt.training - Epoch 480, Step:   165600, Batch Loss:     1.431243, Batch Acc: 0.633210, Tokens per Sec:    13826, Lr: 0.000022\n",
            "2023-01-20 02:05:04,630 - INFO - joeynmt.training - Epoch 480, Step:   165700, Batch Loss:     1.397502, Batch Acc: 0.634117, Tokens per Sec:    13743, Lr: 0.000022\n",
            "2023-01-20 02:05:12,385 - INFO - joeynmt.training - Epoch 480, Step:   165800, Batch Loss:     1.377669, Batch Acc: 0.631121, Tokens per Sec:    14133, Lr: 0.000022\n",
            "2023-01-20 02:05:20,186 - INFO - joeynmt.training - Epoch 480, Step:   165900, Batch Loss:     1.450255, Batch Acc: 0.631705, Tokens per Sec:    14142, Lr: 0.000022\n",
            "2023-01-20 02:05:20,860 - INFO - joeynmt.training - Epoch 480: total training loss 493.39\n",
            "2023-01-20 02:05:20,860 - INFO - joeynmt.training - EPOCH 481\n",
            "2023-01-20 02:05:28,111 - INFO - joeynmt.training - Epoch 481, Step:   166000, Batch Loss:     1.449165, Batch Acc: 0.632674, Tokens per Sec:    13885, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.70ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10451.10ex/s]\n",
            "2023-01-20 02:05:28,374 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=166000\n",
            "2023-01-20 02:05:28,374 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:05:33,191 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:05:33,191 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:05:33,192 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:05:33,193 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:05:33,195 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.86, loss:   2.90, ppl:  18.17, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7683[sec], evaluation: 0.0456[sec]\n",
            "2023-01-20 02:05:33,198 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 02:05:33,201 - INFO - joeynmt.training - \tSource:     و اگر تو را استوار نمی‌داشتیم ، قطعا نزدیک بود کمی به سوی آنان متمایل شوی . \n",
            "2023-01-20 02:05:33,202 - INFO - joeynmt.training - \tReference:  اگر بیز سنه صبات وئرمهسیدیک ، یقین کی ، آز دا اولسا ، اونلارا اویاجاقدین ! \n",
            "2023-01-20 02:05:33,202 - INFO - joeynmt.training - \tHypothesis: اگر سنه قارشێ بیر قدر اولساق ، شبههسیز کی ، بیز سنی اونلارێن آخێر دؤنوشو قبول ائدهدیک .\n",
            "2023-01-20 02:05:33,202 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 02:05:33,208 - INFO - joeynmt.training - \tSource:     اما دیده‌ نمی‌شود . \n",
            "2023-01-20 02:05:33,208 - INFO - joeynmt.training - \tReference:  آمما گؤرونموری . \n",
            "2023-01-20 02:05:33,208 - INFO - joeynmt.training - \tHypothesis: آمما گؤزلرینی آچمیر .\n",
            "2023-01-20 02:05:33,209 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 02:05:33,210 - INFO - joeynmt.training - \tSource:     به‌راستی چه فایده دارد که کسی تمام دنیا را به دست آورد ، اما جان خود را از دست بدهد ؟ \n",
            "2023-01-20 02:05:33,211 - INFO - joeynmt.training - \tReference:  اینسانا بۆتۆن دۆنیانی قازانێب جانێنێ ایتیرمهیینین نه خئیری وار ؟ \n",
            "2023-01-20 02:05:33,211 - INFO - joeynmt.training - \tHypothesis: اینسان بۆتۆن دۆنیانی قازانێب جانێنێ ایتیررسه ، اوندا جانێنێ ایتیررسه ،\n",
            "2023-01-20 02:05:33,211 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 02:05:33,213 - INFO - joeynmt.training - \tSource:     زیرا پروردگار تو همان آفریننده داناست . \n",
            "2023-01-20 02:05:33,213 - INFO - joeynmt.training - \tReference:  حقیقتا ، ربین یاراداندیر ، بیلندیر ! \n",
            "2023-01-20 02:05:33,213 - INFO - joeynmt.training - \tHypothesis: حقیقتا ، سنین ربین یئنیلمز غۆۆت صاحبی ، بیلنلرین انتقامتلی اولان آللاهدێر !\n",
            "2023-01-20 02:05:41,146 - INFO - joeynmt.training - Epoch 481, Step:   166100, Batch Loss:     1.393610, Batch Acc: 0.634500, Tokens per Sec:    13521, Lr: 0.000022\n",
            "2023-01-20 02:05:48,919 - INFO - joeynmt.training - Epoch 481, Step:   166200, Batch Loss:     1.407020, Batch Acc: 0.633079, Tokens per Sec:    14152, Lr: 0.000022\n",
            "2023-01-20 02:05:53,099 - INFO - joeynmt.training - Epoch 481: total training loss 492.04\n",
            "2023-01-20 02:05:53,100 - INFO - joeynmt.training - EPOCH 482\n",
            "2023-01-20 02:05:56,737 - INFO - joeynmt.training - Epoch 482, Step:   166300, Batch Loss:     1.451309, Batch Acc: 0.633579, Tokens per Sec:    14328, Lr: 0.000022\n",
            "2023-01-20 02:06:04,640 - INFO - joeynmt.training - Epoch 482, Step:   166400, Batch Loss:     1.303099, Batch Acc: 0.631860, Tokens per Sec:    13762, Lr: 0.000022\n",
            "2023-01-20 02:06:12,507 - INFO - joeynmt.training - Epoch 482, Step:   166500, Batch Loss:     1.501881, Batch Acc: 0.633376, Tokens per Sec:    13985, Lr: 0.000022\n",
            "2023-01-20 02:06:20,360 - INFO - joeynmt.training - Epoch 482: total training loss 493.18\n",
            "2023-01-20 02:06:20,361 - INFO - joeynmt.training - EPOCH 483\n",
            "2023-01-20 02:06:20,440 - INFO - joeynmt.training - Epoch 483, Step:   166600, Batch Loss:     1.315400, Batch Acc: 0.671642, Tokens per Sec:    12844, Lr: 0.000022\n",
            "2023-01-20 02:06:28,186 - INFO - joeynmt.training - Epoch 483, Step:   166700, Batch Loss:     1.400142, Batch Acc: 0.632002, Tokens per Sec:    14284, Lr: 0.000022\n",
            "2023-01-20 02:06:36,054 - INFO - joeynmt.training - Epoch 483, Step:   166800, Batch Loss:     1.414728, Batch Acc: 0.632117, Tokens per Sec:    13800, Lr: 0.000022\n",
            "2023-01-20 02:06:43,847 - INFO - joeynmt.training - Epoch 483, Step:   166900, Batch Loss:     1.367333, Batch Acc: 0.631481, Tokens per Sec:    13993, Lr: 0.000022\n",
            "2023-01-20 02:06:47,528 - INFO - joeynmt.training - Epoch 483: total training loss 496.13\n",
            "2023-01-20 02:06:47,528 - INFO - joeynmt.training - EPOCH 484\n",
            "2023-01-20 02:06:51,625 - INFO - joeynmt.training - Epoch 484, Step:   167000, Batch Loss:     1.406314, Batch Acc: 0.634857, Tokens per Sec:    14122, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 126.11ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9231.89ex/s]\n",
            "2023-01-20 02:06:51,921 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=167000\n",
            "2023-01-20 02:06:51,921 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:06:56,517 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:06:56,517 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:06:56,517 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:06:56,519 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:06:56,522 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.12, loss:   2.97, ppl:  19.49, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5575[sec], evaluation: 0.0362[sec]\n",
            "2023-01-20 02:06:56,524 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 02:06:56,528 - INFO - joeynmt.training - \tSource:     پس دقت کنید که چگونه می‌شنوید ؛ زیرا هر که دارد ، بیشتر به او داده خواهد شد ، اما آن که ندارد ، حتی آنچه تصور می‌کند دارد نیز از او گرفته خواهد شد . \n",
            "2023-01-20 02:06:56,528 - INFO - joeynmt.training - \tReference:  بونا گؤره ده نئجه دینلهدیگینیزه دقت ائدین . چۆنکی کیمین وارێدێرسا ، اونا داها چوخ وئریلهجک . آمما کیمین یوخودورسا ، اؤزونونکو زن ائتدیگی شئی ده الیندن آلێناجاق . \n",
            "2023-01-20 02:06:56,528 - INFO - joeynmt.training - \tHypothesis: بس نئجه بویورورسونوز ، چۆنکی ائشیتدیگینیز کیمین داها چوخ وئریلهجک . آمما او ، الینده اولانا وئریلهجک ، آمما او دا آلێب یاشایاجاق .\n",
            "2023-01-20 02:06:56,528 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 02:06:56,530 - INFO - joeynmt.training - \tSource:     و خدا شما را به سرای سلامت فرا می‌خواند ، و هر که را بخواهد به راه راست هدایت می‌کند . \n",
            "2023-01-20 02:06:56,531 - INFO - joeynmt.training - \tReference:  آللاه امین آمانلێق یوردونا چاغێرێر وه ایستهدیگینی دوغرو یولا سالێر ! \n",
            "2023-01-20 02:06:56,531 - INFO - joeynmt.training - \tHypothesis: آللاه سیزه سالام گؤندهریر . او ، راحاتلێق ایستهدیگینی ده دۆز یولا یؤنلدر .\n",
            "2023-01-20 02:06:56,531 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 02:06:56,533 - INFO - joeynmt.training - \tSource:     در ایام قدیم ، افرادی بودند که به دلیل ایمانشان ، بر آنان به‌نیکویی شهادت داده شد . \n",
            "2023-01-20 02:06:56,533 - INFO - joeynmt.training - \tReference:  اجدادلاریمیز ایمانلاری اۆچۆن مۆسبت شهادت آلدێلار . \n",
            "2023-01-20 02:06:56,533 - INFO - joeynmt.training - \tHypothesis: ائی قدیم زامانلار ، ایمانی اونلارێن شهادتی ایله شهادت ائتدی .\n",
            "2023-01-20 02:06:56,533 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 02:06:56,535 - INFO - joeynmt.training - \tSource:     و به جای خدا ، چیزهایی را می‌پرستند که نه به آنان زیان می‌رساند و نه به آنان سود می‌دهد . و می‌گویند : اینها نزد خدا شفاعتگران ما هستند . بگو : آیا خدا را به چیزی که در آسمانها و در زمین نمی‌داند ، آگاه می‌گردانید ؟ او پاک و برتر است از آنچه با وی‌ شریک می‌سازند . \n",
            "2023-01-20 02:06:56,535 - INFO - joeynmt.training - \tReference:  اونلار آللاهێ قویوب اؤزلرینه نه بیر خئیر ، نه ده بیر زرر وئره بیلن بۆتلره ابادت ائدیر وه : بونلار آللاه یانیندا بیزدن اؤترو شفاعت ائدنلردیر ! دئییرلر . دئ : آللاها گؤیلرده وه یئرده بیلمهدیگی بیر شئییمی خبر وئریرسینیز ؟ آللاه اؤزونه شریک قوشولان بۆتلردن اوزاقدێر وه اوجادێر ! \n",
            "2023-01-20 02:06:56,536 - INFO - joeynmt.training - \tHypothesis: آللاهێ قویوب گئتدیکلرینین خئیر ، نه ده زرر وئره بیلنلره ، نه ده خئیر وئررلر . دئ : آللاه گؤیلرده وه یئرده نه وارسا ، اونلار هئچ بیر شفاعت ائده بیلر . آللاه پاکدێر ، مۆقدسدر . او ، پاکدێر ، مۆقدسدر . اونا شریک قوشدوقلاریندان اوجادێر !\n",
            "2023-01-20 02:07:04,394 - INFO - joeynmt.training - Epoch 484, Step:   167100, Batch Loss:     1.364885, Batch Acc: 0.632314, Tokens per Sec:    13470, Lr: 0.000022\n",
            "2023-01-20 02:07:12,262 - INFO - joeynmt.training - Epoch 484, Step:   167200, Batch Loss:     1.402507, Batch Acc: 0.633193, Tokens per Sec:    14085, Lr: 0.000022\n",
            "2023-01-20 02:07:19,537 - INFO - joeynmt.training - Epoch 484: total training loss 491.09\n",
            "2023-01-20 02:07:19,538 - INFO - joeynmt.training - EPOCH 485\n",
            "2023-01-20 02:07:20,175 - INFO - joeynmt.training - Epoch 485, Step:   167300, Batch Loss:     1.475043, Batch Acc: 0.637421, Tokens per Sec:    13633, Lr: 0.000022\n",
            "2023-01-20 02:07:28,109 - INFO - joeynmt.training - Epoch 485, Step:   167400, Batch Loss:     1.427980, Batch Acc: 0.632315, Tokens per Sec:    13793, Lr: 0.000022\n",
            "2023-01-20 02:07:36,050 - INFO - joeynmt.training - Epoch 485, Step:   167500, Batch Loss:     1.406982, Batch Acc: 0.633969, Tokens per Sec:    13938, Lr: 0.000022\n",
            "2023-01-20 02:07:44,052 - INFO - joeynmt.training - Epoch 485, Step:   167600, Batch Loss:     1.486212, Batch Acc: 0.632604, Tokens per Sec:    13852, Lr: 0.000022\n",
            "2023-01-20 02:07:46,946 - INFO - joeynmt.training - Epoch 485: total training loss 490.19\n",
            "2023-01-20 02:07:46,946 - INFO - joeynmt.training - EPOCH 486\n",
            "2023-01-20 02:07:52,572 - INFO - joeynmt.training - Epoch 486, Step:   167700, Batch Loss:     1.452647, Batch Acc: 0.633702, Tokens per Sec:    12156, Lr: 0.000022\n",
            "2023-01-20 02:08:02,653 - INFO - joeynmt.training - Epoch 486, Step:   167800, Batch Loss:     1.399481, Batch Acc: 0.635358, Tokens per Sec:    10929, Lr: 0.000022\n",
            "2023-01-20 02:08:10,533 - INFO - joeynmt.training - Epoch 486, Step:   167900, Batch Loss:     1.407700, Batch Acc: 0.630913, Tokens per Sec:    14065, Lr: 0.000022\n",
            "2023-01-20 02:08:16,934 - INFO - joeynmt.training - Epoch 486: total training loss 490.31\n",
            "2023-01-20 02:08:16,934 - INFO - joeynmt.training - EPOCH 487\n",
            "2023-01-20 02:08:18,338 - INFO - joeynmt.training - Epoch 487, Step:   168000, Batch Loss:     1.380248, Batch Acc: 0.630811, Tokens per Sec:    14025, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.43ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9206.56ex/s]\n",
            "2023-01-20 02:08:18,630 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=168000\n",
            "2023-01-20 02:08:18,630 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:08:23,182 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:08:23,183 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:08:23,183 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:08:23,184 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:08:23,187 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.10, loss:   2.98, ppl:  19.62, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5132[sec], evaluation: 0.0362[sec]\n",
            "2023-01-20 02:08:23,189 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 02:08:23,192 - INFO - joeynmt.training - \tSource:     از طایفهٔ یهودا ۱۲ ۰۰۰ نفر مهر شدند ؛ از طایفهٔ رئوبین ۱۲ ۰۰۰ نفر ؛ از طایفهٔ جاد ۱۲ ۰۰۰ نفر ؛ \n",
            "2023-01-20 02:08:23,193 - INFO - joeynmt.training - \tReference:  یهودا قبیلهسیندن 12 ، 000 مۆهورلنمیش نفر ؛ رووئن قبیلهسیندن 12 ، 000 نفر ؛ قاد قبیلهسیندن 12 ، 000 نفر ؛ \n",
            "2023-01-20 02:08:23,193 - INFO - joeynmt.training - \tHypothesis: قبیلهسیندن 12 ، 0000 نفر ؛ قبیلهسیندن 12 ، 000 نفر ؛ مئلئناهر قبیلهسیندن 12 ، 000 نفر ؛\n",
            "2023-01-20 02:08:23,193 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 02:08:23,195 - INFO - joeynmt.training - \tSource:     من با پذیرفتن کمک‌های مالی از جماعت‌های دیگر ، آن‌ها را در محرومیت قرار دادم تا بتوانم به شما خدمت کنم . \n",
            "2023-01-20 02:08:23,195 - INFO - joeynmt.training - \tReference:  سیزه خدمت ائتمک اۆچۆن باشقا جمیتلردن تمینات آلاراق سانکی اونلارێ سویدوم . \n",
            "2023-01-20 02:08:23,195 - INFO - joeynmt.training - \tHypothesis: من ده جمیتین ماللارێنێز باشقالارینا دایما بزیلریمه قدر بؤیوک بیر جمیته قدر اویاق .\n",
            "2023-01-20 02:08:23,195 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 02:08:23,197 - INFO - joeynmt.training - \tSource:     او حکومت‌ها و قدرت‌ها را از طریق تیر شکنجه مغلوب و نزد همگان رسوا ساخت و به این ترتیب ، پیروزی خود را بر آن‌ها به نمایش گذاشت . \n",
            "2023-01-20 02:08:23,198 - INFO - joeynmt.training - \tReference:  آللاه باشچیلاری وه حاکملری ترک سلاح ائدیب ، اونلارا چارمێخدا قلبه چالاراق آلمده رۆسۆای ائتدی . \n",
            "2023-01-20 02:08:23,198 - INFO - joeynmt.training - \tHypothesis: او ، الاهی ایله ، ملتلرین قانێ ایله چارمێخا چکیلهرک ملتلره گؤستردی .\n",
            "2023-01-20 02:08:23,198 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 02:08:23,200 - INFO - joeynmt.training - \tSource:     آیا وقتی ما استخوان‌ریزه‌های پوسیده شدیم زندگی را از سر می‌گیریم‌ ؟ \n",
            "2023-01-20 02:08:23,200 - INFO - joeynmt.training - \tReference:  اؤزو ده چۆرۆمۆش سۆمۆکلر اولدوغوموز زامانمێ \n",
            "2023-01-20 02:08:23,200 - INFO - joeynmt.training - \tHypothesis: مگر بیزده فئستیک ائشیکمی اوتون هیاتی اولاجاغێق ؟\n",
            "2023-01-20 02:08:31,016 - INFO - joeynmt.training - Epoch 487, Step:   168100, Batch Loss:     1.443634, Batch Acc: 0.634502, Tokens per Sec:    13549, Lr: 0.000022\n",
            "2023-01-20 02:08:38,821 - INFO - joeynmt.training - Epoch 487, Step:   168200, Batch Loss:     1.380855, Batch Acc: 0.631418, Tokens per Sec:    13954, Lr: 0.000022\n",
            "2023-01-20 02:08:46,585 - INFO - joeynmt.training - Epoch 487, Step:   168300, Batch Loss:     1.502576, Batch Acc: 0.632389, Tokens per Sec:    14039, Lr: 0.000022\n",
            "2023-01-20 02:08:48,910 - INFO - joeynmt.training - Epoch 487: total training loss 494.55\n",
            "2023-01-20 02:08:48,911 - INFO - joeynmt.training - EPOCH 488\n",
            "2023-01-20 02:08:54,479 - INFO - joeynmt.training - Epoch 488, Step:   168400, Batch Loss:     1.367307, Batch Acc: 0.634532, Tokens per Sec:    14236, Lr: 0.000022\n",
            "2023-01-20 02:09:02,308 - INFO - joeynmt.training - Epoch 488, Step:   168500, Batch Loss:     1.402755, Batch Acc: 0.634731, Tokens per Sec:    14042, Lr: 0.000022\n",
            "2023-01-20 02:09:10,208 - INFO - joeynmt.training - Epoch 488, Step:   168600, Batch Loss:     1.493846, Batch Acc: 0.632786, Tokens per Sec:    13833, Lr: 0.000022\n",
            "2023-01-20 02:09:16,012 - INFO - joeynmt.training - Epoch 488: total training loss 489.82\n",
            "2023-01-20 02:09:16,012 - INFO - joeynmt.training - EPOCH 489\n",
            "2023-01-20 02:09:18,060 - INFO - joeynmt.training - Epoch 489, Step:   168700, Batch Loss:     1.423483, Batch Acc: 0.634653, Tokens per Sec:    14230, Lr: 0.000022\n",
            "2023-01-20 02:09:25,817 - INFO - joeynmt.training - Epoch 489, Step:   168800, Batch Loss:     1.425192, Batch Acc: 0.636012, Tokens per Sec:    14193, Lr: 0.000022\n",
            "2023-01-20 02:09:33,595 - INFO - joeynmt.training - Epoch 489, Step:   168900, Batch Loss:     1.481230, Batch Acc: 0.632058, Tokens per Sec:    14126, Lr: 0.000022\n",
            "2023-01-20 02:09:41,433 - INFO - joeynmt.training - Epoch 489, Step:   169000, Batch Loss:     1.420000, Batch Acc: 0.630917, Tokens per Sec:    14013, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.30ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10622.89ex/s]\n",
            "2023-01-20 02:09:41,710 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=169000\n",
            "2023-01-20 02:09:41,710 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:09:46,032 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:09:46,032 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:09:46,032 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:09:46,033 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:09:46,036 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.02, loss:   2.79, ppl:  16.21, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2834[sec], evaluation: 0.0360[sec]\n",
            "2023-01-20 02:09:46,039 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 02:09:46,042 - INFO - joeynmt.training - \tSource:     اما ، تو آنچه را از دیگران آموختی و با دلیل و برهان به آن متقاعد شدی ، دنبال کن ؛ زیرا می‌دانی آن‌ها را از چه کسانی آموخته‌ای . \n",
            "2023-01-20 02:09:46,042 - INFO - joeynmt.training - \tReference:  سنسه اؤیرندیگین وه گۆۆندیگین شئیلره صادق قال . چۆنکی بونلارێ کیملردن اؤیرندیگینی بیلیرسن ، \n",
            "2023-01-20 02:09:46,043 - INFO - joeynmt.training - \tHypothesis: سنسه سن ده مۆقدسلرین اؤیرت ، اونا سبیرلی اؤیرت وه اونلارێ اؤیرت . چۆنکی سندن اؤیرنلریسن .\n",
            "2023-01-20 02:09:46,043 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 02:09:46,045 - INFO - joeynmt.training - \tSource:     و یاد کن‌ هنگامی را که به فرشتگان گفتیم : برای آدم سجده کنید . پس ، جز ابلیس که سر باز زد همه‌ سجده کردند . \n",
            "2023-01-20 02:09:46,045 - INFO - joeynmt.training - \tReference:  بیر زامان ملکلره : آدمه سجده ائدین ! دئیه بویورموشدوق . ایبلیسدن باشقا هامێسێ سجده ائتدی . او ، بویون قاچێرتدێ . \n",
            "2023-01-20 02:09:46,045 - INFO - joeynmt.training - \tHypothesis: خاطرلا کی ، بیر زامان ملکلره : آدمه سجده ائدین ! دئمیشدیک . ایبلیسدن باشقا سجده ائتدی . ایبلیس اویدو .\n",
            "2023-01-20 02:09:46,045 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 02:09:46,047 - INFO - joeynmt.training - \tSource:     تا خدا به هر کس هر چه به دست آورده است جزا دهد ، که خدا زودشمار است . \n",
            "2023-01-20 02:09:46,047 - INFO - joeynmt.training - \tReference:  آللاه هر کسه ائتدیگی امللرین جزاسێنێ وئرمک اۆچۆن بئله ائدهجکدیر ! شبههسیز کی ، آللاه تئزلیکله حاقق حساب چکندیر ! \n",
            "2023-01-20 02:09:46,048 - INFO - joeynmt.training - \tHypothesis: آللاه ایستهدیگی شخصه هر کسه اؤز جزاسێنێ وئرسین .\n",
            "2023-01-20 02:09:46,048 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 02:09:46,049 - INFO - joeynmt.training - \tSource:     کسی باشد که خانوادهٔ خود را به‌خوبی اداره کند و فرزندانی مطیع و مؤدب داشته باشد\n",
            "2023-01-20 02:09:46,050 - INFO - joeynmt.training - \tReference:  اؤز ائوینی یاخشی اداره ائتمهلی ، تام بیر لیاقتله اوشاقلارێنێ اؤز تابعلیگینده ساخلامالێدێر . \n",
            "2023-01-20 02:09:46,050 - INFO - joeynmt.training - \tHypothesis: اؤزونو اؤیرنمک اۆچۆن یاخشی ادارهچیلێق ائتسین وه اؤؤلادلارێنێن اؤؤلادلارێ اولاسێنێز .\n",
            "2023-01-20 02:09:47,562 - INFO - joeynmt.training - Epoch 489: total training loss 489.99\n",
            "2023-01-20 02:09:47,563 - INFO - joeynmt.training - EPOCH 490\n",
            "2023-01-20 02:09:53,847 - INFO - joeynmt.training - Epoch 490, Step:   169100, Batch Loss:     1.441900, Batch Acc: 0.635253, Tokens per Sec:    14140, Lr: 0.000022\n",
            "2023-01-20 02:10:01,568 - INFO - joeynmt.training - Epoch 490, Step:   169200, Batch Loss:     1.397160, Batch Acc: 0.634649, Tokens per Sec:    14238, Lr: 0.000022\n",
            "2023-01-20 02:10:09,611 - INFO - joeynmt.training - Epoch 490, Step:   169300, Batch Loss:     1.530650, Batch Acc: 0.634558, Tokens per Sec:    13761, Lr: 0.000022\n",
            "2023-01-20 02:10:14,667 - INFO - joeynmt.training - Epoch 490: total training loss 490.86\n",
            "2023-01-20 02:10:14,667 - INFO - joeynmt.training - EPOCH 491\n",
            "2023-01-20 02:10:17,413 - INFO - joeynmt.training - Epoch 491, Step:   169400, Batch Loss:     1.370545, Batch Acc: 0.638194, Tokens per Sec:    13985, Lr: 0.000022\n",
            "2023-01-20 02:10:25,260 - INFO - joeynmt.training - Epoch 491, Step:   169500, Batch Loss:     1.444388, Batch Acc: 0.632265, Tokens per Sec:    14162, Lr: 0.000022\n",
            "2023-01-20 02:10:33,172 - INFO - joeynmt.training - Epoch 491, Step:   169600, Batch Loss:     1.329618, Batch Acc: 0.636260, Tokens per Sec:    13798, Lr: 0.000022\n",
            "2023-01-20 02:10:41,125 - INFO - joeynmt.training - Epoch 491, Step:   169700, Batch Loss:     1.402001, Batch Acc: 0.633085, Tokens per Sec:    13816, Lr: 0.000022\n",
            "2023-01-20 02:10:42,022 - INFO - joeynmt.training - Epoch 491: total training loss 490.47\n",
            "2023-01-20 02:10:42,023 - INFO - joeynmt.training - EPOCH 492\n",
            "2023-01-20 02:10:48,933 - INFO - joeynmt.training - Epoch 492, Step:   169800, Batch Loss:     1.391121, Batch Acc: 0.636361, Tokens per Sec:    14093, Lr: 0.000022\n",
            "2023-01-20 02:10:56,676 - INFO - joeynmt.training - Epoch 492, Step:   169900, Batch Loss:     1.365635, Batch Acc: 0.635477, Tokens per Sec:    14201, Lr: 0.000022\n",
            "2023-01-20 02:11:04,448 - INFO - joeynmt.training - Epoch 492, Step:   170000, Batch Loss:     1.512002, Batch Acc: 0.633329, Tokens per Sec:    14092, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.66ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10523.33ex/s]\n",
            "2023-01-20 02:11:04,709 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=170000\n",
            "2023-01-20 02:11:04,710 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:11:09,706 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:11:09,707 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:11:09,707 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:11:09,708 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:11:09,711 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.00, loss:   2.95, ppl:  19.18, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9576[sec], evaluation: 0.0364[sec]\n",
            "2023-01-20 02:11:09,714 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 02:11:09,717 - INFO - joeynmt.training - \tSource:     وقتی به طور ناگهانی‌ بر داوود درآمدند ، و او از آنان به هراس افتاد ، گفتند : مترس ، ما دو مدعی هستیم‌ که یکی از ما بر دیگری تجاوز کرده ، پس میان ما به حق داوری کن ، و از حق دور مشو ، و ما را به راه راست راهبر باش . \n",
            "2023-01-20 02:11:09,717 - INFO - joeynmt.training - \tReference:  اونلار داوودون یانینا گلیکده اونلاردان قورخدو . اونلار دئدیلر : قورخما ، بیز بیر بیریمیزه حاقسێزلێق ائتمیش ایکی ادیاچییییق . آرامێزدا عدالتله هؤکم ائت ، حاققێ تاپدالاما وه بیزه دوغرو یولو گؤستر ! \n",
            "2023-01-20 02:11:09,717 - INFO - joeynmt.training - \tHypothesis: او زامان داوود مینباشیغا چاغێراندا اونلار دئدیلر : قورخما ، بیز ده ایکیسی بیریمیزدن باشقاسێنا ابادت ائت . آرتێق آرامێزدا مۆحاکمه ائت ، بیزه حاقق یولو گؤستهرن بیر آدام اولاسان . بیزه دوغرو یول گؤستهرنلری دوغرو یولا یؤنل !\n",
            "2023-01-20 02:11:09,718 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 02:11:09,719 - INFO - joeynmt.training - \tSource:     دیر میرسی . \n",
            "2023-01-20 02:11:09,720 - INFO - joeynmt.training - \tReference:  بئواختا قالیرسان . \n",
            "2023-01-20 02:11:09,720 - INFO - joeynmt.training - \tHypothesis: یورقون بیر سان .\n",
            "2023-01-20 02:11:09,720 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 02:11:09,722 - INFO - joeynmt.training - \tSource:     می‌گفتند : تو که می‌خواستی معبد را خراب و در سه روز آن را بنا کنی ، خود را نجات بده ! اگر یکی از پسران خدایی ، از تیر شکنجه پایین بیا ! \n",
            "2023-01-20 02:11:09,722 - INFO - joeynmt.training - \tReference:   ائی مبدی داغێدێب اۆچ گۆنده تیکن ! اگر آللاهێن اوغلوسانسا ، اؤزونو خلاص ائت وه چارمێخدان دۆش ! دئییردیلر . \n",
            "2023-01-20 02:11:09,722 - INFO - joeynmt.training - \tHypothesis: اونلار دئدیلر : سن مبدینی داغێتماق اۆچۆن اۆچ گۆنهدک ، اونو خلاص ائت . اگر سن آللاهێن اوغلوسان ، اؤزونو چارمێخدان دۆشن ، چارمێخدان دۆشن !\n",
            "2023-01-20 02:11:09,722 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 02:11:09,724 - INFO - joeynmt.training - \tSource:     با این حال ، این عیب را در تو می‌بینم که محبت نخستین خود را از دست داده‌ای . \n",
            "2023-01-20 02:11:09,725 - INFO - joeynmt.training - \tReference:  آمما سندن بیر شکایتیم وار : اوهلکی محبتندن ال چکمیسن . \n",
            "2023-01-20 02:11:09,725 - INFO - joeynmt.training - \tHypothesis: آمما ایندی سنین محبتین الیندهی سنه هیات وئریرم .\n",
            "2023-01-20 02:11:14,266 - INFO - joeynmt.training - Epoch 492: total training loss 492.09\n",
            "2023-01-20 02:11:14,267 - INFO - joeynmt.training - EPOCH 493\n",
            "2023-01-20 02:11:17,569 - INFO - joeynmt.training - Epoch 493, Step:   170100, Batch Loss:     1.356919, Batch Acc: 0.637224, Tokens per Sec:    13885, Lr: 0.000022\n",
            "2023-01-20 02:11:25,497 - INFO - joeynmt.training - Epoch 493, Step:   170200, Batch Loss:     1.379809, Batch Acc: 0.637016, Tokens per Sec:    13860, Lr: 0.000022\n",
            "2023-01-20 02:11:33,298 - INFO - joeynmt.training - Epoch 493, Step:   170300, Batch Loss:     1.285845, Batch Acc: 0.635083, Tokens per Sec:    14066, Lr: 0.000022\n",
            "2023-01-20 02:11:44,074 - INFO - joeynmt.training - Epoch 493, Step:   170400, Batch Loss:     1.430497, Batch Acc: 0.631638, Tokens per Sec:    10182, Lr: 0.000022\n",
            "2023-01-20 02:11:44,503 - INFO - joeynmt.training - Epoch 493: total training loss 491.43\n",
            "2023-01-20 02:11:44,504 - INFO - joeynmt.training - EPOCH 494\n",
            "2023-01-20 02:11:51,896 - INFO - joeynmt.training - Epoch 494, Step:   170500, Batch Loss:     1.349186, Batch Acc: 0.638615, Tokens per Sec:    14026, Lr: 0.000022\n",
            "2023-01-20 02:11:59,709 - INFO - joeynmt.training - Epoch 494, Step:   170600, Batch Loss:     1.446365, Batch Acc: 0.636516, Tokens per Sec:    14022, Lr: 0.000022\n",
            "2023-01-20 02:12:07,562 - INFO - joeynmt.training - Epoch 494, Step:   170700, Batch Loss:     1.544264, Batch Acc: 0.630690, Tokens per Sec:    14018, Lr: 0.000022\n",
            "2023-01-20 02:12:11,680 - INFO - joeynmt.training - Epoch 494: total training loss 490.77\n",
            "2023-01-20 02:12:11,680 - INFO - joeynmt.training - EPOCH 495\n",
            "2023-01-20 02:12:15,447 - INFO - joeynmt.training - Epoch 495, Step:   170800, Batch Loss:     1.376690, Batch Acc: 0.636364, Tokens per Sec:    14190, Lr: 0.000022\n",
            "2023-01-20 02:12:23,226 - INFO - joeynmt.training - Epoch 495, Step:   170900, Batch Loss:     1.446844, Batch Acc: 0.635618, Tokens per Sec:    14055, Lr: 0.000022\n",
            "2023-01-20 02:12:31,038 - INFO - joeynmt.training - Epoch 495, Step:   171000, Batch Loss:     1.407343, Batch Acc: 0.633479, Tokens per Sec:    14196, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.19ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10257.72ex/s]\n",
            "2023-01-20 02:12:31,311 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=171000\n",
            "2023-01-20 02:12:31,311 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:12:35,964 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:12:35,964 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:12:35,965 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:12:35,965 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:12:35,968 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.52, loss:   2.91, ppl:  18.32, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6138[sec], evaluation: 0.0365[sec]\n",
            "2023-01-20 02:12:35,971 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 02:12:35,975 - INFO - joeynmt.training - \tSource:     و این ، قوم‌ عاد بود که آیات پروردگارشان را انکار کردند ، و فرستادگانش را نافرمانی نمودند ، و به دنبال فرمان هر زورگوی ستیزه‌جوی رفتند . \n",
            "2023-01-20 02:12:35,975 - INFO - joeynmt.training - \tReference:  بو دا آد تایفاسیدیر ! اونلار ربینین آیهلرینی اینکار ائتدیلر ، اونون پیغمبرلرینه قارشێ چێخدێلار ، باشلارێنێن اۆستۆنده دوران هر بیر اینادکار بؤیوگون امرینه تابع اولدولار . \n",
            "2023-01-20 02:12:35,975 - INFO - joeynmt.training - \tHypothesis: آد تایفاسینین آیهلرینی اینکار ائتدیلر . پیغمبرلری وه پیغمبرلری ایتاعت ائتدیکلری ، ایتاااالا گئتمیشلر .\n",
            "2023-01-20 02:12:35,975 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 02:12:35,977 - INFO - joeynmt.training - \tSource:     همچنین ایشان هماهنگ با آنچه در شریعت یهوه گفته شده است ، قربانی گذراندند ؛ یعنی یک جفت قمری یا دو جوجه‌کبوتر . \n",
            "2023-01-20 02:12:35,978 - INFO - joeynmt.training - \tReference:  همچنین اونلار قوربان تقدیم ائتدی ، نئجه کی ربین قانونوندا امر اولونوب : ایکتی قومرو قوشو یاخود ایکی گؤیرچین بالاسێ . \n",
            "2023-01-20 02:12:35,978 - INFO - joeynmt.training - \tHypothesis: ربین قانونونا مۆعجود اولان بو سؤزلر قوربانێ ، ینی ده ایکی یاخود ایکی جۆت جۆزاملێ بیر جۆزاملێ وه یاخود اۆچده بیر جۆتله باشلاییر .\n",
            "2023-01-20 02:12:35,978 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 02:12:35,980 - INFO - joeynmt.training - \tSource:     پطرس به او گفت : ما همه چیز خود را رها کرده‌ایم و از تو پیروی می‌کنیم . \n",
            "2023-01-20 02:12:35,980 - INFO - joeynmt.training - \tReference:  پئتئر ایسه دئدی : باخ بیز هر شئیی قویوب سنین آردێنجا گلمیشیک . \n",
            "2023-01-20 02:12:35,981 - INFO - joeynmt.training - \tHypothesis: پئتئر اونا دئدی : هر شئیی قویوب سنه هر شئیی قویوب تابع ائدهجهییک .\n",
            "2023-01-20 02:12:35,981 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 02:12:35,983 - INFO - joeynmt.training - \tSource:     و کسانی که از گناهان بزرگ و زشتکاریها خود را به دور می‌دارند و چون به خشم درمی‌آیند درمی‌گذرند . \n",
            "2023-01-20 02:12:35,984 - INFO - joeynmt.training - \tReference:  او کسلر اۆچۆن کی ، بؤیوک گۆناهلاردان رزیل ایشلردن چکینر ، قزبلندیکلری زامان باغیشلایارلار ؛ \n",
            "2023-01-20 02:12:35,984 - INFO - joeynmt.training - \tHypothesis: گۆناهلارێندان وه بؤیوک گۆناهلارێ قێسقانجلێقلا اوزاقلاشێب جانێنێ قێجێرتێب قورتارماق اۆچۆن گلهجکلر .\n",
            "2023-01-20 02:12:43,642 - INFO - joeynmt.training - Epoch 495: total training loss 488.87\n",
            "2023-01-20 02:12:43,642 - INFO - joeynmt.training - EPOCH 496\n",
            "2023-01-20 02:12:43,877 - INFO - joeynmt.training - Epoch 496, Step:   171100, Batch Loss:     1.439821, Batch Acc: 0.620183, Tokens per Sec:    13080, Lr: 0.000022\n",
            "2023-01-20 02:12:51,609 - INFO - joeynmt.training - Epoch 496, Step:   171200, Batch Loss:     1.416952, Batch Acc: 0.634577, Tokens per Sec:    14216, Lr: 0.000022\n",
            "2023-01-20 02:12:59,401 - INFO - joeynmt.training - Epoch 496, Step:   171300, Batch Loss:     1.456139, Batch Acc: 0.634645, Tokens per Sec:    14043, Lr: 0.000022\n",
            "2023-01-20 02:13:07,347 - INFO - joeynmt.training - Epoch 496, Step:   171400, Batch Loss:     1.415637, Batch Acc: 0.633513, Tokens per Sec:    13945, Lr: 0.000022\n",
            "2023-01-20 02:13:10,741 - INFO - joeynmt.training - Epoch 496: total training loss 488.60\n",
            "2023-01-20 02:13:10,741 - INFO - joeynmt.training - EPOCH 497\n",
            "2023-01-20 02:13:15,206 - INFO - joeynmt.training - Epoch 497, Step:   171500, Batch Loss:     1.417976, Batch Acc: 0.637024, Tokens per Sec:    14398, Lr: 0.000022\n",
            "2023-01-20 02:13:22,961 - INFO - joeynmt.training - Epoch 497, Step:   171600, Batch Loss:     1.411836, Batch Acc: 0.639329, Tokens per Sec:    14197, Lr: 0.000022\n",
            "2023-01-20 02:13:30,680 - INFO - joeynmt.training - Epoch 497, Step:   171700, Batch Loss:     1.197197, Batch Acc: 0.631908, Tokens per Sec:    14212, Lr: 0.000022\n",
            "2023-01-20 02:13:37,486 - INFO - joeynmt.training - Epoch 497: total training loss 487.90\n",
            "2023-01-20 02:13:37,486 - INFO - joeynmt.training - EPOCH 498\n",
            "2023-01-20 02:13:38,501 - INFO - joeynmt.training - Epoch 498, Step:   171800, Batch Loss:     1.350527, Batch Acc: 0.635508, Tokens per Sec:    14246, Lr: 0.000022\n",
            "2023-01-20 02:13:46,377 - INFO - joeynmt.training - Epoch 498, Step:   171900, Batch Loss:     1.554543, Batch Acc: 0.633441, Tokens per Sec:    14071, Lr: 0.000022\n",
            "2023-01-20 02:13:54,173 - INFO - joeynmt.training - Epoch 498, Step:   172000, Batch Loss:     1.483785, Batch Acc: 0.636820, Tokens per Sec:    14005, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.35ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8427.92ex/s]\n",
            "2023-01-20 02:13:54,474 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=172000\n",
            "2023-01-20 02:13:54,474 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:13:59,081 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:13:59,082 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:13:59,082 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:13:59,083 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:13:59,086 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.17, loss:   2.88, ppl:  17.86, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5632[sec], evaluation: 0.0408[sec]\n",
            "2023-01-20 02:13:59,089 - INFO - joeynmt.training - Example #0\n",
            "2023-01-20 02:13:59,092 - INFO - joeynmt.training - \tSource:     ای کسانی که ایمان آورده‌اید ، هرگاه با پیامبر خدا گفتگوی محرمانه می‌کنید ، پیش از گفتگوی محرمانه خود صدقه‌ای تقدیم بدارید . این کار برای شما بهتر و پاکیزه‌تر است ؛ و اگر چیزی نیافتید بدانید که خدا آمرزنده مهربان است . \n",
            "2023-01-20 02:13:59,092 - INFO - joeynmt.training - \tReference:  ائی ایمان گتیرنلر ! پیغمبرله مخفی دانیشآجاغینیز زامان بو دانێشێقدان اول صدقه وئرین . بو سیزین اۆچۆن داها خئیرلی ، داکا پاکدێر . اگر بیر شئی تاپماسانێز . چۆنکی آللاه باغیشلایاندیر ، رحم ائدندیر ! \n",
            "2023-01-20 02:13:59,092 - INFO - joeynmt.training - \tHypothesis: ائی ایمان گتیرنلر ! آللاهێن وه پیغمبرینه مۆقابلنده دانێشێن . آللاهدان قورخمایین ، سیزه کاش کی ، یاخشی ایشلر گؤرنلرین آراسێندا بیر خئیرلی اولار . اگر سیزه هئچ بیر خئیری یوخدور . اگر آللاه ایستهسه ، اونلارێ تمیزله بیلمز . حقیقتا ، آللاه باغیشلایاندیر ، رحم ائدندیر !\n",
            "2023-01-20 02:13:59,093 - INFO - joeynmt.training - Example #1\n",
            "2023-01-20 02:13:59,095 - INFO - joeynmt.training - \tSource:     دیدگانشان فرو افتاده ، غبار مذلت آنان را فرو گرفته است . این است همان روزی که به ایشان وعده داده می‌شد . \n",
            "2023-01-20 02:13:59,095 - INFO - joeynmt.training - \tReference:  اونلارێن گؤزلری زلیلجهسینه یئره دیکیلهجک ، اؤزلرینی ده ذلت بۆرویهجکدیر . بو اونلارا وه د اولونموش همین قیامت گۆنۆدۆر ! \n",
            "2023-01-20 02:13:59,095 - INFO - joeynmt.training - \tHypothesis: گؤزلری زلیلجهسینه یئره دیکدی . اونلارا وئردیگی روزی ده وئریلدی . بو ، د اولوندو .\n",
            "2023-01-20 02:13:59,095 - INFO - joeynmt.training - Example #2\n",
            "2023-01-20 02:13:59,097 - INFO - joeynmt.training - \tSource:     چرا از شما چیزی نپذیرفته‌ام ؟ آیا به این دلیل است که شما را دوست ندارم ؟ خدا می‌داند که دوستتان دارم . \n",
            "2023-01-20 02:13:59,097 - INFO - joeynmt.training - \tReference:  نیه ؟ سیزی سئومهدیگیمه گؤرهمی ؟ آللاه بیلیر کی ، سئویرم . \n",
            "2023-01-20 02:13:59,097 - INFO - joeynmt.training - \tHypothesis: نیه باخمادێمێ ؟ مگر سیزی سئومهدیگیم شئیلره گؤرهدیر . آللاه بیلیر .\n",
            "2023-01-20 02:13:59,098 - INFO - joeynmt.training - Example #3\n",
            "2023-01-20 02:13:59,100 - INFO - joeynmt.training - \tSource:     عیسی دلش به حال او سوخت . پس دستش را دراز کرد ، آن مرد را لمس نمود و به او گفت : می‌خواهم . پاک شو . \n",
            "2023-01-20 02:13:59,100 - INFO - joeynmt.training - \tReference:  ایسنانین اونا رحمی گلدی . الینی اوزادێب اونا توخوندو وه دئدی : ایستهییرهم ، پاک اول ! \n",
            "2023-01-20 02:13:59,100 - INFO - joeynmt.training - \tHypothesis: ایسا اونون اۆرهیینه قێزدێرمادێ . او آدام اونو الینه توخوندو وه دئدی : من پاک اولاجاغام !\n",
            "2023-01-20 02:14:07,024 - INFO - joeynmt.training - Epoch 498, Step:   172100, Batch Loss:     1.523133, Batch Acc: 0.633314, Tokens per Sec:    13314, Lr: 0.000022\n",
            "2023-01-20 02:14:09,591 - INFO - joeynmt.training - Epoch 498: total training loss 488.07\n",
            "2023-01-20 02:14:09,591 - INFO - joeynmt.training - EPOCH 499\n",
            "2023-01-20 02:14:14,909 - INFO - joeynmt.training - Epoch 499, Step:   172200, Batch Loss:     1.421967, Batch Acc: 0.636763, Tokens per Sec:    14016, Lr: 0.000022\n",
            "2023-01-20 02:14:22,764 - INFO - joeynmt.training - Epoch 499, Step:   172300, Batch Loss:     1.416947, Batch Acc: 0.636477, Tokens per Sec:    14107, Lr: 0.000022\n",
            "2023-01-20 02:14:30,605 - INFO - joeynmt.training - Epoch 499, Step:   172400, Batch Loss:     1.341109, Batch Acc: 0.635381, Tokens per Sec:    14055, Lr: 0.000022\n",
            "2023-01-20 02:14:36,665 - INFO - joeynmt.training - Epoch 499: total training loss 488.69\n",
            "2023-01-20 02:14:36,665 - INFO - joeynmt.training - EPOCH 500\n",
            "2023-01-20 02:14:38,484 - INFO - joeynmt.training - Epoch 500, Step:   172500, Batch Loss:     1.446764, Batch Acc: 0.634402, Tokens per Sec:    13863, Lr: 0.000022\n",
            "2023-01-20 02:14:46,315 - INFO - joeynmt.training - Epoch 500, Step:   172600, Batch Loss:     1.389856, Batch Acc: 0.635488, Tokens per Sec:    14123, Lr: 0.000022\n",
            "2023-01-20 02:14:54,124 - INFO - joeynmt.training - Epoch 500, Step:   172700, Batch Loss:     1.441600, Batch Acc: 0.634059, Tokens per Sec:    14060, Lr: 0.000022\n",
            "2023-01-20 02:15:01,968 - INFO - joeynmt.training - Epoch 500, Step:   172800, Batch Loss:     1.400737, Batch Acc: 0.635262, Tokens per Sec:    14202, Lr: 0.000022\n",
            "2023-01-20 02:15:03,646 - INFO - joeynmt.training - Epoch 500: total training loss 486.42\n",
            "2023-01-20 02:15:03,646 - INFO - joeynmt.training - Training ended after 500 epochs.\n",
            "2023-01-20 02:15:03,646 - INFO - joeynmt.training - Best validation result (greedy) at step    42000:   2.63 loss.\n",
            "2023-01-20 02:15:03,659 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2023-01-20 02:15:03,740 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2023-01-20 02:15:03,742 - INFO - joeynmt.model - Total params: 4199424\n",
            "2023-01-20 02:15:03,888 - INFO - joeynmt.helpers - Load model from /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/model/42000.ckpt.\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.76ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10426.75ex/s]\n",
            "2023-01-20 02:15:04,173 - INFO - joeynmt.prediction - Decoding on dev set...\n",
            "2023-01-20 02:15:04,173 - INFO - joeynmt.prediction - Predicting 1497 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:16:03,030 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:16:03,030 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:16:03,030 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:16:03,036 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:16:03,060 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:   7.98, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 58.5723[sec], evaluation: 0.2763[sec]\n",
            "2023-01-20 02:16:03,067 - INFO - joeynmt.prediction - Translations saved to: /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/model/00042000.hyps.dev.\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 116.21ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8852.42ex/s]\n",
            "2023-01-20 02:16:03,350 - INFO - joeynmt.prediction - Decoding on test set...\n",
            "2023-01-20 02:16:03,350 - INFO - joeynmt.prediction - Predicting 1497 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:17:00,107 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:17:00,108 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:17:00,108 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:17:00,114 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:17:00,144 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:   8.07, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 56.4903[sec], evaluation: 0.2661[sec]\n",
            "2023-01-20 02:17:00,151 - INFO - joeynmt.prediction - Translations saved to: /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/model/00042000.hyps.test.\n"
          ]
        }
      ],
      "source": [
        "!python3 -m joeynmt train {data_dir}/config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7qULQu0B5Yl"
      },
      "source": [
        "## Continue training after interruption\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr3mEDga2BiJ"
      },
      "outputs": [],
      "source": [
        "resume_config = config\\\n",
        "  .replace('#load_model:', 'load_model:')\\\n",
        "  .replace('#reset_best_ckpt: False', 'reset_best_ckpt: False')\\\n",
        "  .replace('#reset_scheduler: False', 'reset_scheduler: False')\\\n",
        "  .replace('#reset_optimizer: False', 'reset_optimizer: False')\\\n",
        "  .replace('#reset_iter_state: False', 'reset_iter_state: False')\\\n",
        "  .replace(f'model_dir: \"{model_dir}\"', f'model_dir: \"{model_dir}_resume\"')\n",
        "\n",
        "with (Path(data_dir) / \"resume_config.yaml\").open('w') as f:\n",
        "    f.write(resume_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgH1vAsV2Bkw"
      },
      "outputs": [],
      "source": [
        "!python3 -m joeynmt train {data_dir}/resume_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVv1ja0eCk66"
      },
      "source": [
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "k5T0OEp22BnX",
        "outputId": "a3260e51-91b2-4955-f617-b33f0fc06467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-20 02:17:04,348 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
            "2023-01-20 02:17:04,349 - INFO - joeynmt.data - Building tokenizer...\n",
            "2023-01-20 02:17:04,360 - INFO - joeynmt.tokenizers - fa tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
            "2023-01-20 02:17:04,360 - INFO - joeynmt.tokenizers - azb tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
            "2023-01-20 02:17:04,360 - INFO - joeynmt.data - Building vocabulary...\n",
            "2023-01-20 02:17:04,405 - INFO - joeynmt.data - Loading dev set...\n",
            "2023-01-20 02:17:04,538 - INFO - numexpr.utils - NumExpr defaulting to 2 threads.\n",
            "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
            "  warnings.warn(\n",
            "2023-01-20 02:17:05,089 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/data/validation/cache-7e4ae8d39229e840.arrow\n",
            "2023-01-20 02:17:05,124 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/data/validation/cache-7f5ec06696ff8003.arrow\n",
            "2023-01-20 02:17:05,130 - INFO - joeynmt.data - Loading test set...\n",
            "2023-01-20 02:17:05,174 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/data/test/cache-a5aba5939ea0b7c5.arrow\n",
            "2023-01-20 02:17:05,213 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/data/test/cache-8fcb0902b40e23a9.arrow\n",
            "2023-01-20 02:17:05,216 - INFO - joeynmt.data - Data loaded.\n",
            "2023-01-20 02:17:05,217 - INFO - joeynmt.helpers - Train dataset: None\n",
            "2023-01-20 02:17:05,217 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1497, src_lang=fa, trg_lang=azb, has_trg=True, random_subset=200, split=validation)\n",
            "2023-01-20 02:17:05,217 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1497, src_lang=fa, trg_lang=azb, has_trg=True, random_subset=-1, split=test)\n",
            "2023-01-20 02:17:05,217 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁، (5) ی (6) ▁. (7) ه (8) ا (9) ▁و\n",
            "2023-01-20 02:17:05,217 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁، (5) ی (6) ▁. (7) ه (8) ا (9) ▁و\n",
            "2023-01-20 02:17:05,218 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 2000\n",
            "2023-01-20 02:17:05,218 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 2000\n",
            "2023-01-20 02:17:05,218 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2023-01-20 02:17:05,299 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2023-01-20 02:17:05,300 - INFO - joeynmt.model - Total params: 4199424\n",
            "2023-01-20 02:17:06,963 - INFO - joeynmt.helpers - Load model from /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/model/42000.ckpt.\n",
            "2023-01-20 02:17:07,027 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/data/validation/cache-f9ef727b6e49e348.arrow\n",
            "2023-01-20 02:17:07,060 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/data/validation/cache-d81a8e5072135b81.arrow\n",
            "2023-01-20 02:17:07,063 - INFO - joeynmt.prediction - Decoding on dev set...\n",
            "2023-01-20 02:17:07,063 - INFO - joeynmt.prediction - Predicting 1497 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:18:03,054 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:18:03,054 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:18:03,054 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:18:03,059 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:18:03,081 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:   7.98, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 55.7224[sec], evaluation: 0.2564[sec]\n",
            "2023-01-20 02:18:03,124 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/data/test/cache-5b9184ce7fc27494.arrow\n",
            "2023-01-20 02:18:03,158 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1miI8mnwvODEHiRbyHFmXEJoRDTk2ep4L/1401Bahar-azb2fa-translation/RESULTS_fa2azb/data/test/cache-2006f690a44aa70c.arrow\n",
            "2023-01-20 02:18:03,161 - INFO - joeynmt.prediction - Decoding on test set...\n",
            "2023-01-20 02:18:03,161 - INFO - joeynmt.prediction - Predicting 1497 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-20 02:18:59,115 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-20 02:18:59,116 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-20 02:18:59,116 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-20 02:18:59,121 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-20 02:18:59,143 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:   8.07, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 55.6987[sec], evaluation: 0.2462[sec]\n"
          ]
        }
      ],
      "source": [
        "!python3 -m joeynmt test {data_dir}/config.yaml --ckpt {model_dir}/best.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEcyEwpS1Pvi"
      },
      "outputs": [],
      "source": [
        "!python3 -m joeynmt translate {data_dir}/config.yaml --ckpt {model_dir}/best.ckpt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "121f09611d5b4737b3cb8fb465d84599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4bfa21cdecc14374a1bdf01554fff916",
              "IPY_MODEL_cc26dbb86aa8456e9ef5e515263b5811",
              "IPY_MODEL_7bd0582440244b8e8ffb70ed7e9c94b9"
            ],
            "layout": "IPY_MODEL_db20d278b31546888f19c07ef80b92cd"
          }
        },
        "4bfa21cdecc14374a1bdf01554fff916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a13430b6a83c428cb767ae9ed8c2da3d",
            "placeholder": "​",
            "style": "IPY_MODEL_4e92eea5d0174995b62c2cc24cd06c3b",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "cc26dbb86aa8456e9ef5e515263b5811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ab3dcfa2f5c44c783f661bc716bde1d",
            "max": 11978,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b29ebb14d2ec409b9d1ec8702d915d9e",
            "value": 11978
          }
        },
        "7bd0582440244b8e8ffb70ed7e9c94b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53bd855d8d5e42afbf9e79acdaaf5c50",
            "placeholder": "​",
            "style": "IPY_MODEL_546a204f976148278ba0dc2c1d1df958",
            "value": " 11978/11978 [00:00&lt;00:00, 150721.44 examples/s]"
          }
        },
        "db20d278b31546888f19c07ef80b92cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a13430b6a83c428cb767ae9ed8c2da3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e92eea5d0174995b62c2cc24cd06c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ab3dcfa2f5c44c783f661bc716bde1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b29ebb14d2ec409b9d1ec8702d915d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53bd855d8d5e42afbf9e79acdaaf5c50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "546a204f976148278ba0dc2c1d1df958": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "103312696eb14c95a81a212cb00f6c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1422d0dd9e34f7b997ea28e35022444",
              "IPY_MODEL_1a886e5736ac42f895a4bd3f8431522f",
              "IPY_MODEL_c53c622071d548be85e10903e30a1b6d"
            ],
            "layout": "IPY_MODEL_ba1309e170fd4fc69031656b3a9faef8"
          }
        },
        "e1422d0dd9e34f7b997ea28e35022444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0a1fa06c81942b18d7f3d9e9f304990",
            "placeholder": "​",
            "style": "IPY_MODEL_bdda6420da584a62ae4ac1a6e180e338",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "1a886e5736ac42f895a4bd3f8431522f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_818d390f94044bccb7183b98fd428f8a",
            "max": 1497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_882a29f154c042e8813e7f74953ab527",
            "value": 1497
          }
        },
        "c53c622071d548be85e10903e30a1b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b777ab0b704c9cbd35f3ba79b33a33",
            "placeholder": "​",
            "style": "IPY_MODEL_6481faf34a8e4a5faa64a4ad48af95da",
            "value": " 1497/1497 [00:00&lt;00:00, 43639.35 examples/s]"
          }
        },
        "ba1309e170fd4fc69031656b3a9faef8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "c0a1fa06c81942b18d7f3d9e9f304990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdda6420da584a62ae4ac1a6e180e338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "818d390f94044bccb7183b98fd428f8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "882a29f154c042e8813e7f74953ab527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44b777ab0b704c9cbd35f3ba79b33a33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6481faf34a8e4a5faa64a4ad48af95da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a452dcf3cf794d4ca39b26610f614cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f41c6f9c2ce74f6c98e0651f195a00d2",
              "IPY_MODEL_33f3bad294294e04b3c02c857a6483f5",
              "IPY_MODEL_c9c4b5f8ac1c4faf8fd21b17d9dd2a0c"
            ],
            "layout": "IPY_MODEL_ca40d9400f00406db1bcd03b287bc63e"
          }
        },
        "f41c6f9c2ce74f6c98e0651f195a00d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6b8e53cd8f845eea5bf3b48e2758258",
            "placeholder": "​",
            "style": "IPY_MODEL_d52d9c92b8804004af8d8dd6b66275fd",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "33f3bad294294e04b3c02c857a6483f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a227d83015a94b65ba1f6ba90204b676",
            "max": 1497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8072b31667614e71aa5b47751abb6478",
            "value": 1497
          }
        },
        "c9c4b5f8ac1c4faf8fd21b17d9dd2a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a046a5f1c344fba909d0654200d012e",
            "placeholder": "​",
            "style": "IPY_MODEL_943ac4e671994f789bb7413e4dea10bc",
            "value": " 1497/1497 [00:00&lt;00:00, 40891.66 examples/s]"
          }
        },
        "ca40d9400f00406db1bcd03b287bc63e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "c6b8e53cd8f845eea5bf3b48e2758258": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d52d9c92b8804004af8d8dd6b66275fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a227d83015a94b65ba1f6ba90204b676": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8072b31667614e71aa5b47751abb6478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a046a5f1c344fba909d0654200d012e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "943ac4e671994f789bb7413e4dea10bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
