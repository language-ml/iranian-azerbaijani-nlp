{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marzinouri/AzeriPipeline/blob/main/Notebooks/MachineTranslation/Translation_azb2fa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k051L-Y3rTu"
      },
      "source": [
        "Acknowledgment: This portion of the code is based on the work available at [JoeyNMT](https://github.com/joeynmt/joeynmt).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRRatWnCZFdd",
        "outputId": "affacd0d-8151-4ae9-ed0c-735c142a7f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Azari/translation"
      ],
      "metadata": {
        "id": "x-YYYitPmhZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites"
      ],
      "metadata": {
        "id": "w0vXZLx9fdLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/drive/MyDrive/Azari/translation\""
      ],
      "metadata": {
        "id": "kPQ3tbJ-mfaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "K7ZOEq3znIGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pQwoOS-OvMLf",
        "outputId": "3f287994-65b0-485d-9023-3e21df58b01f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJBa6lx26Hdx"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def load_data_to_df(path):\n",
        "    \"\"\"\n",
        "    Load dataset\n",
        "    \"\"\"\n",
        "    lines = Path(path).open(encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
        "\n",
        "    data = {\n",
        "        \"id\": range(len(lines)),\n",
        "    }\n",
        "\n",
        "    #load data into a DataFrame object:\n",
        "    df = pd.DataFrame(data)\n",
        "    sents = []\n",
        "    for i, l in enumerate(lines):\n",
        "        lang1, lang2 = l.split(\"\\t\")\n",
        "        sents.append({\"azb\": lang1, \"fa\": lang2})\n",
        "    random.shuffle(sents)\n",
        "    df[\"translation\"] = sents\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "1ZzzL9FF_UXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1p32slBTthm"
      },
      "outputs": [],
      "source": [
        "df = load_data_to_df(\"/content/drive/MyDrive/Azari/Preprocessed_Datasets/Bilingual/ALL_v2.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTPhy7M8vWw0",
        "outputId": "4cb16b4b-4017-403e-c19a-5f3fb4c1c943"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['id', 'translation'],\n",
              "     num_rows: 11978\n",
              " }), Dataset({\n",
              "     features: ['id', 'translation'],\n",
              "     num_rows: 1497\n",
              " }), Dataset({\n",
              "     features: ['id', 'translation'],\n",
              "     num_rows: 1497\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_frac = 0.8\n",
        "dev_frac = 0.1\n",
        "test_frac = 0.1\n",
        "\n",
        "total = len(df)\n",
        "train_num = int(train_frac * len(df))\n",
        "dev_num = int(dev_frac * len(df))\n",
        "\n",
        "data_train = Dataset.from_pandas(df[:train_num+1])\n",
        "data_dev = Dataset.from_pandas(df[train_num+1:train_num+1+dev_num])\n",
        "data_test = Dataset.from_pandas(df[train_num+1+dev_num:])\n",
        "\n",
        "data_train, data_dev, data_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mibraaK86JP0"
      },
      "source": [
        "Inspect the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr_YM040Tthp",
        "outputId": "f1d5d387-2436-4a3d-f70e-871fecd074f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'azb': 'اونا یئشآیا پیغمبرین کیتابینی وئردیلر . ایسا تومارێ آچێب بو سؤزلر یازیلان هیسسهنی تاپدێ : ',\n",
              "  'fa': 'طومار اشعْیای نبی به او داده شد ، او طومار را گشود و بخشی را یافت که چنین نوشته شده است : '},\n",
              " {'azb': 'وه اؤزو ده جهنهمه آتیلاجاقدیر . ',\n",
              "  'fa': 'و فرجامش\\u200c درافتادن به جهنم است . '},\n",
              " {'azb': 'البته عيلتی ده بودور کی', 'fa': 'البته علتش هم اين است كه '}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data_train['translation'][:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUFSJnWYvWzq",
        "outputId": "ddf13bb5-6fec-4aec-aef9-fcbf637123e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'azb': 'بئلهجه یهودا بیر آلای اسگرله باشچێ کاهنلرین وه فاریسئیلرین گؤندردیگی بزی مۆحافظهچیلری گؤتوروب چێراقلار ، مشلر وه سلاحلارلا اورایا گلدی . ',\n",
              "  'fa': 'پس یهودا گروهی از سربازان و مأموران سران کاهنان و فریسیان را با خود همراه کرد و آنان با مشعل و چراغ و سلاح به آنجا رسیدند . '},\n",
              " {'azb': 'دئدیم : ائله بیل یاتمیشدین آییلدین هه ! ',\n",
              "  'fa': 'گفتم\\u200c : خوابت\\u200c پرید ! '},\n",
              " {'azb': 'بیز کیتابدان سونرا زبوردا دا تورپاغا یالنیز منیم سالئه بندهلریمین داخل اولاجاغێنێ یازمیشدیق . ',\n",
              "  'fa': 'و در حقیقت ، در زبور پس از تورات نوشتیم که زمین را بندگان شایسته ما به ارث خواهند برد . '}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data_dev['translation'][:3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test['translation'][:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECqj2TRfAjHX",
        "outputId": "9da600eb-2043-47c0-d885-005a3031884e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'azb': 'ائشیتدیک کی ، بیزلردن بزی آداملار یانینیزا گلیب سؤزلری ایله سیزی لرزهیه سالاراق دۆشۆنجهلرینیزی قارێشدێرێبلار . لاکین بونو اونلارا بیز تاپشێرمامێشدێق . ',\n",
              "  'fa': 'شنیده\\u200cایم که بعضی از میان ما ، هرچند که ما به آنان حکم نکرده بودیم ، شما را با سخنان خود مضطرب ساخته\\u200cاند و در تلاش بوده\\u200cاند ذهنتان را آشفته سازند . '},\n",
              " {'azb': 'آمما اؤز دینینی آرالارێندا پارچالاییب فرقه فرقه اولدولار . هر فرقه اؤز دینینه سئوینیر ',\n",
              "  'fa': 'تا کار دین\\u200c شان را میان خود قطعه قطعه کردند و دسته دسته شدند : هر دسته\\u200cای به آنچه نزدشان بود ، دل خوش کردند . '},\n",
              " {'azb': 'گۆندهلیک چؤرهییمیزی بیزه بو گۆن وئر . ',\n",
              "  'fa': 'نان روزانهٔ ما را امروز به ما عطا کن . '}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlCBTXOH6KSw"
      },
      "source": [
        "Save the train-dev-test splits in local dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz97rpCkvW7w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "036b9b7baa9e4e17b2af8bb6c7085f26",
            "e3ff0ecbce794a4da8b5f6f59f0084bb",
            "9bb410da850d4f3fa4f4acff023df102",
            "ac5d345c7ee646efb47b460fd4b946fe",
            "be72bda1ce9c463aa523934407aaf9b4",
            "3ce517ae36a64d279dcc2cfc6ffbdf9c",
            "39476d0a1c194b8da9a6650b3e0a3ac4",
            "ef5f6024ae5f4ff4b4e3176a50d1c22e",
            "e480deda15e04e808e5cfdd0a8622251",
            "e925ea4ebbf040a4ade80f6a21be1862",
            "c73cc904e62146ec86b07c4375d09db6",
            "21b5636aeb4c4e34ad6d204b6b612583",
            "8043dc1ff02a4481bc528a1a6f8ff072",
            "2d52b1faf61f4d3b9c520050c0a488de",
            "6b62d7c0d7f84736b73b2d5f36aad769",
            "75e49c5551c7438d83371a87375b6d73",
            "a765fbff31a1466c84aad17dc5d47d50",
            "b5d9eff48e66421f96006061f26ddb9b",
            "fc851c481b7045e18847b34f0df2d466",
            "9fdf7ee5ad924f1aa6fe34281699d1e3",
            "9e7d6734845a4b35a56a9cd44d21a1ea",
            "240f2c21ce7e46f6bb7df482b83cbbe0",
            "1b96b2c1679a45318ae3cce2d4b7108a",
            "8c05bf4480034e5fa8d74dfd5d42b57f",
            "5f0b346485f44b83a88bf2d2be9a07d4",
            "6976b922730e40179b892b5a4a293840",
            "ba70e67ee73749608d49e0b6ae94b1d0",
            "85b9b1bd34334f278350aa113e446183",
            "3418e0c3414f4cce897067215325be54",
            "e060a4d4aed349618959b9cb434f6a0b",
            "fd9241dc0f0e4e248d6076d347e6e253",
            "898e70ccdb8848818ef8b6d6821b6fb4",
            "f69412e8a2f54c22adbf57dfa33794ec"
          ]
        },
        "outputId": "ef301384-8270-4320-96cb-25457f2165ed"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/11978 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "036b9b7baa9e4e17b2af8bb6c7085f26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1497 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21b5636aeb4c4e34ad6d204b6b612583"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1497 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b96b2c1679a45318ae3cce2d4b7108a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets.dataset_dict import DatasetDict\n",
        "\n",
        "dataset_dict = DatasetDict({\n",
        "  \"train\": data_train,\n",
        "  \"validation\": data_dev,\n",
        "  \"test\": data_test\n",
        "})\n",
        "\n",
        "data_dir = \"RESULTS_azb2fa/data\"\n",
        "dataset_dict.save_to_disk(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1RMfXeT-V1m"
      },
      "source": [
        "## Build Vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = 'azb'\n",
        "trg = 'fa'"
      ],
      "metadata": {
        "id": "5wY9pGMekRjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJML2jYR1PlG"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create the config\n",
        "config = f\"\"\"\n",
        "name: \"data_sp\"\n",
        "joeynmt_version: \"2.0.0\"\n",
        "\n",
        "data:\n",
        "    train: \"{data_dir}/train\"\n",
        "    dev: \"{data_dir}/validation\"\n",
        "    test: \"{data_dir}/test\"\n",
        "    dataset_type: \"huggingface\"\n",
        "    sample_dev_subset: 200\n",
        "    src:\n",
        "        lang: \"{src}\"\n",
        "        max_length: 100\n",
        "        lowercase: False\n",
        "        normalize: False\n",
        "        level: \"bpe\"\n",
        "        voc_limit: 2000\n",
        "        voc_min_freq: 1\n",
        "        voc_file: \"{data_dir}/vocab.txt\"\n",
        "        tokenizer_type: \"sentencepiece\"\n",
        "        tokenizer_cfg:\n",
        "            model_file: \"{data_dir}/sp.model\"\n",
        "\n",
        "    trg:\n",
        "        lang: \"{trg}\"\n",
        "        max_length: 100\n",
        "        lowercase: False\n",
        "        normalize: False\n",
        "        level: \"bpe\"\n",
        "        voc_limit: 2000\n",
        "        voc_min_freq: 1\n",
        "        voc_file: \"{data_dir}/vocab.txt\"\n",
        "        tokenizer_type: \"sentencepiece\"\n",
        "        tokenizer_cfg:\n",
        "            model_file: \"{data_dir}/sp.model\"\n",
        "\n",
        "\"\"\".format(data_dir=data_dir)\n",
        "with (Path(data_dir) / \"config.yaml\").open('w') as f:\n",
        "    f.write(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EMkbAmPz1Pnx",
        "outputId": "568e013b-bf33-4a19-eb9b-af3832d784ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
            "  warnings.warn(\n",
            "Dropping NaN...: 100% 12/12 [00:00<00:00, 112.32ba/s]\n",
            "Preprocessing...: 100% 11978/11978 [00:01<00:00, 10812.58ex/s]\n",
            "### Training sentencepiece...\n",
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/tmp/sentencepiece_umr_8188.txt --model_prefix=RESULTS/data/sp --model_type=unigram --vocab_size=2000 --character_coverage=1.0 --accept_language=azb,fa --unk_piece=<unk> --bos_piece=<s> --eos_piece=</s> --pad_piece=<pad> --unk_id=0 --bos_id=2 --eos_id=3 --pad_id=1 --vocabulary_output_piece_score=false\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: /tmp/sentencepiece_umr_8188.txt\n",
            "  input_format: \n",
            "  model_prefix: RESULTS/data/sp\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 2000\n",
            "  accept_language: azb\n",
            "  accept_language: fa\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 1\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 0\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 2\n",
            "  eos_id: 3\n",
            "  pad_id: 1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(181) LOG(INFO) Loading corpus: /tmp/sentencepiece_umr_8188.txt\n",
            "trainer_interface.cc(406) LOG(INFO) Loaded all 23956 sentences\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <pad>\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(536) LOG(INFO) all chars count=2630293\n",
            "trainer_interface.cc(557) LOG(INFO) Alphabet size=67\n",
            "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 23956 sentences.\n",
            "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
            "unigram_model_trainer.cc(201) LOG(INFO) Initialized 72314 seed sentencepieces\n",
            "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 23956\n",
            "trainer_interface.cc(607) LOG(INFO) Done! 38166\n",
            "unigram_model_trainer.cc(491) LOG(INFO) Using 38166 sentences for EM training\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=25918 obj=10.0248 num_tokens=66708 num_tokens/piece=2.57381\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=20870 obj=8.50938 num_tokens=67358 num_tokens/piece=3.2275\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=15646 obj=8.45861 num_tokens=71367 num_tokens/piece=4.56136\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=15627 obj=8.42387 num_tokens=71513 num_tokens/piece=4.57625\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=11720 obj=8.50191 num_tokens=78263 num_tokens/piece=6.67773\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=11720 obj=8.47385 num_tokens=78429 num_tokens/piece=6.69189\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=8790 obj=8.59031 num_tokens=85496 num_tokens/piece=9.72651\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=8790 obj=8.5585 num_tokens=85477 num_tokens/piece=9.72435\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=6592 obj=8.71571 num_tokens=92974 num_tokens/piece=14.1041\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=6592 obj=8.67856 num_tokens=92940 num_tokens/piece=14.0989\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=4944 obj=8.87261 num_tokens=100260 num_tokens/piece=20.2791\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=4944 obj=8.827 num_tokens=100230 num_tokens/piece=20.2731\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=3708 obj=9.06084 num_tokens=107921 num_tokens/piece=29.1049\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=3708 obj=9.00735 num_tokens=107907 num_tokens/piece=29.1011\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2781 obj=9.27944 num_tokens=115910 num_tokens/piece=41.6793\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2781 obj=9.21734 num_tokens=115903 num_tokens/piece=41.6767\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2200 obj=9.46212 num_tokens=122258 num_tokens/piece=55.5718\n",
            "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2200 obj=9.40818 num_tokens=122252 num_tokens/piece=55.5691\n",
            "trainer_interface.cc(685) LOG(INFO) Saving model: RESULTS/data/sp.model\n",
            "trainer_interface.cc(697) LOG(INFO) Saving vocabs: RESULTS/data/sp.vocab\n",
            "### Copying RESULTS/data/sp.vocab to RESULTS/data/vocab.txt ...\n",
            "### Done.\n"
          ]
        }
      ],
      "source": [
        "!python3 scripts/build_vocab.py {data_dir}/config.yaml --joint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN1n1gTKARR7",
        "outputId": "2f575c4a-ad12-4491-e4c6-536e0c94a92f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk>\n",
            "<pad>\n",
            "<s>\n",
            "</s>\n",
            "▁،\n",
            "ی\n",
            "▁.\n",
            "ه\n",
            "ا\n",
            "▁و\n"
          ]
        }
      ],
      "source": [
        "!head -10 {data_dir}/vocab.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6cN9CPtAaPl"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJcLOr_S2BTD"
      },
      "outputs": [],
      "source": [
        "model_dir = \"RESULTS_azb2fa/model\"\n",
        "config += \"\"\"\n",
        "testing:\n",
        "    n_best: 1\n",
        "    beam_size: 5\n",
        "    beam_alpha: 1.0\n",
        "    batch_size: 512\n",
        "    batch_type: \"token\"\n",
        "    max_output_length: 100\n",
        "    eval_metrics: [\"bleu\"]\n",
        "    #return_prob: \"hyp\"\n",
        "    #return_attention: False\n",
        "    sacrebleu_cfg:\n",
        "        tokenize: \"13a\"\n",
        "\n",
        "training:\n",
        "    #load_model: \"{model_dir}/latest.ckpt\"\n",
        "    #reset_best_ckpt: False\n",
        "    #reset_scheduler: False\n",
        "    #reset_optimizer: False\n",
        "    #reset_iter_state: False\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999]\n",
        "    scheduling: \"warmupinversesquareroot\"\n",
        "    learning_rate_warmup: 2000\n",
        "    learning_rate: 0.0002\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    loss: \"crossentropy\"\n",
        "    batch_size: 512\n",
        "    batch_type: \"token\"\n",
        "    batch_multiplier: 4\n",
        "    early_stopping_metric: \"bleu\"\n",
        "    epochs: 500\n",
        "    updates: 2000000000\n",
        "    validation_freq: 1000\n",
        "    logging_freq: 100\n",
        "    model_dir: \"{model_dir}\"\n",
        "    overwrite: True\n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_best_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 2\n",
        "        num_heads: 4\n",
        "        embeddings:\n",
        "            embedding_dim: 256\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256\n",
        "        ff_size: 1024\n",
        "        dropout: 0.1\n",
        "        layer_norm: \"pre\"\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 2\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 256\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256\n",
        "        ff_size: 1024\n",
        "        dropout: 0.1\n",
        "        layer_norm: \"pre\"\n",
        "\n",
        "\"\"\".format(model_dir=model_dir)\n",
        "with (Path(data_dir) / \"config.yaml\").open('w') as f:\n",
        "    f.write(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w45HbBfeMW38"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTbfgVOq2BfB",
        "outputId": "55ce8feb-ede8-4294-a15f-ba1432a1651d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2023-01-19 12:43:22,473 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:43:22,473 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:43:22,474 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:43:22,477 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.83, loss:   2.67, ppl:  14.38, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9234[sec], evaluation: 0.0436[sec]\n",
            "2023-01-19 12:43:22,480 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:43:22,483 - INFO - joeynmt.training - \tSource:     چۆنکی هکمت وه بیلیگین بۆتۆن خزینهلری مصیحده گیزلهدیلمیشدیر . \n",
            "2023-01-19 12:43:22,483 - INFO - joeynmt.training - \tReference:  در او همهٔ گنج‌های حکمت و شناخت نهفته است . \n",
            "2023-01-19 12:43:22,483 - INFO - joeynmt.training - \tHypothesis: زیرا حکمت و حکمت و حکمت و تمامی مایهٔ مسیح به همهٔ آنان داده شده است .\n",
            "2023-01-19 12:43:22,483 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:43:22,485 - INFO - joeynmt.training - \tSource:     او ، سؤزونه بئله داوام ائتدی : سیزه دئییرم : کیمین وارێدێرسا ، داها چوخ وئریلهجک ، کیمین یوخودورسا ، الینده اولان دا آلێناجاق . \n",
            "2023-01-19 12:43:22,486 - INFO - joeynmt.training - \tReference:  در جواب گفت : به شما می‌گویم ، به هر که دارد ، بیشتر داده می‌شود ، اما آن که ندارد ، حتی آنچه دارد هم از او گرفته خواهد شد . \n",
            "2023-01-19 12:43:22,486 - INFO - joeynmt.training - \tHypothesis: او سخن می گوید : من به شما می گویم ، اما مانند کسی است که ندارد ، بلکه آنچه دارد دارد ، همچون آن که گرفته خواهد شد ، اما آن که ندارد ، حتی آنچه دارد ، خواهد گرفت .\n",
            "2023-01-19 12:43:22,486 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:43:22,488 - INFO - joeynmt.training - \tSource:      بویوردو : بیر آزدان پئشمان اولاجاقلار ! \n",
            "2023-01-19 12:43:22,488 - INFO - joeynmt.training - \tReference:  فرمود : به زودی سخت پشیمان خواهند شد . \n",
            "2023-01-19 12:43:22,488 - INFO - joeynmt.training - \tHypothesis: فرمود : از آن ، از جمله در آتشی دورد .\n",
            "2023-01-19 12:43:22,488 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:43:22,490 - INFO - joeynmt.training - \tSource:     بوحبی یئیه ر سن کی تئز تئز یئرینه ایشه مه سن . \n",
            "2023-01-19 12:43:22,490 - INFO - joeynmt.training - \tReference:  این قرصو که بخوری دیگه الکی نمی‌شاشی . \n",
            "2023-01-19 12:43:22,491 - INFO - joeynmt.training - \tHypothesis: این را به زودی به زودی می آیی .\n",
            "2023-01-19 12:43:30,156 - INFO - joeynmt.training - Epoch 153, Step:    53100, Batch Loss:     1.713200, Batch Acc: 0.525388, Tokens per Sec:    15087, Lr: 0.000039\n",
            "2023-01-19 12:43:37,860 - INFO - joeynmt.training - Epoch 153, Step:    53200, Batch Loss:     1.908661, Batch Acc: 0.528831, Tokens per Sec:    15708, Lr: 0.000039\n",
            "2023-01-19 12:43:42,546 - INFO - joeynmt.training - Epoch 153: total training loss 641.66\n",
            "2023-01-19 12:43:42,546 - INFO - joeynmt.training - EPOCH 154\n",
            "2023-01-19 12:43:45,809 - INFO - joeynmt.training - Epoch 154, Step:    53300, Batch Loss:     1.914648, Batch Acc: 0.529190, Tokens per Sec:    15524, Lr: 0.000039\n",
            "2023-01-19 12:43:53,680 - INFO - joeynmt.training - Epoch 154, Step:    53400, Batch Loss:     1.785429, Batch Acc: 0.531664, Tokens per Sec:    15435, Lr: 0.000039\n",
            "2023-01-19 12:44:01,390 - INFO - joeynmt.training - Epoch 154, Step:    53500, Batch Loss:     1.920288, Batch Acc: 0.526874, Tokens per Sec:    15601, Lr: 0.000039\n",
            "2023-01-19 12:44:09,193 - INFO - joeynmt.training - Epoch 154, Step:    53600, Batch Loss:     1.907562, Batch Acc: 0.526119, Tokens per Sec:    15485, Lr: 0.000039\n",
            "2023-01-19 12:44:09,710 - INFO - joeynmt.training - Epoch 154: total training loss 640.58\n",
            "2023-01-19 12:44:09,711 - INFO - joeynmt.training - EPOCH 155\n",
            "2023-01-19 12:44:17,000 - INFO - joeynmt.training - Epoch 155, Step:    53700, Batch Loss:     1.793841, Batch Acc: 0.530986, Tokens per Sec:    15682, Lr: 0.000039\n",
            "2023-01-19 12:44:24,702 - INFO - joeynmt.training - Epoch 155, Step:    53800, Batch Loss:     1.815589, Batch Acc: 0.529100, Tokens per Sec:    15604, Lr: 0.000039\n",
            "2023-01-19 12:44:32,344 - INFO - joeynmt.training - Epoch 155, Step:    53900, Batch Loss:     1.840495, Batch Acc: 0.527904, Tokens per Sec:    15681, Lr: 0.000039\n",
            "2023-01-19 12:44:36,532 - INFO - joeynmt.training - Epoch 155: total training loss 642.09\n",
            "2023-01-19 12:44:36,533 - INFO - joeynmt.training - EPOCH 156\n",
            "2023-01-19 12:44:39,958 - INFO - joeynmt.training - Epoch 156, Step:    54000, Batch Loss:     1.800225, Batch Acc: 0.531840, Tokens per Sec:    15754, Lr: 0.000038\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.31ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10271.21ex/s]\n",
            "2023-01-19 12:44:40,220 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=54000\n",
            "2023-01-19 12:44:40,220 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 12:44:44,760 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 12:44:44,760 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:44:44,761 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:44:44,762 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:44:44,764 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.77, loss:   2.58, ppl:  13.23, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4969[sec], evaluation: 0.0401[sec]\n",
            "2023-01-19 12:44:44,767 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:44:44,770 - INFO - joeynmt.training - \tSource:     آللاه اونو اؤلولر آراسێندان دیریلدرک هئچ واخت چۆرۆمهیه قویمایاجاق . اونا گؤره دئمیشدی : داوودا ود ائتدیگیم صادق محبتی سیزه گؤستهرهجهیم . \n",
            "2023-01-19 12:44:44,770 - INFO - joeynmt.training - \tReference:  این امر که خدا او را از مردگان رستاخیز داد و دیگر هرگز به فسادپذیری بازنمی‌گردد ، چنین بیان شده است : من به شما احسان‌هایی خواهم کرد که به داوود وعده داده شده بود . این وعده قطعی است . \n",
            "2023-01-19 12:44:44,771 - INFO - joeynmt.training - \tHypothesis: خدا او را در میان مردگان برخیزانده نخواهد شد . پس به او گفت : مطابق محبت ، داوود به شما وفادار است . او به شما می گویم ، محبت کرد که به شما وفادار می انجامد .\n",
            "2023-01-19 12:44:44,771 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:44:44,773 - INFO - joeynmt.training - \tSource:     ائرتهسی گۆن موسا دالاشان ایکی ایسرایللینی گؤرنده اونلارێ بارێشدێرماق ایستهدی وه دئدی : آی کیشیلر ، سیز سویداشسینیز ، نیه بیر بیرینیزه زرر وورورسونوز ؟ \n",
            "2023-01-19 12:44:44,773 - INFO - joeynmt.training - \tReference:  روز بعد ، دو نفر را دید که نزاع می‌کنند ، پس سعی کرد آنان را آشتی دهد و گفت : ای مردان ، شما با هم برادرید . چرا با یکدیگر چنین بدرفتاری می‌کنید ؟ \n",
            "2023-01-19 12:44:44,773 - INFO - joeynmt.training - \tHypothesis: روز بعد ، موسی به موسی گفت : شما دو مرد را می بینید و به یکدیگر می گفت : چرا با شما همچون مرده است و شما را در گلهٔ خود بدرید ؟\n",
            "2023-01-19 12:44:44,773 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:44:44,775 - INFO - joeynmt.training - \tSource:     هئچ کیم چێراغێ یاندیریب قابێن آلتێنا قویماز . اکسینه ، چێراقدانا قویار کی ، ائودهکیلرین هامیسینی ایشیقلاندیرسین . \n",
            "2023-01-19 12:44:44,775 - INFO - joeynmt.training - \tReference:  چراغ را نمی‌افروزند که زیر سرپوش قرار دهند ، بلکه آن را روی پایه می‌گذارند تا نورش بر همهٔ اهالی خانه بتابد . \n",
            "2023-01-19 12:44:44,775 - INFO - joeynmt.training - \tHypothesis: کسی که چراغی می کند ، چراغی را می کند و نمی کارد بفشد . بلکه تمام کسانی را که در خانهٔ خود می سازد ، نورزدهٔ کارگران است .\n",
            "2023-01-19 12:44:44,776 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:44:44,777 - INFO - joeynmt.training - \tSource:     بوقادین گاهدان او قدر من دن اوزاقلاشیر\n",
            "2023-01-19 12:44:44,777 - INFO - joeynmt.training - \tReference:  این زن گاهی این قدر از من دور می‌شود \n",
            "2023-01-19 12:44:44,778 - INFO - joeynmt.training - \tHypothesis: گاه مين ميد مي مين ميد مين ميددددمان را از مين ميد\n",
            "2023-01-19 12:44:52,488 - INFO - joeynmt.training - Epoch 156, Step:    54100, Batch Loss:     1.818216, Batch Acc: 0.528406, Tokens per Sec:    15213, Lr: 0.000038\n",
            "2023-01-19 12:45:00,155 - INFO - joeynmt.training - Epoch 156, Step:    54200, Batch Loss:     1.694645, Batch Acc: 0.533322, Tokens per Sec:    15744, Lr: 0.000038\n",
            "2023-01-19 12:45:07,965 - INFO - joeynmt.training - Epoch 156, Step:    54300, Batch Loss:     1.790890, Batch Acc: 0.525945, Tokens per Sec:    15396, Lr: 0.000038\n",
            "2023-01-19 12:45:08,219 - INFO - joeynmt.training - Epoch 156: total training loss 639.26\n",
            "2023-01-19 12:45:08,220 - INFO - joeynmt.training - EPOCH 157\n",
            "2023-01-19 12:45:15,853 - INFO - joeynmt.training - Epoch 157, Step:    54400, Batch Loss:     1.943411, Batch Acc: 0.532380, Tokens per Sec:    15257, Lr: 0.000038\n",
            "2023-01-19 12:45:23,528 - INFO - joeynmt.training - Epoch 157, Step:    54500, Batch Loss:     1.843923, Batch Acc: 0.528395, Tokens per Sec:    15845, Lr: 0.000038\n",
            "2023-01-19 12:45:31,151 - INFO - joeynmt.training - Epoch 157, Step:    54600, Batch Loss:     1.758651, Batch Acc: 0.528558, Tokens per Sec:    15589, Lr: 0.000038\n",
            "2023-01-19 12:45:35,101 - INFO - joeynmt.training - Epoch 157: total training loss 640.06\n",
            "2023-01-19 12:45:35,102 - INFO - joeynmt.training - EPOCH 158\n",
            "2023-01-19 12:45:38,874 - INFO - joeynmt.training - Epoch 158, Step:    54700, Batch Loss:     1.804558, Batch Acc: 0.535275, Tokens per Sec:    15473, Lr: 0.000038\n",
            "2023-01-19 12:45:46,537 - INFO - joeynmt.training - Epoch 158, Step:    54800, Batch Loss:     1.896467, Batch Acc: 0.533140, Tokens per Sec:    15955, Lr: 0.000038\n",
            "2023-01-19 12:45:54,232 - INFO - joeynmt.training - Epoch 158, Step:    54900, Batch Loss:     1.767354, Batch Acc: 0.529175, Tokens per Sec:    15655, Lr: 0.000038\n",
            "2023-01-19 12:46:01,826 - INFO - joeynmt.training - Epoch 158, Step:    55000, Batch Loss:     1.733659, Batch Acc: 0.526408, Tokens per Sec:    15710, Lr: 0.000038\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.62ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10022.00ex/s]\n",
            "2023-01-19 12:46:02,100 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=55000\n",
            "2023-01-19 12:46:02,100 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 12:46:08,299 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 12:46:08,300 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:46:08,301 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:46:08,303 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:46:08,308 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.08, loss:   2.59, ppl:  13.29, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 6.1150[sec], evaluation: 0.0793[sec]\n",
            "2023-01-19 12:46:08,311 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:46:08,314 - INFO - joeynmt.training - \tSource:      دئ : من سیزی آنجاق وحی ایله قورخودورام . کارلار قورخودولدوغو زامان چاغێرێشێ ائشیتمزلر ! \n",
            "2023-01-19 12:46:08,315 - INFO - joeynmt.training - \tReference:  بگو : من شما را فقط به وسیله وحی هشدار می‌دهم . و لی‌ چون کران بیم داده شوند ، دعوت را نمی‌شنوند . \n",
            "2023-01-19 12:46:08,315 - INFO - joeynmt.training - \tHypothesis: بگو : من شما را وحی می کنم ، و چون هشدارهای شما را بیمناکم می کنند ، و هر که را شنیدند .\n",
            "2023-01-19 12:46:08,315 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:46:08,318 - INFO - joeynmt.training - \tSource:     قارداشلار ، اینسان دۆشۆنجهسی اساسێندا دئییرم : هتا اینسانین بئله تصدیق ائتدیگی اهدی هئچ کس لغو ائده بیلمز یاخود دا اونا هئچ بیر شئی الاوه ائده بیلمز . \n",
            "2023-01-19 12:46:08,318 - INFO - joeynmt.training - \tReference:  ای برادران ، مثلی از زندگی روزمره برایتان می‌آورم : وقتی عهدی معتبر می‌شود ، حتی اگر فقط از طرف یک انسان باشد ، هیچ کس نمی‌تواند آن را لغو کند یا چیزی بر آن بیفزاید . \n",
            "2023-01-19 12:46:08,318 - INFO - joeynmt.training - \tHypothesis: ای برادران ، می گویم که هیچ کس نمی تواند به خاطر آن عمل نکند ، مگر کسی که عهد پیش از آن عهد که عهد پیش از آن داشته باشد ، به او افتخار کند .\n",
            "2023-01-19 12:46:08,318 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:46:08,321 - INFO - joeynmt.training - \tSource:     پاوللا بارنابا ایسه جصارتله دئدی : آللاهێن کلامێ اولجه سیزه بیان اولونمالێ ایدی . سیزین اوندان امتنا ائتدیگینیز وه اؤزونوزو ابدی هیاتا لایق گؤرمهدیگینیز اۆچۆن بیز ایندی باشقا ملتلره اۆز توتوروق . \n",
            "2023-01-19 12:46:08,321 - INFO - joeynmt.training - \tReference:  سپس پولس و برنابا با شهامت به آنان گفتند : لازم بود که کلام خدا نخست به شما گفته شود . حال که آن را رد می‌کنید و خود را شایستهٔ زندگی ابدی نمی‌دانید ، ما نیز آن را به غیریهودیان موعظه می‌کنیم . \n",
            "2023-01-19 12:46:08,321 - INFO - joeynmt.training - \tHypothesis: پولس به برنابا گفت : روح خدا با شما سخن می گفتم . شما در واقع ، کلام خدا که در پیشگاه شما بود ، می توانید به حیات دست یابید و به حیات جاودان یابید .\n",
            "2023-01-19 12:46:08,322 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:46:08,324 - INFO - joeynmt.training - \tSource:     چۆنکی ملکلر واسطهسیله بیان ائدیلن سؤز قۆۆهیه میندی ، هر جۆر تقصیر وه ایتااائسیزلیه حاقلێ جزا وئریلدیسه ، \n",
            "2023-01-19 12:46:08,324 - INFO - joeynmt.training - \tReference:  زیرا اگر آن پیام که از طریق فرشتگان گفته شد ، استوار می‌مانْد و مجازات هر خطا و نافرمانی مطابق عدالت می‌بود\n",
            "2023-01-19 12:46:08,325 - INFO - joeynmt.training - \tHypothesis: زیرا اگر با فرشتگان به تحقق رسیده است ، با قدرت قدرت و معاشرت به نحوی که به دست انسان داده شده بود ، به نحوی داده شدن به نحوی داده شد .\n",
            "2023-01-19 12:46:08,329 - INFO - joeynmt.training - Epoch 158: total training loss 637.24\n",
            "2023-01-19 12:46:08,330 - INFO - joeynmt.training - EPOCH 159\n",
            "2023-01-19 12:46:16,485 - INFO - joeynmt.training - Epoch 159, Step:    55100, Batch Loss:     1.804728, Batch Acc: 0.532734, Tokens per Sec:    14829, Lr: 0.000038\n",
            "2023-01-19 12:46:24,331 - INFO - joeynmt.training - Epoch 159, Step:    55200, Batch Loss:     1.687120, Batch Acc: 0.533282, Tokens per Sec:    15282, Lr: 0.000038\n",
            "2023-01-19 12:46:31,993 - INFO - joeynmt.training - Epoch 159, Step:    55300, Batch Loss:     1.746560, Batch Acc: 0.528876, Tokens per Sec:    15850, Lr: 0.000038\n",
            "2023-01-19 12:46:35,718 - INFO - joeynmt.training - Epoch 159: total training loss 634.92\n",
            "2023-01-19 12:46:35,719 - INFO - joeynmt.training - EPOCH 160\n",
            "2023-01-19 12:46:39,806 - INFO - joeynmt.training - Epoch 160, Step:    55400, Batch Loss:     1.829946, Batch Acc: 0.528894, Tokens per Sec:    15670, Lr: 0.000038\n",
            "2023-01-19 12:46:47,645 - INFO - joeynmt.training - Epoch 160, Step:    55500, Batch Loss:     1.752591, Batch Acc: 0.531257, Tokens per Sec:    15306, Lr: 0.000038\n",
            "2023-01-19 12:46:55,522 - INFO - joeynmt.training - Epoch 160, Step:    55600, Batch Loss:     1.888582, Batch Acc: 0.530221, Tokens per Sec:    15259, Lr: 0.000038\n",
            "2023-01-19 12:47:03,063 - INFO - joeynmt.training - Epoch 160: total training loss 639.49\n",
            "2023-01-19 12:47:03,063 - INFO - joeynmt.training - EPOCH 161\n",
            "2023-01-19 12:47:03,309 - INFO - joeynmt.training - Epoch 161, Step:    55700, Batch Loss:     1.787808, Batch Acc: 0.553514, Tokens per Sec:    15099, Lr: 0.000038\n",
            "2023-01-19 12:47:10,966 - INFO - joeynmt.training - Epoch 161, Step:    55800, Batch Loss:     1.867144, Batch Acc: 0.533282, Tokens per Sec:    15829, Lr: 0.000038\n",
            "2023-01-19 12:47:18,573 - INFO - joeynmt.training - Epoch 161, Step:    55900, Batch Loss:     1.733495, Batch Acc: 0.529064, Tokens per Sec:    15597, Lr: 0.000038\n",
            "2023-01-19 12:47:26,134 - INFO - joeynmt.training - Epoch 161, Step:    56000, Batch Loss:     1.871443, Batch Acc: 0.534666, Tokens per Sec:    16194, Lr: 0.000038\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 143.48ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10001.28ex/s]\n",
            "2023-01-19 12:47:26,414 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=56000\n",
            "2023-01-19 12:47:26,415 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 12:47:31,344 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 12:47:31,344 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:47:31,345 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:47:31,346 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:47:31,348 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  10.06, loss:   2.59, ppl:  13.30, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.8851[sec], evaluation: 0.0415[sec]\n",
            "2023-01-19 12:47:31,349 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
            "2023-01-19 12:47:31,555 - INFO - joeynmt.helpers - delete RESULTS/model/34000.ckpt\n",
            "2023-01-19 12:47:31,572 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:47:31,576 - INFO - joeynmt.training - \tSource:     البته بو دیل دئیه ن قادینی چوخ واخت لار حیاتیم دان سیلیره م . \n",
            "2023-01-19 12:47:31,576 - INFO - joeynmt.training - \tReference:  این زن مطرود را بیشتر اوقات حذف می‌كنم . \n",
            "2023-01-19 12:47:31,576 - INFO - joeynmt.training - \tHypothesis: البته این سخنان را به زبان های بسیار كنم می كند .\n",
            "2023-01-19 12:47:31,576 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:47:31,578 - INFO - joeynmt.training - \tSource:     یئر اؤزونه مخصوص بیر شدتله لرزهیه گلیب تیترهیهجهیی زامان ؛ \n",
            "2023-01-19 12:47:31,578 - INFO - joeynmt.training - \tReference:  آنگاه که زمین به لرزش شدید خود لرزانیده شود ، \n",
            "2023-01-19 12:47:31,579 - INFO - joeynmt.training - \tHypothesis: و چون از آن ، زمین به علاوه ای بر آن برده ای ، به سوی آن برده شوند .\n",
            "2023-01-19 12:47:31,579 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:47:31,581 - INFO - joeynmt.training - \tSource:     ابراهیم نه یهودی ، نه ده خاچپرست ایدی . او آنجاق حنیف مۆسلمان ایدی وه شریک قوشانلاردان دئییلدی . \n",
            "2023-01-19 12:47:31,581 - INFO - joeynmt.training - \tReference:  ابراهیم نه یهودی بود و نه نصرانی ، بلکه حق گرایی فرمانبردار بود ، و از مشرکان نبود . \n",
            "2023-01-19 12:47:31,581 - INFO - joeynmt.training - \tHypothesis: و یهودی و منطقهٔ یهودی بود . او از حرف و مفرف بود و از مشرکان نبود .\n",
            "2023-01-19 12:47:31,581 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:47:31,583 - INFO - joeynmt.training - \tSource:     اونلار درحال تورلارێ کنارا آتێب اونون آردێنجا گئتدیلر . \n",
            "2023-01-19 12:47:31,583 - INFO - joeynmt.training - \tReference:  آن دو همان دم تورهای خود را رها کردند و به دنبال او رفتند . \n",
            "2023-01-19 12:47:31,583 - INFO - joeynmt.training - \tHypothesis: پس آنان را به کناری انداختند و به دنبال او رفتند .\n",
            "2023-01-19 12:47:35,065 - INFO - joeynmt.training - Epoch 161: total training loss 634.08\n",
            "2023-01-19 12:47:35,065 - INFO - joeynmt.training - EPOCH 162\n",
            "2023-01-19 12:47:39,508 - INFO - joeynmt.training - Epoch 162, Step:    56100, Batch Loss:     1.899743, Batch Acc: 0.533246, Tokens per Sec:    14964, Lr: 0.000038\n",
            "2023-01-19 12:47:47,184 - INFO - joeynmt.training - Epoch 162, Step:    56200, Batch Loss:     1.725777, Batch Acc: 0.535229, Tokens per Sec:    15640, Lr: 0.000038\n",
            "2023-01-19 12:47:54,860 - INFO - joeynmt.training - Epoch 162, Step:    56300, Batch Loss:     1.659543, Batch Acc: 0.529980, Tokens per Sec:    15766, Lr: 0.000038\n",
            "2023-01-19 12:48:01,998 - INFO - joeynmt.training - Epoch 162: total training loss 632.69\n",
            "2023-01-19 12:48:01,998 - INFO - joeynmt.training - EPOCH 163\n",
            "2023-01-19 12:48:02,646 - INFO - joeynmt.training - Epoch 163, Step:    56400, Batch Loss:     1.973272, Batch Acc: 0.525925, Tokens per Sec:    14573, Lr: 0.000038\n",
            "2023-01-19 12:48:10,390 - INFO - joeynmt.training - Epoch 163, Step:    56500, Batch Loss:     1.619738, Batch Acc: 0.538964, Tokens per Sec:    15509, Lr: 0.000038\n",
            "2023-01-19 12:48:18,134 - INFO - joeynmt.training - Epoch 163, Step:    56600, Batch Loss:     1.786562, Batch Acc: 0.532138, Tokens per Sec:    15787, Lr: 0.000038\n",
            "2023-01-19 12:48:25,767 - INFO - joeynmt.training - Epoch 163, Step:    56700, Batch Loss:     1.875995, Batch Acc: 0.530762, Tokens per Sec:    15723, Lr: 0.000038\n",
            "2023-01-19 12:48:28,857 - INFO - joeynmt.training - Epoch 163: total training loss 632.94\n",
            "2023-01-19 12:48:28,858 - INFO - joeynmt.training - EPOCH 164\n",
            "2023-01-19 12:48:33,426 - INFO - joeynmt.training - Epoch 164, Step:    56800, Batch Loss:     1.731247, Batch Acc: 0.536195, Tokens per Sec:    15957, Lr: 0.000038\n",
            "2023-01-19 12:48:41,088 - INFO - joeynmt.training - Epoch 164, Step:    56900, Batch Loss:     1.846472, Batch Acc: 0.532477, Tokens per Sec:    15775, Lr: 0.000037\n",
            "2023-01-19 12:48:48,631 - INFO - joeynmt.training - Epoch 164, Step:    57000, Batch Loss:     1.740048, Batch Acc: 0.531979, Tokens per Sec:    16015, Lr: 0.000037\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.66ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10776.79ex/s]\n",
            "2023-01-19 12:48:48,904 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=57000\n",
            "2023-01-19 12:48:48,904 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 12:48:53,667 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 12:48:53,668 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:48:53,668 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:48:53,669 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:48:53,672 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.16, loss:   2.61, ppl:  13.66, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7020[sec], evaluation: 0.0588[sec]\n",
            "2023-01-19 12:48:53,679 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:48:53,682 - INFO - joeynmt.training - \tSource:     یاخود گؤیلرین ، یئرین وه اونلارێن آراسێندا اولان هر شئین اختیاری اونلارێن الیندهدیر ائله ایسه قوی ایپلردن یاپیشیب قالخسێنلار ! \n",
            "2023-01-19 12:48:53,682 - INFO - joeynmt.training - \tReference:  آیا فرمانروایی آسمانها و زمین و آنچه میان آن دو است از آن ایشان است ؟ اگر چنین است‌ پس با چنگ زدن‌ در آن اسباب به بالا روند . \n",
            "2023-01-19 12:48:53,682 - INFO - joeynmt.training - \tHypothesis: یا آنچه در آسمانها و زمین است از آن دو است ، و آنچه میان آن دو است ، از دست برمی خیزند .\n",
            "2023-01-19 12:48:53,683 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:48:53,685 - INFO - joeynmt.training - \tSource:      اونو اؤز نسلی آراسێندا همیشهلیک قالان بیر سؤز ائتدی . بلکه ، قاییدالار ! \n",
            "2023-01-19 12:48:53,685 - INFO - joeynmt.training - \tReference:  و او آن را در پی خود سخنی جاویدان کرد ، باشد که آنان به توحید بازگردند . \n",
            "2023-01-19 12:48:53,685 - INFO - joeynmt.training - \tHypothesis: و او را در میان نسل خود باقی مانده بود ، شاید پس شاید درنگ ها .\n",
            "2023-01-19 12:48:53,685 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:48:53,687 - INFO - joeynmt.training - \tSource:     گون باتا بات دا . \n",
            "2023-01-19 12:48:53,688 - INFO - joeynmt.training - \tReference:  گاهی‌ وقت‌ غروب‌ . \n",
            "2023-01-19 12:48:53,688 - INFO - joeynmt.training - \tHypothesis: قرعه كتاك .\n",
            "2023-01-19 12:48:53,688 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:48:53,690 - INFO - joeynmt.training - \tSource:     او گۆن سیز گتیریلهجکسینزینز . سیزین هئچ بیر سیررینیز گیزلی قالمایاجاقدیر ! \n",
            "2023-01-19 12:48:53,690 - INFO - joeynmt.training - \tReference:  در آن روز ، شما به پیشگاه خدا عرضه می‌شوید ، و پوشیده‌ای از شما پوشیده نمی‌ماند . \n",
            "2023-01-19 12:48:53,690 - INFO - joeynmt.training - \tHypothesis: روزی که شما را در آن روز باز می شمارید ، و هیچ شکی نیست .\n",
            "2023-01-19 12:49:00,443 - INFO - joeynmt.training - Epoch 164: total training loss 630.24\n",
            "2023-01-19 12:49:00,444 - INFO - joeynmt.training - EPOCH 165\n",
            "2023-01-19 12:49:01,469 - INFO - joeynmt.training - Epoch 165, Step:    57100, Batch Loss:     1.811963, Batch Acc: 0.547794, Tokens per Sec:    15282, Lr: 0.000037\n",
            "2023-01-19 12:49:09,140 - INFO - joeynmt.training - Epoch 165, Step:    57200, Batch Loss:     1.893355, Batch Acc: 0.534266, Tokens per Sec:    15653, Lr: 0.000037\n",
            "2023-01-19 12:49:17,662 - INFO - joeynmt.training - Epoch 165, Step:    57300, Batch Loss:     1.934430, Batch Acc: 0.536912, Tokens per Sec:    14136, Lr: 0.000037\n",
            "2023-01-19 12:49:25,961 - INFO - joeynmt.training - Epoch 165, Step:    57400, Batch Loss:     1.931797, Batch Acc: 0.529031, Tokens per Sec:    14613, Lr: 0.000037\n",
            "2023-01-19 12:49:28,681 - INFO - joeynmt.training - Epoch 165: total training loss 630.86\n",
            "2023-01-19 12:49:28,681 - INFO - joeynmt.training - EPOCH 166\n",
            "2023-01-19 12:49:33,768 - INFO - joeynmt.training - Epoch 166, Step:    57500, Batch Loss:     1.786179, Batch Acc: 0.537514, Tokens per Sec:    15404, Lr: 0.000037\n",
            "2023-01-19 12:49:41,491 - INFO - joeynmt.training - Epoch 166, Step:    57600, Batch Loss:     1.787723, Batch Acc: 0.536902, Tokens per Sec:    15537, Lr: 0.000037\n",
            "2023-01-19 12:49:49,379 - INFO - joeynmt.training - Epoch 166, Step:    57700, Batch Loss:     1.850651, Batch Acc: 0.532309, Tokens per Sec:    15441, Lr: 0.000037\n",
            "2023-01-19 12:49:55,829 - INFO - joeynmt.training - Epoch 166: total training loss 631.10\n",
            "2023-01-19 12:49:55,830 - INFO - joeynmt.training - EPOCH 167\n",
            "2023-01-19 12:49:57,173 - INFO - joeynmt.training - Epoch 167, Step:    57800, Batch Loss:     1.893033, Batch Acc: 0.540221, Tokens per Sec:    14854, Lr: 0.000037\n",
            "2023-01-19 12:50:05,032 - INFO - joeynmt.training - Epoch 167, Step:    57900, Batch Loss:     1.809244, Batch Acc: 0.532152, Tokens per Sec:    15443, Lr: 0.000037\n",
            "2023-01-19 12:50:12,818 - INFO - joeynmt.training - Epoch 167, Step:    58000, Batch Loss:     1.735882, Batch Acc: 0.537260, Tokens per Sec:    15513, Lr: 0.000037\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.83ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9675.84ex/s] \n",
            "2023-01-19 12:50:13,098 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=58000\n",
            "2023-01-19 12:50:13,098 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 12:50:18,342 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 12:50:18,343 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:50:18,343 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:50:18,344 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:50:18,347 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.79, loss:   2.63, ppl:  13.81, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1003[sec], evaluation: 0.1418[sec]\n",
            "2023-01-19 12:50:18,350 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:50:18,354 - INFO - joeynmt.training - \tSource:      قۆرئیشین اۆلفتی خاطرینه ، \n",
            "2023-01-19 12:50:18,354 - INFO - joeynmt.training - \tReference:  برای الفت‌دادن قریش ، \n",
            "2023-01-19 12:50:18,354 - INFO - joeynmt.training - \tHypothesis: بیش از غروبد ،\n",
            "2023-01-19 12:50:18,354 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:50:18,356 - INFO - joeynmt.training - \tSource:     ایبلیسین هییلهلرینه قارشێ دورا بیلمک اۆچۆن آللاهێن وئردیگی بۆتۆن زیرئه وه سلاحلارا بۆرۆنۆن . \n",
            "2023-01-19 12:50:18,356 - INFO - joeynmt.training - \tReference:  لباس کامل رزم را که از خداست بر تن کنید تا بتوانید در برابر حیله‌های ابلیس استوار بایستید ؛ \n",
            "2023-01-19 12:50:18,357 - INFO - joeynmt.training - \tHypothesis: و باید در برابر مبرید ، باید در برابر خدا هر آنچه را که در دست خداست ، به دست داده است ، به تمام آنچه خداست ، می سازند .\n",
            "2023-01-19 12:50:18,357 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:50:18,359 - INFO - joeynmt.training - \tSource:     ربینین مرحمت ائتدیگی کیمسهلر استصنادیر . اونلارێ بونون اۆچۆن یاراتمیشدیر آرتێق ربینین : من جهنمی بۆتۆن جینلر وه اینسانلارلا دولدورآجاغام ! سؤزو تامامیله یئرینه یئتدی ! \n",
            "2023-01-19 12:50:18,359 - INFO - joeynmt.training - \tReference:  مگر کسانی که پروردگار تو به آنان رحم کرده ، و برای همین آنان را آفریده است . و وعده پروردگارت چنین‌ تحقق پذیرفته است که : البته جهنم را از جن و انس یکسره پر خواهم کرد . \n",
            "2023-01-19 12:50:18,359 - INFO - joeynmt.training - \tHypothesis: و کسانی که به پروردگارشان مصر یافته اند ، این را برای آنان آفریده است . و به راستی آنان را از آتش آفریده است . و لی آنچه را که به سوی آتش می رسد به تحقق رسید ، و این کارها را به انجام داد .\n",
            "2023-01-19 12:50:18,359 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:50:18,361 - INFO - joeynmt.training - \tSource:     آمما اونلار ایسهآدان تکیدله خاهش ائدیب دئدیلر : بیزیمله قال ، چۆنکی آخشام دۆشۆر وه گۆن باتماق اۆزرهدیر . او دا اونلارلا قالماق اۆچۆن ایچری گیردی . \n",
            "2023-01-19 12:50:18,361 - INFO - joeynmt.training - \tReference:  اما آنان به او اصرار ورزیدند که بماند و گفتند : با ما بمان ؛ زیرا چیزی به پایان روز نمانده و نزدیک غروب است . سپس با آنان به خانه رفت تا نزدشان بماند . \n",
            "2023-01-19 12:50:18,361 - INFO - joeynmt.training - \tHypothesis: اما آنان به آنان گفتند : از تو خواستند و به ما خواهد گفت : غروب شوید ؛ زیرا نه با آنان داخل شود و نه در آن خانه بمانیم .\n",
            "2023-01-19 12:50:26,123 - INFO - joeynmt.training - Epoch 167, Step:    58100, Batch Loss:     1.865777, Batch Acc: 0.534703, Tokens per Sec:    15039, Lr: 0.000037\n",
            "2023-01-19 12:50:28,528 - INFO - joeynmt.training - Epoch 167: total training loss 630.22\n",
            "2023-01-19 12:50:28,528 - INFO - joeynmt.training - EPOCH 168\n",
            "2023-01-19 12:50:33,838 - INFO - joeynmt.training - Epoch 168, Step:    58200, Batch Loss:     1.781158, Batch Acc: 0.535712, Tokens per Sec:    15783, Lr: 0.000037\n",
            "2023-01-19 12:50:41,574 - INFO - joeynmt.training - Epoch 168, Step:    58300, Batch Loss:     1.763872, Batch Acc: 0.538073, Tokens per Sec:    15525, Lr: 0.000037\n",
            "2023-01-19 12:50:49,361 - INFO - joeynmt.training - Epoch 168, Step:    58400, Batch Loss:     1.751680, Batch Acc: 0.534706, Tokens per Sec:    15354, Lr: 0.000037\n",
            "2023-01-19 12:50:55,569 - INFO - joeynmt.training - Epoch 168: total training loss 630.73\n",
            "2023-01-19 12:50:55,569 - INFO - joeynmt.training - EPOCH 169\n",
            "2023-01-19 12:50:57,105 - INFO - joeynmt.training - Epoch 169, Step:    58500, Batch Loss:     1.790724, Batch Acc: 0.530499, Tokens per Sec:    15473, Lr: 0.000037\n",
            "2023-01-19 12:51:04,797 - INFO - joeynmt.training - Epoch 169, Step:    58600, Batch Loss:     1.895029, Batch Acc: 0.536474, Tokens per Sec:    15881, Lr: 0.000037\n",
            "2023-01-19 12:51:12,470 - INFO - joeynmt.training - Epoch 169, Step:    58700, Batch Loss:     1.869089, Batch Acc: 0.535302, Tokens per Sec:    15610, Lr: 0.000037\n",
            "2023-01-19 12:51:20,133 - INFO - joeynmt.training - Epoch 169, Step:    58800, Batch Loss:     1.744819, Batch Acc: 0.537736, Tokens per Sec:    15722, Lr: 0.000037\n",
            "2023-01-19 12:51:22,280 - INFO - joeynmt.training - Epoch 169: total training loss 627.02\n",
            "2023-01-19 12:51:22,281 - INFO - joeynmt.training - EPOCH 170\n",
            "2023-01-19 12:51:27,773 - INFO - joeynmt.training - Epoch 170, Step:    58900, Batch Loss:     1.865358, Batch Acc: 0.537971, Tokens per Sec:    15851, Lr: 0.000037\n",
            "2023-01-19 12:51:35,511 - INFO - joeynmt.training - Epoch 170, Step:    59000, Batch Loss:     1.861942, Batch Acc: 0.537239, Tokens per Sec:    15439, Lr: 0.000037\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.68ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9986.84ex/s]\n",
            "2023-01-19 12:51:35,782 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=59000\n",
            "2023-01-19 12:51:35,782 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 12:51:41,357 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 12:51:41,358 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:51:41,358 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:51:41,359 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:51:41,362 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.03, loss:   2.81, ppl:  16.68, acc:   0.40, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.5279[sec], evaluation: 0.0444[sec]\n",
            "2023-01-19 12:51:41,365 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:51:41,368 - INFO - joeynmt.training - \tSource:     آللاه ایسنانی کفاره قوربانێ اولاراق تقدیم ائتدی کی ، ایمان ائدنلرین گۆناهلارێ اونون قانێ واسطهسیله باغیشلانسین . بونو اؤز سالئهلیگینی گؤسترمک اۆچۆن ائتدی . چۆنکی سبیرلی اولوب قاباقجا ائدیلن گۆناهلارێ جزاسێز قویموشدو . \n",
            "2023-01-19 12:51:41,368 - INFO - joeynmt.training - \tReference:  خدا مسیح را همچون قربانی کفاره تقدیم کرد تا انسان‌ها را از طریق ایمان به خون او با خود آشتی دهد . او با انجام دادن این کار می‌خواست عدالت خود را نشان دهد ؛ زیرا به سبب شکیبایی خود گناهانی را که در گذشته انجام می‌شد ، می‌بخشید . \n",
            "2023-01-19 12:51:41,369 - INFO - joeynmt.training - \tHypothesis: خدا از طریق عیسی قربانی های گناه کرد تا به کسانی که ایمان آورده اند ، گناهانش را بخشیده است تا خون خون او را بردارد ؛ زیرا گناهانش آشکار ساخته است . آری ، آنچه در پی گناه پیشه شده بود ، گناه را به تحقق رساند .\n",
            "2023-01-19 12:51:41,369 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:51:41,371 - INFO - joeynmt.training - \tSource:     بو ، همین موسادێر کی ، اونو ردد ائدیب دئمیشدیلر : سنی بیزیم اۆستۆمۆزه کیم باشچێ وه حاکم قویوب ؟ آمما آللاه موسانێ بیر کول آراسێندا گؤرۆنن ملهین الی ایله باشچێ وه قورتارێجێ اولاراق گؤندردی . \n",
            "2023-01-19 12:51:41,371 - INFO - joeynmt.training - \tReference:  آری ، خدا همین موسی را که آنان رد کرده ، به او گفته بودند : چه کسی تو را حاکم و داور ساخته است ، توسط فرشته‌ای که در بوته بر او ظاهر شد ، فرستاد تا هم حاکم و هم رهاننده باشد . \n",
            "2023-01-19 12:51:41,371 - INFO - joeynmt.training - \tHypothesis: و موسی به او گفت : این همان کسی است که تو را رد کرده است ، و می گویند : آیا در واقع ، خدا تو را از میان ما بیرون آورده است ؟\n",
            "2023-01-19 12:51:41,371 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:51:41,373 - INFO - joeynmt.training - \tSource:     هامێ سیناقوق رعیسی سوستئنی توتوب هؤکم کۆرسۆسۆنۆن قارشیسیندا دؤیدو . قاللیو ایسه بو حادثهلره احمیت وئرمهدی . \n",
            "2023-01-19 12:51:41,373 - INFO - joeynmt.training - \tReference:  پس همگی ، سوستنیس ، مسئول کنیسه را گرفتند و او را در مقابل مسند داوری زدند . اما گالیو به این امور کاملا بی‌اعتنا بود . \n",
            "2023-01-19 12:51:41,374 - INFO - joeynmt.training - \tHypothesis: همهٔ شاگردان در کنیسه ای زدند و در مقابل مسند داوری نشسته بود . اما در مقابل مسند داوری نشست و به این ترتیب اجازه ندادند .\n",
            "2023-01-19 12:51:41,376 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:51:41,379 - INFO - joeynmt.training - \tSource:     خوش بیر سؤز وه گۆناهلارێ باغێشلاماق ازییتله وئریلن سدهقدن داها یاخشیدیر . آللاه احتیاجسیزدیر ، هلیمدیر ! \n",
            "2023-01-19 12:51:41,379 - INFO - joeynmt.training - \tReference:  گفتاری پسندیده در برابر نیازمندان‌ و گذشت از اصرار و تندی آنان‌ بهتر از صدقه‌ای است که آزاری به دنبال آن باشد ، و خداوند بی‌نیاز بردبار است . \n",
            "2023-01-19 12:51:41,379 - INFO - joeynmt.training - \tHypothesis: و هر که را خوشگذران و گناهانش بخشیده شود ، به زودی خدا نیاز ستاره دارد .\n",
            "2023-01-19 12:51:49,060 - INFO - joeynmt.training - Epoch 170, Step:    59100, Batch Loss:     1.999007, Batch Acc: 0.535372, Tokens per Sec:    15208, Lr: 0.000037\n",
            "2023-01-19 12:51:55,023 - INFO - joeynmt.training - Epoch 170: total training loss 626.93\n",
            "2023-01-19 12:51:55,023 - INFO - joeynmt.training - EPOCH 171\n",
            "2023-01-19 12:51:56,896 - INFO - joeynmt.training - Epoch 171, Step:    59200, Batch Loss:     1.809410, Batch Acc: 0.539622, Tokens per Sec:    15869, Lr: 0.000037\n",
            "2023-01-19 12:52:04,741 - INFO - joeynmt.training - Epoch 171, Step:    59300, Batch Loss:     1.772394, Batch Acc: 0.538734, Tokens per Sec:    15447, Lr: 0.000037\n",
            "2023-01-19 12:52:12,542 - INFO - joeynmt.training - Epoch 171, Step:    59400, Batch Loss:     1.802671, Batch Acc: 0.537267, Tokens per Sec:    15425, Lr: 0.000037\n",
            "2023-01-19 12:52:20,297 - INFO - joeynmt.training - Epoch 171, Step:    59500, Batch Loss:     1.768430, Batch Acc: 0.536145, Tokens per Sec:    15584, Lr: 0.000037\n",
            "2023-01-19 12:52:22,156 - INFO - joeynmt.training - Epoch 171: total training loss 626.26\n",
            "2023-01-19 12:52:22,156 - INFO - joeynmt.training - EPOCH 172\n",
            "2023-01-19 12:52:29,287 - INFO - joeynmt.training - Epoch 172, Step:    59600, Batch Loss:     1.907625, Batch Acc: 0.538066, Tokens per Sec:    12841, Lr: 0.000037\n",
            "2023-01-19 12:52:37,040 - INFO - joeynmt.training - Epoch 172, Step:    59700, Batch Loss:     1.838714, Batch Acc: 0.539848, Tokens per Sec:    15605, Lr: 0.000037\n",
            "2023-01-19 12:52:44,952 - INFO - joeynmt.training - Epoch 172, Step:    59800, Batch Loss:     1.740892, Batch Acc: 0.537999, Tokens per Sec:    15230, Lr: 0.000037\n",
            "2023-01-19 12:52:50,537 - INFO - joeynmt.training - Epoch 172: total training loss 625.31\n",
            "2023-01-19 12:52:50,537 - INFO - joeynmt.training - EPOCH 173\n",
            "2023-01-19 12:52:52,723 - INFO - joeynmt.training - Epoch 173, Step:    59900, Batch Loss:     1.823832, Batch Acc: 0.540845, Tokens per Sec:    15563, Lr: 0.000037\n",
            "2023-01-19 12:53:00,309 - INFO - joeynmt.training - Epoch 173, Step:    60000, Batch Loss:     1.782564, Batch Acc: 0.541880, Tokens per Sec:    15920, Lr: 0.000037\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 146.94ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10484.41ex/s]\n",
            "2023-01-19 12:53:00,583 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=60000\n",
            "2023-01-19 12:53:00,584 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 12:53:06,286 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 12:53:06,287 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:53:06,287 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:53:06,288 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:53:06,291 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.45, loss:   2.70, ppl:  14.94, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.6558[sec], evaluation: 0.0436[sec]\n",
            "2023-01-19 12:53:06,293 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:53:06,297 - INFO - joeynmt.training - \tSource:     بونلار اؤزلرینی قادێنلارلا لکهلهمهمیش شخصلردیر ، چۆنکی باکردیرلر . قوزو هارا گئتسه ، اونلار دا اونون آردێنجا گئدیر . اونلار آللاه وه قوزو اۆچۆن نۆبار اولاراق اینسانلار آراسێندان ساتێن آلێندێ . \n",
            "2023-01-19 12:53:06,297 - INFO - joeynmt.training - \tReference:  اینان همان کسانی هستند که خود را با زنان آلوده نکردند ، آری باکره‌اند . اینان همان کسانی هستند که هر جا بره می‌رود ، همچنان او را دنبال می‌کنند و از میان انسان‌ها خریده شدند تا به عنوان نوبر به خدا و بره تقدیم شوند . \n",
            "2023-01-19 12:53:06,297 - INFO - joeynmt.training - \tHypothesis: اینانند که زنانشان به زنان جهاد می گویند ؛ زیرا آنان به شما وعده داده شده است . اما هنگامی که به دنبال آن بروند ، بره ای که از آن منافقان او هست ، برهٔ خود و برهٔ انسان ها .\n",
            "2023-01-19 12:53:06,297 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:53:06,299 - INFO - joeynmt.training - \tSource:     ائلهجه ده سمود ، لوت قؤومو ، ایکه اهلی بو فرقهلرین\n",
            "2023-01-19 12:53:06,299 - INFO - joeynmt.training - \tReference:  و ثمود و قوم لوط و اصحاب ایکه نیز به تکذیب پرداختند آنها دسته‌های مخالف بودند . \n",
            "2023-01-19 12:53:06,300 - INFO - joeynmt.training - \tHypothesis: و قوم من و قوم لوط و قوم لوط را که می دانند .\n",
            "2023-01-19 12:53:06,300 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:53:06,302 - INFO - joeynmt.training - \tSource:     بدهنینین چێراغێ گؤزوندور . گؤزۆن ساغلام اولاندا بۆتۆن بدهنین ده نورلو اولور . گؤزۆن ظعیف گؤرنده ایسه بۆتۆن بدهنین ده قارانلێق اولور . \n",
            "2023-01-19 12:53:06,302 - INFO - joeynmt.training - \tReference:  چشم تو ، چراغ بدنت است . اگر چشمت متمرکز باشد ، تمام وجودت روشن خواهد بود . اما اگر چشمت پر از حسد باشد ، وجودت نیز تاریک خواهد بود . \n",
            "2023-01-19 12:53:06,302 - INFO - joeynmt.training - \tHypothesis: چشمت چشم است . اما چشمت پر از چشمت است و هر که چشمت تاریکی تاریکی است ، نور را در تاریکی می کند ، اما تمام بدن خود را در تاریکی می بیند ، تاریکی می بیند .\n",
            "2023-01-19 12:53:06,302 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:53:06,304 - INFO - joeynmt.training - \tSource:     اونلارێ نلر ساردێ ، نلر ! \n",
            "2023-01-19 12:53:06,304 - INFO - joeynmt.training - \tReference:  پوشاند بر آن دو شهر ، از باران گوگردی‌ آنچه را پوشاند . \n",
            "2023-01-19 12:53:06,304 - INFO - joeynmt.training - \tHypothesis: و آنها را در سینه ها نگردند .\n",
            "2023-01-19 12:53:14,128 - INFO - joeynmt.training - Epoch 173, Step:    60100, Batch Loss:     1.742610, Batch Acc: 0.537751, Tokens per Sec:    15027, Lr: 0.000036\n",
            "2023-01-19 12:53:21,882 - INFO - joeynmt.training - Epoch 173, Step:    60200, Batch Loss:     1.761463, Batch Acc: 0.537101, Tokens per Sec:    15570, Lr: 0.000036\n",
            "2023-01-19 12:53:23,367 - INFO - joeynmt.training - Epoch 173: total training loss 622.82\n",
            "2023-01-19 12:53:23,368 - INFO - joeynmt.training - EPOCH 174\n",
            "2023-01-19 12:53:29,666 - INFO - joeynmt.training - Epoch 174, Step:    60300, Batch Loss:     1.757061, Batch Acc: 0.541095, Tokens per Sec:    15389, Lr: 0.000036\n",
            "2023-01-19 12:53:37,456 - INFO - joeynmt.training - Epoch 174, Step:    60400, Batch Loss:     1.835695, Batch Acc: 0.540220, Tokens per Sec:    15380, Lr: 0.000036\n",
            "2023-01-19 12:53:45,193 - INFO - joeynmt.training - Epoch 174, Step:    60500, Batch Loss:     1.897320, Batch Acc: 0.540045, Tokens per Sec:    15595, Lr: 0.000036\n",
            "2023-01-19 12:53:50,416 - INFO - joeynmt.training - Epoch 174: total training loss 625.04\n",
            "2023-01-19 12:53:50,416 - INFO - joeynmt.training - EPOCH 175\n",
            "2023-01-19 12:53:52,874 - INFO - joeynmt.training - Epoch 175, Step:    60600, Batch Loss:     1.825402, Batch Acc: 0.544283, Tokens per Sec:    15760, Lr: 0.000036\n",
            "2023-01-19 12:54:00,569 - INFO - joeynmt.training - Epoch 175, Step:    60700, Batch Loss:     1.997658, Batch Acc: 0.543349, Tokens per Sec:    15514, Lr: 0.000036\n",
            "2023-01-19 12:54:08,354 - INFO - joeynmt.training - Epoch 175, Step:    60800, Batch Loss:     1.749363, Batch Acc: 0.538760, Tokens per Sec:    15595, Lr: 0.000036\n",
            "2023-01-19 12:54:16,049 - INFO - joeynmt.training - Epoch 175, Step:    60900, Batch Loss:     1.876513, Batch Acc: 0.538201, Tokens per Sec:    15647, Lr: 0.000036\n",
            "2023-01-19 12:54:17,379 - INFO - joeynmt.training - Epoch 175: total training loss 624.65\n",
            "2023-01-19 12:54:17,379 - INFO - joeynmt.training - EPOCH 176\n",
            "2023-01-19 12:54:23,848 - INFO - joeynmt.training - Epoch 176, Step:    61000, Batch Loss:     1.674037, Batch Acc: 0.540542, Tokens per Sec:    15570, Lr: 0.000036\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 122.67ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9446.13ex/s]\n",
            "2023-01-19 12:54:24,133 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=61000\n",
            "2023-01-19 12:54:24,133 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 12:54:28,459 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 12:54:28,459 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:54:28,459 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:54:28,460 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:54:28,463 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.89, loss:   2.63, ppl:  13.86, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2824[sec], evaluation: 0.0406[sec]\n",
            "2023-01-19 12:54:28,655 - INFO - joeynmt.helpers - delete RESULTS/model/48000.ckpt\n",
            "2023-01-19 12:54:28,669 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:54:28,672 - INFO - joeynmt.training - \tSource:     بئلهجه ده رب مۆژدنی ائلان ائدنلره مۆژدن دولانماغێ امر ائتمیشدیر . \n",
            "2023-01-19 12:54:28,672 - INFO - joeynmt.training - \tReference:  به همین‌سان ، سرور در خصوص کسانی که بشارت را اعلام می‌کنند ، فرمان داد که روزی خود را از طریق بشارت دریافت کنند . \n",
            "2023-01-19 12:54:28,672 - INFO - joeynmt.training - \tHypothesis: پس به کسانی که بشارت را اعلام کرده اند ، به بشارت داده اند که به آنان فرمان داده اند .\n",
            "2023-01-19 12:54:28,673 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:54:28,675 - INFO - joeynmt.training - \tSource:      باخ یئروسهلیمه قالخێرێق ، بشر اوغلو باشچێ کاهنلره وه الاهییاتچیلارا تسلیم اولوناجاق . اونو اؤلومه محکوم ائدیب باشقا ملتلره تسلیم ائدهجکلر . \n",
            "2023-01-19 12:54:28,675 - INFO - joeynmt.training - \tReference:   اکنون به اورشلیم می‌رویم . در آنجا پسر انسان به سران کاهنان و علمای دین سپرده خواهد شد . آنان او را به مرگ محکوم خواهند کرد و به دست غیریهودیان تحویل خواهند داد ، \n",
            "2023-01-19 12:54:28,684 - INFO - joeynmt.training - \tHypothesis: بنگر ! او به اورشلیم می روییم و در مقابل سران کاهنان ، پسر انسان را به دست دشمنان تسلیم خواهند کرد و به قوم ها محکوم خواهند کرد .\n",
            "2023-01-19 12:54:28,684 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:54:28,687 - INFO - joeynmt.training - \tSource:     لاکین اونلار یالانچی سایدیلار وه اونو توتوب کسدیلر . ربی ده بو گۆناهلارێنا گؤره اونلارێن کؤکونو کسیب یئرله یئکسان ائتدی . \n",
            "2023-01-19 12:54:28,688 - INFO - joeynmt.training - \tReference:  و لی‌ دروغزنش خواندند و آن ماده‌شتر را پی کردند ، و پروردگارشان به سزای‌ گناهشان بر سرشان عذاب آورد و آنان را با خاک یکسان کرد . \n",
            "2023-01-19 12:54:28,688 - INFO - joeynmt.training - \tHypothesis: اما آنان دروغین بودند و او را گرفتند و او را به گناهش بخشیدند ، پس در نتیجه گناهانشان ریشه کن و ریشه در آن ریشهٔ آن ها را بخوری .\n",
            "2023-01-19 12:54:28,688 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:54:28,691 - INFO - joeynmt.training - \tSource:     کؤرپهنین آتاسێندان کؤرپهیه نه آد قویولماسینی اشاره ایله سوروشدولار . \n",
            "2023-01-19 12:54:28,691 - INFO - joeynmt.training - \tReference:  پس با اشاره از پدر نوزاد سؤال کردند که می‌خواهد او را چه بنامد . \n",
            "2023-01-19 12:54:28,691 - INFO - joeynmt.training - \tHypothesis: آنان از پدرش به نام پدر خود ، نه به نام یعقوب و نه به نام او گفتند .\n",
            "2023-01-19 12:54:36,772 - INFO - joeynmt.training - Epoch 176, Step:    61100, Batch Loss:     1.762902, Batch Acc: 0.537847, Tokens per Sec:    13998, Lr: 0.000036\n",
            "2023-01-19 12:54:44,566 - INFO - joeynmt.training - Epoch 176, Step:    61200, Batch Loss:     1.803584, Batch Acc: 0.539335, Tokens per Sec:    15605, Lr: 0.000036\n",
            "2023-01-19 12:54:49,632 - INFO - joeynmt.training - Epoch 176: total training loss 620.93\n",
            "2023-01-19 12:54:49,632 - INFO - joeynmt.training - EPOCH 177\n",
            "2023-01-19 12:54:52,462 - INFO - joeynmt.training - Epoch 177, Step:    61300, Batch Loss:     1.853946, Batch Acc: 0.536732, Tokens per Sec:    15232, Lr: 0.000036\n",
            "2023-01-19 12:55:00,189 - INFO - joeynmt.training - Epoch 177, Step:    61400, Batch Loss:     1.866852, Batch Acc: 0.540215, Tokens per Sec:    15655, Lr: 0.000036\n",
            "2023-01-19 12:55:08,020 - INFO - joeynmt.training - Epoch 177, Step:    61500, Batch Loss:     1.822309, Batch Acc: 0.541009, Tokens per Sec:    15460, Lr: 0.000036\n",
            "2023-01-19 12:55:15,838 - INFO - joeynmt.training - Epoch 177, Step:    61600, Batch Loss:     1.884077, Batch Acc: 0.542313, Tokens per Sec:    15297, Lr: 0.000036\n",
            "2023-01-19 12:55:16,879 - INFO - joeynmt.training - Epoch 177: total training loss 622.80\n",
            "2023-01-19 12:55:16,879 - INFO - joeynmt.training - EPOCH 178\n",
            "2023-01-19 12:55:23,565 - INFO - joeynmt.training - Epoch 178, Step:    61700, Batch Loss:     1.697206, Batch Acc: 0.542202, Tokens per Sec:    15609, Lr: 0.000036\n",
            "2023-01-19 12:55:31,195 - INFO - joeynmt.training - Epoch 178, Step:    61800, Batch Loss:     1.775846, Batch Acc: 0.538598, Tokens per Sec:    16104, Lr: 0.000036\n",
            "2023-01-19 12:55:40,267 - INFO - joeynmt.training - Epoch 178, Step:    61900, Batch Loss:     1.881692, Batch Acc: 0.540041, Tokens per Sec:    13279, Lr: 0.000036\n",
            "2023-01-19 12:55:44,968 - INFO - joeynmt.training - Epoch 178: total training loss 620.96\n",
            "2023-01-19 12:55:44,968 - INFO - joeynmt.training - EPOCH 179\n",
            "2023-01-19 12:55:47,980 - INFO - joeynmt.training - Epoch 179, Step:    62000, Batch Loss:     1.814841, Batch Acc: 0.546891, Tokens per Sec:    15906, Lr: 0.000036\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.98ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9604.62ex/s]\n",
            "2023-01-19 12:55:48,262 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=62000\n",
            "2023-01-19 12:55:48,262 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 12:55:52,833 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 12:55:52,833 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:55:52,833 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:55:52,834 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:55:52,837 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.24, loss:   2.70, ppl:  14.95, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5213[sec], evaluation: 0.0459[sec]\n",
            "2023-01-19 12:55:52,840 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:55:52,843 - INFO - joeynmt.training - \tSource:     منیم یانیما گلن ، سؤزلریمی ائشیدیب اونلارا امل ائدن هر آدامێن کیمه بنزهدیگینی سیزه اضاح ائدرم : \n",
            "2023-01-19 12:55:52,844 - INFO - joeynmt.training - \tReference:  هر که نزد من آید و سخنانم را بشنود و به آن عمل کند ، به شما می‌گویم مانند چه کسی است : \n",
            "2023-01-19 12:55:52,844 - INFO - joeynmt.training - \tHypothesis: وقتی من به شما گوش فرا می رسد ، سخنان من هر کس را بشنود ، باید هر که را یهوه می گوید : به شما باز می دارد .\n",
            "2023-01-19 12:55:52,844 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:55:52,846 - INFO - joeynmt.training - \tSource:     اؤز جانێنێ سئون کس اونو ایتیرر ، بو دۆنیادا جانێ اۆچۆن قایغی چکمهینسه اونو ابدی هیات اۆچۆن قورویار . \n",
            "2023-01-19 12:55:52,846 - INFO - joeynmt.training - \tReference:  کسی که جان خود را دوست بدارد ، آن را تباه خواهد ساخت ، اما کسی که در این دنیا از جان خود نفرت داشته باشد ، آن را برای زندگی جاودان حفظ خواهد کرد . \n",
            "2023-01-19 12:55:52,847 - INFO - joeynmt.training - \tHypothesis: هر که جان خود را دوست دارد ، او را به این دنیا محبت کند ، برای زندگی جاودان خواهد کرد . اما اگر کسی که جان خود را برای زندگی جاودان می کند ، برای زندگی جاودان خواهد کرد .\n",
            "2023-01-19 12:55:52,847 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:55:52,849 - INFO - joeynmt.training - \tSource:     ائینی ترزده کیشیلر ده قادێنلا اولان تبیعی الاقهدن ال چکیب بیر بیرلهرینه شهوتله قێزدێلار ؛ کیشیلر کیشیلرله رۆسۆایچیلێق ائدیب اؤز داخللرینده رزیللیکلرینه لایق جزانێ آلدێلار . \n",
            "2023-01-19 12:55:52,849 - INFO - joeynmt.training - \tReference:  به همین شکل ، مردان آمیزش طبیعی با زنان را ترک کردند و در آتش شهوتی که نسبت به یکدیگر داشتند سوختند ، مرد با مرد مرتکب اعمال قبیح شده ، سزای رفتار زشتشان را به طور کامل در خود دیدند . \n",
            "2023-01-19 12:55:52,849 - INFO - joeynmt.training - \tHypothesis: با این که شوهران ، با زنی که شوهرش در دست داشتند ، دستش را با دست هایش را با طلایییعهٔ خود را به دختران خود کشیدند .\n",
            "2023-01-19 12:55:52,849 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:55:52,851 - INFO - joeynmt.training - \tSource:     ملکلر بارهسینده ایسه دئییر : اؤز ملکلرینی کۆلکلره ، اؤز خدمتچیلرینی یانار اودا دؤندهریر ! \n",
            "2023-01-19 12:55:52,851 - INFO - joeynmt.training - \tReference:  همچنین دربارهٔ فرشتگان می‌گوید : او فرشتگان خود را چون باد پرقدرت می‌سازد و خادمان خود را همچون شعله‌های آتش . \n",
            "2023-01-19 12:55:52,852 - INFO - joeynmt.training - \tHypothesis: فرشتگان در مورد فرشتگان می گوید : باد فرشتگان خود را در افکار خود نگاه می دارد .\n",
            "2023-01-19 12:56:00,656 - INFO - joeynmt.training - Epoch 179, Step:    62100, Batch Loss:     1.820618, Batch Acc: 0.545111, Tokens per Sec:    14980, Lr: 0.000036\n",
            "2023-01-19 12:56:08,503 - INFO - joeynmt.training - Epoch 179, Step:    62200, Batch Loss:     1.781380, Batch Acc: 0.538479, Tokens per Sec:    15314, Lr: 0.000036\n",
            "2023-01-19 12:56:16,296 - INFO - joeynmt.training - Epoch 179, Step:    62300, Batch Loss:     1.750160, Batch Acc: 0.542315, Tokens per Sec:    15320, Lr: 0.000036\n",
            "2023-01-19 12:56:17,082 - INFO - joeynmt.training - Epoch 179: total training loss 621.86\n",
            "2023-01-19 12:56:17,086 - INFO - joeynmt.training - EPOCH 180\n",
            "2023-01-19 12:56:24,032 - INFO - joeynmt.training - Epoch 180, Step:    62400, Batch Loss:     1.613421, Batch Acc: 0.543624, Tokens per Sec:    15665, Lr: 0.000036\n",
            "2023-01-19 12:56:31,779 - INFO - joeynmt.training - Epoch 180, Step:    62500, Batch Loss:     1.705564, Batch Acc: 0.543387, Tokens per Sec:    15636, Lr: 0.000036\n",
            "2023-01-19 12:56:39,563 - INFO - joeynmt.training - Epoch 180, Step:    62600, Batch Loss:     1.769346, Batch Acc: 0.540370, Tokens per Sec:    15310, Lr: 0.000036\n",
            "2023-01-19 12:56:44,202 - INFO - joeynmt.training - Epoch 180: total training loss 619.69\n",
            "2023-01-19 12:56:44,202 - INFO - joeynmt.training - EPOCH 181\n",
            "2023-01-19 12:56:47,479 - INFO - joeynmt.training - Epoch 181, Step:    62700, Batch Loss:     1.855004, Batch Acc: 0.541289, Tokens per Sec:    15497, Lr: 0.000036\n",
            "2023-01-19 12:56:55,160 - INFO - joeynmt.training - Epoch 181, Step:    62800, Batch Loss:     1.845543, Batch Acc: 0.542958, Tokens per Sec:    15798, Lr: 0.000036\n",
            "2023-01-19 12:57:02,751 - INFO - joeynmt.training - Epoch 181, Step:    62900, Batch Loss:     1.821498, Batch Acc: 0.541724, Tokens per Sec:    16001, Lr: 0.000036\n",
            "2023-01-19 12:57:10,588 - INFO - joeynmt.training - Epoch 181, Step:    63000, Batch Loss:     1.799297, Batch Acc: 0.539629, Tokens per Sec:    15331, Lr: 0.000036\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 141.55ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9718.22ex/s] \n",
            "2023-01-19 12:57:10,865 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=63000\n",
            "2023-01-19 12:57:10,865 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 12:57:15,445 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 12:57:15,445 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:57:15,445 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:57:15,447 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:57:15,450 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.93, loss:   2.55, ppl:  12.84, acc:   0.45, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5328[sec], evaluation: 0.0449[sec]\n",
            "2023-01-19 12:57:15,453 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:57:15,457 - INFO - joeynmt.training - \tSource:     اونلارێن هر ایکیسی تسلیم اولدوغو وه اۆزۆسته یئره یێخدیغی زامان\n",
            "2023-01-19 12:57:15,457 - INFO - joeynmt.training - \tReference:  پس وقتی هر دو تن دردادند و همدیگر را بدرود گفتند و پسر را به پیشانی بر خاک افکند ، \n",
            "2023-01-19 12:57:15,457 - INFO - joeynmt.training - \tHypothesis: و هر دو را به سوی آن دو برده ایم ،\n",
            "2023-01-19 12:57:15,457 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:57:15,459 - INFO - joeynmt.training - \tSource:      نه یئیهجهییک نه ایچهجهییک ؟ دئیه آختارێب ناراهات اولمایین . \n",
            "2023-01-19 12:57:15,460 - INFO - joeynmt.training - \tReference:  پس دیگر در پی این مباشید که چه بخورید و چه بنوشید ؛ دیگر غرق در نگرانی مشوید ؛ \n",
            "2023-01-19 12:57:15,460 - INFO - joeynmt.training - \tHypothesis: چه بپیوندید که چه بخوریم ؟ بنوشید که چه بنوشید و بنوشید .\n",
            "2023-01-19 12:57:15,460 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:57:15,462 - INFO - joeynmt.training - \tSource:     دئدیلر : یهودیلرین آنادان اولموش پادشاهێ هارادادێر ؟ شرقده اونون اولدوزونو گؤردۆک وه اونا سجده قێلماغا گلدیک . \n",
            "2023-01-19 12:57:15,462 - INFO - joeynmt.training - \tReference:  و می‌گفتند : آن پادشاه یهودیان که متولد شده است ، کجاست ؟ زیرا ما ستارهٔ او را زمانی که در مشرق بودیم ، دیدیم و آمده‌ایم تا در برابر او سر تعظیم فرود آوریم . \n",
            "2023-01-19 12:57:15,463 - INFO - joeynmt.training - \tHypothesis: گفتند : آیا از کجا بود ؟ پس پادشاه یهودیان ، از کجا یافته بود ؟ او را دیدیم و به او تعظیم کردند و به او تعظیم کردند و به او تعظیمت افتادیم .\n",
            "2023-01-19 12:57:15,463 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:57:15,465 - INFO - joeynmt.training - \tSource:     اونلار : جهنم اودو بیزه بیر نئچه گۆندن آرتێق اعذاب وئرمز دئیرلر . اونلارا سؤیله : سیز آللاهدان بئله بیر وه د آلمیسینیزمی ؟ آللاه هئچ واخت اؤز اهدیندن دؤنمز . یوخسا آللاها قارشێ بیلمهدیگینیزی سؤیلهییرسینیز ؟ \n",
            "2023-01-19 12:57:15,465 - INFO - joeynmt.training - \tReference:  و گفتند : جز روزهایی چند ، هرگز آتش به ما نخواهد رسید . بگو : مگر پیمانی از خدا گرفته‌اید ؟ که خدا پیمان خود را هرگز خلاف نخواهد کرد یا آنچه را نمی‌دانید به دروغ به خدا نسبت می‌دهید ؟ \n",
            "2023-01-19 12:57:15,465 - INFO - joeynmt.training - \tHypothesis: و گفتند : آیا عذابی از ما به ما نمی رسد که عذاب را عذاب نمی کند . و می گویند : آیا شما را از خدا پیمان نمی کنید ؟ و چون خدا پیمان بستید ، به شما پیمانی نمی کنید ، یا از جانب خدا پیمان نمی کنید ؟\n",
            "2023-01-19 12:57:15,941 - INFO - joeynmt.training - Epoch 181: total training loss 617.27\n",
            "2023-01-19 12:57:15,942 - INFO - joeynmt.training - EPOCH 182\n",
            "2023-01-19 12:57:23,276 - INFO - joeynmt.training - Epoch 182, Step:    63100, Batch Loss:     1.584923, Batch Acc: 0.543439, Tokens per Sec:    15729, Lr: 0.000036\n",
            "2023-01-19 12:57:30,997 - INFO - joeynmt.training - Epoch 182, Step:    63200, Batch Loss:     1.729855, Batch Acc: 0.541849, Tokens per Sec:    15649, Lr: 0.000036\n",
            "2023-01-19 12:57:38,775 - INFO - joeynmt.training - Epoch 182, Step:    63300, Batch Loss:     1.678448, Batch Acc: 0.542540, Tokens per Sec:    15518, Lr: 0.000036\n",
            "2023-01-19 12:57:42,889 - INFO - joeynmt.training - Epoch 182: total training loss 616.54\n",
            "2023-01-19 12:57:42,889 - INFO - joeynmt.training - EPOCH 183\n",
            "2023-01-19 12:57:46,589 - INFO - joeynmt.training - Epoch 183, Step:    63400, Batch Loss:     1.780275, Batch Acc: 0.542057, Tokens per Sec:    15626, Lr: 0.000036\n",
            "2023-01-19 12:57:54,195 - INFO - joeynmt.training - Epoch 183, Step:    63500, Batch Loss:     1.743504, Batch Acc: 0.544128, Tokens per Sec:    15934, Lr: 0.000035\n",
            "2023-01-19 12:58:01,754 - INFO - joeynmt.training - Epoch 183, Step:    63600, Batch Loss:     1.813093, Batch Acc: 0.543292, Tokens per Sec:    15869, Lr: 0.000035\n",
            "2023-01-19 12:58:09,479 - INFO - joeynmt.training - Epoch 183, Step:    63700, Batch Loss:     1.694213, Batch Acc: 0.540553, Tokens per Sec:    15566, Lr: 0.000035\n",
            "2023-01-19 12:58:09,577 - INFO - joeynmt.training - Epoch 183: total training loss 619.17\n",
            "2023-01-19 12:58:09,578 - INFO - joeynmt.training - EPOCH 184\n",
            "2023-01-19 12:58:17,191 - INFO - joeynmt.training - Epoch 184, Step:    63800, Batch Loss:     1.771575, Batch Acc: 0.548975, Tokens per Sec:    15599, Lr: 0.000035\n",
            "2023-01-19 12:58:24,812 - INFO - joeynmt.training - Epoch 184, Step:    63900, Batch Loss:     1.730098, Batch Acc: 0.545500, Tokens per Sec:    15813, Lr: 0.000035\n",
            "2023-01-19 12:58:32,342 - INFO - joeynmt.training - Epoch 184, Step:    64000, Batch Loss:     1.852458, Batch Acc: 0.540584, Tokens per Sec:    15843, Lr: 0.000035\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 139.11ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10803.35ex/s]\n",
            "2023-01-19 12:58:32,599 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=64000\n",
            "2023-01-19 12:58:32,600 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 12:58:37,584 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 12:58:37,584 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 12:58:37,584 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 12:58:37,585 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 12:58:37,588 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.57, loss:   2.60, ppl:  13.42, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9409[sec], evaluation: 0.0405[sec]\n",
            "2023-01-19 12:58:37,779 - INFO - joeynmt.helpers - delete RESULTS/model/46000.ckpt\n",
            "2023-01-19 12:58:37,792 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 12:58:37,795 - INFO - joeynmt.training - \tSource:     اگر بیز سنه صبات وئرمهسیدیک ، یقین کی ، آز دا اولسا ، اونلارا اویاجاقدین ! \n",
            "2023-01-19 12:58:37,796 - INFO - joeynmt.training - \tReference:  و اگر تو را استوار نمی‌داشتیم ، قطعا نزدیک بود کمی به سوی آنان متمایل شوی . \n",
            "2023-01-19 12:58:37,796 - INFO - joeynmt.training - \tHypothesis: و اگر تو را به تو ندادیم ، قطعا آنان بهتر از اندازه خواهد بود .\n",
            "2023-01-19 12:58:37,796 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 12:58:37,798 - INFO - joeynmt.training - \tSource:     یوخسا بیز ملکلری دیشی یاراتمیشیق وه اونلار دا شاهد اولوبلار \n",
            "2023-01-19 12:58:37,798 - INFO - joeynmt.training - \tReference:  یا فرشتگان را مادینه آفریدیم و آنان شاهد بودند ؟ \n",
            "2023-01-19 12:58:37,798 - INFO - joeynmt.training - \tHypothesis: یا فرشتگان را آفریده ایم که آنها شاهدان را آفریده ایم و آنان شاهد اند ؟\n",
            "2023-01-19 12:58:37,798 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 12:58:37,801 - INFO - joeynmt.training - \tSource:      خاطرلا کی ، ابراهیم : ائی ربیم ، اؤلولهری نه جۆر دیریلتدیگینی منه گؤستر ! دئدیکده : مگر اینانمیرسان ؟ بویورموشدو . بلی ، اینانیرام ، لاکین اۆرهییم ساکت اولماق اۆچۆن ، دئیه جاواب وئرمیشدی . بویورموشدو : دؤرد جۆر قوش گؤتوروب اونلارا دقتله باخ ، سونرا هر داغێن باشێنا اونلاردان بیر پارچا آت ، سونرا اونلارێ چاغێر ، تئز یانینا گلهجکلر . بیل کی ، آللاه یئنیلمز غۆۆت ، هکمت صاحبدر ! \n",
            "2023-01-19 12:58:37,801 - INFO - joeynmt.training - \tReference:  و یاد کن‌ آنگاه که ابراهیم گفت : پروردگارا ، به من نشان ده ؛ چگونه مردگان را زنده می‌کنی ؟ فرمود : مگر ایمان نیاورده‌ای ؟ گفت : چرا ، ولی تا دلم آرامش یابد . فرمود : پس ، چهار پرنده برگیر ، و آنها را پیش خود ، ریز ریز گردان ؛ سپس بر هر کوهی پاره‌ای از آنها را قرار ده ؛ آنگاه آنها را فرا خوان ، شتابان به سوی تو می‌آیند ، و بدان که خداوند توانا و حکیم است . \n",
            "2023-01-19 12:58:37,801 - INFO - joeynmt.training - \tHypothesis: و یاد کن هنگامی را که ابراهیم مرده را زنده می گرداند ، گفت : پروردگارا ، آیا ایمان آورده است ؟ گفت : آیا ایمان بیاور ، ایمان آورده ام ؟ بگو : خدا را به آنچه را که دلسوزه بود به سوی آن بیاورم . پس چون سپس به آنان می داد ، می گویند : خدا بی نیاز است . پس خداوند ، آنان را به زودی ناسپاسپاسند .\n",
            "2023-01-19 12:58:37,801 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 12:58:37,803 - INFO - joeynmt.training - \tSource:     مصیحده اولاراق اون دؤرد ایل بوندان اول بدندمی ، یوخسا بدندن خارجمی ، من بیلمیرم ، آللاه بیلیر گؤتورولوب گؤیون اۆچۆنجۆ قاتێنا آپارێلان بیر اینسانی تانیییرام . \n",
            "2023-01-19 12:58:37,803 - INFO - joeynmt.training - \tReference:  شخصی را که در اتحاد با مسیح است می‌شناسم که ۱۴ سال پیش با بدن یا خارج از بدن نمی‌دانم ، خدا می‌داند به آسمان سوم برده شد . \n",
            "2023-01-19 12:58:37,803 - INFO - joeynmt.training - \tHypothesis: من نمی دانم که مسیح از طریق مسیح ۴۴ نسل ، بدن را از بدن می شناسم و در آسمان سوم برده شده ام ، اما خدا را که از بدنش برد ، برمی خاندگیخته است ، نمی شناسم .\n",
            "2023-01-19 12:58:41,743 - INFO - joeynmt.training - Epoch 184: total training loss 617.63\n",
            "2023-01-19 12:58:41,744 - INFO - joeynmt.training - EPOCH 185\n",
            "2023-01-19 12:58:46,076 - INFO - joeynmt.training - Epoch 185, Step:    64100, Batch Loss:     1.872230, Batch Acc: 0.544956, Tokens per Sec:    13692, Lr: 0.000035\n",
            "2023-01-19 12:58:54,878 - INFO - joeynmt.training - Epoch 185, Step:    64200, Batch Loss:     1.802405, Batch Acc: 0.547551, Tokens per Sec:    13798, Lr: 0.000035\n",
            "2023-01-19 12:59:02,760 - INFO - joeynmt.training - Epoch 185, Step:    64300, Batch Loss:     1.732636, Batch Acc: 0.541453, Tokens per Sec:    15260, Lr: 0.000035\n",
            "2023-01-19 12:59:10,479 - INFO - joeynmt.training - Epoch 185: total training loss 617.52\n",
            "2023-01-19 12:59:10,480 - INFO - joeynmt.training - EPOCH 186\n",
            "2023-01-19 12:59:10,563 - INFO - joeynmt.training - Epoch 186, Step:    64400, Batch Loss:     1.792202, Batch Acc: 0.547541, Tokens per Sec:    14769, Lr: 0.000035\n",
            "2023-01-19 12:59:18,792 - INFO - joeynmt.training - Epoch 186, Step:    64500, Batch Loss:     1.722866, Batch Acc: 0.547652, Tokens per Sec:    14620, Lr: 0.000035\n",
            "2023-01-19 12:59:26,648 - INFO - joeynmt.training - Epoch 186, Step:    64600, Batch Loss:     1.838164, Batch Acc: 0.538838, Tokens per Sec:    15281, Lr: 0.000035\n",
            "2023-01-19 12:59:34,494 - INFO - joeynmt.training - Epoch 186, Step:    64700, Batch Loss:     1.853334, Batch Acc: 0.542517, Tokens per Sec:    15423, Lr: 0.000035\n",
            "2023-01-19 12:59:38,300 - INFO - joeynmt.training - Epoch 186: total training loss 616.99\n",
            "2023-01-19 12:59:38,301 - INFO - joeynmt.training - EPOCH 187\n",
            "2023-01-19 12:59:42,461 - INFO - joeynmt.training - Epoch 187, Step:    64800, Batch Loss:     1.939233, Batch Acc: 0.548249, Tokens per Sec:    15166, Lr: 0.000035\n",
            "2023-01-19 12:59:50,389 - INFO - joeynmt.training - Epoch 187, Step:    64900, Batch Loss:     1.724898, Batch Acc: 0.545723, Tokens per Sec:    15209, Lr: 0.000035\n",
            "2023-01-19 12:59:58,207 - INFO - joeynmt.training - Epoch 187, Step:    65000, Batch Loss:     1.740711, Batch Acc: 0.541947, Tokens per Sec:    15445, Lr: 0.000035\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 141.59ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10526.24ex/s]\n",
            "2023-01-19 12:59:58,477 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=65000\n",
            "2023-01-19 12:59:58,477 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:00:04,106 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:00:04,106 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:00:04,107 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:00:04,108 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:00:04,111 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.89, loss:   2.64, ppl:  13.99, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.5818[sec], evaluation: 0.0434[sec]\n",
            "2023-01-19 13:00:04,113 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:00:04,117 - INFO - joeynmt.training - \tSource:     ایسهآیا خیانت ائدن یهودا اونلارا اشاره ایله بیلدیریب دئمیشدی : کیمی اؤپسم ، ایسا اودور ، اونو توتون . \n",
            "2023-01-19 13:00:04,117 - INFO - joeynmt.training - \tReference:  آن خائن به همراهان خود چنین نشانه‌ای داده بود : کسی را که ببوسم ، همان اوست . دستگیرش کنید . \n",
            "2023-01-19 13:00:04,117 - INFO - joeynmt.training - \tHypothesis: آنگاه یهودا به همراه او به آنان گفت : مردی که به دنبالش خیانت کردم ، همان کسی است که او را گرفتم .\n",
            "2023-01-19 13:00:04,117 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:00:04,119 - INFO - joeynmt.training - \tSource:     آللاه اونو اؤلولر آراسێندان دیریلدرک هئچ واخت چۆرۆمهیه قویمایاجاق . اونا گؤره دئمیشدی : داوودا ود ائتدیگیم صادق محبتی سیزه گؤستهرهجهیم . \n",
            "2023-01-19 13:00:04,119 - INFO - joeynmt.training - \tReference:  این امر که خدا او را از مردگان رستاخیز داد و دیگر هرگز به فسادپذیری بازنمی‌گردد ، چنین بیان شده است : من به شما احسان‌هایی خواهم کرد که به داوود وعده داده شده بود . این وعده قطعی است . \n",
            "2023-01-19 13:00:04,120 - INFO - joeynmt.training - \tHypothesis: او را در میان مردگان برخیزانده نخواهد شد . پس به او گفت : مطابق محبت ، داوود به شما می گویم ، محبتی که از ما برخوردار خواهم کرد .\n",
            "2023-01-19 13:00:04,120 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:00:04,122 - INFO - joeynmt.training - \tSource:     یهودی باشچیلاری اوللر کور اولان آدامێ ایکینجی دفه چاغێرێب اونا دئدیلر : آللاهێن ایزهتی نامنه دوغرو سؤیله . بیز بو آدامێن گۆناهکار اولدوغونو بیلیریک . \n",
            "2023-01-19 13:00:04,122 - INFO - joeynmt.training - \tReference:  پس آنان برای بار دوم آن مرد را که پیش از این نابینا بود ، فراخواندند و به او گفتند : خدا را تمجید کن ؛ ما می‌دانیم که این مرد گناهکار است . \n",
            "2023-01-19 13:00:04,122 - INFO - joeynmt.training - \tHypothesis: یهودیان باید نخستین بسیاری از این که پیش از این دوم بود ، به او گفتند : ما می دانیم که جلال خدا را به او گفته ایم .\n",
            "2023-01-19 13:00:04,122 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:00:04,124 - INFO - joeynmt.training - \tSource:     داش آکلین یاخشی یادیندا ایدی کی اوچ گون بوندان اول دومیل قفه خاناسیندا ، کاکا رستم اونا خط نشان چکمیشدی ، \n",
            "2023-01-19 13:00:04,124 - INFO - joeynmt.training - \tReference:  داش آكل خوب یادش بود كه سه روز پیش در قهوه خانه دو میل كاكا رستم برایش خط و نشان كشید ، \n",
            "2023-01-19 13:00:04,125 - INFO - joeynmt.training - \tHypothesis: داش آكل از آكل قرافه یكل قفسه آكل قرعه یكه یكل و كاكا رستش بود ،\n",
            "2023-01-19 13:00:11,625 - INFO - joeynmt.training - Epoch 187: total training loss 612.32\n",
            "2023-01-19 13:00:11,625 - INFO - joeynmt.training - EPOCH 188\n",
            "2023-01-19 13:00:12,040 - INFO - joeynmt.training - Epoch 188, Step:    65100, Batch Loss:     1.735464, Batch Acc: 0.550521, Tokens per Sec:    13902, Lr: 0.000035\n",
            "2023-01-19 13:00:19,940 - INFO - joeynmt.training - Epoch 188, Step:    65200, Batch Loss:     1.795809, Batch Acc: 0.546078, Tokens per Sec:    15387, Lr: 0.000035\n",
            "2023-01-19 13:00:27,778 - INFO - joeynmt.training - Epoch 188, Step:    65300, Batch Loss:     1.770201, Batch Acc: 0.543809, Tokens per Sec:    15281, Lr: 0.000035\n",
            "2023-01-19 13:00:35,726 - INFO - joeynmt.training - Epoch 188, Step:    65400, Batch Loss:     1.779929, Batch Acc: 0.543896, Tokens per Sec:    15234, Lr: 0.000035\n",
            "2023-01-19 13:00:39,112 - INFO - joeynmt.training - Epoch 188: total training loss 611.65\n",
            "2023-01-19 13:00:39,112 - INFO - joeynmt.training - EPOCH 189\n",
            "2023-01-19 13:00:43,780 - INFO - joeynmt.training - Epoch 189, Step:    65500, Batch Loss:     1.811519, Batch Acc: 0.549227, Tokens per Sec:    15223, Lr: 0.000035\n",
            "2023-01-19 13:00:51,577 - INFO - joeynmt.training - Epoch 189, Step:    65600, Batch Loss:     1.810479, Batch Acc: 0.545879, Tokens per Sec:    15370, Lr: 0.000035\n",
            "2023-01-19 13:00:59,504 - INFO - joeynmt.training - Epoch 189, Step:    65700, Batch Loss:     1.766349, Batch Acc: 0.544461, Tokens per Sec:    15462, Lr: 0.000035\n",
            "2023-01-19 13:01:06,477 - INFO - joeynmt.training - Epoch 189: total training loss 610.79\n",
            "2023-01-19 13:01:06,478 - INFO - joeynmt.training - EPOCH 190\n",
            "2023-01-19 13:01:07,345 - INFO - joeynmt.training - Epoch 190, Step:    65800, Batch Loss:     1.771736, Batch Acc: 0.553715, Tokens per Sec:    15244, Lr: 0.000035\n",
            "2023-01-19 13:01:15,183 - INFO - joeynmt.training - Epoch 190, Step:    65900, Batch Loss:     1.804466, Batch Acc: 0.544371, Tokens per Sec:    15368, Lr: 0.000035\n",
            "2023-01-19 13:01:22,987 - INFO - joeynmt.training - Epoch 190, Step:    66000, Batch Loss:     1.750670, Batch Acc: 0.548591, Tokens per Sec:    15462, Lr: 0.000035\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 124.77ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10046.15ex/s]\n",
            "2023-01-19 13:01:23,259 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=66000\n",
            "2023-01-19 13:01:23,259 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:01:27,885 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:01:27,885 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:01:27,886 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:01:27,887 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:01:27,889 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.23, loss:   2.62, ppl:  13.77, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5811[sec], evaluation: 0.0417[sec]\n",
            "2023-01-19 13:01:27,892 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:01:27,895 - INFO - joeynmt.training - \tSource:     هر کس آللاهێن چوخجهتلی لۆتفنۆن یاخشی ادارهچیلری کیمی آلدێغێ اناما گؤره بیر بیرینه خدمت ائتسین . \n",
            "2023-01-19 13:01:27,896 - INFO - joeynmt.training - \tReference:  همچون کارگزاران نیکوی لطف ال هی ، لطفی که عطایای گوناگون می‌بخشد ، متناسب با عطیه‌ای که یافته‌اید یکدیگر را خدمت کنید . \n",
            "2023-01-19 13:01:27,896 - INFO - joeynmt.training - \tHypothesis: هر کس باید بیشتری از طریق لطف خدا را به خاطر ثروتمندگی کند ؛ زیرا همان طور که به خاطر خدمتگزاران است ، به خاطر خدمتگزاران را همچون خدمتگزار سازد .\n",
            "2023-01-19 13:01:27,896 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:01:27,898 - INFO - joeynmt.training - \tSource:     اکسینه ، سۆنتلیلره مۆژده یایماق ایشی پئتئره تاپشیریلدیغی کیمی سۆنتسیزلر آراسێندا مۆژده یایماق ایشینین منه تاپشیریلدیغینی گؤردولر . \n",
            "2023-01-19 13:01:27,898 - INFO - joeynmt.training - \tReference:  برعکس ، آنان متوجه شدند که بشارت دادن به غیریهودیان به من سپرده شده است ، همان طور که بشارت دادن به یهودیان به پطرس واگذار شده بود \n",
            "2023-01-19 13:01:27,898 - INFO - joeynmt.training - \tHypothesis: بلکه باید از ختنه شدگان ، پطرس به من اهمیت کرده اند و کسانی که ختنه شده بودند ، به من دادند ؛ همان طور که در میان من به من سپرده شده است .\n",
            "2023-01-19 13:01:27,899 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:01:27,900 - INFO - joeynmt.training - \tSource:     بیزه بورجلو اولان هر آدامێ باغیشلادیغیمیزا گؤره بیزیم گۆناهلاریمیزی دا باغێشلا . بیزی سێناغا چکمه . \n",
            "2023-01-19 13:01:27,901 - INFO - joeynmt.training - \tReference:  گناهان ما را ببخش ؛ زیرا خود ما نیز هر که را به ما مقروض است ، می‌بخشیم . همچنین ما را در وسوسه میاور . \n",
            "2023-01-19 13:01:27,901 - INFO - joeynmt.training - \tHypothesis: اما هر که بدهید ، هر که را به ما بخشیده است ، گناهان ما را بخشیده است .\n",
            "2023-01-19 13:01:27,901 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:01:27,903 - INFO - joeynmt.training - \tSource:     اونلار اؤز امللرینین هامیسینی اینسانلار گؤرسۆن دئیه ائدرلر . چۆنکی اونلار دوعا قوتوجوقلارینی گئنیشلندیریب گئییملرینین قوتازلارێنێ اوزادارلار . \n",
            "2023-01-19 13:01:27,903 - INFO - joeynmt.training - \tReference:  هر کاری که می‌کنند برای این است که توجه مردم را به خود جلب کنند ؛ آیه‌دان‌های خود را بزرگ‌تر و حاشیهٔ رداهای خود را پهن‌تر می‌کنند . \n",
            "2023-01-19 13:01:27,903 - INFO - joeynmt.training - \tHypothesis: و می گویند : همهٔ کارهای خود را می بینند ؛ زیرا می گویند که آنان را با دعاهای خود دعا می کنند و سعی هایشان را در تن می کنند .\n",
            "2023-01-19 13:01:35,725 - INFO - joeynmt.training - Epoch 190, Step:    66100, Batch Loss:     1.794061, Batch Acc: 0.545449, Tokens per Sec:    14934, Lr: 0.000035\n",
            "2023-01-19 13:01:38,659 - INFO - joeynmt.training - Epoch 190: total training loss 612.36\n",
            "2023-01-19 13:01:38,660 - INFO - joeynmt.training - EPOCH 191\n",
            "2023-01-19 13:01:43,616 - INFO - joeynmt.training - Epoch 191, Step:    66200, Batch Loss:     1.704661, Batch Acc: 0.550565, Tokens per Sec:    15187, Lr: 0.000035\n",
            "2023-01-19 13:01:51,403 - INFO - joeynmt.training - Epoch 191, Step:    66300, Batch Loss:     1.807901, Batch Acc: 0.547131, Tokens per Sec:    15597, Lr: 0.000035\n",
            "2023-01-19 13:02:00,427 - INFO - joeynmt.training - Epoch 191, Step:    66400, Batch Loss:     1.651620, Batch Acc: 0.546275, Tokens per Sec:    13431, Lr: 0.000035\n",
            "2023-01-19 13:02:07,054 - INFO - joeynmt.training - Epoch 191: total training loss 609.43\n",
            "2023-01-19 13:02:07,054 - INFO - joeynmt.training - EPOCH 192\n",
            "2023-01-19 13:02:08,321 - INFO - joeynmt.training - Epoch 192, Step:    66500, Batch Loss:     1.844867, Batch Acc: 0.547709, Tokens per Sec:    15736, Lr: 0.000035\n",
            "2023-01-19 13:02:16,081 - INFO - joeynmt.training - Epoch 192, Step:    66600, Batch Loss:     1.777602, Batch Acc: 0.549565, Tokens per Sec:    15411, Lr: 0.000035\n",
            "2023-01-19 13:02:23,800 - INFO - joeynmt.training - Epoch 192, Step:    66700, Batch Loss:     1.779527, Batch Acc: 0.545551, Tokens per Sec:    15643, Lr: 0.000035\n",
            "2023-01-19 13:02:31,494 - INFO - joeynmt.training - Epoch 192, Step:    66800, Batch Loss:     1.759299, Batch Acc: 0.547943, Tokens per Sec:    15470, Lr: 0.000035\n",
            "2023-01-19 13:02:34,090 - INFO - joeynmt.training - Epoch 192: total training loss 613.89\n",
            "2023-01-19 13:02:34,090 - INFO - joeynmt.training - EPOCH 193\n",
            "2023-01-19 13:02:39,189 - INFO - joeynmt.training - Epoch 193, Step:    66900, Batch Loss:     1.670667, Batch Acc: 0.550799, Tokens per Sec:    15642, Lr: 0.000035\n",
            "2023-01-19 13:02:46,931 - INFO - joeynmt.training - Epoch 193, Step:    67000, Batch Loss:     1.744324, Batch Acc: 0.549446, Tokens per Sec:    15690, Lr: 0.000035\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.51ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9796.69ex/s] \n",
            "2023-01-19 13:02:47,210 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=67000\n",
            "2023-01-19 13:02:47,210 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:02:51,297 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:02:51,297 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:02:51,297 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:02:51,298 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:02:51,301 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.08, loss:   2.59, ppl:  13.29, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.0428[sec], evaluation: 0.0412[sec]\n",
            "2023-01-19 13:02:51,304 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:02:51,308 - INFO - joeynmt.training - \tSource:     سور چالیناجاق ، آللاهێن گؤیلرده وه یئرده اولان ایستهدیگی کیمسهلردن باشقا ، درحال هامێ یێخیلیب اؤلهجک . سونرا بیر داها چالێنان کیمی اونلار قالخێب مۆنتزر اولاجاقلار ! \n",
            "2023-01-19 13:02:51,308 - INFO - joeynmt.training - \tReference:  و در صور دمیده می‌شود ، پس هر که در آسمانها و هر که در زمین است بیهوش درمی‌افتد ، مگر کسی که خدا بخواهد ؛ سپس بار دیگر در آن دمیده می‌شود و بناگاه آنان بر پای ایستاده می‌نگرند . \n",
            "2023-01-19 13:02:51,308 - INFO - joeynmt.training - \tHypothesis: و در صور دمیده شود ، کسانی که در آسمانها و زمین است میرند ، سپس همه آنان را خواهند مرد و در میان آنان جایگاهی که در زمین می خوانند ، برخیزند و برخی از آنان درآمدند .\n",
            "2023-01-19 13:02:51,308 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:02:51,310 - INFO - joeynmt.training - \tSource:     ایسا گئری قاییتدیقدا خالق اونو قبول ائتدی ، چۆنکی هامێ اونو گؤزلهییردی . \n",
            "2023-01-19 13:02:51,310 - INFO - joeynmt.training - \tReference:  وقتی عیسی بازگشت ، مردم به گرمی از او استقبال کردند ؛ زیرا همه منتظرش بودند . \n",
            "2023-01-19 13:02:51,310 - INFO - joeynmt.training - \tHypothesis: عیسی پس از آن که عیسی را به پایان رساند ، او را پذیرفت ؛ زیرا همهٔ آنانی است .\n",
            "2023-01-19 13:02:51,311 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:02:51,312 - INFO - joeynmt.training - \tSource:     بوش صندل ده اوتورور سان . \n",
            "2023-01-19 13:02:51,313 - INFO - joeynmt.training - \tReference:  روی صندلی خالی مینشینی . \n",
            "2023-01-19 13:02:51,313 - INFO - joeynmt.training - \tHypothesis: همین نشسته یك كه یكر نشست .\n",
            "2023-01-19 13:02:51,313 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:02:51,315 - INFO - joeynmt.training - \tSource:     او ، سیناقوقدا جصارتله دانێشماغا باشلادێ . اونا قولاق آسان پریسکیلا ایله آکیلا اونو یانلارینا چکیب آللاه یولو بارهده داها دقیق اضاهات وئردی . \n",
            "2023-01-19 13:02:51,315 - INFO - joeynmt.training - \tReference:  یک بار که آپولس در کنیسه با شهامت شروع به سخن گفتن کرد ، پریسکیلا و آکیلا سخنانش را شنیده ، او را نزد خود بردند و طریق خدا را دقیق‌تر به او آموزش دادند . \n",
            "2023-01-19 13:02:51,315 - INFO - joeynmt.training - \tHypothesis: در کنیسه به یکدیگر گفت : دلگرمی به سخنانش گوش داده است . او را با سخنان خود به او گوش داد و در مورد خدا به احوالی فرمان داد که خدا از طریق او اطاعت کرد .\n",
            "2023-01-19 13:02:59,088 - INFO - joeynmt.training - Epoch 193, Step:    67100, Batch Loss:     1.744125, Batch Acc: 0.542913, Tokens per Sec:    14872, Lr: 0.000035\n",
            "2023-01-19 13:03:05,506 - INFO - joeynmt.training - Epoch 193: total training loss 610.30\n",
            "2023-01-19 13:03:05,506 - INFO - joeynmt.training - EPOCH 194\n",
            "2023-01-19 13:03:06,926 - INFO - joeynmt.training - Epoch 194, Step:    67200, Batch Loss:     1.714990, Batch Acc: 0.547113, Tokens per Sec:    14829, Lr: 0.000035\n",
            "2023-01-19 13:03:14,625 - INFO - joeynmt.training - Epoch 194, Step:    67300, Batch Loss:     1.675200, Batch Acc: 0.549055, Tokens per Sec:    15654, Lr: 0.000034\n",
            "2023-01-19 13:03:22,446 - INFO - joeynmt.training - Epoch 194, Step:    67400, Batch Loss:     1.779112, Batch Acc: 0.546617, Tokens per Sec:    15685, Lr: 0.000034\n",
            "2023-01-19 13:03:30,140 - INFO - joeynmt.training - Epoch 194, Step:    67500, Batch Loss:     1.713610, Batch Acc: 0.549289, Tokens per Sec:    15742, Lr: 0.000034\n",
            "2023-01-19 13:03:32,380 - INFO - joeynmt.training - Epoch 194: total training loss 607.04\n",
            "2023-01-19 13:03:32,380 - INFO - joeynmt.training - EPOCH 195\n",
            "2023-01-19 13:03:37,790 - INFO - joeynmt.training - Epoch 195, Step:    67600, Batch Loss:     1.654692, Batch Acc: 0.551258, Tokens per Sec:    15791, Lr: 0.000034\n",
            "2023-01-19 13:03:45,401 - INFO - joeynmt.training - Epoch 195, Step:    67700, Batch Loss:     1.722007, Batch Acc: 0.547302, Tokens per Sec:    15865, Lr: 0.000034\n",
            "2023-01-19 13:03:53,051 - INFO - joeynmt.training - Epoch 195, Step:    67800, Batch Loss:     1.801147, Batch Acc: 0.547366, Tokens per Sec:    15765, Lr: 0.000034\n",
            "2023-01-19 13:03:59,005 - INFO - joeynmt.training - Epoch 195: total training loss 609.03\n",
            "2023-01-19 13:03:59,006 - INFO - joeynmt.training - EPOCH 196\n",
            "2023-01-19 13:04:00,770 - INFO - joeynmt.training - Epoch 196, Step:    67900, Batch Loss:     1.716615, Batch Acc: 0.558576, Tokens per Sec:    15416, Lr: 0.000034\n",
            "2023-01-19 13:04:08,472 - INFO - joeynmt.training - Epoch 196, Step:    68000, Batch Loss:     1.632675, Batch Acc: 0.550225, Tokens per Sec:    15576, Lr: 0.000034\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 75.00ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10431.36ex/s]\n",
            "2023-01-19 13:04:08,751 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=68000\n",
            "2023-01-19 13:04:08,752 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:04:12,998 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:04:12,998 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:04:12,999 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:04:12,999 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:04:13,002 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.95, loss:   2.68, ppl:  14.60, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2011[sec], evaluation: 0.0421[sec]\n",
            "2023-01-19 13:04:13,005 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:04:13,008 - INFO - joeynmt.training - \tSource:     فیر اون دا ، اوندان اوهلکیلر ده ، آلت اۆست اولموش معؤ تفیکه اهلی ده گۆناه تؤرتمیشدیلر . \n",
            "2023-01-19 13:04:13,009 - INFO - joeynmt.training - \tReference:  و فرعون و کسانی که پیش از او بودند و مردم‌ شهرهای سرنگون شده سدوم و عاموره‌ مرتکب خطا شدند . \n",
            "2023-01-19 13:04:13,009 - INFO - joeynmt.training - \tHypothesis: و فرعون فرعون و کسانی که پیش از این بودند از این رو ، پیش از آنکه نافرمانی کردند .\n",
            "2023-01-19 13:04:13,009 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:04:13,011 - INFO - joeynmt.training - \tSource:     یوخسا اونلار آللاهێن اؤز نئ متیندن بخش ائتدیگی شئیلره گؤره اینسانلارا حسد آپارێرلار ؟ حالبوکی بیز ابراهیم اؤؤلادێنا دا کیتاب وه هکمت وئرمیشدیک وه اونلارا بؤیوک مۆلک بخش ائتمیشدیک . \n",
            "2023-01-19 13:04:13,011 - INFO - joeynmt.training - \tReference:  بلکه به مردم ، برای آنچه خدا از فضل خویش به آنان عطا کرده رشک می‌ورزند ؛ در حقیقت ، ما به خاندان ابراهیم کتاب و حکمت دادیم ، و به آنان ملکی بزرگ بخشیدیم . \n",
            "2023-01-19 13:04:13,011 - INFO - joeynmt.training - \tHypothesis: آیا ندیده اند که خدا از آنچه به آنچه به ایشان عطا کرده است به وسیله آن ، به ابراهیم و حکمت دادیم ؟ و به راستی ابراهیم و حکمت و حکمت و حکمت و حکمت و دانشی بود .\n",
            "2023-01-19 13:04:13,012 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:04:13,013 - INFO - joeynmt.training - \tSource:     مصیح اسعادا اولان هیات ودینه گؤره آللاهێن ارادهسی ایله مصیح ایسنانین هوارسی من پاولدان\n",
            "2023-01-19 13:04:13,014 - INFO - joeynmt.training - \tReference:  از طرف پولس که به خواست خدا رسول مسیحْ عیسی گشت تا وعدهٔ آن حیاتی را اعلام کند که از طریق مسیحْ عیسی میسر شده است ، \n",
            "2023-01-19 13:04:13,014 - INFO - joeynmt.training - \tHypothesis: از طرف پولس که در اتحاد با مسیحْ عیسی به خواست خدا ، به خاطر مسیحْ عیسی ، از طرف پولس ،\n",
            "2023-01-19 13:04:13,014 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:04:13,016 - INFO - joeynmt.training - \tSource:     هیات سؤزونو تقدیم ائدین . قوی مسیحین زۆهور ائدهجهگی گۆنده فخر ائده بیلیم کی ، نه بوش یئره جهد ائتدیم ، نه ده بوش یئره ظهمت چکدیم . \n",
            "2023-01-19 13:04:13,016 - INFO - joeynmt.training - \tReference:  و به کلام حیات‌بخش پای‌بند می‌مانید . به همین خاطر ، من دلیلی برای شادی در روز مسیح خواهم داشت ؛ زیرا آنگاه خواهم دانست که بیهوده ندویده و عبث زحمت نکشیده‌ام . \n",
            "2023-01-19 13:04:13,016 - INFO - joeynmt.training - \tHypothesis: همچنین گفتهٔ حیات را تقدیم کنید . پس بیایید تا بتوانم در روزها را در روز بازپسین مسیح بگردمیدم و چه کنم که چه در میان شما به زمین افتادم .\n",
            "2023-01-19 13:04:20,846 - INFO - joeynmt.training - Epoch 196, Step:    68100, Batch Loss:     1.821337, Batch Acc: 0.546125, Tokens per Sec:    14879, Lr: 0.000034\n",
            "2023-01-19 13:04:28,526 - INFO - joeynmt.training - Epoch 196, Step:    68200, Batch Loss:     1.759457, Batch Acc: 0.547487, Tokens per Sec:    15709, Lr: 0.000034\n",
            "2023-01-19 13:04:30,615 - INFO - joeynmt.training - Epoch 196: total training loss 611.35\n",
            "2023-01-19 13:04:30,615 - INFO - joeynmt.training - EPOCH 197\n",
            "2023-01-19 13:04:36,208 - INFO - joeynmt.training - Epoch 197, Step:    68300, Batch Loss:     1.816439, Batch Acc: 0.554626, Tokens per Sec:    15688, Lr: 0.000034\n",
            "2023-01-19 13:04:43,814 - INFO - joeynmt.training - Epoch 197, Step:    68400, Batch Loss:     1.849400, Batch Acc: 0.546569, Tokens per Sec:    15989, Lr: 0.000034\n",
            "2023-01-19 13:04:51,528 - INFO - joeynmt.training - Epoch 197, Step:    68500, Batch Loss:     1.728194, Batch Acc: 0.550138, Tokens per Sec:    15700, Lr: 0.000034\n",
            "2023-01-19 13:04:57,398 - INFO - joeynmt.training - Epoch 197: total training loss 607.10\n",
            "2023-01-19 13:04:57,398 - INFO - joeynmt.training - EPOCH 198\n",
            "2023-01-19 13:04:59,333 - INFO - joeynmt.training - Epoch 198, Step:    68600, Batch Loss:     1.636658, Batch Acc: 0.549576, Tokens per Sec:    15928, Lr: 0.000034\n",
            "2023-01-19 13:05:07,183 - INFO - joeynmt.training - Epoch 198, Step:    68700, Batch Loss:     1.781870, Batch Acc: 0.553622, Tokens per Sec:    15228, Lr: 0.000034\n",
            "2023-01-19 13:05:16,411 - INFO - joeynmt.training - Epoch 198, Step:    68800, Batch Loss:     1.724801, Batch Acc: 0.548110, Tokens per Sec:    13142, Lr: 0.000034\n",
            "2023-01-19 13:05:24,212 - INFO - joeynmt.training - Epoch 198, Step:    68900, Batch Loss:     1.724064, Batch Acc: 0.548124, Tokens per Sec:    15248, Lr: 0.000034\n",
            "2023-01-19 13:05:26,207 - INFO - joeynmt.training - Epoch 198: total training loss 609.44\n",
            "2023-01-19 13:05:26,208 - INFO - joeynmt.training - EPOCH 199\n",
            "2023-01-19 13:05:32,092 - INFO - joeynmt.training - Epoch 199, Step:    69000, Batch Loss:     1.652962, Batch Acc: 0.550702, Tokens per Sec:    15445, Lr: 0.000034\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 136.20ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10384.83ex/s]\n",
            "2023-01-19 13:05:32,368 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=69000\n",
            "2023-01-19 13:05:32,368 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:05:37,680 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:05:37,680 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:05:37,681 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:05:37,682 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:05:37,685 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.33, loss:   2.71, ppl:  15.10, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.2632[sec], evaluation: 0.0466[sec]\n",
            "2023-01-19 13:05:37,688 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:05:37,691 - INFO - joeynmt.training - \tSource:     بیز کیتابدان سونرا زبوردا دا تورپاغا یالنیز منیم سالئه بندهلریمین داخل اولاجاغێنێ یازمیشدیق . \n",
            "2023-01-19 13:05:37,691 - INFO - joeynmt.training - \tReference:  و در حقیقت ، در زبور پس از تورات نوشتیم که زمین را بندگان شایسته ما به ارث خواهند برد . \n",
            "2023-01-19 13:05:37,692 - INFO - joeynmt.training - \tHypothesis: و در کتابی که از این کتاب به زورمت در زمین است ، جز بنده من به جای آورده بودیم .\n",
            "2023-01-19 13:05:37,692 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:05:37,694 - INFO - joeynmt.training - \tSource:     مگر اونلار ایندی یوردلاریندا گزیب دولاشدێقلاری اؤزلریندن اوهلکی نئچه نئچه نسیللری محو ائتدیگیمیزی گؤرمورلرمی ؟ حقیقتا ، بوندا ابرتلر واردێر . مگر قولاق آسمایاجاقلار ؟ \n",
            "2023-01-19 13:05:37,694 - INFO - joeynmt.training - \tReference:  آیا برای آنان روشن نگردیده که چه بسیار نسلها را پیش از آنها نابود گردانیدیم که اینان‌ در سراهایشان راه می‌روند ؟ قطعا در این امر عبرتهاست ، مگر نمی‌شنوند ؟ \n",
            "2023-01-19 13:05:37,694 - INFO - joeynmt.training - \tHypothesis: آیا ندیده اند که آنان در نتیجه های پیش از آنان ساکنان زمین سراسرهای پیشینیان را هلاک کردیم ؟ قطعا در این نسلهایی که پیش از آنان بودند ، به گوش می دهند ؟ آیا در این راه نمی شنوند ؟\n",
            "2023-01-19 13:05:37,694 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:05:37,696 - INFO - joeynmt.training - \tSource:     ایسنانین اونا رحمی گلدی . الینی اوزادێب اونا توخوندو وه دئدی : ایستهییرهم ، پاک اول ! \n",
            "2023-01-19 13:05:37,696 - INFO - joeynmt.training - \tReference:  عیسی دلش به حال او سوخت . پس دستش را دراز کرد ، آن مرد را لمس نمود و به او گفت : می‌خواهم . پاک شو . \n",
            "2023-01-19 13:05:37,697 - INFO - joeynmt.training - \tHypothesis: سپس عیسی به او رحم شد و دستش را لمس نمود و به او گفت : من می خواهم ، پاک شو و پاک شوم .\n",
            "2023-01-19 13:05:37,697 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:05:37,699 - INFO - joeynmt.training - \tSource:     ائی ربیم ! منی ده ، نسلیمدن اولانلارێ دا ناماز قێلان ائت . ائی ربیمیز ! دوعامێ قبول بویور ! \n",
            "2023-01-19 13:05:37,699 - INFO - joeynmt.training - \tReference:  پروردگارا ، مرا برپادارنده نماز قرار ده ، و از فرزندان من نیز . پروردگارا ، و دعای مرا بپذیر . \n",
            "2023-01-19 13:05:37,699 - INFO - joeynmt.training - \tHypothesis: پروردگارا ، مرا به پاس کسانی که از نسل من نماز برپا داشته اند ، پروردگارا ، پس برکتاید .\n",
            "2023-01-19 13:05:45,564 - INFO - joeynmt.training - Epoch 199, Step:    69100, Batch Loss:     1.663350, Batch Acc: 0.551020, Tokens per Sec:    14713, Lr: 0.000034\n",
            "2023-01-19 13:05:53,464 - INFO - joeynmt.training - Epoch 199, Step:    69200, Batch Loss:     1.672590, Batch Acc: 0.551663, Tokens per Sec:    15133, Lr: 0.000034\n",
            "2023-01-19 13:05:59,279 - INFO - joeynmt.training - Epoch 199: total training loss 607.62\n",
            "2023-01-19 13:05:59,280 - INFO - joeynmt.training - EPOCH 200\n",
            "2023-01-19 13:06:01,291 - INFO - joeynmt.training - Epoch 200, Step:    69300, Batch Loss:     1.708467, Batch Acc: 0.548138, Tokens per Sec:    15304, Lr: 0.000034\n",
            "2023-01-19 13:06:09,151 - INFO - joeynmt.training - Epoch 200, Step:    69400, Batch Loss:     1.808022, Batch Acc: 0.551846, Tokens per Sec:    15154, Lr: 0.000034\n",
            "2023-01-19 13:06:17,026 - INFO - joeynmt.training - Epoch 200, Step:    69500, Batch Loss:     1.704852, Batch Acc: 0.550029, Tokens per Sec:    15387, Lr: 0.000034\n",
            "2023-01-19 13:06:24,834 - INFO - joeynmt.training - Epoch 200, Step:    69600, Batch Loss:     1.696417, Batch Acc: 0.548464, Tokens per Sec:    15459, Lr: 0.000034\n",
            "2023-01-19 13:06:26,647 - INFO - joeynmt.training - Epoch 200: total training loss 607.31\n",
            "2023-01-19 13:06:26,647 - INFO - joeynmt.training - EPOCH 201\n",
            "2023-01-19 13:06:32,558 - INFO - joeynmt.training - Epoch 201, Step:    69700, Batch Loss:     1.856766, Batch Acc: 0.553763, Tokens per Sec:    15906, Lr: 0.000034\n",
            "2023-01-19 13:06:40,265 - INFO - joeynmt.training - Epoch 201, Step:    69800, Batch Loss:     1.816697, Batch Acc: 0.552279, Tokens per Sec:    15494, Lr: 0.000034\n",
            "2023-01-19 13:06:48,029 - INFO - joeynmt.training - Epoch 201, Step:    69900, Batch Loss:     1.671770, Batch Acc: 0.549052, Tokens per Sec:    15592, Lr: 0.000034\n",
            "2023-01-19 13:06:53,578 - INFO - joeynmt.training - Epoch 201: total training loss 604.23\n",
            "2023-01-19 13:06:53,578 - INFO - joeynmt.training - EPOCH 202\n",
            "2023-01-19 13:06:55,825 - INFO - joeynmt.training - Epoch 202, Step:    70000, Batch Loss:     1.688775, Batch Acc: 0.551971, Tokens per Sec:    15631, Lr: 0.000034\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.50ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9406.68ex/s] \n",
            "2023-01-19 13:06:56,126 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=70000\n",
            "2023-01-19 13:06:56,126 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:07:02,416 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:07:02,417 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:07:02,417 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:07:02,418 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:07:02,421 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.61, loss:   2.71, ppl:  15.10, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 6.2355[sec], evaluation: 0.0510[sec]\n",
            "2023-01-19 13:07:02,626 - INFO - joeynmt.helpers - delete RESULTS/model/64000.ckpt\n",
            "2023-01-19 13:07:02,635 - INFO - joeynmt.helpers - delete /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/64000.ckpt\n",
            "2023-01-19 13:07:02,636 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/64000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/64000.ckpt')\n",
            "2023-01-19 13:07:02,638 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:07:02,641 - INFO - joeynmt.training - \tSource:      دئدی : هؤکمدارلار بیر اؤلکهیه گیردیکلری زامان اونو خارابازارا چئویرر ، خالقێنێن بؤیوکلرینی ده زلیل ائدرلر . اونلار محض بئله هرهکت ائدرلر \n",
            "2023-01-19 13:07:02,642 - INFO - joeynmt.training - \tReference:   ملکه‌ گفت : پادشاهان چون به شهری درآیند ، آن را تباه و عزیزانش را خوار می‌گردانند ، و این گونه می‌کنند . \n",
            "2023-01-19 13:07:02,642 - INFO - joeynmt.training - \tHypothesis: و موسی گفت : مردمی که در حال آنکه مردم به سبب آن وارد شدند ، و کسانی را که مرتکب اعمال نامشروع می کنند ، زور می کنند .\n",
            "2023-01-19 13:07:02,642 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:07:02,644 - INFO - joeynmt.training - \tSource:     ایسنانین آناسێ وه قارداشلارێ اونون یانینا گلدی . آمما ازدههاما گؤره اونا یاخینلاشا بیلمهدیلر . \n",
            "2023-01-19 13:07:02,645 - INFO - joeynmt.training - \tReference:  آنگاه مادر و برادران عیسی برای دیدنش آمدند ، اما به دلیل ازدحام جمعیت نتوانستند پیش او بروند . \n",
            "2023-01-19 13:07:02,645 - INFO - joeynmt.training - \tHypothesis: مادر و برادرانش نزد عیسی آمدند ، اما شاگردانش نزد او آمدند . پس او را نیافتند .\n",
            "2023-01-19 13:07:02,645 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:07:02,647 - INFO - joeynmt.training - \tSource:     آمما دوت اولونانلار ائتیناسیزلێق گؤستهرهک کیمی تارلاسێنا ، کیمی تجارتنه گئتدی . \n",
            "2023-01-19 13:07:02,647 - INFO - joeynmt.training - \tReference:  اما آنان با بی‌اعتنایی ، یکی پی مزرعهٔ خود رفت ، دیگری پی تجارتش\n",
            "2023-01-19 13:07:02,647 - INFO - joeynmt.training - \tHypothesis: پس باید از آن که به ساحل دعوت کن ، به ستیزه جویی باید به ساوت بپندید .\n",
            "2023-01-19 13:07:02,648 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:07:02,650 - INFO - joeynmt.training - \tSource:     دوهنین اییینه گؤزوندن کئچمهسی وارلێ آدامێن آللاهێن پادشاهلیغینا داخل اولماسێندان آساندێر . \n",
            "2023-01-19 13:07:02,650 - INFO - joeynmt.training - \tReference:  گذشتن شتر از سوراخ سوزن آسان‌تر است ، از راه یافتن شخص ثروتمند به پادشاهی خدا ! \n",
            "2023-01-19 13:07:02,650 - INFO - joeynmt.training - \tHypothesis: از طرف چنگ شتر از چشم دوم ، آسان تر است که برای پادشاهی خدا آسان تر است .\n",
            "2023-01-19 13:07:10,788 - INFO - joeynmt.training - Epoch 202, Step:    70100, Batch Loss:     1.725414, Batch Acc: 0.553115, Tokens per Sec:    13940, Lr: 0.000034\n",
            "2023-01-19 13:07:18,578 - INFO - joeynmt.training - Epoch 202, Step:    70200, Batch Loss:     1.810726, Batch Acc: 0.549018, Tokens per Sec:    15527, Lr: 0.000034\n",
            "2023-01-19 13:07:26,469 - INFO - joeynmt.training - Epoch 202, Step:    70300, Batch Loss:     1.755203, Batch Acc: 0.551456, Tokens per Sec:    15389, Lr: 0.000034\n",
            "2023-01-19 13:07:27,926 - INFO - joeynmt.training - Epoch 202: total training loss 601.97\n",
            "2023-01-19 13:07:27,926 - INFO - joeynmt.training - EPOCH 203\n",
            "2023-01-19 13:07:34,197 - INFO - joeynmt.training - Epoch 203, Step:    70400, Batch Loss:     1.691776, Batch Acc: 0.555403, Tokens per Sec:    15763, Lr: 0.000034\n",
            "2023-01-19 13:07:41,864 - INFO - joeynmt.training - Epoch 203, Step:    70500, Batch Loss:     1.704270, Batch Acc: 0.547902, Tokens per Sec:    15529, Lr: 0.000034\n",
            "2023-01-19 13:07:49,524 - INFO - joeynmt.training - Epoch 203, Step:    70600, Batch Loss:     1.761179, Batch Acc: 0.551144, Tokens per Sec:    15784, Lr: 0.000034\n",
            "2023-01-19 13:07:54,668 - INFO - joeynmt.training - Epoch 203: total training loss 605.60\n",
            "2023-01-19 13:07:54,668 - INFO - joeynmt.training - EPOCH 204\n",
            "2023-01-19 13:07:57,193 - INFO - joeynmt.training - Epoch 204, Step:    70700, Batch Loss:     1.693459, Batch Acc: 0.554146, Tokens per Sec:    15736, Lr: 0.000034\n",
            "2023-01-19 13:08:04,877 - INFO - joeynmt.training - Epoch 204, Step:    70800, Batch Loss:     1.599224, Batch Acc: 0.551389, Tokens per Sec:    15797, Lr: 0.000034\n",
            "2023-01-19 13:08:12,600 - INFO - joeynmt.training - Epoch 204, Step:    70900, Batch Loss:     1.758716, Batch Acc: 0.551582, Tokens per Sec:    15689, Lr: 0.000034\n",
            "2023-01-19 13:08:20,317 - INFO - joeynmt.training - Epoch 204, Step:    71000, Batch Loss:     1.709736, Batch Acc: 0.550526, Tokens per Sec:    15549, Lr: 0.000034\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.70ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10177.54ex/s]\n",
            "2023-01-19 13:08:20,599 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=71000\n",
            "2023-01-19 13:08:20,600 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:08:26,731 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:08:26,732 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:08:26,732 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:08:26,733 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:08:26,736 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.67, loss:   2.67, ppl:  14.48, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 6.0861[sec], evaluation: 0.0428[sec]\n",
            "2023-01-19 13:08:26,933 - INFO - joeynmt.helpers - delete RESULTS/model/70000.ckpt\n",
            "2023-01-19 13:08:26,942 - INFO - joeynmt.helpers - delete /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/70000.ckpt\n",
            "2023-01-19 13:08:26,943 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/70000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/70000.ckpt')\n",
            "2023-01-19 13:08:26,945 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:08:26,949 - INFO - joeynmt.training - \tSource:     قول ایسه دئدی : آغا ، سنین امرین یئرینه یئتیریلدی ، هله آرتێق یئر ده قالێب . \n",
            "2023-01-19 13:08:26,949 - INFO - joeynmt.training - \tReference:  اندکی بعد غلام گفت : ارباب فرمانت به اجرا درآمد . اما هنوز هم جا هست . \n",
            "2023-01-19 13:08:26,949 - INFO - joeynmt.training - \tHypothesis: اما غلام گفت : سرور ، به این فرمان فرمان داده شد که تو نیز بماند .\n",
            "2023-01-19 13:08:26,949 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:08:26,951 - INFO - joeynmt.training - \tSource:     دوغروسو ، منیم بندهلریم اۆزهرینده سنین هئچ بیر هؤکمون اولا بیلمز . ربینین وکیل اولماسێ کفایت ائدر ! \n",
            "2023-01-19 13:08:26,952 - INFO - joeynmt.training - \tReference:   در حقیقت ، تو را بر بندگان من تسلطی نیست ، و حمایتگری چون‌ پروردگارت بس است . \n",
            "2023-01-19 13:08:26,952 - INFO - joeynmt.training - \tHypothesis: و حقیقت ، بندگان من را بر بندگانت ستم نمی کند ، و بر توکل نیست ، و خدا بس بس است .\n",
            "2023-01-19 13:08:26,952 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:08:26,954 - INFO - joeynmt.training - \tSource:     مگر اونلار ایندی یوردلاریندا گزیب دولاشدێقلاری اؤزلریندن اوهلکی نئچه نئچه نسیللری محو ائتدیگیمیزی گؤرمورلرمی ؟ حقیقتا ، بوندا ابرتلر واردێر . مگر قولاق آسمایاجاقلار ؟ \n",
            "2023-01-19 13:08:26,954 - INFO - joeynmt.training - \tReference:  آیا برای آنان روشن نگردیده که چه بسیار نسلها را پیش از آنها نابود گردانیدیم که اینان‌ در سراهایشان راه می‌روند ؟ قطعا در این امر عبرتهاست ، مگر نمی‌شنوند ؟ \n",
            "2023-01-19 13:08:26,954 - INFO - joeynmt.training - \tHypothesis: آیا ندیده اند که آنان در مقامهایشان سنگهایی از آنان بودند که در گذشته اند و نعمتهای پیش از آنان بودند که پیش از آنان بودند هلاک کردیم ؟ قطعا در این حال برای مردمی که می شنوند ، نمی کنند ؟\n",
            "2023-01-19 13:08:26,955 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:08:26,956 - INFO - joeynmt.training - \tSource:      سنین جاماآتێن اونو ، حاقق اولدوغو حالدا ، یالان حساب ائتدی . دئ : من سیزین وکیلینیز دئییلم ! \n",
            "2023-01-19 13:08:26,957 - INFO - joeynmt.training - \tReference:  و قوم تو آن قرآن‌ را دروغ شمردند ، در حالی که آن بر حق است . بگو : من بر شما نگهبان نیستم . \n",
            "2023-01-19 13:08:26,957 - INFO - joeynmt.training - \tHypothesis: و قوم تو را به حق از آنکه حق را به دروغ گرفتند ، و بگو : من شما نیستم .\n",
            "2023-01-19 13:08:28,152 - INFO - joeynmt.training - Epoch 204: total training loss 602.84\n",
            "2023-01-19 13:08:28,153 - INFO - joeynmt.training - EPOCH 205\n",
            "2023-01-19 13:08:34,657 - INFO - joeynmt.training - Epoch 205, Step:    71100, Batch Loss:     1.849300, Batch Acc: 0.554854, Tokens per Sec:    15629, Lr: 0.000034\n",
            "2023-01-19 13:08:42,590 - INFO - joeynmt.training - Epoch 205, Step:    71200, Batch Loss:     1.662710, Batch Acc: 0.555511, Tokens per Sec:    15149, Lr: 0.000034\n",
            "2023-01-19 13:08:50,324 - INFO - joeynmt.training - Epoch 205, Step:    71300, Batch Loss:     1.652780, Batch Acc: 0.550400, Tokens per Sec:    15758, Lr: 0.000033\n",
            "2023-01-19 13:08:55,306 - INFO - joeynmt.training - Epoch 205: total training loss 602.93\n",
            "2023-01-19 13:08:55,307 - INFO - joeynmt.training - EPOCH 206\n",
            "2023-01-19 13:08:58,073 - INFO - joeynmt.training - Epoch 206, Step:    71400, Batch Loss:     1.811834, Batch Acc: 0.557493, Tokens per Sec:    15651, Lr: 0.000033\n",
            "2023-01-19 13:09:05,853 - INFO - joeynmt.training - Epoch 206, Step:    71500, Batch Loss:     1.632279, Batch Acc: 0.554735, Tokens per Sec:    15434, Lr: 0.000033\n",
            "2023-01-19 13:09:13,635 - INFO - joeynmt.training - Epoch 206, Step:    71600, Batch Loss:     1.814122, Batch Acc: 0.550640, Tokens per Sec:    15466, Lr: 0.000033\n",
            "2023-01-19 13:09:21,379 - INFO - joeynmt.training - Epoch 206, Step:    71700, Batch Loss:     1.653271, Batch Acc: 0.550120, Tokens per Sec:    15718, Lr: 0.000033\n",
            "2023-01-19 13:09:22,438 - INFO - joeynmt.training - Epoch 206: total training loss 603.24\n",
            "2023-01-19 13:09:22,438 - INFO - joeynmt.training - EPOCH 207\n",
            "2023-01-19 13:09:29,089 - INFO - joeynmt.training - Epoch 207, Step:    71800, Batch Loss:     1.634364, Batch Acc: 0.557019, Tokens per Sec:    16014, Lr: 0.000033\n",
            "2023-01-19 13:09:36,773 - INFO - joeynmt.training - Epoch 207, Step:    71900, Batch Loss:     1.804322, Batch Acc: 0.554151, Tokens per Sec:    15621, Lr: 0.000033\n",
            "2023-01-19 13:09:44,469 - INFO - joeynmt.training - Epoch 207, Step:    72000, Batch Loss:     1.576680, Batch Acc: 0.550922, Tokens per Sec:    15593, Lr: 0.000033\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 124.43ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10203.36ex/s]\n",
            "2023-01-19 13:09:44,743 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=72000\n",
            "2023-01-19 13:09:44,743 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:09:49,742 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:09:49,743 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:09:49,743 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:09:49,744 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:09:49,747 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.30, loss:   2.68, ppl:  14.65, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9521[sec], evaluation: 0.0433[sec]\n",
            "2023-01-19 13:09:49,749 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:09:49,752 - INFO - joeynmt.training - \tSource:     بو بارهده یئنه دئییب : اونلار منیم راحاتلێق دییاریما گیرمهیهجکلر . \n",
            "2023-01-19 13:09:49,753 - INFO - joeynmt.training - \tReference:  و باز گفته است : آنان به آرامی من راه نخواهند یافت . \n",
            "2023-01-19 13:09:49,753 - INFO - joeynmt.training - \tHypothesis: در مورد این امور به آنان گفته است : آنان در مورد من جایی که به آنان گفته نخواهند شد .\n",
            "2023-01-19 13:09:49,753 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:09:49,755 - INFO - joeynmt.training - \tSource:     ائله بیر کیمسه یوخدور کی ، اونون اۆستۆنده بیر گؤزتچی اولماسێن ! \n",
            "2023-01-19 13:09:49,755 - INFO - joeynmt.training - \tReference:  هیچ کس نیست مگر اینکه نگاهبانی بر او گماشته شده‌ است . \n",
            "2023-01-19 13:09:49,755 - INFO - joeynmt.training - \tHypothesis: هیچ کس بر او نیست مگر آنکه بر سر او نگهبانی باشد .\n",
            "2023-01-19 13:09:49,755 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:09:49,757 - INFO - joeynmt.training - \tSource:     بئله اولدوقدا ربینیزین هانسێ نئ متلرینی یالان سایا بیلرسینیز \n",
            "2023-01-19 13:09:49,757 - INFO - joeynmt.training - \tReference:  پس کدام یک از نعمتهای پروردگارتان را منکرید ؟ \n",
            "2023-01-19 13:09:49,758 - INFO - joeynmt.training - \tHypothesis: پس کدام یک از نعمتهای پروردگارتان را دروغ می شمارید ؟\n",
            "2023-01-19 13:09:49,758 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:09:49,760 - INFO - joeynmt.training - \tSource:     او گۆن اونلارێ بیر یئره یێغاجاق ، سونرا دا ملکلره بئله دئیهجک : بونلار سیزمی ابادت ائدیردیلر ؟ \n",
            "2023-01-19 13:09:49,760 - INFO - joeynmt.training - \tReference:  و یاد کن‌ روزی را که همه آنان را محشور می‌کند ، آنگاه به فرشتگان می‌فرماید : آیا اینها بودند که شما را می‌پرستیدند ؟ \n",
            "2023-01-19 13:09:49,760 - INFO - joeynmt.training - \tHypothesis: روزی که آنان را گرد آورد ، سپس آنان را گرد می آورد و می گویند : آیا اینها را می پرستید ؟\n",
            "2023-01-19 13:09:54,537 - INFO - joeynmt.training - Epoch 207: total training loss 599.72\n",
            "2023-01-19 13:09:54,538 - INFO - joeynmt.training - EPOCH 208\n",
            "2023-01-19 13:09:57,588 - INFO - joeynmt.training - Epoch 208, Step:    72100, Batch Loss:     1.782594, Batch Acc: 0.556523, Tokens per Sec:    15361, Lr: 0.000033\n",
            "2023-01-19 13:10:05,296 - INFO - joeynmt.training - Epoch 208, Step:    72200, Batch Loss:     1.711372, Batch Acc: 0.552503, Tokens per Sec:    15614, Lr: 0.000033\n",
            "2023-01-19 13:10:13,001 - INFO - joeynmt.training - Epoch 208, Step:    72300, Batch Loss:     1.757750, Batch Acc: 0.554220, Tokens per Sec:    15691, Lr: 0.000033\n",
            "2023-01-19 13:10:20,716 - INFO - joeynmt.training - Epoch 208, Step:    72400, Batch Loss:     1.649410, Batch Acc: 0.554044, Tokens per Sec:    15656, Lr: 0.000033\n",
            "2023-01-19 13:10:21,458 - INFO - joeynmt.training - Epoch 208: total training loss 599.52\n",
            "2023-01-19 13:10:21,459 - INFO - joeynmt.training - EPOCH 209\n",
            "2023-01-19 13:10:28,509 - INFO - joeynmt.training - Epoch 209, Step:    72500, Batch Loss:     1.665269, Batch Acc: 0.556651, Tokens per Sec:    15564, Lr: 0.000033\n",
            "2023-01-19 13:10:36,156 - INFO - joeynmt.training - Epoch 209, Step:    72600, Batch Loss:     1.597077, Batch Acc: 0.559397, Tokens per Sec:    15536, Lr: 0.000033\n",
            "2023-01-19 13:10:43,836 - INFO - joeynmt.training - Epoch 209, Step:    72700, Batch Loss:     1.657201, Batch Acc: 0.549627, Tokens per Sec:    15746, Lr: 0.000033\n",
            "2023-01-19 13:10:48,326 - INFO - joeynmt.training - Epoch 209: total training loss 600.57\n",
            "2023-01-19 13:10:48,327 - INFO - joeynmt.training - EPOCH 210\n",
            "2023-01-19 13:10:51,564 - INFO - joeynmt.training - Epoch 210, Step:    72800, Batch Loss:     1.693024, Batch Acc: 0.558355, Tokens per Sec:    15585, Lr: 0.000033\n",
            "2023-01-19 13:10:59,358 - INFO - joeynmt.training - Epoch 210, Step:    72900, Batch Loss:     1.773789, Batch Acc: 0.556147, Tokens per Sec:    15386, Lr: 0.000033\n",
            "2023-01-19 13:11:07,184 - INFO - joeynmt.training - Epoch 210, Step:    73000, Batch Loss:     1.616912, Batch Acc: 0.555814, Tokens per Sec:    15329, Lr: 0.000033\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.95ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8121.16ex/s]\n",
            "2023-01-19 13:11:07,500 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=73000\n",
            "2023-01-19 13:11:07,500 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:11:12,655 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:11:12,655 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:11:12,656 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:11:12,657 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:11:12,660 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.14, loss:   2.75, ppl:  15.64, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1093[sec], evaluation: 0.0430[sec]\n",
            "2023-01-19 13:11:12,662 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:11:12,666 - INFO - joeynmt.training - \tSource:     چۆنکی اونا سنین ربین وحی ائتمیشدیر ! \n",
            "2023-01-19 13:11:12,666 - INFO - joeynmt.training - \tReference:   همان گونه‌ که پروردگارت بدان وحی کرده است . \n",
            "2023-01-19 13:11:12,666 - INFO - joeynmt.training - \tHypothesis: زیرا به او وحی کرده است .\n",
            "2023-01-19 13:11:12,666 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:11:12,668 - INFO - joeynmt.training - \tSource:     اۆچ گۆن ارزینده شاعلون گؤزلری گؤرمهدی وه او نه یئدی ، نه ده ایچدی . \n",
            "2023-01-19 13:11:12,668 - INFO - joeynmt.training - \tReference:  او برای سه روز نابینا بود و چیزی نمی‌خورد و نمی‌آشامید . \n",
            "2023-01-19 13:11:12,669 - INFO - joeynmt.training - \tHypothesis: سه روز بعد ، سوم با دیدن آن بود و بخور ، خورد و خوردن آن ها نشین .\n",
            "2023-01-19 13:11:12,669 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:11:12,671 - INFO - joeynmt.training - \tSource:     کیتابین آلملرین ربی ترهفیندن نازل ائدیلمهسینده هئچ بیر شک شبههه یوخدور ! \n",
            "2023-01-19 13:11:12,671 - INFO - joeynmt.training - \tReference:  نازل شدن این کتاب که هیچ جای‌ شک در آن نیست از طرف پروردگار جهانهاست . \n",
            "2023-01-19 13:11:12,671 - INFO - joeynmt.training - \tHypothesis: و کتابی که از جانب پروردگار جهانیان به سوی پروردگار جهانیان نمی شود ،\n",
            "2023-01-19 13:11:12,671 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:11:12,673 - INFO - joeynmt.training - \tSource:     ائی قؤوموم ! اؤلچوده وه چکیده دۆز اولون . اینسانلارین هئچ بیر شئیده حاققێنێ اسکیلتمهیین . یئر اۆزۆنده گزیب فیتنه فصاد تؤرتمهیین ! \n",
            "2023-01-19 13:11:12,673 - INFO - joeynmt.training - \tReference:   و ای قوم من ، پیمانه و ترازو را به داد ، تمام دهید ، و حقوق مردم را کم مدهید ، و در زمین به فساد سر برمدارید . \n",
            "2023-01-19 13:11:12,674 - INFO - joeynmt.training - \tHypothesis: ای قوم من ، مردم و شهادتی را که در خاکستر مردم قرار می دهند . و در زمین فساد مکنید . در زمین فساد مدارید .\n",
            "2023-01-19 13:11:20,393 - INFO - joeynmt.training - Epoch 210, Step:    73100, Batch Loss:     1.781394, Batch Acc: 0.551950, Tokens per Sec:    15116, Lr: 0.000033\n",
            "2023-01-19 13:11:20,961 - INFO - joeynmt.training - Epoch 210: total training loss 600.27\n",
            "2023-01-19 13:11:20,961 - INFO - joeynmt.training - EPOCH 211\n",
            "2023-01-19 13:11:28,208 - INFO - joeynmt.training - Epoch 211, Step:    73200, Batch Loss:     1.718873, Batch Acc: 0.555183, Tokens per Sec:    15604, Lr: 0.000033\n",
            "2023-01-19 13:11:35,873 - INFO - joeynmt.training - Epoch 211, Step:    73300, Batch Loss:     1.775846, Batch Acc: 0.558242, Tokens per Sec:    15719, Lr: 0.000033\n",
            "2023-01-19 13:11:44,844 - INFO - joeynmt.training - Epoch 211, Step:    73400, Batch Loss:     1.734852, Batch Acc: 0.552710, Tokens per Sec:    13457, Lr: 0.000033\n",
            "2023-01-19 13:11:49,022 - INFO - joeynmt.training - Epoch 211: total training loss 596.02\n",
            "2023-01-19 13:11:49,022 - INFO - joeynmt.training - EPOCH 212\n",
            "2023-01-19 13:11:52,566 - INFO - joeynmt.training - Epoch 212, Step:    73500, Batch Loss:     1.596514, Batch Acc: 0.555496, Tokens per Sec:    15778, Lr: 0.000033\n",
            "2023-01-19 13:12:00,292 - INFO - joeynmt.training - Epoch 212, Step:    73600, Batch Loss:     1.679339, Batch Acc: 0.556679, Tokens per Sec:    15552, Lr: 0.000033\n",
            "2023-01-19 13:12:08,107 - INFO - joeynmt.training - Epoch 212, Step:    73700, Batch Loss:     1.700402, Batch Acc: 0.553010, Tokens per Sec:    15516, Lr: 0.000033\n",
            "2023-01-19 13:12:15,971 - INFO - joeynmt.training - Epoch 212, Step:    73800, Batch Loss:     1.697255, Batch Acc: 0.554506, Tokens per Sec:    15356, Lr: 0.000033\n",
            "2023-01-19 13:12:16,129 - INFO - joeynmt.training - Epoch 212: total training loss 596.34\n",
            "2023-01-19 13:12:16,130 - INFO - joeynmt.training - EPOCH 213\n",
            "2023-01-19 13:12:23,791 - INFO - joeynmt.training - Epoch 213, Step:    73900, Batch Loss:     1.689319, Batch Acc: 0.555921, Tokens per Sec:    15744, Lr: 0.000033\n",
            "2023-01-19 13:12:31,612 - INFO - joeynmt.training - Epoch 213, Step:    74000, Batch Loss:     1.768873, Batch Acc: 0.558131, Tokens per Sec:    15437, Lr: 0.000033\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 140.43ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9821.07ex/s] \n",
            "2023-01-19 13:12:31,887 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=74000\n",
            "2023-01-19 13:12:31,887 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:12:37,315 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:12:37,315 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:12:37,315 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:12:37,316 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:12:37,319 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.20, loss:   2.69, ppl:  14.75, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.3807[sec], evaluation: 0.0441[sec]\n",
            "2023-01-19 13:12:37,322 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:12:37,326 - INFO - joeynmt.training - \tSource:     خئیرلی اولان هر شئیی سیزه بیان ائدهرک ایستر آچێق ، ایسترسه ده ائودن ائوه گزیب دولاشاراق تعلیم وئرمکدن چکینمهدیم . \n",
            "2023-01-19 13:12:37,326 - INFO - joeynmt.training - \tReference:  در عین حال ، از گفتن هر آنچه برای شما مفید بود ، دریغ نکردم و شما را چه در جمع و چه خانه به خانه تعلیم دادم . \n",
            "2023-01-19 13:12:37,326 - INFO - joeynmt.training - \tHypothesis: اما چنین نیست که هر چیزی را که در خانه اش نداده بود ، به شما نشان داده است و در خانهٔ خود تعلیم دهد ، نداد .\n",
            "2023-01-19 13:12:37,326 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:12:37,328 - INFO - joeynmt.training - \tSource:     سنسه اؤیرندیگین وه گۆۆندیگین شئیلره صادق قال . چۆنکی بونلارێ کیملردن اؤیرندیگینی بیلیرسن ، \n",
            "2023-01-19 13:12:37,328 - INFO - joeynmt.training - \tReference:  اما ، تو آنچه را از دیگران آموختی و با دلیل و برهان به آن متقاعد شدی ، دنبال کن ؛ زیرا می‌دانی آن‌ها را از چه کسانی آموخته‌ای . \n",
            "2023-01-19 13:12:37,329 - INFO - joeynmt.training - \tHypothesis: اما تو از آنچه می دانستی و آنچه را که منصوب می کند ، می دانی . در واقع ، این ها را می داند ؛\n",
            "2023-01-19 13:12:37,329 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:12:37,331 - INFO - joeynmt.training - \tSource:      . اونا : جنته داخل اول ! دئییلدی . دئدی : کاش قؤوموم بیلهیدی کی ، \n",
            "2023-01-19 13:12:37,331 - INFO - joeynmt.training - \tReference:   سرانجام به جرم ایمان کشته شد ، و بدو گفته شد : به بهشت درآی . گفت : ای کاش ، قوم من می‌دانستند ، \n",
            "2023-01-19 13:12:37,331 - INFO - joeynmt.training - \tHypothesis: و به او گفت : در بهشت داخل شو . و گفت : کاشته در آن وارد شو ، که گروهی از ستمکاران را بدانید .\n",
            "2023-01-19 13:12:37,331 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:12:37,333 - INFO - joeynmt.training - \tSource:     بونلارێن خبری یئروسلیمدکی جمیته چاتدێ وه بارنابانێ آنتاقیایا گؤندردیلر . \n",
            "2023-01-19 13:12:37,333 - INFO - joeynmt.training - \tReference:  خبر فعالیت‌های آنان به گوش جماعت اورشلیم رسید و ایشان برنابا را به انطاکیه فرستادند . \n",
            "2023-01-19 13:12:37,333 - INFO - joeynmt.training - \tHypothesis: پس از این که خبر این امور را به جماعت خود سپردند و به برنابا و برنابا را به قیصریه فرستادند .\n",
            "2023-01-19 13:12:45,029 - INFO - joeynmt.training - Epoch 213, Step:    74100, Batch Loss:     1.804726, Batch Acc: 0.550852, Tokens per Sec:    15158, Lr: 0.000033\n",
            "2023-01-19 13:12:48,648 - INFO - joeynmt.training - Epoch 213: total training loss 592.88\n",
            "2023-01-19 13:12:48,649 - INFO - joeynmt.training - EPOCH 214\n",
            "2023-01-19 13:12:52,744 - INFO - joeynmt.training - Epoch 214, Step:    74200, Batch Loss:     1.649827, Batch Acc: 0.561945, Tokens per Sec:    15894, Lr: 0.000033\n",
            "2023-01-19 13:13:00,368 - INFO - joeynmt.training - Epoch 214, Step:    74300, Batch Loss:     1.707763, Batch Acc: 0.556720, Tokens per Sec:    15702, Lr: 0.000033\n",
            "2023-01-19 13:13:08,106 - INFO - joeynmt.training - Epoch 214, Step:    74400, Batch Loss:     1.650040, Batch Acc: 0.556914, Tokens per Sec:    15637, Lr: 0.000033\n",
            "2023-01-19 13:13:15,404 - INFO - joeynmt.training - Epoch 214: total training loss 595.75\n",
            "2023-01-19 13:13:15,404 - INFO - joeynmt.training - EPOCH 215\n",
            "2023-01-19 13:13:15,811 - INFO - joeynmt.training - Epoch 215, Step:    74500, Batch Loss:     1.671600, Batch Acc: 0.551888, Tokens per Sec:    13925, Lr: 0.000033\n",
            "2023-01-19 13:13:23,556 - INFO - joeynmt.training - Epoch 215, Step:    74600, Batch Loss:     1.723246, Batch Acc: 0.556265, Tokens per Sec:    15614, Lr: 0.000033\n",
            "2023-01-19 13:13:31,257 - INFO - joeynmt.training - Epoch 215, Step:    74700, Batch Loss:     1.808250, Batch Acc: 0.557719, Tokens per Sec:    15654, Lr: 0.000033\n",
            "2023-01-19 13:13:38,944 - INFO - joeynmt.training - Epoch 215, Step:    74800, Batch Loss:     1.594957, Batch Acc: 0.556436, Tokens per Sec:    15658, Lr: 0.000033\n",
            "2023-01-19 13:13:42,316 - INFO - joeynmt.training - Epoch 215: total training loss 595.27\n",
            "2023-01-19 13:13:42,317 - INFO - joeynmt.training - EPOCH 216\n",
            "2023-01-19 13:13:46,787 - INFO - joeynmt.training - Epoch 216, Step:    74900, Batch Loss:     1.824218, Batch Acc: 0.556416, Tokens per Sec:    15486, Lr: 0.000033\n",
            "2023-01-19 13:13:54,493 - INFO - joeynmt.training - Epoch 216, Step:    75000, Batch Loss:     1.740891, Batch Acc: 0.561299, Tokens per Sec:    15454, Lr: 0.000033\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.44ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10428.31ex/s]\n",
            "2023-01-19 13:13:54,757 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=75000\n",
            "2023-01-19 13:13:54,757 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:14:00,077 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:14:00,077 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:14:00,078 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:14:00,079 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:14:00,081 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.64, loss:   2.65, ppl:  14.16, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.2663[sec], evaluation: 0.0505[sec]\n",
            "2023-01-19 13:14:00,084 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:14:00,087 - INFO - joeynmt.training - \tSource:     ایسا اترافینداکیلارا غزبله باخدێ وه اونلارێن اینادکار اۆرکلی اولدوقلارینا گؤره کدرلهنیب او آداما دئدی : الینی اوزات ! او ، الینی اوزاتدێ وه الی اوهلکی هالێنا قایێتدی . \n",
            "2023-01-19 13:14:00,088 - INFO - joeynmt.training - \tReference:  عیسی از سنگدلی‌شان عمیقا اندوهگین شد و با خشم به آنان نظر افکند . سپس به آن مرد گفت : دستت را دراز کن . او دستش را دراز کرد و دست او شفا یافت . \n",
            "2023-01-19 13:14:00,088 - INFO - joeynmt.training - \tHypothesis: عیسی با دیدن این که به ناحیهٔ آفتابت می کرد ، بر دلشان پرتلال کرد و گفت : دستت را در دل خود بردار و دست هایش را دراز کرد . آنگاه دستش از دستش برخواهد خاست .\n",
            "2023-01-19 13:14:00,088 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:14:00,090 - INFO - joeynmt.training - \tSource:     آللاهێن روحونو بوندان تانییا بیلرسینیز : ایسا مسیحین جسما گلدیگینی اقرار ائدن هر روح آللاهداندێر ، \n",
            "2023-01-19 13:14:00,090 - INFO - joeynmt.training - \tReference:  به این طریق تشخیص می‌دهید که گفته‌ای الهام‌شده از خداست : هر گفتهٔ الهام‌شده که تصدیق کند عیسی مسیح به صورت انسان آمد ، از جانب خداست . \n",
            "2023-01-19 13:14:00,091 - INFO - joeynmt.training - \tHypothesis: همچنین روح خدا را می شناسید و می توانید از او می شناسید : روحیه ای که خدا را تصدیق می کند ، خدا را تصدیق می کند .\n",
            "2023-01-19 13:14:00,091 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:14:00,093 - INFO - joeynmt.training - \tSource:      اونلارێ تورپاغێندان قوووب چێخارتماق ایستهدی . بیز ایسه اونو وه اونونلا بیرلیکده اولانلارێن هامیسینی قرق ائتدیک . \n",
            "2023-01-19 13:14:00,093 - INFO - joeynmt.training - \tReference:  پس فرعون‌ تصمیم گرفت که آنان را از سرزمین مصر برکند ، پس او و هر که را با وی بود همه را غرق کردیم . \n",
            "2023-01-19 13:14:00,093 - INFO - joeynmt.training - \tHypothesis: و از زمین بیرون رانیم ، تا خواستیم از سرزمین خود بیرون راند و او و همه همه را با شایستگان قرار دادیم .\n",
            "2023-01-19 13:14:00,093 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:14:00,095 - INFO - joeynmt.training - \tSource:     رب روحدور ، ربین روحو هارادادێرسا ، آزادلێق دا اورادادێر . \n",
            "2023-01-19 13:14:00,095 - INFO - joeynmt.training - \tReference:  یهوه روح است و هر جا روح یهوه باشد ، آنجا آزادی است . \n",
            "2023-01-19 13:14:00,095 - INFO - joeynmt.training - \tHypothesis: سرور روح یهوه روح است و اگر روح یهوه آزاد می شود ، از آنجا آزاد خواهد شد .\n",
            "2023-01-19 13:14:07,857 - INFO - joeynmt.training - Epoch 216, Step:    75100, Batch Loss:     1.717397, Batch Acc: 0.557477, Tokens per Sec:    15034, Lr: 0.000033\n",
            "2023-01-19 13:14:14,951 - INFO - joeynmt.training - Epoch 216: total training loss 594.79\n",
            "2023-01-19 13:14:14,952 - INFO - joeynmt.training - EPOCH 217\n",
            "2023-01-19 13:14:15,678 - INFO - joeynmt.training - Epoch 217, Step:    75200, Batch Loss:     1.563490, Batch Acc: 0.562472, Tokens per Sec:    15451, Lr: 0.000033\n",
            "2023-01-19 13:14:23,412 - INFO - joeynmt.training - Epoch 217, Step:    75300, Batch Loss:     1.729074, Batch Acc: 0.556809, Tokens per Sec:    15652, Lr: 0.000033\n",
            "2023-01-19 13:14:31,163 - INFO - joeynmt.training - Epoch 217, Step:    75400, Batch Loss:     1.617120, Batch Acc: 0.557806, Tokens per Sec:    15625, Lr: 0.000033\n",
            "2023-01-19 13:14:38,846 - INFO - joeynmt.training - Epoch 217, Step:    75500, Batch Loss:     1.657185, Batch Acc: 0.557619, Tokens per Sec:    15850, Lr: 0.000033\n",
            "2023-01-19 13:14:41,741 - INFO - joeynmt.training - Epoch 217: total training loss 591.22\n",
            "2023-01-19 13:14:41,741 - INFO - joeynmt.training - EPOCH 218\n",
            "2023-01-19 13:14:46,562 - INFO - joeynmt.training - Epoch 218, Step:    75600, Batch Loss:     1.790564, Batch Acc: 0.557604, Tokens per Sec:    15752, Lr: 0.000033\n",
            "2023-01-19 13:14:54,214 - INFO - joeynmt.training - Epoch 218, Step:    75700, Batch Loss:     1.856138, Batch Acc: 0.558510, Tokens per Sec:    15884, Lr: 0.000033\n",
            "2023-01-19 13:15:03,225 - INFO - joeynmt.training - Epoch 218, Step:    75800, Batch Loss:     1.689177, Batch Acc: 0.558229, Tokens per Sec:    13353, Lr: 0.000032\n",
            "2023-01-19 13:15:09,885 - INFO - joeynmt.training - Epoch 218: total training loss 594.18\n",
            "2023-01-19 13:15:09,886 - INFO - joeynmt.training - EPOCH 219\n",
            "2023-01-19 13:15:11,068 - INFO - joeynmt.training - Epoch 219, Step:    75900, Batch Loss:     1.711581, Batch Acc: 0.546968, Tokens per Sec:    14999, Lr: 0.000032\n",
            "2023-01-19 13:15:18,891 - INFO - joeynmt.training - Epoch 219, Step:    76000, Batch Loss:     1.588158, Batch Acc: 0.560160, Tokens per Sec:    15580, Lr: 0.000032\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.50ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9187.60ex/s]\n",
            "2023-01-19 13:15:19,179 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=76000\n",
            "2023-01-19 13:15:19,180 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:15:23,683 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:15:23,684 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:15:23,684 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:15:23,685 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:15:23,688 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.60, loss:   2.67, ppl:  14.40, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4607[sec], evaluation: 0.0400[sec]\n",
            "2023-01-19 13:15:23,690 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:15:23,693 - INFO - joeynmt.training - \tSource:     حقیقتا ، یئتیملرین ماللارێنێ حاقسێزلێقلا یئینلرین یئدیکلری قارینلاریندا اودا چئوریلهجک وه اونلار آلوولو جهنهمه گیرهجکلر . \n",
            "2023-01-19 13:15:23,694 - INFO - joeynmt.training - \tReference:  در حقیقت ، کسانی که اموال یتیمان را به ستم می‌خورند ، جز این نیست که آتشی در شکم خود فرو می‌برند ، و به زودی در آتشی فروزان درآیند . \n",
            "2023-01-19 13:15:23,694 - INFO - joeynmt.training - \tHypothesis: در حقیقت ، هرچند اموال کسانی که مرتکب اعمال نامشروع جنسی می شوند ، در کنیسه هایشان پر در آتش و چراغدان خواهند شد .\n",
            "2023-01-19 13:15:23,694 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:15:23,696 - INFO - joeynmt.training - \tSource:     هر کس اۆچۆن ائتدیگی امللره گؤره درهجهلر واردێر . ربین اونلارێن نه ائتدیکلریندن قافل دئییلدیر ! \n",
            "2023-01-19 13:15:23,696 - INFO - joeynmt.training - \tReference:  و برای هر یک از این دو گروه‌ ، از آنچه انجام داده‌اند ، در جزا مراتبی خواهد بود ، و پروردگارت از آنچه می‌کنند غافل نیست . \n",
            "2023-01-19 13:15:23,696 - INFO - joeynmt.training - \tHypothesis: و هر کس به سزای آنچه انجام داده اند برایشان است ، قطعا پروردگارت به آنچه انجام می دادند غافل نیست .\n",
            "2023-01-19 13:15:23,696 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:15:23,698 - INFO - joeynmt.training - \tSource:     آللاهێن کؤمگی ایله . ایستهدیگینه کؤمک ائدر . او ، یئنیلمز غۆۆت صاحبی ، مرحمت صاحبدر ! \n",
            "2023-01-19 13:15:23,698 - INFO - joeynmt.training - \tReference:  هر که را بخواهد یاری می‌کند ، و اوست شکست‌ناپذیر مهربان . \n",
            "2023-01-19 13:15:23,699 - INFO - joeynmt.training - \tHypothesis: و به یاری خدا ارجمند می کند ، و اوست ارجمند مهربان .\n",
            "2023-01-19 13:15:23,699 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:15:23,701 - INFO - joeynmt.training - \tSource:     فاریسئیلر ایسنانین یانینا گلیب اونونلا مۆباحصه ائتمهیه باشلادێ . اونو سێناماق اۆچۆن ایستهدیلر کی ، گؤیدن گلن بیر علامت گؤسترسین . \n",
            "2023-01-19 13:15:23,701 - INFO - joeynmt.training - \tReference:  در آنجا فریسیان نزد عیسی آمدند و با او به بحث پرداختند . آنان برای آزمودن عیسی ، از وی خواستند نشانه‌ای از آسمان برایشان نمایان سازد . \n",
            "2023-01-19 13:15:23,701 - INFO - joeynmt.training - \tHypothesis: فریسیان نزد عیسی آمدند و با او چنین سؤال کردند . او خواستند که نشانه ای از آسمان برای او بیاورد .\n",
            "2023-01-19 13:15:31,487 - INFO - joeynmt.training - Epoch 219, Step:    76100, Batch Loss:     1.741331, Batch Acc: 0.557073, Tokens per Sec:    14794, Lr: 0.000032\n",
            "2023-01-19 13:15:39,289 - INFO - joeynmt.training - Epoch 219, Step:    76200, Batch Loss:     1.731900, Batch Acc: 0.558367, Tokens per Sec:    15757, Lr: 0.000032\n",
            "2023-01-19 13:15:41,732 - INFO - joeynmt.training - Epoch 219: total training loss 591.46\n",
            "2023-01-19 13:15:41,732 - INFO - joeynmt.training - EPOCH 220\n",
            "2023-01-19 13:15:46,974 - INFO - joeynmt.training - Epoch 220, Step:    76300, Batch Loss:     1.634653, Batch Acc: 0.560308, Tokens per Sec:    15697, Lr: 0.000032\n",
            "2023-01-19 13:15:54,668 - INFO - joeynmt.training - Epoch 220, Step:    76400, Batch Loss:     1.563116, Batch Acc: 0.559636, Tokens per Sec:    15556, Lr: 0.000032\n",
            "2023-01-19 13:16:02,305 - INFO - joeynmt.training - Epoch 220, Step:    76500, Batch Loss:     1.720473, Batch Acc: 0.558575, Tokens per Sec:    15588, Lr: 0.000032\n",
            "2023-01-19 13:16:08,594 - INFO - joeynmt.training - Epoch 220: total training loss 594.10\n",
            "2023-01-19 13:16:08,594 - INFO - joeynmt.training - EPOCH 221\n",
            "2023-01-19 13:16:10,044 - INFO - joeynmt.training - Epoch 221, Step:    76600, Batch Loss:     1.698831, Batch Acc: 0.562271, Tokens per Sec:    15608, Lr: 0.000032\n",
            "2023-01-19 13:16:17,772 - INFO - joeynmt.training - Epoch 221, Step:    76700, Batch Loss:     1.711475, Batch Acc: 0.557359, Tokens per Sec:    15525, Lr: 0.000032\n",
            "2023-01-19 13:16:25,577 - INFO - joeynmt.training - Epoch 221, Step:    76800, Batch Loss:     1.701699, Batch Acc: 0.558676, Tokens per Sec:    15534, Lr: 0.000032\n",
            "2023-01-19 13:16:33,353 - INFO - joeynmt.training - Epoch 221, Step:    76900, Batch Loss:     1.690177, Batch Acc: 0.556977, Tokens per Sec:    15377, Lr: 0.000032\n",
            "2023-01-19 13:16:35,723 - INFO - joeynmt.training - Epoch 221: total training loss 593.92\n",
            "2023-01-19 13:16:35,723 - INFO - joeynmt.training - EPOCH 222\n",
            "2023-01-19 13:16:41,181 - INFO - joeynmt.training - Epoch 222, Step:    77000, Batch Loss:     1.775125, Batch Acc: 0.558107, Tokens per Sec:    15591, Lr: 0.000032\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.89ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10649.65ex/s]\n",
            "2023-01-19 13:16:41,461 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=77000\n",
            "2023-01-19 13:16:41,462 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:16:45,867 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:16:45,868 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:16:45,868 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:16:45,869 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:16:45,872 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.67, loss:   2.80, ppl:  16.41, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3591[sec], evaluation: 0.0433[sec]\n",
            "2023-01-19 13:16:45,874 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:16:45,878 - INFO - joeynmt.training - \tSource:      یئ ایچ ، گؤزۆن آیدین اولسون . اگر بیر آدام گؤرهجک اولسان ، بئله دئ : من رهمان یولوندا اوروج توتماغێ نظیر ائلهمیشم ، اونا گؤره ده بو گۆن هئچ کسله دانیشمایاجاغام ! \n",
            "2023-01-19 13:16:45,878 - INFO - joeynmt.training - \tReference:  و بخور و بنوش و دیده روشن دار . پس اگر کسی از آدمیان را دیدی ، بگوی : من برای خدای‌ رحمان روزه نذر کرده‌ام ، و امروز مطلقا با انسانی سخن نخواهم گفت . \n",
            "2023-01-19 13:16:45,879 - INFO - joeynmt.training - \tHypothesis: باید بگویید : ای مرد ، اگر کسی باید در راه خود بگوید : به راه راست من در راه خدا گم کرده ام ، و هر که را در راه خودمایی کرده ام ، دیگر بام تقوا در آن روز باز خواهم کرد .\n",
            "2023-01-19 13:16:45,879 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:16:45,881 - INFO - joeynmt.training - \tSource:     آمما او منه دئدی : لۆتفۆم سنه کفایتدیر ، چۆنکی زیفلیک اولاندا قۆۆه تام اولور . مسیحین قۆۆهسی منده مسکن سالسێن دئیه زیفلیکلریمله داها چوخ سئوینه سئوینه اؤیونهجهیهم . \n",
            "2023-01-19 13:16:45,881 - INFO - joeynmt.training - \tReference:  اما او به من گفت : لطف من برای تو کافی است ؛ زیرا قدرت من در ضعف تو کاملا آشکار می‌شود . پس با شادی هر چه بیشتر در مورد ضعف‌های خود فخر خواهم کرد تا قدرت مسیح همچون خیمه‌ای مرا در پناه خود نگاه دارد . \n",
            "2023-01-19 13:16:45,881 - INFO - joeynmt.training - \tHypothesis: اما او به من گفت : لطفی که من است ، من فقط به تو عطا شده است ؛ زیرا من قدرت را از آن ها متفاوت می کند تا به خاطر مسیح ، بسیاری از شادمانم .\n",
            "2023-01-19 13:16:45,881 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:16:45,883 - INFO - joeynmt.training - \tSource:     ایسا اونلارا جاواب وئردی : سیز ده هله درک ائتمهمیسینیز ؟ باشا دۆشمۆرسۆنۆز کی ، کناردان اینسانین داخلینه گیرن هر بیر شئی اینسانی موردار ائده بیلمز ؟ \n",
            "2023-01-19 13:16:45,884 - INFO - joeynmt.training - \tReference:  عیسی گفت : آیا شما نیز مانند آنان درک نمی‌کنید ؟ آیا نمی‌دانید که هیچ چیز نمی‌تواند از بیرون به انسان وارد شود و او را نجس کند ؟ \n",
            "2023-01-19 13:16:45,884 - INFO - joeynmt.training - \tHypothesis: عیسی در جواب گفت : آیا شما نیز هنوز نمی کنید ؟ آیا نمی دانید که انسان از هر جهاد نمی توانید به جا آورد ، اما انسان وارد شود ؟\n",
            "2023-01-19 13:16:45,884 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:16:45,886 - INFO - joeynmt.training - \tSource:     او ، نؤکرلردن بیرینی یانینا چاغێرێب سوروشدو : بو ندیر ؟ \n",
            "2023-01-19 13:16:45,886 - INFO - joeynmt.training - \tReference:  پس یکی از خدمتکاران را صدا کرد و از او پرسید : چه خبر است ؟ \n",
            "2023-01-19 13:16:45,886 - INFO - joeynmt.training - \tHypothesis: پس از یکی از مأموران خود را فراخواند و گفت : این چیست ؟\n",
            "2023-01-19 13:16:53,615 - INFO - joeynmt.training - Epoch 222, Step:    77100, Batch Loss:     1.631575, Batch Acc: 0.559954, Tokens per Sec:    15090, Lr: 0.000032\n",
            "2023-01-19 13:17:01,261 - INFO - joeynmt.training - Epoch 222, Step:    77200, Batch Loss:     1.611443, Batch Acc: 0.560631, Tokens per Sec:    15825, Lr: 0.000032\n",
            "2023-01-19 13:17:07,141 - INFO - joeynmt.training - Epoch 222: total training loss 588.10\n",
            "2023-01-19 13:17:07,142 - INFO - joeynmt.training - EPOCH 223\n",
            "2023-01-19 13:17:08,990 - INFO - joeynmt.training - Epoch 223, Step:    77300, Batch Loss:     1.626614, Batch Acc: 0.562828, Tokens per Sec:    15247, Lr: 0.000032\n",
            "2023-01-19 13:17:16,704 - INFO - joeynmt.training - Epoch 223, Step:    77400, Batch Loss:     1.673675, Batch Acc: 0.559620, Tokens per Sec:    15794, Lr: 0.000032\n",
            "2023-01-19 13:17:24,432 - INFO - joeynmt.training - Epoch 223, Step:    77500, Batch Loss:     1.792785, Batch Acc: 0.562445, Tokens per Sec:    15721, Lr: 0.000032\n",
            "2023-01-19 13:17:32,132 - INFO - joeynmt.training - Epoch 223, Step:    77600, Batch Loss:     1.659636, Batch Acc: 0.559893, Tokens per Sec:    15705, Lr: 0.000032\n",
            "2023-01-19 13:17:33,958 - INFO - joeynmt.training - Epoch 223: total training loss 587.95\n",
            "2023-01-19 13:17:33,959 - INFO - joeynmt.training - EPOCH 224\n",
            "2023-01-19 13:17:39,849 - INFO - joeynmt.training - Epoch 224, Step:    77700, Batch Loss:     1.822358, Batch Acc: 0.563415, Tokens per Sec:    15696, Lr: 0.000032\n",
            "2023-01-19 13:17:47,580 - INFO - joeynmt.training - Epoch 224, Step:    77800, Batch Loss:     1.685123, Batch Acc: 0.561241, Tokens per Sec:    15749, Lr: 0.000032\n",
            "2023-01-19 13:17:55,226 - INFO - joeynmt.training - Epoch 224, Step:    77900, Batch Loss:     1.579067, Batch Acc: 0.558808, Tokens per Sec:    15902, Lr: 0.000032\n",
            "2023-01-19 13:18:00,573 - INFO - joeynmt.training - Epoch 224: total training loss 587.95\n",
            "2023-01-19 13:18:00,573 - INFO - joeynmt.training - EPOCH 225\n",
            "2023-01-19 13:18:02,909 - INFO - joeynmt.training - Epoch 225, Step:    78000, Batch Loss:     1.610746, Batch Acc: 0.565918, Tokens per Sec:    15378, Lr: 0.000032\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 140.67ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10798.93ex/s]\n",
            "2023-01-19 13:18:03,194 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=78000\n",
            "2023-01-19 13:18:03,194 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:18:07,936 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:18:07,936 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:18:07,937 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:18:07,938 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:18:07,941 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.11, loss:   2.74, ppl:  15.46, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6912[sec], evaluation: 0.0478[sec]\n",
            "2023-01-19 13:18:07,943 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:18:07,947 - INFO - joeynmt.training - \tSource:     سیزه نه اولوب کی ، اۆستۆنده آللاهێن آدێ چکیلمیش اتیندن یئمهیهسینیز ؟ حالبوکی مجبوریت قارشیسیندا اولدوغونوز شئیلر استثنا ائدیلمکله ، سیزه هارام بویوردوقلارین آرتێق او مۆفصل شکیلده سیزه بیلدیرمیشدیر . شبههسیز کی ، چوخلارێ بیلمهدیکلریندن نفصلرینین ایستکلرینه اویاراق دۆز یولدان آزدێرارلار . ربین هدی آشانلارێ ان یاخشی تانییاندیر ! \n",
            "2023-01-19 13:18:07,947 - INFO - joeynmt.training - \tReference:  و شما را چه شده است که از آنچه نام خدا بر آن برده شده است نمی‌خورید ؟ با اینکه خدا آنچه را بر شما حرام کرده جز آنچه بدان ناچار شده‌اید برای شما به تفصیل بیان نموده است . و به راستی ، بسیاری از مردم ، دیگران را از روی نادانی ، با هوسهای خود گمراه می‌کنند . آری ، پروردگار تو به حال‌ تجاوزکاران داناتر است . \n",
            "2023-01-19 13:18:07,947 - INFO - joeynmt.training - \tHypothesis: آیا به شما وعده داده شده است که با آنکه گوشت داده اید ، با آنکه از آنچه به دست خدا داده شده است ؟ مگر آنچه را که به شما داده شده است به شکل کرده اید بر ضد شما حرام ساخته ایم ، و آنان ستم کرده اند . او بهترین دار و کسانی اند که خود را در گمراه ترند ، و خدا گمراه ترند .\n",
            "2023-01-19 13:18:07,947 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:18:07,949 - INFO - joeynmt.training - \tSource:     یهودا قبیلهسیندن 12 ، 000 مۆهورلنمیش نفر ؛ رووئن قبیلهسیندن 12 ، 000 نفر ؛ قاد قبیلهسیندن 12 ، 000 نفر ؛ \n",
            "2023-01-19 13:18:07,950 - INFO - joeynmt.training - \tReference:  از طایفهٔ یهودا ۱۲ ۰۰۰ نفر مهر شدند ؛ از طایفهٔ رئوبین ۱۲ ۰۰۰ نفر ؛ از طایفهٔ جاد ۱۲ ۰۰۰ نفر ؛ \n",
            "2023-01-19 13:18:07,950 - INFO - joeynmt.training - \tHypothesis: یهودا ۰۰۰ نفر از طایفهٔ شمعون ۱۲ ۰۰۰ نفر ؛ از طایفهٔ طایفهٔ ۱۲ ۰۰۰ نفر ؛\n",
            "2023-01-19 13:18:07,950 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:18:07,952 - INFO - joeynmt.training - \tSource:     چۆنکی آللاه قاریشیقلێق دئییل ، سۆلح قایناغیدیر . مۆقدسلرین بۆتۆن جمیتلرینده بئلهدیر . \n",
            "2023-01-19 13:18:07,952 - INFO - joeynmt.training - \tReference:  زیرا خدا ، خدای بی‌نظمی نیست ، بلکه خدای آرامش است . همان گونه که در همهٔ جماعت‌های مقدسان مرسوم است ، \n",
            "2023-01-19 13:18:07,952 - INFO - joeynmt.training - \tHypothesis: زیرا خدا در میان ابریش نیست ، بلکه صلح را از میان جماعت است .\n",
            "2023-01-19 13:18:07,952 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:18:07,954 - INFO - joeynmt.training - \tSource:     بۆتۆن ایمانلیلار جمیتینی وه بو هاداسنی ائشیدنلرین هامیسینی بؤیوک بیر واهمه بۆرۆدۆ . \n",
            "2023-01-19 13:18:07,955 - INFO - joeynmt.training - \tReference:  پس ترسی شدید در دل تمامی جماعت و هر که این ماجرا را می‌شنید ، افتاد . \n",
            "2023-01-19 13:18:07,955 - INFO - joeynmt.training - \tHypothesis: وقتی همهٔ جماعت و این ها را شنید ، این ها را شنیدم که همهٔ جماعت ها را بر پایهٔ بسیار کشید .\n",
            "2023-01-19 13:18:16,995 - INFO - joeynmt.training - Epoch 225, Step:    78100, Batch Loss:     1.742220, Batch Acc: 0.562732, Tokens per Sec:    13020, Lr: 0.000032\n",
            "2023-01-19 13:18:24,693 - INFO - joeynmt.training - Epoch 225, Step:    78200, Batch Loss:     1.648498, Batch Acc: 0.559532, Tokens per Sec:    15811, Lr: 0.000032\n",
            "2023-01-19 13:18:32,346 - INFO - joeynmt.training - Epoch 225, Step:    78300, Batch Loss:     1.802289, Batch Acc: 0.558407, Tokens per Sec:    15597, Lr: 0.000032\n",
            "2023-01-19 13:18:33,817 - INFO - joeynmt.training - Epoch 225: total training loss 589.01\n",
            "2023-01-19 13:18:33,817 - INFO - joeynmt.training - EPOCH 226\n",
            "2023-01-19 13:18:40,081 - INFO - joeynmt.training - Epoch 226, Step:    78400, Batch Loss:     1.730557, Batch Acc: 0.563649, Tokens per Sec:    15790, Lr: 0.000032\n",
            "2023-01-19 13:18:47,697 - INFO - joeynmt.training - Epoch 226, Step:    78500, Batch Loss:     1.810256, Batch Acc: 0.558724, Tokens per Sec:    15843, Lr: 0.000032\n",
            "2023-01-19 13:18:55,423 - INFO - joeynmt.training - Epoch 226, Step:    78600, Batch Loss:     1.725919, Batch Acc: 0.558037, Tokens per Sec:    15715, Lr: 0.000032\n",
            "2023-01-19 13:19:00,463 - INFO - joeynmt.training - Epoch 226: total training loss 589.77\n",
            "2023-01-19 13:19:00,464 - INFO - joeynmt.training - EPOCH 227\n",
            "2023-01-19 13:19:03,102 - INFO - joeynmt.training - Epoch 227, Step:    78700, Batch Loss:     1.564489, Batch Acc: 0.562600, Tokens per Sec:    15478, Lr: 0.000032\n",
            "2023-01-19 13:19:10,898 - INFO - joeynmt.training - Epoch 227, Step:    78800, Batch Loss:     1.648183, Batch Acc: 0.561593, Tokens per Sec:    15446, Lr: 0.000032\n",
            "2023-01-19 13:19:18,702 - INFO - joeynmt.training - Epoch 227, Step:    78900, Batch Loss:     1.609997, Batch Acc: 0.561698, Tokens per Sec:    15465, Lr: 0.000032\n",
            "2023-01-19 13:19:26,443 - INFO - joeynmt.training - Epoch 227, Step:    79000, Batch Loss:     1.809642, Batch Acc: 0.556594, Tokens per Sec:    15696, Lr: 0.000032\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 109.90ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9616.31ex/s]\n",
            "2023-01-19 13:19:26,728 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=79000\n",
            "2023-01-19 13:19:26,728 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:19:31,499 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:19:31,499 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:19:31,499 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:19:31,500 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:19:31,503 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.64, loss:   2.82, ppl:  16.77, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7234[sec], evaluation: 0.0436[sec]\n",
            "2023-01-19 13:19:31,506 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:19:31,509 - INFO - joeynmt.training - \tSource:      دئ : آللاهێن ایستهدیگیندن باشقا ، من اؤزومه نه بیر خئیر ، نه ده بیر زرر وئره بیلهرم . هر اممتین بیر اجل واختێ واردێر . اونلارێن اجهلی گلیب چاتدێقدا بیرجه ساعات بئله نه گئری قالار ، نه ده ایرهلی کئچرلر . \n",
            "2023-01-19 13:19:31,510 - INFO - joeynmt.training - \tReference:  بگو : برای خود زیان و سودی در اختیار ندارم ، مگر آنچه را که خدا بخواهد . هر امتی را زمانی محدود است . آنگاه که زمانشان به سر رسد ، پس نه ساعتی از آن‌ تأخیر کنند و نه پیشی گیرند . \n",
            "2023-01-19 13:19:31,510 - INFO - joeynmt.training - \tHypothesis: بگو : هرگز خدا را خواهم داد ، نه سود خود و نه سودی برای خودشان سود می رسانم . و نه پیش از آن ، نه افزود که ساعتی را اقتداری می کنند و نه اجلشان .\n",
            "2023-01-19 13:19:31,510 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:19:31,512 - INFO - joeynmt.training - \tSource:     اونلارا اوخوندوغو زامان : بیز اونا ایناندیق . دوغرودان دا ، ربیمیزدن حاقدێر . بیز اوندان اول ده مۆسلمان ایدیک ! دئییرلر . \n",
            "2023-01-19 13:19:31,512 - INFO - joeynmt.training - \tReference:  و چون بر ایشان فرو خوانده می‌شود ، می‌گویند : بدان ایمان آوردیم که آن درست است و از طرف پروردگار ماست ؛ ما پیش از آن هم‌ از تسلیم‌شوندگان بودیم . \n",
            "2023-01-19 13:19:31,512 - INFO - joeynmt.training - \tHypothesis: و چون بر آنان خوانده شود ، می گویند : ما به آنچه از آن ایمان آورده ایم ، پیش از آنکه به پروردگارمانمان بودیم .\n",
            "2023-01-19 13:19:31,512 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:19:31,514 - INFO - joeynmt.training - \tSource:     پئتئر اونا جاواب وئرهرک دئدی : بو مسهلی بیزه اضاح ائت . \n",
            "2023-01-19 13:19:31,515 - INFO - joeynmt.training - \tReference:  پطرس به او گفت : مفهوم مثلی را که پیش از این گفتی برای ما روشن کن . \n",
            "2023-01-19 13:19:31,515 - INFO - joeynmt.training - \tHypothesis: پطرس در جواب گفت : این مثل برای ما بیان کن .\n",
            "2023-01-19 13:19:31,515 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:19:31,517 - INFO - joeynmt.training - \tSource:     اونلاردان سدهقهلر بارهسینده سنه ائییب توتانلار دا وار . اگر اونلارا بیر شئی وئریلسه ، رازێ قالار ، وئریلمهسه ، درحال قزبلهنهرلر . \n",
            "2023-01-19 13:19:31,517 - INFO - joeynmt.training - \tReference:  و برخی از آنان در تقسیم‌ صدقات بر تو خرده می‌گیرند ، پس اگر از آن اموال‌ به ایشان داده شود خشنود می‌گردند ، و اگر از آن به ایشان داده نشود بناگاه به خشم می‌آیند . \n",
            "2023-01-19 13:19:31,517 - INFO - joeynmt.training - \tHypothesis: و در باره کسانی که در باره آنان در باره تو هستند ، می خواهند به حال چیزی نگذشته نخواهند داد ، و اگر به نکوهشت ، رکا می شوند ، بنا می کنند .\n",
            "2023-01-19 13:19:32,583 - INFO - joeynmt.training - Epoch 227: total training loss 586.32\n",
            "2023-01-19 13:19:32,583 - INFO - joeynmt.training - EPOCH 228\n",
            "2023-01-19 13:19:39,367 - INFO - joeynmt.training - Epoch 228, Step:    79100, Batch Loss:     1.688600, Batch Acc: 0.560741, Tokens per Sec:    15499, Lr: 0.000032\n",
            "2023-01-19 13:19:47,090 - INFO - joeynmt.training - Epoch 228, Step:    79200, Batch Loss:     1.869351, Batch Acc: 0.558978, Tokens per Sec:    15576, Lr: 0.000032\n",
            "2023-01-19 13:19:54,716 - INFO - joeynmt.training - Epoch 228, Step:    79300, Batch Loss:     1.647286, Batch Acc: 0.559713, Tokens per Sec:    15929, Lr: 0.000032\n",
            "2023-01-19 13:19:59,418 - INFO - joeynmt.training - Epoch 228: total training loss 588.03\n",
            "2023-01-19 13:19:59,418 - INFO - joeynmt.training - EPOCH 229\n",
            "2023-01-19 13:20:02,400 - INFO - joeynmt.training - Epoch 229, Step:    79400, Batch Loss:     1.688934, Batch Acc: 0.559237, Tokens per Sec:    15865, Lr: 0.000032\n",
            "2023-01-19 13:20:10,213 - INFO - joeynmt.training - Epoch 229, Step:    79500, Batch Loss:     1.871222, Batch Acc: 0.563965, Tokens per Sec:    15612, Lr: 0.000032\n",
            "2023-01-19 13:20:17,993 - INFO - joeynmt.training - Epoch 229, Step:    79600, Batch Loss:     1.810674, Batch Acc: 0.560983, Tokens per Sec:    15436, Lr: 0.000032\n",
            "2023-01-19 13:20:25,663 - INFO - joeynmt.training - Epoch 229, Step:    79700, Batch Loss:     1.797464, Batch Acc: 0.559868, Tokens per Sec:    15575, Lr: 0.000032\n",
            "2023-01-19 13:20:26,414 - INFO - joeynmt.training - Epoch 229: total training loss 587.84\n",
            "2023-01-19 13:20:26,414 - INFO - joeynmt.training - EPOCH 230\n",
            "2023-01-19 13:20:33,460 - INFO - joeynmt.training - Epoch 230, Step:    79800, Batch Loss:     1.701171, Batch Acc: 0.561056, Tokens per Sec:    15813, Lr: 0.000032\n",
            "2023-01-19 13:20:41,179 - INFO - joeynmt.training - Epoch 230, Step:    79900, Batch Loss:     1.715427, Batch Acc: 0.563234, Tokens per Sec:    15515, Lr: 0.000032\n",
            "2023-01-19 13:20:48,814 - INFO - joeynmt.training - Epoch 230, Step:    80000, Batch Loss:     1.792761, Batch Acc: 0.561952, Tokens per Sec:    15829, Lr: 0.000032\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.89ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9809.11ex/s] \n",
            "2023-01-19 13:20:49,109 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=80000\n",
            "2023-01-19 13:20:49,109 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:20:53,881 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:20:53,882 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:20:53,882 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:20:53,883 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:20:53,886 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.65, loss:   2.63, ppl:  13.94, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7278[sec], evaluation: 0.0413[sec]\n",
            "2023-01-19 13:20:53,888 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:20:53,892 - INFO - joeynmt.training - \tSource:     فێرتینایا دۆشن گمی کۆلهیه مۆقاۆمت گؤستهره بیلمیردی ، الآجسیز قالدێق وه کۆلک بیزی آپارێردێ . \n",
            "2023-01-19 13:20:53,892 - INFO - joeynmt.training - \tReference:  کشتی شدیدا گرفتار باد شد و نتوانست خلاف جهت باد پیش رود . پس خود را به باد سپردیم و با آن رانده شدیم . \n",
            "2023-01-19 13:20:53,892 - INFO - joeynmt.training - \tHypothesis: فورا درد کشتی به سوی کشتی می رسید و نمی تواند در حالی که به باد مجرمهٔ کشتی درمی آورد ، اما نمی توانیم باد باد به باد رسیدیم و ما را به باد برد .\n",
            "2023-01-19 13:20:53,892 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:20:53,894 - INFO - joeynmt.training - \tSource:     آنجاق سؤزونوزده بلی نیز بلی ، خئیر اینیز خئیر اولسون . بوندان قالانێ شر اولانداندێر . \n",
            "2023-01-19 13:20:53,894 - INFO - joeynmt.training - \tReference:  بله‌تان فقط بله و نه‌تان فقط نه باشد ؛ چون سخنی بیش از این ، از آن شریر است . \n",
            "2023-01-19 13:20:53,895 - INFO - joeynmt.training - \tHypothesis: مگر کسی که سخن را در این امور نهید ، بلکه از آن آتش است که از آن باقی ماهی است .\n",
            "2023-01-19 13:20:53,895 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:20:53,897 - INFO - joeynmt.training - \tSource:     اگر باشقا اینسانلارین تقصیرلرینی باغیشلاسانیز ، سماوی آتانێز دا سیزی باغیشلایار . \n",
            "2023-01-19 13:20:53,897 - INFO - joeynmt.training - \tReference:   زیرا اگر شما خطاهای دیگران را ببخشید ، پدر آسمانی شما نیز شما را خواهد بخشید ؛ \n",
            "2023-01-19 13:20:53,897 - INFO - joeynmt.training - \tHypothesis: اگر دیگران به مردم بخشیده شدید ، پدر آسمانی شما را ببخشید .\n",
            "2023-01-19 13:20:53,897 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:20:53,899 - INFO - joeynmt.training - \tSource:     خئیرلی اولان هر شئیی سیزه بیان ائدهرک ایستر آچێق ، ایسترسه ده ائودن ائوه گزیب دولاشاراق تعلیم وئرمکدن چکینمهدیم . \n",
            "2023-01-19 13:20:53,899 - INFO - joeynmt.training - \tReference:  در عین حال ، از گفتن هر آنچه برای شما مفید بود ، دریغ نکردم و شما را چه در جمع و چه خانه به خانه تعلیم دادم . \n",
            "2023-01-19 13:20:53,899 - INFO - joeynmt.training - \tHypothesis: اما چنین نیست که هر چیزی را که در خانه اش نداده بود ، به شما نشان داده است و در خانهٔ خود تعلیم دهد ، نداد .\n",
            "2023-01-19 13:20:58,239 - INFO - joeynmt.training - Epoch 230: total training loss 586.63\n",
            "2023-01-19 13:20:58,239 - INFO - joeynmt.training - EPOCH 231\n",
            "2023-01-19 13:21:01,556 - INFO - joeynmt.training - Epoch 231, Step:    80100, Batch Loss:     1.727118, Batch Acc: 0.562873, Tokens per Sec:    15693, Lr: 0.000032\n",
            "2023-01-19 13:21:09,307 - INFO - joeynmt.training - Epoch 231, Step:    80200, Batch Loss:     1.835644, Batch Acc: 0.562329, Tokens per Sec:    15620, Lr: 0.000032\n",
            "2023-01-19 13:21:17,054 - INFO - joeynmt.training - Epoch 231, Step:    80300, Batch Loss:     1.743538, Batch Acc: 0.561888, Tokens per Sec:    15600, Lr: 0.000032\n",
            "2023-01-19 13:21:24,779 - INFO - joeynmt.training - Epoch 231, Step:    80400, Batch Loss:     1.586313, Batch Acc: 0.561625, Tokens per Sec:    15699, Lr: 0.000032\n",
            "2023-01-19 13:21:25,117 - INFO - joeynmt.training - Epoch 231: total training loss 585.30\n",
            "2023-01-19 13:21:25,117 - INFO - joeynmt.training - EPOCH 232\n",
            "2023-01-19 13:21:32,884 - INFO - joeynmt.training - Epoch 232, Step:    80500, Batch Loss:     1.726051, Batch Acc: 0.560066, Tokens per Sec:    14995, Lr: 0.000032\n",
            "2023-01-19 13:21:41,655 - INFO - joeynmt.training - Epoch 232, Step:    80600, Batch Loss:     1.838117, Batch Acc: 0.563494, Tokens per Sec:    13774, Lr: 0.000032\n",
            "2023-01-19 13:21:49,387 - INFO - joeynmt.training - Epoch 232, Step:    80700, Batch Loss:     1.627737, Batch Acc: 0.562145, Tokens per Sec:    15651, Lr: 0.000031\n",
            "2023-01-19 13:21:53,390 - INFO - joeynmt.training - Epoch 232: total training loss 584.14\n",
            "2023-01-19 13:21:53,390 - INFO - joeynmt.training - EPOCH 233\n",
            "2023-01-19 13:21:57,176 - INFO - joeynmt.training - Epoch 233, Step:    80800, Batch Loss:     1.631098, Batch Acc: 0.561178, Tokens per Sec:    15821, Lr: 0.000031\n",
            "2023-01-19 13:22:04,831 - INFO - joeynmt.training - Epoch 233, Step:    80900, Batch Loss:     1.769772, Batch Acc: 0.564357, Tokens per Sec:    15716, Lr: 0.000031\n",
            "2023-01-19 13:22:12,568 - INFO - joeynmt.training - Epoch 233, Step:    81000, Batch Loss:     1.560534, Batch Acc: 0.562281, Tokens per Sec:    15575, Lr: 0.000031\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 144.88ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10826.97ex/s]\n",
            "2023-01-19 13:22:12,828 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=81000\n",
            "2023-01-19 13:22:12,828 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:22:17,743 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:22:17,743 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:22:17,743 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:22:17,744 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:22:17,747 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.57, loss:   2.77, ppl:  16.00, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.8665[sec], evaluation: 0.0444[sec]\n",
            "2023-01-19 13:22:17,750 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:22:17,753 - INFO - joeynmt.training - \tSource:     بیز آزێ دا آخێرینجیلارداندیر . \n",
            "2023-01-19 13:22:17,753 - INFO - joeynmt.training - \tReference:  و اندکی از متأخران . \n",
            "2023-01-19 13:22:17,754 - INFO - joeynmt.training - \tHypothesis: و از گمراهان ،\n",
            "2023-01-19 13:22:17,754 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:22:17,756 - INFO - joeynmt.training - \tSource:     قادێن ایسه یاخینلاشدی وه اونا سجده قێلێب دئدی : یا رب ، منه امداد ائت . \n",
            "2023-01-19 13:22:17,756 - INFO - joeynmt.training - \tReference:  اما آن زن پیش عیسی آمد ، در مقابل او به زانو افتاد و گفت : سرورم ، به من کمک کن ! \n",
            "2023-01-19 13:22:17,756 - INFO - joeynmt.training - \tHypothesis: پس زن برخاست و به او گفت : سرور ، به من گفت : سرور ، من را با من بنویس کن .\n",
            "2023-01-19 13:22:17,756 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:22:17,758 - INFO - joeynmt.training - \tSource:     آراباسێنا اوتوروب گئری قاییدارکن یئشآیا پیغمبرین کیتابینی اوخویوردو . \n",
            "2023-01-19 13:22:17,758 - INFO - joeynmt.training - \tReference:  او در راه بازگشت در ارابهٔ خود نشسته بود و با صدای بلند نوشته‌های اشعْیای نبی را می‌خواند . \n",
            "2023-01-19 13:22:17,758 - INFO - joeynmt.training - \tHypothesis: آنگاه که میانشان به تن افتاد و آن را برگشت و آنگاه فرشته ای را خواند .\n",
            "2023-01-19 13:22:17,759 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:22:17,761 - INFO - joeynmt.training - \tSource:     ابراهیم نه یهودی ، نه ده خاچپرست ایدی . او آنجاق حنیف مۆسلمان ایدی وه شریک قوشانلاردان دئییلدی . \n",
            "2023-01-19 13:22:17,761 - INFO - joeynmt.training - \tReference:  ابراهیم نه یهودی بود و نه نصرانی ، بلکه حق گرایی فرمانبردار بود ، و از مشرکان نبود . \n",
            "2023-01-19 13:22:17,761 - INFO - joeynmt.training - \tHypothesis: و ابراهیم و یهودی و خاندان ابراهیم بود . ویژه از دانش نبود .\n",
            "2023-01-19 13:22:25,500 - INFO - joeynmt.training - Epoch 233, Step:    81100, Batch Loss:     1.605030, Batch Acc: 0.564395, Tokens per Sec:    14864, Lr: 0.000031\n",
            "2023-01-19 13:22:25,538 - INFO - joeynmt.training - Epoch 233: total training loss 586.53\n",
            "2023-01-19 13:22:25,538 - INFO - joeynmt.training - EPOCH 234\n",
            "2023-01-19 13:22:33,355 - INFO - joeynmt.training - Epoch 234, Step:    81200, Batch Loss:     1.731372, Batch Acc: 0.563050, Tokens per Sec:    15528, Lr: 0.000031\n",
            "2023-01-19 13:22:41,086 - INFO - joeynmt.training - Epoch 234, Step:    81300, Batch Loss:     1.726806, Batch Acc: 0.565629, Tokens per Sec:    15567, Lr: 0.000031\n",
            "2023-01-19 13:22:48,742 - INFO - joeynmt.training - Epoch 234, Step:    81400, Batch Loss:     1.590276, Batch Acc: 0.561471, Tokens per Sec:    15865, Lr: 0.000031\n",
            "2023-01-19 13:22:52,389 - INFO - joeynmt.training - Epoch 234: total training loss 584.09\n",
            "2023-01-19 13:22:52,390 - INFO - joeynmt.training - EPOCH 235\n",
            "2023-01-19 13:22:56,357 - INFO - joeynmt.training - Epoch 235, Step:    81500, Batch Loss:     1.546619, Batch Acc: 0.566336, Tokens per Sec:    15770, Lr: 0.000031\n",
            "2023-01-19 13:23:03,918 - INFO - joeynmt.training - Epoch 235, Step:    81600, Batch Loss:     1.598592, Batch Acc: 0.562801, Tokens per Sec:    15725, Lr: 0.000031\n",
            "2023-01-19 13:23:11,705 - INFO - joeynmt.training - Epoch 235, Step:    81700, Batch Loss:     1.690739, Batch Acc: 0.562641, Tokens per Sec:    15623, Lr: 0.000031\n",
            "2023-01-19 13:23:19,254 - INFO - joeynmt.training - Epoch 235: total training loss 585.87\n",
            "2023-01-19 13:23:19,254 - INFO - joeynmt.training - EPOCH 236\n",
            "2023-01-19 13:23:19,485 - INFO - joeynmt.training - Epoch 236, Step:    81800, Batch Loss:     1.744210, Batch Acc: 0.572251, Tokens per Sec:    16595, Lr: 0.000031\n",
            "2023-01-19 13:23:27,168 - INFO - joeynmt.training - Epoch 236, Step:    81900, Batch Loss:     1.691917, Batch Acc: 0.567781, Tokens per Sec:    15766, Lr: 0.000031\n",
            "2023-01-19 13:23:34,793 - INFO - joeynmt.training - Epoch 236, Step:    82000, Batch Loss:     1.554246, Batch Acc: 0.565391, Tokens per Sec:    15593, Lr: 0.000031\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.16ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10385.34ex/s]\n",
            "2023-01-19 13:23:35,064 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=82000\n",
            "2023-01-19 13:23:35,064 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:23:40,296 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:23:40,296 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:23:40,296 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:23:40,297 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:23:40,300 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.99, loss:   2.75, ppl:  15.58, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1858[sec], evaluation: 0.0429[sec]\n",
            "2023-01-19 13:23:40,303 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:23:40,306 - INFO - joeynmt.training - \tSource:     پوللارێنێ آلدیقلاری زامان ائو صاحبینه شکایت ائدهرک دئدیلر : \n",
            "2023-01-19 13:23:40,306 - INFO - joeynmt.training - \tReference:  آنان پس از دریافت مزدشان ، شروع به گله و شکایت از مالک کردند . \n",
            "2023-01-19 13:23:40,306 - INFO - joeynmt.training - \tHypothesis: پس وقتی پولس به خانه هایشان رسیدند ، به شکستند :\n",
            "2023-01-19 13:23:40,307 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:23:40,309 - INFO - joeynmt.training - \tSource:     سونرا جهنهمه وارد قالاجاقلار . \n",
            "2023-01-19 13:23:40,309 - INFO - joeynmt.training - \tReference:  آنگاه به یقین ، آنان به جهنم درآیند . \n",
            "2023-01-19 13:23:40,309 - INFO - joeynmt.training - \tHypothesis: آنگاه به دوزخ درآیند .\n",
            "2023-01-19 13:23:40,309 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:23:40,311 - INFO - joeynmt.training - \tSource:     پاسخایا هازێرلێق گۆنۆ ایدی . آلتێنجێ ساعات رادهلری ایدی . پیلات یهودیلره پادشاهینیز بودور ! دئدی . \n",
            "2023-01-19 13:23:40,311 - INFO - joeynmt.training - \tReference:  آن روز ، روز تهیه برای عید پسح و حدود ساعت ششم بود . پیلاتس به یهودیان گفت : ببینید ! این هم پادشاهتان ! \n",
            "2023-01-19 13:23:40,311 - INFO - joeynmt.training - \tHypothesis: پس یکی از آن روز که برای تمجید بود ، حدود ساعتی بود . پیلاتس گفت : ببینید !\n",
            "2023-01-19 13:23:40,311 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:23:40,317 - INFO - joeynmt.training - \tSource:     کیم من آللاهێ سئویرم دئییر ، آمما اؤز قارداشێنا نفرت ائدیرسه ، یالانچیدیر . آخێ گؤردوگو قارداشێنێ سئومهین گؤرمهدیگی آللاهێ سئوه بیلمز . \n",
            "2023-01-19 13:23:40,318 - INFO - joeynmt.training - \tReference:  اگر کسی بگوید : خدا را دوست دارم ، اما از برادر خود نفرت داشته باشد دروغگوست ؛ زیرا هر که برادر خود را که دیده است دوست نداشته باشد ، نمی‌تواند خدایی را که ندیده است دوست داشته باشد . \n",
            "2023-01-19 13:23:40,318 - INFO - joeynmt.training - \tHypothesis: کسی که به خدا محبت می کنم ، اما اگر خدا را دوست می دارد ، از برادر خود نفرت دارد . اما کسی که برادر خود را دوست می دارد ، از او دوست دارد .\n",
            "2023-01-19 13:23:47,965 - INFO - joeynmt.training - Epoch 236, Step:    82100, Batch Loss:     1.673986, Batch Acc: 0.564364, Tokens per Sec:    15254, Lr: 0.000031\n",
            "2023-01-19 13:23:51,535 - INFO - joeynmt.training - Epoch 236: total training loss 583.90\n",
            "2023-01-19 13:23:51,535 - INFO - joeynmt.training - EPOCH 237\n",
            "2023-01-19 13:23:55,695 - INFO - joeynmt.training - Epoch 237, Step:    82200, Batch Loss:     1.660467, Batch Acc: 0.565465, Tokens per Sec:    15758, Lr: 0.000031\n",
            "2023-01-19 13:24:03,352 - INFO - joeynmt.training - Epoch 237, Step:    82300, Batch Loss:     1.766817, Batch Acc: 0.568153, Tokens per Sec:    15816, Lr: 0.000031\n",
            "2023-01-19 13:24:11,026 - INFO - joeynmt.training - Epoch 237, Step:    82400, Batch Loss:     1.620867, Batch Acc: 0.564002, Tokens per Sec:    15805, Lr: 0.000031\n",
            "2023-01-19 13:24:18,262 - INFO - joeynmt.training - Epoch 237: total training loss 582.44\n",
            "2023-01-19 13:24:18,263 - INFO - joeynmt.training - EPOCH 238\n",
            "2023-01-19 13:24:18,734 - INFO - joeynmt.training - Epoch 238, Step:    82500, Batch Loss:     1.636471, Batch Acc: 0.557866, Tokens per Sec:    15529, Lr: 0.000031\n",
            "2023-01-19 13:24:26,491 - INFO - joeynmt.training - Epoch 238, Step:    82600, Batch Loss:     1.709754, Batch Acc: 0.567608, Tokens per Sec:    15620, Lr: 0.000031\n",
            "2023-01-19 13:24:34,217 - INFO - joeynmt.training - Epoch 238, Step:    82700, Batch Loss:     1.602270, Batch Acc: 0.565100, Tokens per Sec:    15629, Lr: 0.000031\n",
            "2023-01-19 13:24:41,967 - INFO - joeynmt.training - Epoch 238, Step:    82800, Batch Loss:     1.748331, Batch Acc: 0.563694, Tokens per Sec:    15445, Lr: 0.000031\n",
            "2023-01-19 13:24:45,209 - INFO - joeynmt.training - Epoch 238: total training loss 583.15\n",
            "2023-01-19 13:24:45,210 - INFO - joeynmt.training - EPOCH 239\n",
            "2023-01-19 13:24:49,645 - INFO - joeynmt.training - Epoch 239, Step:    82900, Batch Loss:     1.847699, Batch Acc: 0.562563, Tokens per Sec:    15747, Lr: 0.000031\n",
            "2023-01-19 13:24:57,629 - INFO - joeynmt.training - Epoch 239, Step:    83000, Batch Loss:     1.644350, Batch Acc: 0.567024, Tokens per Sec:    14878, Lr: 0.000031\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 101.46ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 5594.82ex/s]\n",
            "2023-01-19 13:24:58,092 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=83000\n",
            "2023-01-19 13:24:58,092 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:25:03,671 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:25:03,672 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:25:03,672 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:25:03,673 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:25:03,676 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.73, loss:   2.73, ppl:  15.32, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.5330[sec], evaluation: 0.0431[sec]\n",
            "2023-01-19 13:25:03,679 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:25:03,682 - INFO - joeynmt.training - \tSource:     سونرا داروازآیا چێخاندا باشقا بیر قاراباش اونو گؤروب اوراداکێلارا دئدی : بو آدام نازارائتلی ایسا ایله بیرلیکده ایدی . \n",
            "2023-01-19 13:25:03,682 - INFO - joeynmt.training - \tReference:  بعد از آن که پطرس به طرف در خانه رفت ، زنی دیگر متوجه او شد و به کسانی که آنجا بودند ، گفت : این مرد با عیسای ناصری بود . \n",
            "2023-01-19 13:25:03,682 - INFO - joeynmt.training - \tHypothesis: سپس هنگامی که دیگر رفت ، مردی دیگر دید که او را دیدند و به او گفت : این مرد ناصری بود که با او نازل کرد .\n",
            "2023-01-19 13:25:03,683 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:25:03,684 - INFO - joeynmt.training - \tSource:     او ، لؤؤهی محفوزدادێر ! \n",
            "2023-01-19 13:25:03,685 - INFO - joeynmt.training - \tReference:  که در لوحی محفوظ است . \n",
            "2023-01-19 13:25:03,685 - INFO - joeynmt.training - \tHypothesis: و او در سفینه ای بیفکن .\n",
            "2023-01-19 13:25:03,685 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:25:03,687 - INFO - joeynmt.training - \tSource:     داها سونرا اونلارا آشکار سؤیلهدیم وه گیزلی بیلدیردیم . \n",
            "2023-01-19 13:25:03,687 - INFO - joeynmt.training - \tReference:  باز من به آنان اعلام نمودم و در خلوت و پوشیده نیز به ایشان گفتم . \n",
            "2023-01-19 13:25:03,687 - INFO - joeynmt.training - \tHypothesis: سپس به آنان گفتم که آشکارا به آنان گفته ام و پنهان کردم .\n",
            "2023-01-19 13:25:03,687 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:25:03,690 - INFO - joeynmt.training - \tSource:     شبههسیز کی ، آللاه گؤیلرین وه یئرین غیبینی بیلیر . او ، اۆرکلرده اولانلارێ دا بیلندیر ! \n",
            "2023-01-19 13:25:03,690 - INFO - joeynmt.training - \tReference:  خدا ست که‌ دانای نهان آسمانها و زمین است ، و اوست که به راز دلها داناست . \n",
            "2023-01-19 13:25:03,690 - INFO - joeynmt.training - \tHypothesis: خداست که نهان آسمانها و زمین را می داند ، و او به دلها داناست .\n",
            "2023-01-19 13:25:11,499 - INFO - joeynmt.training - Epoch 239, Step:    83100, Batch Loss:     1.806091, Batch Acc: 0.562105, Tokens per Sec:    14575, Lr: 0.000031\n",
            "2023-01-19 13:25:18,628 - INFO - joeynmt.training - Epoch 239: total training loss 583.26\n",
            "2023-01-19 13:25:18,628 - INFO - joeynmt.training - EPOCH 240\n",
            "2023-01-19 13:25:19,327 - INFO - joeynmt.training - Epoch 240, Step:    83200, Batch Loss:     1.673425, Batch Acc: 0.573602, Tokens per Sec:    15728, Lr: 0.000031\n",
            "2023-01-19 13:25:27,054 - INFO - joeynmt.training - Epoch 240, Step:    83300, Batch Loss:     1.696653, Batch Acc: 0.567945, Tokens per Sec:    15682, Lr: 0.000031\n",
            "2023-01-19 13:25:34,758 - INFO - joeynmt.training - Epoch 240, Step:    83400, Batch Loss:     1.648194, Batch Acc: 0.568219, Tokens per Sec:    15594, Lr: 0.000031\n",
            "2023-01-19 13:25:42,528 - INFO - joeynmt.training - Epoch 240, Step:    83500, Batch Loss:     1.669164, Batch Acc: 0.564168, Tokens per Sec:    15538, Lr: 0.000031\n",
            "2023-01-19 13:25:45,520 - INFO - joeynmt.training - Epoch 240: total training loss 578.72\n",
            "2023-01-19 13:25:45,520 - INFO - joeynmt.training - EPOCH 241\n",
            "2023-01-19 13:25:50,331 - INFO - joeynmt.training - Epoch 241, Step:    83600, Batch Loss:     1.667103, Batch Acc: 0.573246, Tokens per Sec:    15561, Lr: 0.000031\n",
            "2023-01-19 13:25:58,069 - INFO - joeynmt.training - Epoch 241, Step:    83700, Batch Loss:     1.601530, Batch Acc: 0.566868, Tokens per Sec:    15412, Lr: 0.000031\n",
            "2023-01-19 13:26:05,719 - INFO - joeynmt.training - Epoch 241, Step:    83800, Batch Loss:     1.777969, Batch Acc: 0.562783, Tokens per Sec:    15747, Lr: 0.000031\n",
            "2023-01-19 13:26:12,577 - INFO - joeynmt.training - Epoch 241: total training loss 583.66\n",
            "2023-01-19 13:26:12,578 - INFO - joeynmt.training - EPOCH 242\n",
            "2023-01-19 13:26:13,499 - INFO - joeynmt.training - Epoch 242, Step:    83900, Batch Loss:     1.693000, Batch Acc: 0.566973, Tokens per Sec:    16196, Lr: 0.000031\n",
            "2023-01-19 13:26:21,126 - INFO - joeynmt.training - Epoch 242, Step:    84000, Batch Loss:     1.675218, Batch Acc: 0.568493, Tokens per Sec:    15921, Lr: 0.000031\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.28ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10766.35ex/s]\n",
            "2023-01-19 13:26:21,387 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=84000\n",
            "2023-01-19 13:26:21,387 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:26:26,589 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:26:26,590 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:26:26,590 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:26:26,591 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:26:26,594 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.02, loss:   2.83, ppl:  16.98, acc:   0.40, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1554[sec], evaluation: 0.0433[sec]\n",
            "2023-01-19 13:26:26,597 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:26:26,600 - INFO - joeynmt.training - \tSource:     سیز محض بونون اۆچۆن چاغێرێلدێنێز . چۆنکی مصیح ده سیزین اۆچۆن اعذاب چکدی وه سیزه نمونه اولدو کی ، سیز ده اونون ایزی ایله گئدهسینیز . \n",
            "2023-01-19 13:26:26,601 - INFO - joeynmt.training - \tReference:  در واقع ، شما نیز فراخوانده شده‌اید تا این راه را دنبال کنید ؛ زیرا حتی مسیح برای شما رنج کشید و سرمشقی برای شما قرار داد تا به‌دقت در جای پای او گام بردارید . \n",
            "2023-01-19 13:26:26,601 - INFO - joeynmt.training - \tHypothesis: شما نیز نمی دانید که مسیحْد ، در واقع شما رنج کشید ؛ زیرا همان مسیح که در رنج رنج کشیده است و شما نیز به خاطر مسیح رنج کشیده اید .\n",
            "2023-01-19 13:26:26,601 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:26:26,603 - INFO - joeynmt.training - \tSource:     سالامیسه چاتاندا یهودیلرین سیناقوقلاریندا آللاهێن کلامێنێ بیان ائتمهیه باشلادێلار . یهیا ایسه اونلارێن کؤمکچیسی ایدی . \n",
            "2023-01-19 13:26:26,603 - INFO - joeynmt.training - \tReference:  وقتی به سالامیس رسیدند ، به اعلام کلام خدا در کنیسه‌های یهودیان پرداختند . یوحنا نیز دستیارشان بود . \n",
            "2023-01-19 13:26:26,603 - INFO - joeynmt.training - \tHypothesis: اما وقتی به کنیسه ای به کنیسه رسیدند ، کلام خدا را در کنیسه به سخن گفت و یحیای در کنیسه خدا یاری کرد .\n",
            "2023-01-19 13:26:26,603 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:26:26,605 - INFO - joeynmt.training - \tSource:     بو واخت او گئدیب اؤزوندن داها بئتر یئددی باشقا روحو گؤتورر . اونلار دا اورایا گیریب مسکونلاشار . اوندا بو آدامێن آخێرێ اوهلکیندن داها پیس اولار . \n",
            "2023-01-19 13:26:26,605 - INFO - joeynmt.training - \tReference:  آنگاه می‌رود و هفت روح شریرتر از خود را می‌آورد و با آنان به آن خانه داخل گشته و در آن ساکن می‌شوند . در نتیجه ، عاقبت آن شخص از ابتدایش بدتر می‌شود . \n",
            "2023-01-19 13:26:26,605 - INFO - joeynmt.training - \tHypothesis: در آن زمان رفت و هفت روح خدا را به کوهی دیگر گرفت و آنان نیز به آن سوی رود اردن می شود . پس او از این که دربتید ، زمان چنین می شود .\n",
            "2023-01-19 13:26:26,606 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:26:26,608 - INFO - joeynmt.training - \tSource:     آمما داش آکل چوخ أل آچیق و مال قدری بیلمز ایدی\n",
            "2023-01-19 13:26:26,608 - INFO - joeynmt.training - \tReference:  ولی داش آكل پشت گوش فراخ و گشاد باز بود ، \n",
            "2023-01-19 13:26:26,608 - INFO - joeynmt.training - \tHypothesis: اما داش آكل از دست داش آكل روی میكند ،\n",
            "2023-01-19 13:26:34,314 - INFO - joeynmt.training - Epoch 242, Step:    84100, Batch Loss:     1.642061, Batch Acc: 0.564579, Tokens per Sec:    15144, Lr: 0.000031\n",
            "2023-01-19 13:26:42,077 - INFO - joeynmt.training - Epoch 242, Step:    84200, Batch Loss:     1.646435, Batch Acc: 0.566347, Tokens per Sec:    15547, Lr: 0.000031\n",
            "2023-01-19 13:26:44,856 - INFO - joeynmt.training - Epoch 242: total training loss 579.17\n",
            "2023-01-19 13:26:44,856 - INFO - joeynmt.training - EPOCH 243\n",
            "2023-01-19 13:26:49,791 - INFO - joeynmt.training - Epoch 243, Step:    84300, Batch Loss:     1.588800, Batch Acc: 0.569529, Tokens per Sec:    15565, Lr: 0.000031\n",
            "2023-01-19 13:26:57,492 - INFO - joeynmt.training - Epoch 243, Step:    84400, Batch Loss:     1.646618, Batch Acc: 0.566997, Tokens per Sec:    15652, Lr: 0.000031\n",
            "2023-01-19 13:27:05,023 - INFO - joeynmt.training - Epoch 243, Step:    84500, Batch Loss:     1.667595, Batch Acc: 0.564259, Tokens per Sec:    15993, Lr: 0.000031\n",
            "2023-01-19 13:27:11,613 - INFO - joeynmt.training - Epoch 243: total training loss 581.00\n",
            "2023-01-19 13:27:11,614 - INFO - joeynmt.training - EPOCH 244\n",
            "2023-01-19 13:27:12,774 - INFO - joeynmt.training - Epoch 244, Step:    84600, Batch Loss:     1.624581, Batch Acc: 0.566165, Tokens per Sec:    15816, Lr: 0.000031\n",
            "2023-01-19 13:27:20,465 - INFO - joeynmt.training - Epoch 244, Step:    84700, Batch Loss:     1.602363, Batch Acc: 0.568750, Tokens per Sec:    15737, Lr: 0.000031\n",
            "2023-01-19 13:27:28,209 - INFO - joeynmt.training - Epoch 244, Step:    84800, Batch Loss:     1.638038, Batch Acc: 0.565865, Tokens per Sec:    15581, Lr: 0.000031\n",
            "2023-01-19 13:27:36,003 - INFO - joeynmt.training - Epoch 244, Step:    84900, Batch Loss:     1.574752, Batch Acc: 0.564993, Tokens per Sec:    15456, Lr: 0.000031\n",
            "2023-01-19 13:27:38,606 - INFO - joeynmt.training - Epoch 244: total training loss 579.41\n",
            "2023-01-19 13:27:38,607 - INFO - joeynmt.training - EPOCH 245\n",
            "2023-01-19 13:27:43,848 - INFO - joeynmt.training - Epoch 245, Step:    85000, Batch Loss:     1.658663, Batch Acc: 0.565409, Tokens per Sec:    15673, Lr: 0.000031\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.61ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10055.80ex/s]\n",
            "2023-01-19 13:27:44,132 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=85000\n",
            "2023-01-19 13:27:44,132 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:27:48,310 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:27:48,311 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:27:48,311 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:27:48,312 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:27:48,315 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.98, loss:   2.66, ppl:  14.27, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.1342[sec], evaluation: 0.0413[sec]\n",
            "2023-01-19 13:27:48,317 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:27:48,320 - INFO - joeynmt.training - \tSource:     اگر ایستهسهیدیک ، سنه وحی ائتدیگیمیزی چێخاردێب آپاراردێق . سونرا بیزه قارشێ اؤزون اۆچۆن بیر مۆدافهچی ده تاپا بیلمزدین \n",
            "2023-01-19 13:27:48,321 - INFO - joeynmt.training - \tReference:  و اگر بخواهیم ، قطعا آنچه را به تو وحی کرده‌ایم می‌بریم ، آنگاه برای حفظ آن ، در برابر ما ، برای خود مدافعی نمی‌یابی ، \n",
            "2023-01-19 13:27:48,321 - INFO - joeynmt.training - \tHypothesis: و اگر بخواهیم ، آنچه را که به تو وحی کرده ایم به سوی تو وحی کردیم ، در برابر ما مشرعه ای برای ما مجادله کنیم ، و برای ما مهی نخواهد یافت .\n",
            "2023-01-19 13:27:48,321 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:27:48,323 - INFO - joeynmt.training - \tSource:     ایسا اونلارا جاواب وئردی : منیم آنام وه قارداشلارێم آللاهێن کلامێنێ ائشیدیب اونا امل ائدنلردیر . \n",
            "2023-01-19 13:27:48,323 - INFO - joeynmt.training - \tReference:  او در جواب به آنان گفت : مادر و برادرانم آنانی هستند که کلام خدا را می‌شنوند و به آن عمل می‌کنند . \n",
            "2023-01-19 13:27:48,323 - INFO - joeynmt.training - \tHypothesis: عیسی در جواب به آنان گفت : مادر من و برادران من هستند و کلام خدا را شنیده اند .\n",
            "2023-01-19 13:27:48,323 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:27:48,325 - INFO - joeynmt.training - \tSource:     آناسێندان ، آتاسێندان ؛ \n",
            "2023-01-19 13:27:48,325 - INFO - joeynmt.training - \tReference:  و از مادرش و پدرش . \n",
            "2023-01-19 13:27:48,325 - INFO - joeynmt.training - \tHypothesis: و مادرش ،\n",
            "2023-01-19 13:27:48,326 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:27:48,327 - INFO - joeynmt.training - \tSource:     ربینین آیهلرینه اینانلار ؛ \n",
            "2023-01-19 13:27:48,328 - INFO - joeynmt.training - \tReference:  و کسانی که به نشانه‌های پروردگارشان ایمان می‌آورند ، \n",
            "2023-01-19 13:27:48,328 - INFO - joeynmt.training - \tHypothesis: و کسانی که به آیات پروردگارشان ایمان ندارند ،\n",
            "2023-01-19 13:27:56,026 - INFO - joeynmt.training - Epoch 245, Step:    85100, Batch Loss:     1.466363, Batch Acc: 0.566275, Tokens per Sec:    15154, Lr: 0.000031\n",
            "2023-01-19 13:28:03,682 - INFO - joeynmt.training - Epoch 245, Step:    85200, Batch Loss:     1.536329, Batch Acc: 0.568521, Tokens per Sec:    15767, Lr: 0.000031\n",
            "2023-01-19 13:28:09,898 - INFO - joeynmt.training - Epoch 245: total training loss 577.16\n",
            "2023-01-19 13:28:09,898 - INFO - joeynmt.training - EPOCH 246\n",
            "2023-01-19 13:28:11,424 - INFO - joeynmt.training - Epoch 246, Step:    85300, Batch Loss:     1.582007, Batch Acc: 0.574547, Tokens per Sec:    15617, Lr: 0.000031\n",
            "2023-01-19 13:28:20,145 - INFO - joeynmt.training - Epoch 246, Step:    85400, Batch Loss:     1.588866, Batch Acc: 0.568549, Tokens per Sec:    13841, Lr: 0.000031\n",
            "2023-01-19 13:28:28,373 - INFO - joeynmt.training - Epoch 246, Step:    85500, Batch Loss:     1.670864, Batch Acc: 0.567722, Tokens per Sec:    14513, Lr: 0.000031\n",
            "2023-01-19 13:28:36,196 - INFO - joeynmt.training - Epoch 246, Step:    85600, Batch Loss:     1.802136, Batch Acc: 0.567228, Tokens per Sec:    15381, Lr: 0.000031\n",
            "2023-01-19 13:28:38,622 - INFO - joeynmt.training - Epoch 246: total training loss 580.18\n",
            "2023-01-19 13:28:38,622 - INFO - joeynmt.training - EPOCH 247\n",
            "2023-01-19 13:28:44,060 - INFO - joeynmt.training - Epoch 247, Step:    85700, Batch Loss:     1.760212, Batch Acc: 0.564335, Tokens per Sec:    15331, Lr: 0.000031\n",
            "2023-01-19 13:28:51,876 - INFO - joeynmt.training - Epoch 247, Step:    85800, Batch Loss:     1.570700, Batch Acc: 0.569963, Tokens per Sec:    15424, Lr: 0.000031\n",
            "2023-01-19 13:28:59,662 - INFO - joeynmt.training - Epoch 247, Step:    85900, Batch Loss:     1.523206, Batch Acc: 0.566430, Tokens per Sec:    15450, Lr: 0.000031\n",
            "2023-01-19 13:29:05,832 - INFO - joeynmt.training - Epoch 247: total training loss 579.03\n",
            "2023-01-19 13:29:05,832 - INFO - joeynmt.training - EPOCH 248\n",
            "2023-01-19 13:29:07,506 - INFO - joeynmt.training - Epoch 248, Step:    86000, Batch Loss:     1.638812, Batch Acc: 0.571584, Tokens per Sec:    15423, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.14ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10397.31ex/s]\n",
            "2023-01-19 13:29:07,775 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=86000\n",
            "2023-01-19 13:29:07,775 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:29:12,667 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:29:12,668 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:29:12,668 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:29:12,669 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:29:12,672 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.12, loss:   2.66, ppl:  14.25, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.8386[sec], evaluation: 0.0501[sec]\n",
            "2023-01-19 13:29:12,674 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:29:12,677 - INFO - joeynmt.training - \tSource:     اونلار اونونلا قاینار سو آراسێندا دولانێب دوراجاقلار . \n",
            "2023-01-19 13:29:12,677 - INFO - joeynmt.training - \tReference:  میان آتش‌ و میان آب جوشان سرگردان باشند . \n",
            "2023-01-19 13:29:12,678 - INFO - joeynmt.training - \tHypothesis: و میان آن جوشاه آب جوشان خواهند کرد .\n",
            "2023-01-19 13:29:12,678 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:29:12,680 - INFO - joeynmt.training - \tSource:     مگر آللاه یالنیز یهودیلرین آللاهێدێر ؟ باشقا ملتلرین ده آللاهێ دئییلمی ؟ البته ، باشقا ملتلرین ده آللاهێدێر . \n",
            "2023-01-19 13:29:12,680 - INFO - joeynmt.training - \tReference:  آیا خدا فقط خدای یهودیان است ؟ مگر خدای غیریهودیان نیز نیست ؟ البته که خدای غیریهودیان نیز است . \n",
            "2023-01-19 13:29:12,680 - INFO - joeynmt.training - \tHypothesis: آیا خدا تنها یهودیان است ؟ آیا جز خدا نیست ؟ نه ، بلکه خدا را نیز و غیریهودیان ، بلکه خدا نیز خداست ؟\n",
            "2023-01-19 13:29:12,680 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:29:12,682 - INFO - joeynmt.training - \tSource:     سهربازلار گلن کیمی فیر اونا دئدیلر : اگر بیز غالب گلسک ، یقین کی ، بیزه بیر موزد وئریلهجک ، ائله دئییلمی ؟ \n",
            "2023-01-19 13:29:12,682 - INFO - joeynmt.training - \tReference:  و چون ساحران پیش فرعون آمدند ، گفتند : آیا اگر ما غالب آییم واقعا برای ما مزدی خواهد بود ؟ \n",
            "2023-01-19 13:29:12,682 - INFO - joeynmt.training - \tHypothesis: و صبحان به او گفتند : چرا ما غالب آمده است ؟ اگر ما غالب آمده ایم ، به ما وعده می دهد که ما به ما وعده می داده خواهد شد ؟\n",
            "2023-01-19 13:29:12,683 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:29:12,684 - INFO - joeynmt.training - \tSource:     ایسا قالیلئیانین کئفئرناهوم شهرینه گلدی وه شنبه گۆنۆ اونلارا تعلیم اؤیرتمهیه باشلادێ . \n",
            "2023-01-19 13:29:12,685 - INFO - joeynmt.training - \tReference:  سپس به کفرناحوم ، شهری در جلیل رفت . او در روزهای سبت به مردم تعلیم می‌داد . \n",
            "2023-01-19 13:29:12,685 - INFO - joeynmt.training - \tHypothesis: عیسی در حالی که در جلیل جلیل بود ، به شهر رفت و به تعلیم دادن پرداخت .\n",
            "2023-01-19 13:29:20,566 - INFO - joeynmt.training - Epoch 248, Step:    86100, Batch Loss:     1.690394, Batch Acc: 0.571969, Tokens per Sec:    14791, Lr: 0.000030\n",
            "2023-01-19 13:29:28,436 - INFO - joeynmt.training - Epoch 248, Step:    86200, Batch Loss:     1.558815, Batch Acc: 0.570226, Tokens per Sec:    15370, Lr: 0.000030\n",
            "2023-01-19 13:29:36,215 - INFO - joeynmt.training - Epoch 248, Step:    86300, Batch Loss:     1.702907, Batch Acc: 0.563477, Tokens per Sec:    15239, Lr: 0.000030\n",
            "2023-01-19 13:29:38,504 - INFO - joeynmt.training - Epoch 248: total training loss 580.33\n",
            "2023-01-19 13:29:38,505 - INFO - joeynmt.training - EPOCH 249\n",
            "2023-01-19 13:29:44,041 - INFO - joeynmt.training - Epoch 249, Step:    86400, Batch Loss:     1.710091, Batch Acc: 0.571850, Tokens per Sec:    15311, Lr: 0.000030\n",
            "2023-01-19 13:29:51,952 - INFO - joeynmt.training - Epoch 249, Step:    86500, Batch Loss:     1.553716, Batch Acc: 0.569123, Tokens per Sec:    15360, Lr: 0.000030\n",
            "2023-01-19 13:29:59,810 - INFO - joeynmt.training - Epoch 249, Step:    86600, Batch Loss:     1.660334, Batch Acc: 0.567191, Tokens per Sec:    15428, Lr: 0.000030\n",
            "2023-01-19 13:30:05,968 - INFO - joeynmt.training - Epoch 249: total training loss 577.93\n",
            "2023-01-19 13:30:05,968 - INFO - joeynmt.training - EPOCH 250\n",
            "2023-01-19 13:30:07,721 - INFO - joeynmt.training - Epoch 250, Step:    86700, Batch Loss:     1.751080, Batch Acc: 0.576089, Tokens per Sec:    15181, Lr: 0.000030\n",
            "2023-01-19 13:30:15,707 - INFO - joeynmt.training - Epoch 250, Step:    86800, Batch Loss:     1.550508, Batch Acc: 0.569893, Tokens per Sec:    15112, Lr: 0.000030\n",
            "2023-01-19 13:30:23,689 - INFO - joeynmt.training - Epoch 250, Step:    86900, Batch Loss:     1.666260, Batch Acc: 0.567342, Tokens per Sec:    15138, Lr: 0.000030\n",
            "2023-01-19 13:30:31,565 - INFO - joeynmt.training - Epoch 250, Step:    87000, Batch Loss:     1.543588, Batch Acc: 0.568872, Tokens per Sec:    15367, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 139.75ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9968.06ex/s] \n",
            "2023-01-19 13:30:31,852 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=87000\n",
            "2023-01-19 13:30:31,852 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:30:37,030 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:30:37,031 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:30:37,031 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:30:37,032 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:30:37,035 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.70, loss:   2.73, ppl:  15.35, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1311[sec], evaluation: 0.0442[sec]\n",
            "2023-01-19 13:30:37,227 - INFO - joeynmt.helpers - delete RESULTS/model/71000.ckpt\n",
            "2023-01-19 13:30:37,237 - INFO - joeynmt.helpers - delete /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/71000.ckpt\n",
            "2023-01-19 13:30:37,238 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/71000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/71000.ckpt')\n",
            "2023-01-19 13:30:37,240 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:30:37,244 - INFO - joeynmt.training - \tSource:     فاریسئیلر بونو گؤردۆکده اونون شاگردلرینه دئدیلر : نیه مۆعلمنظ وئرگیییغانلار وه گۆناهکارلارلا بیرگه یئمک یئییر ؟ \n",
            "2023-01-19 13:30:37,244 - INFO - joeynmt.training - \tReference:  فریسیان وقتی این را دیدند ، به شاگردان عیسی گفتند : چرا استاد شما با خراجگیران و گناهکاران غذا می‌خورد ؟ \n",
            "2023-01-19 13:30:37,244 - INFO - joeynmt.training - \tHypothesis: فریسیان با دیدن این که فریسیان ، شاگردانش گفتند : استاد ، چرا خراجگیران و گناهکاران غذا می خورد ؟\n",
            "2023-01-19 13:30:37,244 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:30:37,247 - INFO - joeynmt.training - \tSource:     ایسا اونلارا بیر مسل ده چکدی : هئچ کیم تزه پالتاردان بیر پارچا قوپارێب کؤهنه پالتارا یاماق وورماز . یوخسا تزه پالتار هم جێرێلار ، هم ده تزه پالتاردان قوپارێلان پارچا کؤهنه پالتارا یاراشماز . \n",
            "2023-01-19 13:30:37,247 - INFO - joeynmt.training - \tReference:  همچنین این مثل را برای آنان زد : هیچ کس تکه‌ای از لباس نو را نمی‌برد تا بر لباسی کهنه وصله زند . اگر چنین کند ، نه فقط وصلهٔ نو کنده می‌شود ، بلکه آن تکه از لباس نو نیز وصلهٔ ناجوری برای لباس کهنه است . \n",
            "2023-01-19 13:30:37,247 - INFO - joeynmt.training - \tHypothesis: عیسی مثلی برای آنان آورد و گفت : کسی از لباسی که رداهایشان را بر تن کند ، ردای سفید بر تن کند . جامه ای بر تن نکند و لباسی که رداهای سفید بر تن راند ، ردایش را بر تن کن .\n",
            "2023-01-19 13:30:37,247 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:30:37,249 - INFO - joeynmt.training - \tSource:     چۆنکی ابراهیم مئمارێ وه بانیسی آللاه اولان بۆنؤورهلی شهری گؤزلهییردی . \n",
            "2023-01-19 13:30:37,249 - INFO - joeynmt.training - \tReference:  زیرا او در انتظار شهری با بنیاد حقیقی بود ، که طراح و سازندهٔ آن خداست . \n",
            "2023-01-19 13:30:37,250 - INFO - joeynmt.training - \tHypothesis: زیرا خداست که به سبب وادت و شهری در انتظار آن خواهد بود .\n",
            "2023-01-19 13:30:37,250 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:30:37,252 - INFO - joeynmt.training - \tSource:     گینه کی اؤزون با تیردین . \n",
            "2023-01-19 13:30:37,252 - INFO - joeynmt.training - \tReference:  باز که خودتو کثیف کردی . \n",
            "2023-01-19 13:30:37,252 - INFO - joeynmt.training - \tHypothesis: سرش میشد .\n",
            "2023-01-19 13:30:39,326 - INFO - joeynmt.training - Epoch 250: total training loss 575.81\n",
            "2023-01-19 13:30:39,326 - INFO - joeynmt.training - EPOCH 251\n",
            "2023-01-19 13:30:45,407 - INFO - joeynmt.training - Epoch 251, Step:    87100, Batch Loss:     1.655625, Batch Acc: 0.571206, Tokens per Sec:    14869, Lr: 0.000030\n",
            "2023-01-19 13:30:53,272 - INFO - joeynmt.training - Epoch 251, Step:    87200, Batch Loss:     1.608635, Batch Acc: 0.567717, Tokens per Sec:    14958, Lr: 0.000030\n",
            "2023-01-19 13:31:01,137 - INFO - joeynmt.training - Epoch 251, Step:    87300, Batch Loss:     1.612106, Batch Acc: 0.569578, Tokens per Sec:    15437, Lr: 0.000030\n",
            "2023-01-19 13:31:07,023 - INFO - joeynmt.training - Epoch 251: total training loss 575.74\n",
            "2023-01-19 13:31:07,024 - INFO - joeynmt.training - EPOCH 252\n",
            "2023-01-19 13:31:09,097 - INFO - joeynmt.training - Epoch 252, Step:    87400, Batch Loss:     1.575124, Batch Acc: 0.578008, Tokens per Sec:    15198, Lr: 0.000030\n",
            "2023-01-19 13:31:17,066 - INFO - joeynmt.training - Epoch 252, Step:    87500, Batch Loss:     1.608989, Batch Acc: 0.570079, Tokens per Sec:    15227, Lr: 0.000030\n",
            "2023-01-19 13:31:25,295 - INFO - joeynmt.training - Epoch 252, Step:    87600, Batch Loss:     1.635219, Batch Acc: 0.566988, Tokens per Sec:    14556, Lr: 0.000030\n",
            "2023-01-19 13:31:33,331 - INFO - joeynmt.training - Epoch 252, Step:    87700, Batch Loss:     1.683415, Batch Acc: 0.569167, Tokens per Sec:    15023, Lr: 0.000030\n",
            "2023-01-19 13:31:35,099 - INFO - joeynmt.training - Epoch 252: total training loss 575.42\n",
            "2023-01-19 13:31:35,100 - INFO - joeynmt.training - EPOCH 253\n",
            "2023-01-19 13:31:42,306 - INFO - joeynmt.training - Epoch 253, Step:    87800, Batch Loss:     1.725552, Batch Acc: 0.572680, Tokens per Sec:    13183, Lr: 0.000030\n",
            "2023-01-19 13:31:50,735 - INFO - joeynmt.training - Epoch 253, Step:    87900, Batch Loss:     1.614329, Batch Acc: 0.571365, Tokens per Sec:    14292, Lr: 0.000030\n",
            "2023-01-19 13:31:58,766 - INFO - joeynmt.training - Epoch 253, Step:    88000, Batch Loss:     1.743302, Batch Acc: 0.567369, Tokens per Sec:    14865, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.42ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9316.18ex/s]\n",
            "2023-01-19 13:31:59,052 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=88000\n",
            "2023-01-19 13:31:59,052 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:32:04,176 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:32:04,176 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:32:04,176 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:32:04,177 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:32:04,180 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.26, loss:   2.67, ppl:  14.49, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0767[sec], evaluation: 0.0429[sec]\n",
            "2023-01-19 13:32:04,183 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:32:04,186 - INFO - joeynmt.training - \tSource:     اینسانا بۆتۆن دۆنیانی قازانێب جانێنێ ایتیرمهیینین نه خئیری وار ؟ \n",
            "2023-01-19 13:32:04,186 - INFO - joeynmt.training - \tReference:  به‌راستی چه فایده دارد که کسی تمام دنیا را به دست آورد ، اما جان خود را از دست بدهد ؟ \n",
            "2023-01-19 13:32:04,186 - INFO - joeynmt.training - \tHypothesis: چه فایدهٔ انسان ها را حفظ کنید ؟ نه ، بلکه آنچه را دنیاست ، انجام می دهید ؟\n",
            "2023-01-19 13:32:04,186 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:32:04,188 - INFO - joeynmt.training - \tSource:     من یئروسهلیمه قاییدیب مبده دوعا ائتمکده ایدیم کی ، فکریمی بیر گؤرۆنتۆ آپاردێ\n",
            "2023-01-19 13:32:04,188 - INFO - joeynmt.training - \tReference:   وقتی به اورشلیم بازگشتم ، در معبد مشغول دعا بودم که به حالت خلسه فرو رفتم\n",
            "2023-01-19 13:32:04,188 - INFO - joeynmt.training - \tHypothesis: وقتی به اورشلیم بازگشتم ، دعا کردم که در معبد بودم و تصمیم گرفتم ، چون بودم ، بودم .\n",
            "2023-01-19 13:32:04,188 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:32:04,190 - INFO - joeynmt.training - \tSource:     اونلار اونون اۆچۆن بئله بیر هییه قورماق ایستهدیلر ، بیز ایسه اونلارێ چوخ صفیل بیر وزیته سالدێق \n",
            "2023-01-19 13:32:04,190 - INFO - joeynmt.training - \tReference:  پس خواستند به او نیرنگی زنند ؛ و لی‌ ما آنان را پست گردانیدیم . \n",
            "2023-01-19 13:32:04,191 - INFO - joeynmt.training - \tHypothesis: و چون چنین خواستند برای او پند دهیم ، و لی آنان را در سفری استوار کردیم ، و ما آنان را در سرکشی فرو ریختیم .\n",
            "2023-01-19 13:32:04,191 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:32:04,193 - INFO - joeynmt.training - \tSource:     اینسانلارین تانرێسێنا ؛ \n",
            "2023-01-19 13:32:04,193 - INFO - joeynmt.training - \tReference:  معبود مردم ، \n",
            "2023-01-19 13:32:04,193 - INFO - joeynmt.training - \tHypothesis: و مردم را به خدای بپرستید .\n",
            "2023-01-19 13:32:09,869 - INFO - joeynmt.training - Epoch 253: total training loss 573.81\n",
            "2023-01-19 13:32:09,870 - INFO - joeynmt.training - EPOCH 254\n",
            "2023-01-19 13:32:12,247 - INFO - joeynmt.training - Epoch 254, Step:    88100, Batch Loss:     1.668716, Batch Acc: 0.569611, Tokens per Sec:    15078, Lr: 0.000030\n",
            "2023-01-19 13:32:20,242 - INFO - joeynmt.training - Epoch 254, Step:    88200, Batch Loss:     1.632323, Batch Acc: 0.571119, Tokens per Sec:    15145, Lr: 0.000030\n",
            "2023-01-19 13:32:28,221 - INFO - joeynmt.training - Epoch 254, Step:    88300, Batch Loss:     1.739122, Batch Acc: 0.566835, Tokens per Sec:    15180, Lr: 0.000030\n",
            "2023-01-19 13:32:36,140 - INFO - joeynmt.training - Epoch 254, Step:    88400, Batch Loss:     1.531435, Batch Acc: 0.570766, Tokens per Sec:    15285, Lr: 0.000030\n",
            "2023-01-19 13:32:37,583 - INFO - joeynmt.training - Epoch 254: total training loss 572.35\n",
            "2023-01-19 13:32:37,584 - INFO - joeynmt.training - EPOCH 255\n",
            "2023-01-19 13:32:44,222 - INFO - joeynmt.training - Epoch 255, Step:    88500, Batch Loss:     1.536089, Batch Acc: 0.572820, Tokens per Sec:    15122, Lr: 0.000030\n",
            "2023-01-19 13:32:52,190 - INFO - joeynmt.training - Epoch 255, Step:    88600, Batch Loss:     1.480223, Batch Acc: 0.571970, Tokens per Sec:    15355, Lr: 0.000030\n",
            "2023-01-19 13:33:00,103 - INFO - joeynmt.training - Epoch 255, Step:    88700, Batch Loss:     1.697096, Batch Acc: 0.568166, Tokens per Sec:    15269, Lr: 0.000030\n",
            "2023-01-19 13:33:05,137 - INFO - joeynmt.training - Epoch 255: total training loss 572.31\n",
            "2023-01-19 13:33:05,137 - INFO - joeynmt.training - EPOCH 256\n",
            "2023-01-19 13:33:07,992 - INFO - joeynmt.training - Epoch 256, Step:    88800, Batch Loss:     1.688123, Batch Acc: 0.574698, Tokens per Sec:    15204, Lr: 0.000030\n",
            "2023-01-19 13:33:15,844 - INFO - joeynmt.training - Epoch 256, Step:    88900, Batch Loss:     1.692440, Batch Acc: 0.572007, Tokens per Sec:    15371, Lr: 0.000030\n",
            "2023-01-19 13:33:23,737 - INFO - joeynmt.training - Epoch 256, Step:    89000, Batch Loss:     1.558651, Batch Acc: 0.571259, Tokens per Sec:    15292, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 142.84ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10611.44ex/s]\n",
            "2023-01-19 13:33:24,021 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=89000\n",
            "2023-01-19 13:33:24,021 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:33:29,015 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:33:29,015 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:33:29,016 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:33:29,017 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:33:29,020 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.58, loss:   2.72, ppl:  15.16, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9477[sec], evaluation: 0.0433[sec]\n",
            "2023-01-19 13:33:29,022 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:33:29,026 - INFO - joeynmt.training - \tSource:     هوارلر ایسنانین یانینا قاییدیب گؤردوکلری ایشلر حاققێندا اونا دانێشدێلار . سونرا ایسا آنجاق اونلارێ اؤزو ایله گؤتوروب بئت سایدا آدلانان بیر شهره آپاردێ . \n",
            "2023-01-19 13:33:29,026 - INFO - joeynmt.training - \tReference:  وقتی رسولان بازگشتند ، تمام آنچه را کرده بودند برای او تعریف کردند . آنگاه عیسی آنان را با خود به شهری به نام بیت‌صیدا برد تا در آنجا تنها باشند . \n",
            "2023-01-19 13:33:29,026 - INFO - joeynmt.training - \tHypothesis: رسولان نزد عیسی آمدند و آنچه را که در مورد عیسی به ایشان گفته بود ، در مورد او صحبت کردند . او نیز در مورد اورشلیم ، شهری که او را به شهری مرقس برد ، بردند .\n",
            "2023-01-19 13:33:29,026 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:33:29,028 - INFO - joeynmt.training - \tSource:     چۆنکی یاقوبون یانیندان بزی آداملار گلمهمیشدن اول پئتئر باشقا ملتلردن اولانلارلا یئمک یئییردی . همین آداملار گلدیکده ایسه سۆنت ترفدارلاریندان قورخاراق باشقا ملتلرین یانیندان چکیلیب آیری دوردو . \n",
            "2023-01-19 13:33:29,029 - INFO - joeynmt.training - \tReference:  زیرا پیش از آمدن افرادی از جانب یعقوب ، با غیریهودیان غذا می‌خورد ، اما وقتی آنان آمدند ، از ترس طرفداران ختنه ، دیگر چنین نکرد و خود را از ایشان جدا ساخت . \n",
            "2023-01-19 13:33:29,029 - INFO - joeynmt.training - \tHypothesis: زیرا آنان پیش از این که همراه یعقوب بودند ، از آنان که پیش از آنان بودند ، آمدند . اما وقتی رسولان از ختنه شدند ، از ختنهٔ غیریهودیان ، از ختنهٔ غیریهودیان ، ترسی شدند و از خواندن دوری شدن ترسان شدند .\n",
            "2023-01-19 13:33:29,029 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:33:29,031 - INFO - joeynmt.training - \tSource:     ایسا دا اونلارا دئدی : اونا گؤره ده سماوی پادشاهلێق اۆچۆن شاگردلیک ائتمیش هر الاهییاتچی خزینهسیندن تزه وه کؤهنه شئیلر چێخاران ائو صاحبینه بنزهییر . \n",
            "2023-01-19 13:33:29,031 - INFO - joeynmt.training - \tReference:  سپس به آنان گفت : حال که این‌ها را درک کردید ، بدانید هر معلمی که در مورد پادشاهی آسمان‌ها تعلیم گرفته باشد ، مانند صاحبخانه‌ای است که از گنجینهٔ خود ، هم چیزهای نو و هم چیزهای کهنه بیرون می‌آورد . \n",
            "2023-01-19 13:33:29,031 - INFO - joeynmt.training - \tHypothesis: عیسی به آنان گفت : پس از این رو ، پادشاهی آسمان ها را برای آن شاگرد ساخته است که برای خوردن آن ها به دست دادن به خانهٔ خراجگیر و کاسه ای است .\n",
            "2023-01-19 13:33:29,031 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:33:29,033 - INFO - joeynmt.training - \tSource:     سیز آللاهێ قویوب آنجاق بۆتلره ابادت ائدیر ، یالان اویدورورسونوز . سیزین آللاهدان باشقا ابادت ائتدیکلرینیز سیزه روزی وئرمهیه قادر دئییللر . روزینی آللاهدان دیلهگین . اونا تاپێنێن ، اونا شۆکۆر ائدین . سیز اونون هۆزورونا قایتاریلاجاقسینزیتینز ! \n",
            "2023-01-19 13:33:29,034 - INFO - joeynmt.training - \tReference:  واقعا آنچه را که شما سوای خدا می‌پرستید جز بتانی بیش‌ نیستند و دروغی برمی‌سازید . در حقیقت ، کسانی را که جز خدا می‌پرستید اختیار روزی شما را در دست ندارند . پس روزی را پیش خدا بجویید و او را بپرستید و وی را سپاس گویید ، که به سوی او بازگردانیده می‌شوید . \n",
            "2023-01-19 13:33:29,034 - INFO - joeynmt.training - \tHypothesis: جز خدا می پرستید که تنها تنها او را می پرستید و غیر از خدا می پرستید ، و آنچه را که شما می پرستید ، روزیتان را که شما را می پرستید ، و از خدا روزی می پرستید ، و به او بازمی گردانید .\n",
            "2023-01-19 13:33:36,824 - INFO - joeynmt.training - Epoch 256, Step:    89100, Batch Loss:     1.652560, Batch Acc: 0.569578, Tokens per Sec:    15015, Lr: 0.000030\n",
            "2023-01-19 13:33:37,774 - INFO - joeynmt.training - Epoch 256: total training loss 573.10\n",
            "2023-01-19 13:33:37,774 - INFO - joeynmt.training - EPOCH 257\n",
            "2023-01-19 13:33:44,604 - INFO - joeynmt.training - Epoch 257, Step:    89200, Batch Loss:     1.636681, Batch Acc: 0.570122, Tokens per Sec:    15589, Lr: 0.000030\n",
            "2023-01-19 13:33:52,449 - INFO - joeynmt.training - Epoch 257, Step:    89300, Batch Loss:     1.535331, Batch Acc: 0.571913, Tokens per Sec:    15702, Lr: 0.000030\n",
            "2023-01-19 13:34:00,289 - INFO - joeynmt.training - Epoch 257, Step:    89400, Batch Loss:     1.692721, Batch Acc: 0.569142, Tokens per Sec:    15331, Lr: 0.000030\n",
            "2023-01-19 13:34:04,810 - INFO - joeynmt.training - Epoch 257: total training loss 569.19\n",
            "2023-01-19 13:34:04,811 - INFO - joeynmt.training - EPOCH 258\n",
            "2023-01-19 13:34:08,139 - INFO - joeynmt.training - Epoch 258, Step:    89500, Batch Loss:     1.708248, Batch Acc: 0.571401, Tokens per Sec:    15468, Lr: 0.000030\n",
            "2023-01-19 13:34:15,912 - INFO - joeynmt.training - Epoch 258, Step:    89600, Batch Loss:     1.670360, Batch Acc: 0.571340, Tokens per Sec:    15613, Lr: 0.000030\n",
            "2023-01-19 13:34:23,739 - INFO - joeynmt.training - Epoch 258, Step:    89700, Batch Loss:     1.576249, Batch Acc: 0.574078, Tokens per Sec:    15276, Lr: 0.000030\n",
            "2023-01-19 13:34:31,505 - INFO - joeynmt.training - Epoch 258, Step:    89800, Batch Loss:     1.649054, Batch Acc: 0.568792, Tokens per Sec:    15596, Lr: 0.000030\n",
            "2023-01-19 13:34:31,967 - INFO - joeynmt.training - Epoch 258: total training loss 572.25\n",
            "2023-01-19 13:34:31,968 - INFO - joeynmt.training - EPOCH 259\n",
            "2023-01-19 13:34:39,289 - INFO - joeynmt.training - Epoch 259, Step:    89900, Batch Loss:     1.569314, Batch Acc: 0.569415, Tokens per Sec:    15468, Lr: 0.000030\n",
            "2023-01-19 13:34:47,027 - INFO - joeynmt.training - Epoch 259, Step:    90000, Batch Loss:     1.584715, Batch Acc: 0.573534, Tokens per Sec:    15753, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.24ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10672.82ex/s]\n",
            "2023-01-19 13:34:47,287 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=90000\n",
            "2023-01-19 13:34:47,288 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:34:52,294 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:34:52,294 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:34:52,294 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:34:52,296 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:34:52,299 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.58, loss:   2.73, ppl:  15.36, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9569[sec], evaluation: 0.0457[sec]\n",
            "2023-01-19 13:34:52,301 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:34:52,305 - INFO - joeynmt.training - \tSource:     همچنین : ناماز قێلێن ، آللاهدان قورخون ، هۆزورونا توپلانآجاغینیز محض اودور \n",
            "2023-01-19 13:34:52,305 - INFO - joeynmt.training - \tReference:  و اینکه نماز برپا دارید و از او بترسید ، و هم اوست که نزد وی محشور خواهید گردید . \n",
            "2023-01-19 13:34:52,305 - INFO - joeynmt.training - \tHypothesis: و می گویند : نماز را گرد آورده اید ، و او شما را به سوی خدا گرد می آورد .\n",
            "2023-01-19 13:34:52,305 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:34:52,307 - INFO - joeynmt.training - \tSource:     شاگردلر قرارا آلدێ کی ، هر بیری گۆجۆ چاتان قدر یهودئیآدا یاشایان ایمانلی باجێ قارداشلارێنا یاردیم گؤندرسین . \n",
            "2023-01-19 13:34:52,308 - INFO - joeynmt.training - \tReference:  پس شاگردان مصمم شدند که هر یک به قدر توانایی خود کمکی برای برادران ساکن یهودیه بفرستد . \n",
            "2023-01-19 13:34:52,308 - INFO - joeynmt.training - \tHypothesis: شاگردان در هر موقعیت به نام خدا رسیده است که باید در کنیسهٔ یهودیه و برادرانش را فرستادند .\n",
            "2023-01-19 13:34:52,308 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:34:52,310 - INFO - joeynmt.training - \tSource:     او کسلر کی ، گؤزلری منی آنماقدان قاپالێ ایدی وه ائشیتمهیه ده قادر دئییلدیلر \n",
            "2023-01-19 13:34:52,310 - INFO - joeynmt.training - \tReference:   به‌ همان کسانی که چشمان بصیرت‌ شان از یاد من در پرده بود ، و توانایی شنیدن حق‌ نداشتند . \n",
            "2023-01-19 13:34:52,310 - INFO - joeynmt.training - \tHypothesis: همانان که چشمان مرا از حیواناتی بود ، در حالی که باید شنوای بودند ،\n",
            "2023-01-19 13:34:52,310 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:34:52,312 - INFO - joeynmt.training - \tSource:     کؤرپهنین آتاسێندان کؤرپهیه نه آد قویولماسینی اشاره ایله سوروشدولار . \n",
            "2023-01-19 13:34:52,313 - INFO - joeynmt.training - \tReference:  پس با اشاره از پدر نوزاد سؤال کردند که می‌خواهد او را چه بنامد . \n",
            "2023-01-19 13:34:52,313 - INFO - joeynmt.training - \tHypothesis: آنان از پدرش به نام پدر خود ، صدای شیر بردند و به نام یعقوب پاسخ دادند .\n",
            "2023-01-19 13:35:00,086 - INFO - joeynmt.training - Epoch 259, Step:    90100, Batch Loss:     1.528380, Batch Acc: 0.570350, Tokens per Sec:    15146, Lr: 0.000030\n",
            "2023-01-19 13:35:04,353 - INFO - joeynmt.training - Epoch 259: total training loss 570.28\n",
            "2023-01-19 13:35:04,354 - INFO - joeynmt.training - EPOCH 260\n",
            "2023-01-19 13:35:09,173 - INFO - joeynmt.training - Epoch 260, Step:    90200, Batch Loss:     1.769387, Batch Acc: 0.576228, Tokens per Sec:    11756, Lr: 0.000030\n",
            "2023-01-19 13:35:16,983 - INFO - joeynmt.training - Epoch 260, Step:    90300, Batch Loss:     1.578319, Batch Acc: 0.572175, Tokens per Sec:    15348, Lr: 0.000030\n",
            "2023-01-19 13:35:24,846 - INFO - joeynmt.training - Epoch 260, Step:    90400, Batch Loss:     1.652912, Batch Acc: 0.570241, Tokens per Sec:    15618, Lr: 0.000030\n",
            "2023-01-19 13:35:32,604 - INFO - joeynmt.training - Epoch 260, Step:    90500, Batch Loss:     1.693118, Batch Acc: 0.572923, Tokens per Sec:    15264, Lr: 0.000030\n",
            "2023-01-19 13:35:32,771 - INFO - joeynmt.training - Epoch 260: total training loss 572.80\n",
            "2023-01-19 13:35:32,772 - INFO - joeynmt.training - EPOCH 261\n",
            "2023-01-19 13:35:40,408 - INFO - joeynmt.training - Epoch 261, Step:    90600, Batch Loss:     1.611201, Batch Acc: 0.575004, Tokens per Sec:    15541, Lr: 0.000030\n",
            "2023-01-19 13:35:48,405 - INFO - joeynmt.training - Epoch 261, Step:    90700, Batch Loss:     1.653032, Batch Acc: 0.571386, Tokens per Sec:    15077, Lr: 0.000030\n",
            "2023-01-19 13:35:56,279 - INFO - joeynmt.training - Epoch 261, Step:    90800, Batch Loss:     1.529825, Batch Acc: 0.569402, Tokens per Sec:    15308, Lr: 0.000030\n",
            "2023-01-19 13:36:00,263 - INFO - joeynmt.training - Epoch 261: total training loss 572.25\n",
            "2023-01-19 13:36:00,264 - INFO - joeynmt.training - EPOCH 262\n",
            "2023-01-19 13:36:04,057 - INFO - joeynmt.training - Epoch 262, Step:    90900, Batch Loss:     1.666672, Batch Acc: 0.575083, Tokens per Sec:    15737, Lr: 0.000030\n",
            "2023-01-19 13:36:11,962 - INFO - joeynmt.training - Epoch 262, Step:    91000, Batch Loss:     1.590970, Batch Acc: 0.575535, Tokens per Sec:    15345, Lr: 0.000030\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.17ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10667.27ex/s]\n",
            "2023-01-19 13:36:12,242 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=91000\n",
            "2023-01-19 13:36:12,242 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:36:17,192 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:36:17,192 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:36:17,193 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:36:17,194 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:36:17,197 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.41, loss:   2.76, ppl:  15.82, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9030[sec], evaluation: 0.0430[sec]\n",
            "2023-01-19 13:36:17,199 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:36:17,202 - INFO - joeynmt.training - \tSource:     هوارلایگیمین نیشانهلری آرانێزدا الامتلر ، خارقهلر وه مۆجزلر واسطهسیله ، بؤیوک دؤزوملولوکله گؤستریلدی . \n",
            "2023-01-19 13:36:17,203 - INFO - joeynmt.training - \tReference:  من نشانه‌های رسول بودن خود را با بردباری بسیار ، با نشانه‌ها ، با عجایب و با معجزات در میان شما آشکار ساختم . \n",
            "2023-01-19 13:36:17,203 - INFO - joeynmt.training - \tHypothesis: و نشانه های او در میان شما نشانه ها و معجزات های بسیار به ظهور می رساند ، اما عجیب و بسیار تحمل می شد .\n",
            "2023-01-19 13:36:17,203 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:36:17,205 - INFO - joeynmt.training - \tSource:     بونا امین اولاراق بیلیرم : ساغ قالاجاغام وه هامینیزلا بیرلیکده قالماقدا داوام ائدهجهیم کی ، اماندا هم ایرهلیلهیهسینیز ، هم ده سئوینهسینیز . \n",
            "2023-01-19 13:36:17,205 - INFO - joeynmt.training - \tReference:  پس ، از آنجا که به این امر اطمینان دارم ، می‌دانم که برای پیشرفت و شادی شما در ایمان ، در جسم خواهم ماند و با همهٔ شما به سر خواهم برد . \n",
            "2023-01-19 13:36:17,205 - INFO - joeynmt.training - \tHypothesis: از این رو ، من می دانم که با تو وضعیت دست راست من و با تمامی خود بپردازید تا شادی کنید .\n",
            "2023-01-19 13:36:17,205 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:36:17,207 - INFO - joeynmt.training - \tSource:     یوخ ، اگر اونون کؤینهگی آرخادان جێریلیبسا ، یالان دئییر ، او ایسه دوغرودانیشانلارداندیریندار . \n",
            "2023-01-19 13:36:17,207 - INFO - joeynmt.training - \tReference:  و اگر پیراهن او از پشت دریده شده ، زن دروغ گفته و او از راستگویان است . \n",
            "2023-01-19 13:36:17,208 - INFO - joeynmt.training - \tHypothesis: اما اگر از پشت آن زن ، به جهان او دروغ می گویند ، و از راست می گویند ، از حقیقت پیروی می کند .\n",
            "2023-01-19 13:36:17,208 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:36:17,209 - INFO - joeynmt.training - \tSource:     شبههسیز کی ، بو آلملرین ربی ترهفیندن نازل ائدیلمیشدیر ! \n",
            "2023-01-19 13:36:17,210 - INFO - joeynmt.training - \tReference:  و راستی که این قرآن‌ وحی پروردگار جهانیان است . \n",
            "2023-01-19 13:36:17,210 - INFO - joeynmt.training - \tHypothesis: این است همانا که از جانب پروردگار جهانیان ،\n",
            "2023-01-19 13:36:25,102 - INFO - joeynmt.training - Epoch 262, Step:    91100, Batch Loss:     1.608466, Batch Acc: 0.571626, Tokens per Sec:    14847, Lr: 0.000030\n",
            "2023-01-19 13:36:32,780 - INFO - joeynmt.training - Epoch 262: total training loss 568.20\n",
            "2023-01-19 13:36:32,780 - INFO - joeynmt.training - EPOCH 263\n",
            "2023-01-19 13:36:32,937 - INFO - joeynmt.training - Epoch 263, Step:    91200, Batch Loss:     1.606348, Batch Acc: 0.579982, Tokens per Sec:    14327, Lr: 0.000030\n",
            "2023-01-19 13:36:40,763 - INFO - joeynmt.training - Epoch 263, Step:    91300, Batch Loss:     1.670468, Batch Acc: 0.575315, Tokens per Sec:    15550, Lr: 0.000030\n",
            "2023-01-19 13:36:48,569 - INFO - joeynmt.training - Epoch 263, Step:    91400, Batch Loss:     1.620245, Batch Acc: 0.573654, Tokens per Sec:    15461, Lr: 0.000030\n",
            "2023-01-19 13:36:56,402 - INFO - joeynmt.training - Epoch 263, Step:    91500, Batch Loss:     1.706975, Batch Acc: 0.573955, Tokens per Sec:    15270, Lr: 0.000030\n",
            "2023-01-19 13:37:00,083 - INFO - joeynmt.training - Epoch 263: total training loss 569.96\n",
            "2023-01-19 13:37:00,083 - INFO - joeynmt.training - EPOCH 264\n",
            "2023-01-19 13:37:04,192 - INFO - joeynmt.training - Epoch 264, Step:    91600, Batch Loss:     1.617604, Batch Acc: 0.576100, Tokens per Sec:    15474, Lr: 0.000030\n",
            "2023-01-19 13:37:12,235 - INFO - joeynmt.training - Epoch 264, Step:    91700, Batch Loss:     1.662717, Batch Acc: 0.575548, Tokens per Sec:    15093, Lr: 0.000030\n",
            "2023-01-19 13:37:20,034 - INFO - joeynmt.training - Epoch 264, Step:    91800, Batch Loss:     1.630065, Batch Acc: 0.570535, Tokens per Sec:    15499, Lr: 0.000030\n",
            "2023-01-19 13:37:27,411 - INFO - joeynmt.training - Epoch 264: total training loss 567.42\n",
            "2023-01-19 13:37:27,412 - INFO - joeynmt.training - EPOCH 265\n",
            "2023-01-19 13:37:27,870 - INFO - joeynmt.training - Epoch 265, Step:    91900, Batch Loss:     1.673929, Batch Acc: 0.569464, Tokens per Sec:    16822, Lr: 0.000030\n",
            "2023-01-19 13:37:35,630 - INFO - joeynmt.training - Epoch 265, Step:    92000, Batch Loss:     1.663817, Batch Acc: 0.576277, Tokens per Sec:    15702, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 124.67ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 5258.06ex/s]\n",
            "2023-01-19 13:37:36,042 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=92000\n",
            "2023-01-19 13:37:36,043 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:37:42,018 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:37:42,018 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:37:42,019 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:37:42,020 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:37:42,023 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.18, loss:   2.78, ppl:  16.04, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.9247[sec], evaluation: 0.0473[sec]\n",
            "2023-01-19 13:37:42,025 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:37:42,029 - INFO - joeynmt.training - \tSource:     تاماهکارلێقلارینداندان اویدورما سؤزلرله سیزی استثمار ائدهجکلر . اونلارێن مۆحاکمهسی چوخدان داوام ائدیر وه هلاکێ هازێردێر . \n",
            "2023-01-19 13:37:42,029 - INFO - joeynmt.training - \tReference:  همچنین ، از روی طمع با سخنان جعلی از شما بهره‌کشی خواهند کرد . اما محکومیت ایشان که از گذشته‌های دور تعیین شده است ، تأخیر نمی‌کند و نابودی آنان قطعی است . \n",
            "2023-01-19 13:37:42,029 - INFO - joeynmt.training - \tHypothesis: همچنین از آنچه از چاه برده شده است ، به دلیل سخنان خود اعتماد می کردند ، بسیاری دیگر را محکوم خواهند کرد و بسیاری را نابود می کردند .\n",
            "2023-01-19 13:37:42,029 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:37:42,031 - INFO - joeynmt.training - \tSource:      دئدی : ائی ا یانلار ! بو ایش بارهسینده منه ره یینیزی بیلدیرین . من سیزینله مسلهحتلشمهمیش هئچ بیر ایش گؤرن دئییلم ! \n",
            "2023-01-19 13:37:42,031 - INFO - joeynmt.training - \tReference:  گفت : ای سران کشور در کارم به من نظر دهید که بی‌حضور شما تا به حال‌ کاری را فیصله نداده‌ام . \n",
            "2023-01-19 13:37:42,032 - INFO - joeynmt.training - \tHypothesis: گفت : ای سران قوم ، این کار را با شما در باره من به من داده ام . من از شما در باره مسحرانم ، و کاری که نمی کردم ، کاری بر شما نیستم .\n",
            "2023-01-19 13:37:42,032 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:37:42,034 - INFO - joeynmt.training - \tSource:      ان یاخین بیر یئرده . لاکین اونلار مغلوبگتلریندندهندهنالوبال سونرا غالب گلهجکلر . \n",
            "2023-01-19 13:37:42,034 - INFO - joeynmt.training - \tReference:  در نزدیکترین سرزمین ، و لی‌ بعد از شکستشان ، در ظرف چند سالی ، به زودی پیروز خواهند گردید . \n",
            "2023-01-19 13:37:42,034 - INFO - joeynmt.training - \tHypothesis: نزدیک است ، اما آنان از اصلی غلبه ها برخواهند خاست .\n",
            "2023-01-19 13:37:42,034 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:37:42,036 - INFO - joeynmt.training - \tSource:     اورادا اولوب کئچنلره اوزاقدان باخان بیر چوخ قادێن وار ایدی کی ، خدمت ائتمک اۆچۆن قالیلئیادان ایسنانین آردێنجا گلمیشدیلر . \n",
            "2023-01-19 13:37:42,037 - INFO - joeynmt.training - \tReference:  زنان بسیاری نیز از دور وقایع را نظاره می‌کردند ؛ همان زنانی که برای خدمت به عیسی ، او را از جلیل همراهی کرده بودند . \n",
            "2023-01-19 13:37:42,037 - INFO - joeynmt.training - \tHypothesis: در آنجا زنی بود که از آنجا دور بود که از جلیل بود ، به دنبال عیسی خدمتکاری بود .\n",
            "2023-01-19 13:37:49,919 - INFO - joeynmt.training - Epoch 265, Step:    92100, Batch Loss:     1.663215, Batch Acc: 0.573826, Tokens per Sec:    14541, Lr: 0.000029\n",
            "2023-01-19 13:37:57,910 - INFO - joeynmt.training - Epoch 265, Step:    92200, Batch Loss:     1.806714, Batch Acc: 0.572797, Tokens per Sec:    15039, Lr: 0.000029\n",
            "2023-01-19 13:38:01,214 - INFO - joeynmt.training - Epoch 265: total training loss 566.47\n",
            "2023-01-19 13:38:01,214 - INFO - joeynmt.training - EPOCH 266\n",
            "2023-01-19 13:38:05,982 - INFO - joeynmt.training - Epoch 266, Step:    92300, Batch Loss:     1.589777, Batch Acc: 0.573003, Tokens per Sec:    15059, Lr: 0.000029\n",
            "2023-01-19 13:38:13,792 - INFO - joeynmt.training - Epoch 266, Step:    92400, Batch Loss:     1.708485, Batch Acc: 0.573960, Tokens per Sec:    15358, Lr: 0.000029\n",
            "2023-01-19 13:38:21,523 - INFO - joeynmt.training - Epoch 266, Step:    92500, Batch Loss:     1.642645, Batch Acc: 0.575791, Tokens per Sec:    15733, Lr: 0.000029\n",
            "2023-01-19 13:38:29,588 - INFO - joeynmt.training - Epoch 266: total training loss 567.95\n",
            "2023-01-19 13:38:29,588 - INFO - joeynmt.training - EPOCH 267\n",
            "2023-01-19 13:38:30,568 - INFO - joeynmt.training - Epoch 267, Step:    92600, Batch Loss:     1.584511, Batch Acc: 0.581944, Tokens per Sec:    13474, Lr: 0.000029\n",
            "2023-01-19 13:38:38,268 - INFO - joeynmt.training - Epoch 267, Step:    92700, Batch Loss:     1.608300, Batch Acc: 0.575812, Tokens per Sec:    15837, Lr: 0.000029\n",
            "2023-01-19 13:38:45,966 - INFO - joeynmt.training - Epoch 267, Step:    92800, Batch Loss:     1.606463, Batch Acc: 0.575080, Tokens per Sec:    15694, Lr: 0.000029\n",
            "2023-01-19 13:38:53,642 - INFO - joeynmt.training - Epoch 267, Step:    92900, Batch Loss:     1.503600, Batch Acc: 0.572918, Tokens per Sec:    15645, Lr: 0.000029\n",
            "2023-01-19 13:38:56,466 - INFO - joeynmt.training - Epoch 267: total training loss 566.13\n",
            "2023-01-19 13:38:56,466 - INFO - joeynmt.training - EPOCH 268\n",
            "2023-01-19 13:39:01,428 - INFO - joeynmt.training - Epoch 268, Step:    93000, Batch Loss:     1.720238, Batch Acc: 0.579257, Tokens per Sec:    15445, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.88ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10603.30ex/s]\n",
            "2023-01-19 13:39:01,689 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=93000\n",
            "2023-01-19 13:39:01,689 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:39:07,254 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:39:07,254 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:39:07,254 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:39:07,255 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:39:07,259 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.91, loss:   2.71, ppl:  15.01, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.5166[sec], evaluation: 0.0452[sec]\n",
            "2023-01-19 13:39:07,262 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:39:07,265 - INFO - joeynmt.training - \tSource:     بونا گؤره مۆقدس روحون دئدیگی کیمی : بو گۆن اگر اونون سسینی ائشیتسنیز ، \n",
            "2023-01-19 13:39:07,265 - INFO - joeynmt.training - \tReference:  از این رو ، همان طور که از طریق روح‌القدس نوشته شده است : امروز اگر صدای مرا می‌شنوید ، \n",
            "2023-01-19 13:39:07,265 - INFO - joeynmt.training - \tHypothesis: پس روح القدس به ایشان گفته شد : امروز اگر صدای او را شنیدید ، شنیدید که او را بشنوید .\n",
            "2023-01-19 13:39:07,266 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:39:07,268 - INFO - joeynmt.training - \tSource:     قورد اولسان گرگ قاچاسان و قوواسان . \n",
            "2023-01-19 13:39:07,268 - INFO - joeynmt.training - \tReference:  گرگ كه میشوی باید دنبالشان كنی . \n",
            "2023-01-19 13:39:07,268 - INFO - joeynmt.training - \tHypothesis: اگر این را باید مسیای باید از یتاه ی یعقوب .\n",
            "2023-01-19 13:39:07,268 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:39:07,270 - INFO - joeynmt.training - \tSource:     مصیح اسعادا اولان هیات ودینه گؤره آللاهێن ارادهسی ایله مصیح ایسنانین هوارسی من پاولدان\n",
            "2023-01-19 13:39:07,270 - INFO - joeynmt.training - \tReference:  از طرف پولس که به خواست خدا رسول مسیحْ عیسی گشت تا وعدهٔ آن حیاتی را اعلام کند که از طریق مسیحْ عیسی میسر شده است ، \n",
            "2023-01-19 13:39:07,270 - INFO - joeynmt.training - \tHypothesis: از طرف پولس که به خواست مسیحْ عیسی حیات و مطابق رسول مسیحْ عیسی است ، از طرف پولس به خاطر مسیحْ عیسی است .\n",
            "2023-01-19 13:39:07,270 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:39:07,272 - INFO - joeynmt.training - \tSource:     سیلا وه تیموتئیله بیرلیکده سیزه وز ائتدیگیمیز آللاهێن اوغلو ایسا مصیح هم بلی ، هم ده خئیر دئییل . اوندا یالنیز بلی وار . \n",
            "2023-01-19 13:39:07,272 - INFO - joeynmt.training - \tReference:  همچنین ، پسر خدا عیسی مسیح که من و سیلوانوس و تیموتائوس در مورد او به شما موعظه کردیم ، همزمان بله و نه نشد ، بلکه بله در خصوص او بله شده است ؛ \n",
            "2023-01-19 13:39:07,273 - INFO - joeynmt.training - \tHypothesis: ماییم که با سیلوان و تیموتائوس به شما و در اتحاد با او هستیم ، نه تنها عیسی مسیح ، بلکه هم و نه تنها او .\n",
            "2023-01-19 13:39:15,065 - INFO - joeynmt.training - Epoch 268, Step:    93100, Batch Loss:     1.689646, Batch Acc: 0.572442, Tokens per Sec:    14993, Lr: 0.000029\n",
            "2023-01-19 13:39:22,863 - INFO - joeynmt.training - Epoch 268, Step:    93200, Batch Loss:     1.553756, Batch Acc: 0.573839, Tokens per Sec:    15542, Lr: 0.000029\n",
            "2023-01-19 13:39:29,561 - INFO - joeynmt.training - Epoch 268: total training loss 568.71\n",
            "2023-01-19 13:39:29,561 - INFO - joeynmt.training - EPOCH 269\n",
            "2023-01-19 13:39:30,735 - INFO - joeynmt.training - Epoch 269, Step:    93300, Batch Loss:     1.668748, Batch Acc: 0.570843, Tokens per Sec:    15181, Lr: 0.000029\n",
            "2023-01-19 13:39:38,509 - INFO - joeynmt.training - Epoch 269, Step:    93400, Batch Loss:     1.626784, Batch Acc: 0.577702, Tokens per Sec:    15512, Lr: 0.000029\n",
            "2023-01-19 13:39:46,248 - INFO - joeynmt.training - Epoch 269, Step:    93500, Batch Loss:     1.834697, Batch Acc: 0.573813, Tokens per Sec:    15432, Lr: 0.000029\n",
            "2023-01-19 13:39:53,915 - INFO - joeynmt.training - Epoch 269, Step:    93600, Batch Loss:     1.764910, Batch Acc: 0.571440, Tokens per Sec:    15781, Lr: 0.000029\n",
            "2023-01-19 13:39:56,558 - INFO - joeynmt.training - Epoch 269: total training loss 568.40\n",
            "2023-01-19 13:39:56,558 - INFO - joeynmt.training - EPOCH 270\n",
            "2023-01-19 13:40:01,714 - INFO - joeynmt.training - Epoch 270, Step:    93700, Batch Loss:     1.613240, Batch Acc: 0.575527, Tokens per Sec:    15343, Lr: 0.000029\n",
            "2023-01-19 13:40:09,496 - INFO - joeynmt.training - Epoch 270, Step:    93800, Batch Loss:     1.699373, Batch Acc: 0.574092, Tokens per Sec:    15466, Lr: 0.000029\n",
            "2023-01-19 13:40:17,256 - INFO - joeynmt.training - Epoch 270, Step:    93900, Batch Loss:     1.572315, Batch Acc: 0.577773, Tokens per Sec:    15802, Lr: 0.000029\n",
            "2023-01-19 13:40:23,568 - INFO - joeynmt.training - Epoch 270: total training loss 565.53\n",
            "2023-01-19 13:40:23,568 - INFO - joeynmt.training - EPOCH 271\n",
            "2023-01-19 13:40:24,991 - INFO - joeynmt.training - Epoch 271, Step:    94000, Batch Loss:     1.644425, Batch Acc: 0.576520, Tokens per Sec:    15207, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.08ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9405.35ex/s] \n",
            "2023-01-19 13:40:25,280 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=94000\n",
            "2023-01-19 13:40:25,280 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:40:30,193 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:40:30,194 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:40:30,194 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:40:30,195 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:40:30,197 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.29, loss:   2.76, ppl:  15.79, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.8677[sec], evaluation: 0.0414[sec]\n",
            "2023-01-19 13:40:30,200 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:40:30,203 - INFO - joeynmt.training - \tSource:     چوخلو آدام اونون یانینا گلیب دئدی : یهیا هئچ بیر علامت گؤسترمهسه ده ، اونون بو آدام بارهده دئدیگی بۆتۆن سؤزلر دوغرو چێخدێ . \n",
            "2023-01-19 13:40:30,204 - INFO - joeynmt.training - \tReference:  مردمی بسیار نزد او آمدند و گفتند : یحیی یک معجزه هم به ظهور نرساند ، اما هر چه در مورد این مرد گفت ، درست بود . \n",
            "2023-01-19 13:40:30,204 - INFO - joeynmt.training - \tHypothesis: در آنجا مردی نزد او آمد و گفت : یحیی نشانه ها را به ظهور رساند و نشانه ها را به ظهور رساند . در واقع ، همهٔ این سخنان را به مردم اعلام می کردند .\n",
            "2023-01-19 13:40:30,204 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:40:30,206 - INFO - joeynmt.training - \tSource:     پاسخا بایرامی ارفسی ایدی . ایسا آرتێق بو دۆنیادان آیریلیب آتانێن یانینا گئدهجهگی مقامێن گلدیگینی بیلیردی . او ، دۆنیادا اؤزونه مخصوص اولانلارێ همیشه سئومیشدی وه آخێرا قدر ده سئودی . \n",
            "2023-01-19 13:40:30,206 - INFO - joeynmt.training - \tReference:  پیش از عید پسح بود . عیسی می‌دانست ساعت او رسیده است که این دنیا را ترک کند و نزد پدر برود و چون پیروانش را که در دنیا بودند ، دوست می‌داشت ، تا به آخر به ایشان محبت کرد . \n",
            "2023-01-19 13:40:30,206 - INFO - joeynmt.training - \tHypothesis: عید پسح ، عید پسح عیسی بود . از این رو ، هنگامی که دنیا به دنیا آمد ، نزد پدر می آمد و می دانست که از دنیا دوست داشته باشد . او به این دنیا دوست می داشت و تا آنان که توبه کند ، خشنود شوید .\n",
            "2023-01-19 13:40:30,207 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:40:30,208 - INFO - joeynmt.training - \tSource:      کیتابدا ابراهیمی ده یاد ائت . حقیقتا ، او ، بۆسبۆتۆن دوغرو دانێشان کیمسه بیر پیغمبر ایدی . \n",
            "2023-01-19 13:40:30,209 - INFO - joeynmt.training - \tReference:  و در این کتاب به یاد ابراهیم پرداز ، زیرا او پیامبری بسیار راستگوی بود . \n",
            "2023-01-19 13:40:30,209 - INFO - joeynmt.training - \tHypothesis: و در این کتاب ابراهیم یاد کن ، زیرا کسی را که او راستگو بود .\n",
            "2023-01-19 13:40:30,209 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:40:30,211 - INFO - joeynmt.training - \tSource:     بیز اونو یۆکسک بیر مقاما قالدێردێق . \n",
            "2023-01-19 13:40:30,211 - INFO - joeynmt.training - \tReference:  و ما او را به مقامی بلند ارتقا دادیم . \n",
            "2023-01-19 13:40:30,211 - INFO - joeynmt.training - \tHypothesis: و او را در بار بارش درآوردیم .\n",
            "2023-01-19 13:40:38,121 - INFO - joeynmt.training - Epoch 271, Step:    94100, Batch Loss:     1.542493, Batch Acc: 0.578277, Tokens per Sec:    14848, Lr: 0.000029\n",
            "2023-01-19 13:40:45,914 - INFO - joeynmt.training - Epoch 271, Step:    94200, Batch Loss:     1.575054, Batch Acc: 0.572989, Tokens per Sec:    15530, Lr: 0.000029\n",
            "2023-01-19 13:40:53,693 - INFO - joeynmt.training - Epoch 271, Step:    94300, Batch Loss:     1.530013, Batch Acc: 0.576669, Tokens per Sec:    15522, Lr: 0.000029\n",
            "2023-01-19 13:40:55,987 - INFO - joeynmt.training - Epoch 271: total training loss 564.37\n",
            "2023-01-19 13:40:55,988 - INFO - joeynmt.training - EPOCH 272\n",
            "2023-01-19 13:41:01,589 - INFO - joeynmt.training - Epoch 272, Step:    94400, Batch Loss:     1.657277, Batch Acc: 0.576508, Tokens per Sec:    15360, Lr: 0.000029\n",
            "2023-01-19 13:41:09,378 - INFO - joeynmt.training - Epoch 272, Step:    94500, Batch Loss:     1.584174, Batch Acc: 0.575549, Tokens per Sec:    15496, Lr: 0.000029\n",
            "2023-01-19 13:41:17,143 - INFO - joeynmt.training - Epoch 272, Step:    94600, Batch Loss:     1.546708, Batch Acc: 0.575068, Tokens per Sec:    15427, Lr: 0.000029\n",
            "2023-01-19 13:41:23,176 - INFO - joeynmt.training - Epoch 272: total training loss 566.86\n",
            "2023-01-19 13:41:23,176 - INFO - joeynmt.training - EPOCH 273\n",
            "2023-01-19 13:41:24,890 - INFO - joeynmt.training - Epoch 273, Step:    94700, Batch Loss:     1.564831, Batch Acc: 0.576770, Tokens per Sec:    15655, Lr: 0.000029\n",
            "2023-01-19 13:41:32,760 - INFO - joeynmt.training - Epoch 273, Step:    94800, Batch Loss:     1.502072, Batch Acc: 0.578062, Tokens per Sec:    15395, Lr: 0.000029\n",
            "2023-01-19 13:41:40,551 - INFO - joeynmt.training - Epoch 273, Step:    94900, Batch Loss:     1.634643, Batch Acc: 0.571658, Tokens per Sec:    15333, Lr: 0.000029\n",
            "2023-01-19 13:41:48,268 - INFO - joeynmt.training - Epoch 273, Step:    95000, Batch Loss:     1.684820, Batch Acc: 0.576429, Tokens per Sec:    15727, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.29ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9813.91ex/s] \n",
            "2023-01-19 13:41:48,549 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=95000\n",
            "2023-01-19 13:41:48,549 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:41:54,195 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:41:54,195 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:41:54,196 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:41:54,196 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:41:54,199 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.76, loss:   2.69, ppl:  14.66, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.6020[sec], evaluation: 0.0409[sec]\n",
            "2023-01-19 13:41:54,203 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:41:54,206 - INFO - joeynmt.training - \tSource:     وه اونلارێ اودلو سوبایا آتاجاقلار . اورادا آغلاشما وه دیش قێجێرتێسێ اولاجاق . \n",
            "2023-01-19 13:41:54,206 - INFO - joeynmt.training - \tReference:  و آنان را در کورهٔ آتش خواهند افکند ؛ جایی که در آن گریه خواهند کرد و دندان بر هم خواهند سایید . \n",
            "2023-01-19 13:41:54,206 - INFO - joeynmt.training - \tHypothesis: و آنان را در آتش و سوم آب خواهند افکند . در آن گریه خواهند کرد و دندان بر هم خواهند سایید .\n",
            "2023-01-19 13:41:54,206 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:41:54,208 - INFO - joeynmt.training - \tSource:     اسراعل اؤؤلادلارێ اونون اۆزوندکی اؤتری احتشامین کئچیب گئتمهسینه باخماسێن دئیه اۆزۆنه نیگاب چکن موسا کیمی دئییلیک . \n",
            "2023-01-19 13:41:54,209 - INFO - joeynmt.training - \tReference:  و مانند موسی عمل نمی‌کنیم که پوششی بر چهرهٔ خود می‌کشید تا بنی‌اسرائیل به جلال آنچه از میان می‌رفت ، چشم ندوزند . \n",
            "2023-01-19 13:41:54,209 - INFO - joeynmt.training - \tHypothesis: فرزندان اسرائیل در خصوص حاضران به نحوی که از او روی نگرداند و بنگرد که ما به تحقق نرسد ، بلکه موسی موسی به توطع شود که گویی به وجود آمده ایم .\n",
            "2023-01-19 13:41:54,209 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:41:54,211 - INFO - joeynmt.training - \tSource:     مگر اونلار آللاهێن مخلوقاتێ اولجه نئجه یاراتدیغینی ، سونرا دا اونو یئنیدن دیریلدهجهیینی بیلمیرلرمی ؟ حقیقتا ، بو ، آللاه اۆچۆن آساندێر ! \n",
            "2023-01-19 13:41:54,211 - INFO - joeynmt.training - \tReference:  آیا ندیده‌اند که خدا چگونه آفرینش را آغاز می‌کند سپس آن را باز می‌گرداند ؟ در حقیقت ، این کار بر خدا آسان است . \n",
            "2023-01-19 13:41:54,211 - INFO - joeynmt.training - \tHypothesis: آیا نمی دانند که خدا و آنچه را که آفرینش را آغاز کرده و باز آن را زنده می گرداند ؟ قطعا خداست که این کار برای او آسان است .\n",
            "2023-01-19 13:41:54,211 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:41:54,213 - INFO - joeynmt.training - \tSource:     ائی گۆرزهلر نسلی ، سیز پیس اولدوغونوز حالدا نئجه یاخشی شئیلر سؤیلهیه بیلرسینیز ؟ چۆنکی اۆرک دولولوغوندان آغێز دانێشار . \n",
            "2023-01-19 13:41:54,213 - INFO - joeynmt.training - \tReference:  ای افعی‌زادگان ! چگونه می‌توانید سخن نیکو بگویید ، در حالی که خود شریرید ؟ زیرا زبان از آنچه دل از آن پر است ، سخن می‌گوید . \n",
            "2023-01-19 13:41:54,213 - INFO - joeynmt.training - \tHypothesis: ای مردم ، شما با این حال ، چه بد می کنید ؟ با این حال ، با دل های بدگویی می کنید ؛ زیرا دل هایتان پر از دل خود پر دردید .\n",
            "2023-01-19 13:41:56,248 - INFO - joeynmt.training - Epoch 273: total training loss 564.78\n",
            "2023-01-19 13:41:56,248 - INFO - joeynmt.training - EPOCH 274\n",
            "2023-01-19 13:42:02,070 - INFO - joeynmt.training - Epoch 274, Step:    95100, Batch Loss:     1.598144, Batch Acc: 0.576509, Tokens per Sec:    15242, Lr: 0.000029\n",
            "2023-01-19 13:42:09,972 - INFO - joeynmt.training - Epoch 274, Step:    95200, Batch Loss:     1.614657, Batch Acc: 0.579059, Tokens per Sec:    15207, Lr: 0.000029\n",
            "2023-01-19 13:42:17,738 - INFO - joeynmt.training - Epoch 274, Step:    95300, Batch Loss:     1.629187, Batch Acc: 0.575983, Tokens per Sec:    15597, Lr: 0.000029\n",
            "2023-01-19 13:42:23,513 - INFO - joeynmt.training - Epoch 274: total training loss 564.66\n",
            "2023-01-19 13:42:23,513 - INFO - joeynmt.training - EPOCH 275\n",
            "2023-01-19 13:42:25,522 - INFO - joeynmt.training - Epoch 275, Step:    95400, Batch Loss:     1.553141, Batch Acc: 0.578531, Tokens per Sec:    15736, Lr: 0.000029\n",
            "2023-01-19 13:42:33,377 - INFO - joeynmt.training - Epoch 275, Step:    95500, Batch Loss:     1.518146, Batch Acc: 0.577711, Tokens per Sec:    15439, Lr: 0.000029\n",
            "2023-01-19 13:42:41,219 - INFO - joeynmt.training - Epoch 275, Step:    95600, Batch Loss:     1.494888, Batch Acc: 0.578687, Tokens per Sec:    15408, Lr: 0.000029\n",
            "2023-01-19 13:42:49,002 - INFO - joeynmt.training - Epoch 275, Step:    95700, Batch Loss:     1.609321, Batch Acc: 0.574066, Tokens per Sec:    15560, Lr: 0.000029\n",
            "2023-01-19 13:42:50,662 - INFO - joeynmt.training - Epoch 275: total training loss 561.26\n",
            "2023-01-19 13:42:50,662 - INFO - joeynmt.training - EPOCH 276\n",
            "2023-01-19 13:42:56,784 - INFO - joeynmt.training - Epoch 276, Step:    95800, Batch Loss:     1.757062, Batch Acc: 0.577237, Tokens per Sec:    15661, Lr: 0.000029\n",
            "2023-01-19 13:43:04,597 - INFO - joeynmt.training - Epoch 276, Step:    95900, Batch Loss:     1.625445, Batch Acc: 0.579017, Tokens per Sec:    15564, Lr: 0.000029\n",
            "2023-01-19 13:43:12,416 - INFO - joeynmt.training - Epoch 276, Step:    96000, Batch Loss:     1.589663, Batch Acc: 0.576462, Tokens per Sec:    15501, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 111.62ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10537.99ex/s]\n",
            "2023-01-19 13:43:12,683 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=96000\n",
            "2023-01-19 13:43:12,693 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:43:18,149 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:43:18,149 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:43:18,149 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:43:18,151 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:43:18,156 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.15, loss:   2.70, ppl:  14.93, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.4043[sec], evaluation: 0.0502[sec]\n",
            "2023-01-19 13:43:18,160 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:43:18,163 - INFO - joeynmt.training - \tSource:     سیز تنبل اولمایاسینیز ، آمما ود اولونان شئیلری ایمان وه سبیرله ایرس آلانلاردان نمونه گؤتورهسینیز . \n",
            "2023-01-19 13:43:18,164 - INFO - joeynmt.training - \tReference:  و کاهل نشوید ، بلکه آنانی را سرمشق قرار دهید که وعده‌ها را از طریق ایمان و شکیبایی به میراث می‌برند . \n",
            "2023-01-19 13:43:18,164 - INFO - joeynmt.training - \tHypothesis: شما نیز به سبب ایمانتان ، اما حال شما نیز از طریق ایمان و ثروتمند باشید که به سبب ایمان و مصر ایمان ها و بر پایهٔ آن ها تازهٔ ایمانان را برید .\n",
            "2023-01-19 13:43:18,164 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:43:18,167 - INFO - joeynmt.training - \tSource:     مصیح اسعادا اولان هیات ودینه گؤره آللاهێن ارادهسی ایله مصیح ایسنانین هوارسی من پاولدان\n",
            "2023-01-19 13:43:18,167 - INFO - joeynmt.training - \tReference:  از طرف پولس که به خواست خدا رسول مسیحْ عیسی گشت تا وعدهٔ آن حیاتی را اعلام کند که از طریق مسیحْ عیسی میسر شده است ، \n",
            "2023-01-19 13:43:18,167 - INFO - joeynmt.training - \tHypothesis: از طرف پولس که در اتحاد با مسیحْ عیسی حیات و مطابق خواست خدا ، به خاطر مسیحْ عیسی است .\n",
            "2023-01-19 13:43:18,167 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:43:18,169 - INFO - joeynmt.training - \tSource:     سونرا اونا دئدیلر : بس سن کیمسن ؟ بیزی گؤندهرنلره نه جاواب وئرک ؟ اؤزون بارهده نه دئییرسن ؟ \n",
            "2023-01-19 13:43:18,170 - INFO - joeynmt.training - \tReference:  آنگاه به او گفتند : پس به ما بگو تو کیستی تا بتوانیم برای آنان که ما را فرستاده‌اند ، جوابی ببریم ؛ در مورد خود چه می‌گویی ؟ \n",
            "2023-01-19 13:43:18,170 - INFO - joeynmt.training - \tHypothesis: سپس به او گفتند : تو کیستی ؟ پاسخ دادند : ما را فرستاده است ؟\n",
            "2023-01-19 13:43:18,170 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:43:18,172 - INFO - joeynmt.training - \tSource:     حقیقتا ، ایلیاس دا پیغمبرلردندیر ! \n",
            "2023-01-19 13:43:18,172 - INFO - joeynmt.training - \tReference:  و به راستی الیاس از فرستادگان ما بود . \n",
            "2023-01-19 13:43:18,172 - INFO - joeynmt.training - \tHypothesis: و در حقیقت ، نخستین پیامبران از پیامبران خدا .\n",
            "2023-01-19 13:43:23,546 - INFO - joeynmt.training - Epoch 276: total training loss 561.85\n",
            "2023-01-19 13:43:23,547 - INFO - joeynmt.training - EPOCH 277\n",
            "2023-01-19 13:43:26,101 - INFO - joeynmt.training - Epoch 277, Step:    96100, Batch Loss:     1.518696, Batch Acc: 0.579976, Tokens per Sec:    14884, Lr: 0.000029\n",
            "2023-01-19 13:43:34,026 - INFO - joeynmt.training - Epoch 277, Step:    96200, Batch Loss:     1.535985, Batch Acc: 0.581113, Tokens per Sec:    15308, Lr: 0.000029\n",
            "2023-01-19 13:43:41,824 - INFO - joeynmt.training - Epoch 277, Step:    96300, Batch Loss:     1.659263, Batch Acc: 0.573926, Tokens per Sec:    15634, Lr: 0.000029\n",
            "2023-01-19 13:43:49,606 - INFO - joeynmt.training - Epoch 277, Step:    96400, Batch Loss:     1.648778, Batch Acc: 0.575696, Tokens per Sec:    15654, Lr: 0.000029\n",
            "2023-01-19 13:43:50,715 - INFO - joeynmt.training - Epoch 277: total training loss 560.34\n",
            "2023-01-19 13:43:50,715 - INFO - joeynmt.training - EPOCH 278\n",
            "2023-01-19 13:43:57,402 - INFO - joeynmt.training - Epoch 278, Step:    96500, Batch Loss:     1.526993, Batch Acc: 0.577016, Tokens per Sec:    15575, Lr: 0.000029\n",
            "2023-01-19 13:44:05,121 - INFO - joeynmt.training - Epoch 278, Step:    96600, Batch Loss:     1.623475, Batch Acc: 0.580021, Tokens per Sec:    15652, Lr: 0.000029\n",
            "2023-01-19 13:44:12,917 - INFO - joeynmt.training - Epoch 278, Step:    96700, Batch Loss:     1.627642, Batch Acc: 0.575555, Tokens per Sec:    15388, Lr: 0.000029\n",
            "2023-01-19 13:44:17,726 - INFO - joeynmt.training - Epoch 278: total training loss 562.88\n",
            "2023-01-19 13:44:17,726 - INFO - joeynmt.training - EPOCH 279\n",
            "2023-01-19 13:44:20,639 - INFO - joeynmt.training - Epoch 279, Step:    96800, Batch Loss:     1.659019, Batch Acc: 0.581874, Tokens per Sec:    15709, Lr: 0.000029\n",
            "2023-01-19 13:44:28,384 - INFO - joeynmt.training - Epoch 279, Step:    96900, Batch Loss:     1.712377, Batch Acc: 0.575225, Tokens per Sec:    15707, Lr: 0.000029\n",
            "2023-01-19 13:44:36,115 - INFO - joeynmt.training - Epoch 279, Step:    97000, Batch Loss:     1.629934, Batch Acc: 0.577881, Tokens per Sec:    15587, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.22ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10407.12ex/s]\n",
            "2023-01-19 13:44:36,380 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=97000\n",
            "2023-01-19 13:44:36,380 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:44:41,350 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:44:41,351 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:44:41,351 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:44:41,352 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:44:41,355 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.75, loss:   2.71, ppl:  15.10, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9240[sec], evaluation: 0.0428[sec]\n",
            "2023-01-19 13:44:41,357 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:44:41,361 - INFO - joeynmt.training - \tSource:     هم ماکئدونیایا گئدنده ، هم ده ماکئدونایادان قاییداندا یانینیزا گلمک نیتینده اولدوم . بوندان سونرا سیز منی یهودهیایا یولا سالمالێ ایدینیز . \n",
            "2023-01-19 13:44:41,361 - INFO - joeynmt.training - \tReference:  زیرا تصمیم داشتم در مسیر خود به مقدونیه از شما دیدار کنم و از مقدونیه باز نزدتان بیایم و سپس شما مرا از آنجا راهی یهودیه کنید . \n",
            "2023-01-19 13:44:41,361 - INFO - joeynmt.training - \tHypothesis: در واقع ، از مقدونیه ، به مقدونیه و از پیشاپاسوس ، به من نیز آمدم و در یهودیه نزد شما آمدم . شما وقتی به راه رسیدم ، به راهی که با من بودید به راهی به راهی که به من آمدید ، بودید .\n",
            "2023-01-19 13:44:41,361 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:44:41,363 - INFO - joeynmt.training - \tSource:      ائی ابراهیم ! بوندان ال چک ، ربینین امری آرتێق گلمیشدیر . اونلارا مۆتلق قارشیسیآلینماز بیر اعذاب گلهجکدیر . \n",
            "2023-01-19 13:44:41,363 - INFO - joeynmt.training - \tReference:  ای ابراهیم ! از این چون و چرا روی برتاب ، که فرمان پروردگارت آمده و برای آنان عذابی که بی‌بازگشت است خواهد آمد . \n",
            "2023-01-19 13:44:41,364 - INFO - joeynmt.training - \tHypothesis: ای ابراهیم ، دستور پروردگارا ، پیش از این فرمان پروردگارت به ایشان آمده است ، و قطعا عذابی دردناک خواهد بود .\n",
            "2023-01-19 13:44:41,364 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:44:41,366 - INFO - joeynmt.training - \tSource:     اونلارێن جسدلری سونرادان شکئمه گتیریلدی وه شکئمده ابراهیمین خامور اؤؤلادلاریندان بیر نئچه گۆمۆشه ساتێن آلدێغێ قبیرده دفن اولوندو . \n",
            "2023-01-19 13:44:41,366 - INFO - joeynmt.training - \tReference:  بقایای آنان را به شکیم بردند و در آرامگاهی دفن کردند که ابراهیم با پول نقره از پسران حمور در شکیم خریده بود . \n",
            "2023-01-19 13:44:41,366 - INFO - joeynmt.training - \tHypothesis: پس از آن ، جنیان و شکستند و پس از ابراهیم سراسرازی در آن ها ۱۰۰ نفر بود و بر آن ها ۱۰۰ سلیمان شد .\n",
            "2023-01-19 13:44:41,366 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:44:41,368 - INFO - joeynmt.training - \tSource:      تایفاسینین اؤزلرینه سێغیشدیرمایان تکبۆرلۆ ا یانلاری : ائی شۆعیب ! یا سنی وه سنینله بیرلیکده ایمان گتیرنلری مملهکتیمیزدن مۆتلق قوواجاغێق ، یا دا سیز بیزیم دینیمیزه دؤنهجکسینیز ! دئدیلر . بئله جاواب وئردی : نفرت ائتدیگیم حالدا بئلهمی ؟ \n",
            "2023-01-19 13:44:41,368 - INFO - joeynmt.training - \tReference:  سران قومش که تکبر می‌ورزیدند ، گفتند : ای شعیب ، یا تو و کسانی را که با تو ایمان آورده‌اند ، از شهر خودمان بیرون خواهیم کرد ؛ یا به کیش ما برگردید . گفت : آیا هر چند کراهت داشته باشیم ؟ \n",
            "2023-01-19 13:44:41,368 - INFO - joeynmt.training - \tHypothesis: و قومش که قوم خود را تکذیب کردند ، گفتند : ای قوم من ، تو را با لحاظ و سینه ات و آزارت رسانیم یا با تو واداران خواهیم کرد و گفتند : آیا شما هم آیین مایهان را از گواهی دهیم ؟ گفتند : ما از آنچه از تسلیم می کنید .\n",
            "2023-01-19 13:44:49,101 - INFO - joeynmt.training - Epoch 279, Step:    97100, Batch Loss:     1.532702, Batch Acc: 0.577333, Tokens per Sec:    15106, Lr: 0.000029\n",
            "2023-01-19 13:44:49,851 - INFO - joeynmt.training - Epoch 279: total training loss 560.24\n",
            "2023-01-19 13:44:49,851 - INFO - joeynmt.training - EPOCH 280\n",
            "2023-01-19 13:44:56,774 - INFO - joeynmt.training - Epoch 280, Step:    97200, Batch Loss:     1.595441, Batch Acc: 0.576960, Tokens per Sec:    15817, Lr: 0.000029\n",
            "2023-01-19 13:45:04,455 - INFO - joeynmt.training - Epoch 280, Step:    97300, Batch Loss:     1.505607, Batch Acc: 0.578675, Tokens per Sec:    15543, Lr: 0.000029\n",
            "2023-01-19 13:45:12,122 - INFO - joeynmt.training - Epoch 280, Step:    97400, Batch Loss:     1.667113, Batch Acc: 0.577852, Tokens per Sec:    15836, Lr: 0.000029\n",
            "2023-01-19 13:45:17,532 - INFO - joeynmt.training - Epoch 280: total training loss 563.65\n",
            "2023-01-19 13:45:17,533 - INFO - joeynmt.training - EPOCH 281\n",
            "2023-01-19 13:45:20,987 - INFO - joeynmt.training - Epoch 281, Step:    97500, Batch Loss:     1.587167, Batch Acc: 0.584630, Tokens per Sec:    14668, Lr: 0.000029\n",
            "2023-01-19 13:45:28,639 - INFO - joeynmt.training - Epoch 281, Step:    97600, Batch Loss:     1.553153, Batch Acc: 0.579151, Tokens per Sec:    15784, Lr: 0.000029\n",
            "2023-01-19 13:45:36,309 - INFO - joeynmt.training - Epoch 281, Step:    97700, Batch Loss:     1.659341, Batch Acc: 0.577217, Tokens per Sec:    15839, Lr: 0.000029\n",
            "2023-01-19 13:45:43,929 - INFO - joeynmt.training - Epoch 281, Step:    97800, Batch Loss:     1.650129, Batch Acc: 0.577498, Tokens per Sec:    15834, Lr: 0.000029\n",
            "2023-01-19 13:45:44,393 - INFO - joeynmt.training - Epoch 281: total training loss 561.18\n",
            "2023-01-19 13:45:44,393 - INFO - joeynmt.training - EPOCH 282\n",
            "2023-01-19 13:45:51,504 - INFO - joeynmt.training - Epoch 282, Step:    97900, Batch Loss:     1.586380, Batch Acc: 0.578906, Tokens per Sec:    16096, Lr: 0.000029\n",
            "2023-01-19 13:45:59,056 - INFO - joeynmt.training - Epoch 282, Step:    98000, Batch Loss:     1.618744, Batch Acc: 0.577574, Tokens per Sec:    15959, Lr: 0.000029\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 143.87ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9392.85ex/s] \n",
            "2023-01-19 13:45:59,347 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=98000\n",
            "2023-01-19 13:45:59,347 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:46:03,834 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:46:03,834 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:46:03,834 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:46:03,835 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:46:03,838 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.68, loss:   2.60, ppl:  13.45, acc:   0.45, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4418[sec], evaluation: 0.0422[sec]\n",
            "2023-01-19 13:46:03,841 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:46:03,844 - INFO - joeynmt.training - \tSource:     کنار اینسانلار ترهفیندن ده یاخشی بیر اینسان کیمی تانینمالیدیر . بئله کی ابلیسین قوردوغو تورا دۆشۆب اوتاندێرێلماسێن . \n",
            "2023-01-19 13:46:03,844 - INFO - joeynmt.training - \tReference:  علاوه بر این ، در میان افراد خارج از جماعت نیز نیکنام باشد تا مورد سرزنش قرار نگیرد و در دام ابلیس نیفتد . \n",
            "2023-01-19 13:46:03,844 - INFO - joeynmt.training - \tHypothesis: اما انسان نیز می داند چه کسی از جانب انسان است که مرده است . او همان طور که ابلیس است ، مشرعهٔ آن را می شناسد .\n",
            "2023-01-19 13:46:03,845 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:46:03,847 - INFO - joeynmt.training - \tSource:     نه بختیاردیر اۆرهیی تمیز اولانلار ! چۆنکی اونلار آللاهێ گؤرهجک . \n",
            "2023-01-19 13:46:03,847 - INFO - joeynmt.training - \tReference:   خوشا به حال آنان که دلی پاک دارند ؛ زیرا خدا را خواهند دید . \n",
            "2023-01-19 13:46:03,847 - INFO - joeynmt.training - \tHypothesis: خوشا به حال آنان که دلشان پاک است ، خدا را تمسخیص می کند ؛ زیرا خدا را می بیند .\n",
            "2023-01-19 13:46:03,847 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:46:03,849 - INFO - joeynmt.training - \tSource:     کیم حاقسێزلێق ائدرسه ، ایشلتدیگی حاقسێزلێغێن اوهزینی آلاجاق ، چۆنکی رب ترفکئشلیک ائتمز . \n",
            "2023-01-19 13:46:03,849 - INFO - joeynmt.training - \tReference:  بی‌شک کسی که بدی کند ، سزای عمل خود را خواهد دید ؛ زیرا خدا تبعیض قائل نمی‌شود . \n",
            "2023-01-19 13:46:03,849 - INFO - joeynmt.training - \tHypothesis: هر که بدکاری کند ، به عوض خود بد خواهد کرد ؛ زیرا یهوه را با فریبکاری نمی سازد .\n",
            "2023-01-19 13:46:03,849 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:46:03,851 - INFO - joeynmt.training - \tSource:     آمما ائلیزآوئت سونسوز اولدوغونا گؤره اونلارێن اؤؤلادێ یوخ ایدی . هر ایکیسینین یاشی چوخ ایدی . \n",
            "2023-01-19 13:46:03,851 - INFO - joeynmt.training - \tReference:  اما فرزندی نداشتند ، چون الیزابت نازا بود و هر دو سالخورده بودند . \n",
            "2023-01-19 13:46:03,851 - INFO - joeynmt.training - \tHypothesis: اما او از اصلاعی که از نواحی بود ، فرزندانشان بود .\n",
            "2023-01-19 13:46:11,549 - INFO - joeynmt.training - Epoch 282, Step:    98100, Batch Loss:     1.480007, Batch Acc: 0.579161, Tokens per Sec:    15060, Lr: 0.000029\n",
            "2023-01-19 13:46:15,742 - INFO - joeynmt.training - Epoch 282: total training loss 561.35\n",
            "2023-01-19 13:46:15,742 - INFO - joeynmt.training - EPOCH 283\n",
            "2023-01-19 13:46:19,213 - INFO - joeynmt.training - Epoch 283, Step:    98200, Batch Loss:     1.728689, Batch Acc: 0.579687, Tokens per Sec:    16000, Lr: 0.000029\n",
            "2023-01-19 13:46:26,879 - INFO - joeynmt.training - Epoch 283, Step:    98300, Batch Loss:     1.487748, Batch Acc: 0.578657, Tokens per Sec:    15750, Lr: 0.000029\n",
            "2023-01-19 13:46:34,545 - INFO - joeynmt.training - Epoch 283, Step:    98400, Batch Loss:     1.554018, Batch Acc: 0.577801, Tokens per Sec:    15683, Lr: 0.000029\n",
            "2023-01-19 13:46:42,149 - INFO - joeynmt.training - Epoch 283, Step:    98500, Batch Loss:     1.638422, Batch Acc: 0.579200, Tokens per Sec:    15932, Lr: 0.000028\n",
            "2023-01-19 13:46:42,330 - INFO - joeynmt.training - Epoch 283: total training loss 559.86\n",
            "2023-01-19 13:46:42,331 - INFO - joeynmt.training - EPOCH 284\n",
            "2023-01-19 13:46:49,790 - INFO - joeynmt.training - Epoch 284, Step:    98600, Batch Loss:     1.661502, Batch Acc: 0.577919, Tokens per Sec:    15776, Lr: 0.000028\n",
            "2023-01-19 13:46:57,362 - INFO - joeynmt.training - Epoch 284, Step:    98700, Batch Loss:     1.706200, Batch Acc: 0.580211, Tokens per Sec:    15700, Lr: 0.000028\n",
            "2023-01-19 13:47:04,945 - INFO - joeynmt.training - Epoch 284, Step:    98800, Batch Loss:     1.495043, Batch Acc: 0.579875, Tokens per Sec:    16004, Lr: 0.000028\n",
            "2023-01-19 13:47:09,063 - INFO - joeynmt.training - Epoch 284: total training loss 563.46\n",
            "2023-01-19 13:47:09,064 - INFO - joeynmt.training - EPOCH 285\n",
            "2023-01-19 13:47:12,722 - INFO - joeynmt.training - Epoch 285, Step:    98900, Batch Loss:     1.597240, Batch Acc: 0.580130, Tokens per Sec:    15936, Lr: 0.000028\n",
            "2023-01-19 13:47:20,265 - INFO - joeynmt.training - Epoch 285, Step:    99000, Batch Loss:     1.629780, Batch Acc: 0.583175, Tokens per Sec:    16010, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.07ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10484.85ex/s]\n",
            "2023-01-19 13:47:20,527 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=99000\n",
            "2023-01-19 13:47:20,527 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:47:26,708 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:47:26,708 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:47:26,709 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:47:26,710 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:47:26,712 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.51, loss:   2.87, ppl:  17.71, acc:   0.40, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 6.1320[sec], evaluation: 0.0455[sec]\n",
            "2023-01-19 13:47:26,715 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:47:26,718 - INFO - joeynmt.training - \tSource:     بئلهلیکله ، حقیقتی کمر کیمی بئلینیزه باغلامێش ، سالئهلیگی زیرئه کیمی دؤشونوزه تاخمێش ، بارێشێق مۆژدسنی یایماق حاضرلێغێنێ آیاقلارینیزا گئیینمیش اولاراق یئرینیزده دورون . \n",
            "2023-01-19 13:47:26,718 - INFO - joeynmt.training - \tReference:  حال برای آن که استوار بایستید ، کمربند حقیقت را به کمر ببندید ، سینه‌پوش عدالت را بر تن کنید\n",
            "2023-01-19 13:47:26,718 - INFO - joeynmt.training - \tHypothesis: پس چون شما را با کمال گام برمی دارید ، همچون شما شما را با شیوهٔ درستکاری و فردی که از زیر نظرتان گسترده بودید ، همچون شهٔ گسترده بودید .\n",
            "2023-01-19 13:47:26,719 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:47:26,721 - INFO - joeynmt.training - \tSource:     یادا کی او خانیمین ساچ لاری\n",
            "2023-01-19 13:47:26,721 - INFO - joeynmt.training - \tReference:  يا اين‌كه موهاي آن خانم\n",
            "2023-01-19 13:47:26,721 - INFO - joeynmt.training - \tHypothesis: یادش را موهای آشفته میكند\n",
            "2023-01-19 13:47:26,721 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:47:26,723 - INFO - joeynmt.training - \tSource:     آمما استئفانێن سؤزوندکی هیکمتین وه روحون قارشیسیندا دایانا بیلمهدیلر . \n",
            "2023-01-19 13:47:26,723 - INFO - joeynmt.training - \tReference:  اما نتوانستند با حکمت او و روحی که او را در سخن گفتن هدایت می‌کرد ، مقابله کنند . \n",
            "2023-01-19 13:47:26,723 - INFO - joeynmt.training - \tHypothesis: اما او از کلام الهام رسید ، او را دست ندادند و در مقابل آن نیافتند .\n",
            "2023-01-19 13:47:26,723 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:47:26,725 - INFO - joeynmt.training - \tSource:     اونلارێ حاکملرین قارشیسینا چێخاراراق دئدیلر : بو آداملار یهودیدیر . شهریمیزه قاریشیقلێق سالێبلار . \n",
            "2023-01-19 13:47:26,726 - INFO - joeynmt.training - \tReference:  پس ایشان را نزد والیان شهر حاضر ساختند و گفتند : این مردان نظم شهر ما را برهم زده‌اند . آنان یهودی‌اند\n",
            "2023-01-19 13:47:26,726 - INFO - joeynmt.training - \tHypothesis: و آنان را به شهرها و نابینا گفتند : این یهودی است ، ما شهریها را به شهری که در آن شهر می بریم ، پراکنده ساخته اند .\n",
            "2023-01-19 13:47:34,367 - INFO - joeynmt.training - Epoch 285, Step:    99100, Batch Loss:     1.623943, Batch Acc: 0.576984, Tokens per Sec:    15247, Lr: 0.000028\n",
            "2023-01-19 13:47:42,065 - INFO - joeynmt.training - Epoch 285, Step:    99200, Batch Loss:     1.608655, Batch Acc: 0.578278, Tokens per Sec:    15444, Lr: 0.000028\n",
            "2023-01-19 13:47:42,175 - INFO - joeynmt.training - Epoch 285: total training loss 560.79\n",
            "2023-01-19 13:47:42,175 - INFO - joeynmt.training - EPOCH 286\n",
            "2023-01-19 13:47:49,759 - INFO - joeynmt.training - Epoch 286, Step:    99300, Batch Loss:     1.561771, Batch Acc: 0.579163, Tokens per Sec:    15644, Lr: 0.000028\n",
            "2023-01-19 13:47:57,301 - INFO - joeynmt.training - Epoch 286, Step:    99400, Batch Loss:     1.521517, Batch Acc: 0.579484, Tokens per Sec:    16166, Lr: 0.000028\n",
            "2023-01-19 13:48:04,803 - INFO - joeynmt.training - Epoch 286, Step:    99500, Batch Loss:     1.658651, Batch Acc: 0.579054, Tokens per Sec:    16017, Lr: 0.000028\n",
            "2023-01-19 13:48:08,603 - INFO - joeynmt.training - Epoch 286: total training loss 557.25\n",
            "2023-01-19 13:48:08,603 - INFO - joeynmt.training - EPOCH 287\n",
            "2023-01-19 13:48:12,580 - INFO - joeynmt.training - Epoch 287, Step:    99600, Batch Loss:     1.515771, Batch Acc: 0.581931, Tokens per Sec:    15600, Lr: 0.000028\n",
            "2023-01-19 13:48:20,216 - INFO - joeynmt.training - Epoch 287, Step:    99700, Batch Loss:     1.532290, Batch Acc: 0.578708, Tokens per Sec:    15747, Lr: 0.000028\n",
            "2023-01-19 13:48:27,930 - INFO - joeynmt.training - Epoch 287, Step:    99800, Batch Loss:     1.668997, Batch Acc: 0.581390, Tokens per Sec:    15675, Lr: 0.000028\n",
            "2023-01-19 13:48:35,265 - INFO - joeynmt.training - Epoch 287: total training loss 558.31\n",
            "2023-01-19 13:48:35,265 - INFO - joeynmt.training - EPOCH 288\n",
            "2023-01-19 13:48:35,581 - INFO - joeynmt.training - Epoch 288, Step:    99900, Batch Loss:     1.611659, Batch Acc: 0.564826, Tokens per Sec:    15201, Lr: 0.000028\n",
            "2023-01-19 13:48:44,534 - INFO - joeynmt.training - Epoch 288, Step:   100000, Batch Loss:     1.598149, Batch Acc: 0.580307, Tokens per Sec:    13551, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 97.93ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10543.51ex/s]\n",
            "2023-01-19 13:48:44,820 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=100000\n",
            "2023-01-19 13:48:44,821 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:48:49,813 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:48:49,813 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:48:49,813 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:48:49,814 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:48:49,817 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.73, loss:   2.82, ppl:  16.82, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9444[sec], evaluation: 0.0442[sec]\n",
            "2023-01-19 13:48:49,820 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:48:49,824 - INFO - joeynmt.training - \tSource:      خاطرلا کی ، ابراهیم : ائی ربیم ، اؤلولهری نه جۆر دیریلتدیگینی منه گؤستر ! دئدیکده : مگر اینانمیرسان ؟ بویورموشدو . بلی ، اینانیرام ، لاکین اۆرهییم ساکت اولماق اۆچۆن ، دئیه جاواب وئرمیشدی . بویورموشدو : دؤرد جۆر قوش گؤتوروب اونلارا دقتله باخ ، سونرا هر داغێن باشێنا اونلاردان بیر پارچا آت ، سونرا اونلارێ چاغێر ، تئز یانینا گلهجکلر . بیل کی ، آللاه یئنیلمز غۆۆت ، هکمت صاحبدر ! \n",
            "2023-01-19 13:48:49,824 - INFO - joeynmt.training - \tReference:  و یاد کن‌ آنگاه که ابراهیم گفت : پروردگارا ، به من نشان ده ؛ چگونه مردگان را زنده می‌کنی ؟ فرمود : مگر ایمان نیاورده‌ای ؟ گفت : چرا ، ولی تا دلم آرامش یابد . فرمود : پس ، چهار پرنده برگیر ، و آنها را پیش خود ، ریز ریز گردان ؛ سپس بر هر کوهی پاره‌ای از آنها را قرار ده ؛ آنگاه آنها را فرا خوان ، شتابان به سوی تو می‌آیند ، و بدان که خداوند توانا و حکیم است . \n",
            "2023-01-19 13:48:49,824 - INFO - joeynmt.training - \tHypothesis: و یاد کن هنگامی را که ابراهیم مرده بود ، و می فرستم : پروردگارا ، آیا می دانی که ایمان بیاوری ؟ گفت : آیا ایمان ندارم که ایمان بیاورم ؟ بگو : بی گمان ، خدا را که در دلماییده کرده است . پس به راستی ، از آنان می فرماید : خدا بی نیاز است . پس کسانی که پس از ایشان را به سوی ایشان می نداد . پسند ، پروردگارشان می فرما می فر\n",
            "2023-01-19 13:48:49,824 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:48:49,826 - INFO - joeynmt.training - \tSource:     ائی یوصیف ! سن بو ایشی آچێب آغارتما . سن ده گۆناهێنا گؤره باغیشلانمانی دیله . چۆنکی سن ، حقیقتا ، گۆناه ائدنلردنسن . \n",
            "2023-01-19 13:48:49,827 - INFO - joeynmt.training - \tReference:   ای یوسف ، از این پیشامد روی بگردان . و تو ای زن‌ برای گناه خود آمرزش بخواه که تو از خطاکاران بوده‌ای . \n",
            "2023-01-19 13:48:49,827 - INFO - joeynmt.training - \tHypothesis: ای یوسف ، این کار را می گشایدی و گریه را که تو می پرستی ؛ زیرا به راستی که گناهانت بخشیده ای .\n",
            "2023-01-19 13:48:49,827 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:48:49,829 - INFO - joeynmt.training - \tSource:     اگر اۆز دؤندرسنیز ، سیزدن یئر اۆزۆنده فیتنه فصاد تؤرتمک وه قوهوملوق تئللرینی قێرماق گؤزلهنیلمزمی \n",
            "2023-01-19 13:48:49,830 - INFO - joeynmt.training - \tReference:  پس ای منافقان ، آیا امید بستید که چون از خدا برگشتید یا سرپرست مردم شدید در روی‌ زمین فساد کنید و خویشاوندیهای خود را از هم بگسلید ؟ \n",
            "2023-01-19 13:48:49,831 - INFO - joeynmt.training - \tHypothesis: پس اگر روی برتابید ، قطعا از شما در زمین فسادناپذیر است و از همسران خویشاوندان و در برابر خویش نگردانند ؟\n",
            "2023-01-19 13:48:49,831 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:48:49,833 - INFO - joeynmt.training - \tSource:     ایسا کورون الیندن توتوب کندن کنارا آپاردێ . سونرا اونون گؤزلرینه تۆپۆرۆب اللهرینی اونون اۆزهرینه قویدو وه سوروشدو : بیر شئی گؤرورسنمی ؟ \n",
            "2023-01-19 13:48:49,833 - INFO - joeynmt.training - \tReference:  عیسی دست مرد نابینا را گرفت و او را از روستا بیرون برد . سپس روی چشمان او آب دهان انداخت ، دست‌های خود را روی او گذاشت و پرسید : آیا چیزی می‌بینی ؟ \n",
            "2023-01-19 13:48:49,833 - INFO - joeynmt.training - \tHypothesis: عیسی نابینا را گرفت و دستگیر کرد . آنگاه از او بیرون برد و به او ساحل کرد و به او گفتند : آیا چیزی دیدی ؟\n",
            "2023-01-19 13:48:57,538 - INFO - joeynmt.training - Epoch 288, Step:   100100, Batch Loss:     1.661790, Batch Acc: 0.582085, Tokens per Sec:    15173, Lr: 0.000028\n",
            "2023-01-19 13:49:05,104 - INFO - joeynmt.training - Epoch 288, Step:   100200, Batch Loss:     1.655403, Batch Acc: 0.579175, Tokens per Sec:    15908, Lr: 0.000028\n",
            "2023-01-19 13:49:08,495 - INFO - joeynmt.training - Epoch 288: total training loss 556.72\n",
            "2023-01-19 13:49:08,496 - INFO - joeynmt.training - EPOCH 289\n",
            "2023-01-19 13:49:12,852 - INFO - joeynmt.training - Epoch 289, Step:   100300, Batch Loss:     1.571913, Batch Acc: 0.581812, Tokens per Sec:    15719, Lr: 0.000028\n",
            "2023-01-19 13:49:20,514 - INFO - joeynmt.training - Epoch 289, Step:   100400, Batch Loss:     1.465263, Batch Acc: 0.579714, Tokens per Sec:    15843, Lr: 0.000028\n",
            "2023-01-19 13:49:28,142 - INFO - joeynmt.training - Epoch 289, Step:   100500, Batch Loss:     1.580662, Batch Acc: 0.580972, Tokens per Sec:    15776, Lr: 0.000028\n",
            "2023-01-19 13:49:35,125 - INFO - joeynmt.training - Epoch 289: total training loss 557.57\n",
            "2023-01-19 13:49:35,125 - INFO - joeynmt.training - EPOCH 290\n",
            "2023-01-19 13:49:35,833 - INFO - joeynmt.training - Epoch 290, Step:   100600, Batch Loss:     1.486284, Batch Acc: 0.587233, Tokens per Sec:    15444, Lr: 0.000028\n",
            "2023-01-19 13:49:43,490 - INFO - joeynmt.training - Epoch 290, Step:   100700, Batch Loss:     1.676445, Batch Acc: 0.580986, Tokens per Sec:    15766, Lr: 0.000028\n",
            "2023-01-19 13:49:51,096 - INFO - joeynmt.training - Epoch 290, Step:   100800, Batch Loss:     1.652095, Batch Acc: 0.577683, Tokens per Sec:    15781, Lr: 0.000028\n",
            "2023-01-19 13:49:58,681 - INFO - joeynmt.training - Epoch 290, Step:   100900, Batch Loss:     1.525392, Batch Acc: 0.579431, Tokens per Sec:    15852, Lr: 0.000028\n",
            "2023-01-19 13:50:01,728 - INFO - joeynmt.training - Epoch 290: total training loss 559.19\n",
            "2023-01-19 13:50:01,728 - INFO - joeynmt.training - EPOCH 291\n",
            "2023-01-19 13:50:06,320 - INFO - joeynmt.training - Epoch 291, Step:   101000, Batch Loss:     1.619005, Batch Acc: 0.581994, Tokens per Sec:    15746, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.52ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10288.73ex/s]\n",
            "2023-01-19 13:50:06,593 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=101000\n",
            "2023-01-19 13:50:06,593 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:50:11,612 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:50:11,612 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:50:11,612 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:50:11,613 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:50:11,616 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.00, loss:   2.76, ppl:  15.81, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9735[sec], evaluation: 0.0414[sec]\n",
            "2023-01-19 13:50:11,618 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:50:11,621 - INFO - joeynmt.training - \tSource:     بۆتۆن خالق بو ، داوودون اوغلو دئییلمی ؟ دئیه مات قالدێ . \n",
            "2023-01-19 13:50:11,622 - INFO - joeynmt.training - \tReference:  تمام جمعیت شگفت‌زده شدند و گفتند : آیا ممکن است که او پسر داوود باشد ؟ \n",
            "2023-01-19 13:50:11,622 - INFO - joeynmt.training - \tHypothesis: همهٔ مردم چنین می گوید : آیا تو پسر داوود نیست ؟\n",
            "2023-01-19 13:50:11,622 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:50:11,624 - INFO - joeynmt.training - \tSource:     چۆنکی من ده تابئچیلیک آلتێندا اولان بیر آدامام ، منیم ده تابئچیلیگیمده اسگرلر وار . بیرینه گئت ! دئییرم ، او گئدیر ، دیگرینه ایسه گل ! دئییرم ، او دا گلیر . قولوما بونو ائت ! دئییرم ، او دا ائدیر . \n",
            "2023-01-19 13:50:11,624 - INFO - joeynmt.training - \tReference:  زیرا من خود از دیگران فرمان می‌برم و سربازانی هم زیر دست خود دارم . به یکی می‌گویم : برو ! می‌رود . به دیگری می‌گویم : بیا ! می‌آید و به غلام خود می‌گویم : این کار را انجام بده ! و او انجام می‌دهد . \n",
            "2023-01-19 13:50:11,624 - INFO - joeynmt.training - \tHypothesis: زیرا من خود می گویم ، مردی دیگر زیر دستابستم و دیگری در پیچید به یکی می گویم : برو ! می رود ! می گویم : بیا ! می آید ! می آید ! می گوید : آن که سرم به غلام خود می گویم .\n",
            "2023-01-19 13:50:11,625 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:50:11,627 - INFO - joeynmt.training - \tSource:     ایندی ایکی گئجه دیر کاکا رستم سیزی گؤزله بیر . \n",
            "2023-01-19 13:50:11,627 - INFO - joeynmt.training - \tReference:  تا حالا دو شب است كه كاكا رستم براه شما بود . \n",
            "2023-01-19 13:50:11,627 - INFO - joeynmt.training - \tHypothesis: الكا رستم شب را به شما رستم .\n",
            "2023-01-19 13:50:11,627 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:50:11,630 - INFO - joeynmt.training - \tSource:     آند اولسون سهره ؛ \n",
            "2023-01-19 13:50:11,630 - INFO - joeynmt.training - \tReference:  سوگند به روشنایی روز ، \n",
            "2023-01-19 13:50:11,630 - INFO - joeynmt.training - \tHypothesis: سوگند به ماده ،\n",
            "2023-01-19 13:50:19,375 - INFO - joeynmt.training - Epoch 291, Step:   101100, Batch Loss:     1.637176, Batch Acc: 0.581250, Tokens per Sec:    15032, Lr: 0.000028\n",
            "2023-01-19 13:50:27,078 - INFO - joeynmt.training - Epoch 291, Step:   101200, Batch Loss:     1.529820, Batch Acc: 0.579515, Tokens per Sec:    15659, Lr: 0.000028\n",
            "2023-01-19 13:50:33,903 - INFO - joeynmt.training - Epoch 291: total training loss 557.52\n",
            "2023-01-19 13:50:33,903 - INFO - joeynmt.training - EPOCH 292\n",
            "2023-01-19 13:50:34,827 - INFO - joeynmt.training - Epoch 292, Step:   101300, Batch Loss:     1.421806, Batch Acc: 0.583738, Tokens per Sec:    15389, Lr: 0.000028\n",
            "2023-01-19 13:50:42,582 - INFO - joeynmt.training - Epoch 292, Step:   101400, Batch Loss:     1.466253, Batch Acc: 0.583891, Tokens per Sec:    15681, Lr: 0.000028\n",
            "2023-01-19 13:50:50,252 - INFO - joeynmt.training - Epoch 292, Step:   101500, Batch Loss:     1.548742, Batch Acc: 0.581702, Tokens per Sec:    15729, Lr: 0.000028\n",
            "2023-01-19 13:50:57,839 - INFO - joeynmt.training - Epoch 292, Step:   101600, Batch Loss:     1.579097, Batch Acc: 0.576439, Tokens per Sec:    15846, Lr: 0.000028\n",
            "2023-01-19 13:51:00,564 - INFO - joeynmt.training - Epoch 292: total training loss 556.17\n",
            "2023-01-19 13:51:00,565 - INFO - joeynmt.training - EPOCH 293\n",
            "2023-01-19 13:51:05,413 - INFO - joeynmt.training - Epoch 293, Step:   101700, Batch Loss:     1.575946, Batch Acc: 0.583897, Tokens per Sec:    15923, Lr: 0.000028\n",
            "2023-01-19 13:51:13,109 - INFO - joeynmt.training - Epoch 293, Step:   101800, Batch Loss:     1.491869, Batch Acc: 0.579712, Tokens per Sec:    15615, Lr: 0.000028\n",
            "2023-01-19 13:51:20,713 - INFO - joeynmt.training - Epoch 293, Step:   101900, Batch Loss:     1.707186, Batch Acc: 0.581946, Tokens per Sec:    16070, Lr: 0.000028\n",
            "2023-01-19 13:51:27,201 - INFO - joeynmt.training - Epoch 293: total training loss 557.60\n",
            "2023-01-19 13:51:27,201 - INFO - joeynmt.training - EPOCH 294\n",
            "2023-01-19 13:51:28,363 - INFO - joeynmt.training - Epoch 294, Step:   102000, Batch Loss:     1.590124, Batch Acc: 0.591628, Tokens per Sec:    15300, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 136.25ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10589.09ex/s]\n",
            "2023-01-19 13:51:28,626 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=102000\n",
            "2023-01-19 13:51:28,626 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:51:33,613 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:51:33,613 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:51:33,613 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:51:33,614 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:51:33,617 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.55, loss:   2.72, ppl:  15.22, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9376[sec], evaluation: 0.0460[sec]\n",
            "2023-01-19 13:51:33,634 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:51:33,636 - INFO - joeynmt.training - \tSource:     شیطان دا اؤزونه قارشێ قالخێب اؤز اؤزونه بؤلونوبسه ، داوام گتیرمز ، یالنیز آخێرێ چاتار . \n",
            "2023-01-19 13:51:33,637 - INFO - joeynmt.training - \tReference:  همچنین اگر شیطان بر ضد خود قیام کند و در سلطنتش تفرقه بیفتد ، او نمی‌تواند دوام بیاورد و پایان کارش خواهد رسید . \n",
            "2023-01-19 13:51:33,637 - INFO - joeynmt.training - \tHypothesis: شیطان بر خود سخنی بسته و در خود خود خود نباید و جزاهنگ خود خود را به پایان رساند ، بلکه آخرین کار .\n",
            "2023-01-19 13:51:33,637 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:51:33,639 - INFO - joeynmt.training - \tSource:     آللاه ملتلر آراسێندا بو سیررین ایزهتینین نئجه زنگین اولدوغونو مۆقدسلره بیلدیرمک ایستهدی . بو سیرر سیزده اولان مصیحدر . او سیزه ایزته قوووشماق امیدینی وئریر . \n",
            "2023-01-19 13:51:33,639 - INFO - joeynmt.training - \tReference:  آری ، خشنودی خدا در این بود که شکوه و ارزش این راز مقدس را بر مقدسان آشکار سازد ، چنان که در میان غیریهودیان نیز آشکار می‌شود . آن راز این است که مسیح در اتحاد با شماست ، یعنی امید دارید که در جلال او شریک شوید . \n",
            "2023-01-19 13:51:33,639 - INFO - joeynmt.training - \tHypothesis: خدا این امر را به شما نشان می دهد که چگونه در جلال مقدس مقدسان آن متحمل می شوید . او شما را خواست تا به خاطر مسیحْ عیسی رنج کشد .\n",
            "2023-01-19 13:51:33,640 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:51:33,641 - INFO - joeynmt.training - \tSource:     بئلهجه ده آخێرێنجێلار بیرینجی ، بیرینجیلر ایسه آخێرێنجێ اولاجاق . \n",
            "2023-01-19 13:51:33,642 - INFO - joeynmt.training - \tReference:  به این ترتیب ، آخرین‌ها اولین خواهند بود و اولین‌ها آخرین . \n",
            "2023-01-19 13:51:33,642 - INFO - joeynmt.training - \tHypothesis: پس آخر ، اول اولند ، آخر خواهند بود .\n",
            "2023-01-19 13:51:33,642 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:51:33,644 - INFO - joeynmt.training - \tSource:     اگر ائو لایقدیرسه ، امین آمانلیغینیز اونون اۆزهرینه گلسین ؛ لایق دئییلسه ، امین آمانلیغینیز اؤزونوزه قاییتسین . \n",
            "2023-01-19 13:51:33,644 - INFO - joeynmt.training - \tReference:  اگر لایق باشند ، صلحی که آرزو کرده‌اید ، نصیبشان می‌شود ، اما اگر لایق نباشند ، صلحی که برای آنان آرزو نموده‌اید ، به شما بازمی‌گردد . \n",
            "2023-01-19 13:51:33,645 - INFO - joeynmt.training - \tHypothesis: اگر خانهٔ خود را بیدارید ، یقین داشته باشید که اگر کسی در صلح صلح باشد ، صلح بر شما ببخشد .\n",
            "2023-01-19 13:51:41,449 - INFO - joeynmt.training - Epoch 294, Step:   102100, Batch Loss:     1.526107, Batch Acc: 0.583591, Tokens per Sec:    15195, Lr: 0.000028\n",
            "2023-01-19 13:51:49,136 - INFO - joeynmt.training - Epoch 294, Step:   102200, Batch Loss:     1.689923, Batch Acc: 0.585021, Tokens per Sec:    15579, Lr: 0.000028\n",
            "2023-01-19 13:51:56,752 - INFO - joeynmt.training - Epoch 294, Step:   102300, Batch Loss:     1.581576, Batch Acc: 0.578230, Tokens per Sec:    15734, Lr: 0.000028\n",
            "2023-01-19 13:51:59,300 - INFO - joeynmt.training - Epoch 294: total training loss 554.89\n",
            "2023-01-19 13:51:59,301 - INFO - joeynmt.training - EPOCH 295\n",
            "2023-01-19 13:52:04,336 - INFO - joeynmt.training - Epoch 295, Step:   102400, Batch Loss:     1.706709, Batch Acc: 0.583165, Tokens per Sec:    16032, Lr: 0.000028\n",
            "2023-01-19 13:52:13,401 - INFO - joeynmt.training - Epoch 295, Step:   102500, Batch Loss:     1.562737, Batch Acc: 0.579599, Tokens per Sec:    13295, Lr: 0.000028\n",
            "2023-01-19 13:52:21,110 - INFO - joeynmt.training - Epoch 295, Step:   102600, Batch Loss:     1.620413, Batch Acc: 0.583306, Tokens per Sec:    15884, Lr: 0.000028\n",
            "2023-01-19 13:52:27,273 - INFO - joeynmt.training - Epoch 295: total training loss 555.25\n",
            "2023-01-19 13:52:27,273 - INFO - joeynmt.training - EPOCH 296\n",
            "2023-01-19 13:52:28,725 - INFO - joeynmt.training - Epoch 296, Step:   102700, Batch Loss:     1.429930, Batch Acc: 0.589418, Tokens per Sec:    15936, Lr: 0.000028\n",
            "2023-01-19 13:52:36,417 - INFO - joeynmt.training - Epoch 296, Step:   102800, Batch Loss:     1.616633, Batch Acc: 0.582377, Tokens per Sec:    15674, Lr: 0.000028\n",
            "2023-01-19 13:52:44,171 - INFO - joeynmt.training - Epoch 296, Step:   102900, Batch Loss:     1.545750, Batch Acc: 0.581706, Tokens per Sec:    15603, Lr: 0.000028\n",
            "2023-01-19 13:52:51,855 - INFO - joeynmt.training - Epoch 296, Step:   103000, Batch Loss:     1.506489, Batch Acc: 0.576645, Tokens per Sec:    15854, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 126.64ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10317.18ex/s]\n",
            "2023-01-19 13:52:52,125 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=103000\n",
            "2023-01-19 13:52:52,125 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:52:57,267 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:52:57,268 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:52:57,268 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:52:57,269 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:52:57,272 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  10.01, loss:   2.78, ppl:  16.14, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0939[sec], evaluation: 0.0446[sec]\n",
            "2023-01-19 13:52:57,467 - INFO - joeynmt.helpers - delete RESULTS/model/87000.ckpt\n",
            "2023-01-19 13:52:57,478 - INFO - joeynmt.helpers - delete /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/87000.ckpt\n",
            "2023-01-19 13:52:57,479 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/87000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/87000.ckpt')\n",
            "2023-01-19 13:52:57,481 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:52:57,484 - INFO - joeynmt.training - \tSource:     بوندان سونرا پئتئر جاواب وئرهرک اونا دئدی : باخ بیز هر شئیی قویوب سنین آردێنجا گلمیشیک ، اوهزینده نییمیز اولاجاق ؟ \n",
            "2023-01-19 13:52:57,485 - INFO - joeynmt.training - \tReference:  پطرس در مقابل به او گفت : ما همه چیز را رها کرده‌ایم و از تو پیروی می‌کنیم ؛ پس چه چیز عاید ما خواهد شد ؟ \n",
            "2023-01-19 13:52:57,485 - INFO - joeynmt.training - \tHypothesis: پس پطرس در جواب به او گفت : ما همه چیز را رها کنیم و به هر چیز خواهیم داد ؟\n",
            "2023-01-19 13:52:57,485 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:52:57,487 - INFO - joeynmt.training - \tSource:     رهمان ارشی یارادیب هؤکمو آلتێنا آلمێشدێر . \n",
            "2023-01-19 13:52:57,487 - INFO - joeynmt.training - \tReference:  خدای رحمان که بر عرش استیلا یافته است . \n",
            "2023-01-19 13:52:57,488 - INFO - joeynmt.training - \tHypothesis: و خدای رحمان را داوری کرده بود .\n",
            "2023-01-19 13:52:57,488 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:52:57,490 - INFO - joeynmt.training - \tSource:     سیزی سیناقوقلارا ، باشچیلارین وه هؤکمدارلارێن قارشیسینا گتیرنده اؤزونوزو نئجه مۆدافه ائدهجهیینیز وه یاخود نه دئیهجهیینیز بارهده قایغی چکمهگین . \n",
            "2023-01-19 13:52:57,491 - INFO - joeynmt.training - \tReference:  وقتی شما را به حضور مردم ، مأموران حکومتی و صاحب‌منصبان ببرند ، نگران مشوید که چگونه از خود دفاع کنید یا چه بگویید ؛ \n",
            "2023-01-19 13:52:57,491 - INFO - joeynmt.training - \tHypothesis: شما را به کنیسه ها و در مقابل مسافتگانه نزد آنان می آورند و در مقابل شما نحوی یا در مقابل شما ندادم .\n",
            "2023-01-19 13:52:57,491 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:52:57,493 - INFO - joeynmt.training - \tSource:      ربینه دوعا ائدیب : من مغلوب اولدوم ، بونا گؤره ده انتقام آل ! دئدی . \n",
            "2023-01-19 13:52:57,493 - INFO - joeynmt.training - \tReference:  تا پروردگارش را خواند که : من مغلوب شدم ؛ به داد من برس ! \n",
            "2023-01-19 13:52:57,494 - INFO - joeynmt.training - \tHypothesis: و هنگامی که پروردگارش دعا کرد و گفت : من از مدارکت یافتم ، پس به این دلیل بودم .\n",
            "2023-01-19 13:52:59,637 - INFO - joeynmt.training - Epoch 296: total training loss 553.40\n",
            "2023-01-19 13:52:59,638 - INFO - joeynmt.training - EPOCH 297\n",
            "2023-01-19 13:53:05,118 - INFO - joeynmt.training - Epoch 297, Step:   103100, Batch Loss:     1.544293, Batch Acc: 0.582766, Tokens per Sec:    15873, Lr: 0.000028\n",
            "2023-01-19 13:53:13,002 - INFO - joeynmt.training - Epoch 297, Step:   103200, Batch Loss:     1.660137, Batch Acc: 0.583329, Tokens per Sec:    15362, Lr: 0.000028\n",
            "2023-01-19 13:53:20,720 - INFO - joeynmt.training - Epoch 297, Step:   103300, Batch Loss:     1.574064, Batch Acc: 0.582438, Tokens per Sec:    15719, Lr: 0.000028\n",
            "2023-01-19 13:53:26,564 - INFO - joeynmt.training - Epoch 297: total training loss 553.47\n",
            "2023-01-19 13:53:26,565 - INFO - joeynmt.training - EPOCH 298\n",
            "2023-01-19 13:53:28,456 - INFO - joeynmt.training - Epoch 298, Step:   103400, Batch Loss:     1.462858, Batch Acc: 0.590078, Tokens per Sec:    15477, Lr: 0.000028\n",
            "2023-01-19 13:53:36,134 - INFO - joeynmt.training - Epoch 298, Step:   103500, Batch Loss:     1.567734, Batch Acc: 0.582492, Tokens per Sec:    15808, Lr: 0.000028\n",
            "2023-01-19 13:53:43,807 - INFO - joeynmt.training - Epoch 298, Step:   103600, Batch Loss:     1.679100, Batch Acc: 0.581897, Tokens per Sec:    15478, Lr: 0.000028\n",
            "2023-01-19 13:53:51,373 - INFO - joeynmt.training - Epoch 298, Step:   103700, Batch Loss:     1.658941, Batch Acc: 0.581252, Tokens per Sec:    15946, Lr: 0.000028\n",
            "2023-01-19 13:53:53,269 - INFO - joeynmt.training - Epoch 298: total training loss 555.32\n",
            "2023-01-19 13:53:53,269 - INFO - joeynmt.training - EPOCH 299\n",
            "2023-01-19 13:53:59,018 - INFO - joeynmt.training - Epoch 299, Step:   103800, Batch Loss:     1.512824, Batch Acc: 0.587769, Tokens per Sec:    15570, Lr: 0.000028\n",
            "2023-01-19 13:54:06,582 - INFO - joeynmt.training - Epoch 299, Step:   103900, Batch Loss:     1.666727, Batch Acc: 0.582164, Tokens per Sec:    15984, Lr: 0.000028\n",
            "2023-01-19 13:54:14,248 - INFO - joeynmt.training - Epoch 299, Step:   104000, Batch Loss:     1.630043, Batch Acc: 0.583731, Tokens per Sec:    15746, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.53ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10166.14ex/s]\n",
            "2023-01-19 13:54:14,542 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=104000\n",
            "2023-01-19 13:54:14,542 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:54:19,320 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:54:19,320 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:54:19,321 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:54:19,322 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:54:19,325 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.37, loss:   2.68, ppl:  14.51, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7319[sec], evaluation: 0.0421[sec]\n",
            "2023-01-19 13:54:19,328 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:54:19,332 - INFO - joeynmt.training - \tSource:     اونلارێن خرجلهدیکلرینین قبول اولونماسێنا مانع اولان یالنیز آللاهێ وه اونون پیغمبرینی اینکار ائتمهلری ، نامازا تنبل تنبل گلمهلری وه ایستهمهیه ایستهمهیه خرجلهمهلریدیر . \n",
            "2023-01-19 13:54:19,332 - INFO - joeynmt.training - \tReference:  و هیچ چیز مانع پذیرفته شدن انفاقهای آنان نشد جز اینکه به خدا و پیامبرش کفر ورزیدند ، و جز با حال‌ کسالت نماز به جا نمی‌آورند ، و جز با کراهت انفاق نمی‌کنند . \n",
            "2023-01-19 13:54:19,332 - INFO - joeynmt.training - \tHypothesis: و آنچه را که از ایشان مجادله نمی کنند ، مانع آنان است ، و کسانی که کفر ورزیدند ، و به نماز ورزد که بر نماز ورزد ، و بر نماز برپا دارند ، و خودشان مصر و خود را از راه خدا بیمابند .\n",
            "2023-01-19 13:54:19,332 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:54:19,334 - INFO - joeynmt.training - \tSource:     آند اولسون کی ، بیز قور آنێ ابرت آلماق اۆچۆن بئله آسانلاشدێردێق . آمما هئچ بیر ابرت آلان وارمێ \n",
            "2023-01-19 13:54:19,335 - INFO - joeynmt.training - \tReference:  و قطعا قرآن را برای پندآموزی آسان کرده‌ایم ، پس آیا پندگیرنده‌ای هست ؟ \n",
            "2023-01-19 13:54:19,335 - INFO - joeynmt.training - \tHypothesis: و قطعا قرآن را برای پندآموزی آسان کردیم ، و آیا پندگیرنده ای هست ؟\n",
            "2023-01-19 13:54:19,335 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:54:19,337 - INFO - joeynmt.training - \tSource:     اوندان اول موسانێن رحبر وه مرحمت اولان کیتابی وار ایدی . بو عرب دیلینده تصدیق ائدن ، زالیملاری قورخوتماق وه یاخشی امل صاحبلرینه مۆژده وئرمک اۆچۆن اولان بیر کیتابدیر ! \n",
            "2023-01-19 13:54:19,337 - INFO - joeynmt.training - \tReference:  و حال آنکه‌ پیش از آن ، کتاب موسی ، راهبر و مایه‌ رحمتی بود ؛ و این قرآن‌ کتابی است به زبان عربی که تصدیق‌کننده آن‌ است ، تا کسانی را که ستم کرده‌اند هشدار دهد و برای نیکوکاران مژده‌ای باشد . \n",
            "2023-01-19 13:54:19,337 - INFO - joeynmt.training - \tHypothesis: کتابی از جانب موسی و رحیمت رحمتی از جانب خدای رحمان بود . این است آنچه تصدیق می کرد ، و به زبان و مستمندان کتاب و مژده ده .\n",
            "2023-01-19 13:54:19,337 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:54:19,339 - INFO - joeynmt.training - \tSource:     یاخود گؤیلرین ، یئرین وه اونلارێن آراسێندا اولان هر شئین اختیاری اونلارێن الیندهدیر ائله ایسه قوی ایپلردن یاپیشیب قالخسێنلار ! \n",
            "2023-01-19 13:54:19,340 - INFO - joeynmt.training - \tReference:  آیا فرمانروایی آسمانها و زمین و آنچه میان آن دو است از آن ایشان است ؟ اگر چنین است‌ پس با چنگ زدن‌ در آن اسباب به بالا روند . \n",
            "2023-01-19 13:54:19,340 - INFO - joeynmt.training - \tHypothesis: یا نوشته های آسمانها و زمین و آنچه میان آن دو است ، و آنچه میان آن دو است ، از دست داده اند ، از برخیزان برمی خیزند .\n",
            "2023-01-19 13:54:25,012 - INFO - joeynmt.training - Epoch 299: total training loss 554.74\n",
            "2023-01-19 13:54:25,012 - INFO - joeynmt.training - EPOCH 300\n",
            "2023-01-19 13:54:27,029 - INFO - joeynmt.training - Epoch 300, Step:   104100, Batch Loss:     1.561774, Batch Acc: 0.579613, Tokens per Sec:    15804, Lr: 0.000028\n",
            "2023-01-19 13:54:34,707 - INFO - joeynmt.training - Epoch 300, Step:   104200, Batch Loss:     1.630680, Batch Acc: 0.585992, Tokens per Sec:    15590, Lr: 0.000028\n",
            "2023-01-19 13:54:42,403 - INFO - joeynmt.training - Epoch 300, Step:   104300, Batch Loss:     1.684105, Batch Acc: 0.584551, Tokens per Sec:    15611, Lr: 0.000028\n",
            "2023-01-19 13:54:50,055 - INFO - joeynmt.training - Epoch 300, Step:   104400, Batch Loss:     1.631658, Batch Acc: 0.582470, Tokens per Sec:    15685, Lr: 0.000028\n",
            "2023-01-19 13:54:51,932 - INFO - joeynmt.training - Epoch 300: total training loss 555.48\n",
            "2023-01-19 13:54:51,932 - INFO - joeynmt.training - EPOCH 301\n",
            "2023-01-19 13:54:57,784 - INFO - joeynmt.training - Epoch 301, Step:   104500, Batch Loss:     1.470686, Batch Acc: 0.586690, Tokens per Sec:    15694, Lr: 0.000028\n",
            "2023-01-19 13:55:05,369 - INFO - joeynmt.training - Epoch 301, Step:   104600, Batch Loss:     1.646490, Batch Acc: 0.585356, Tokens per Sec:    15835, Lr: 0.000028\n",
            "2023-01-19 13:55:13,115 - INFO - joeynmt.training - Epoch 301, Step:   104700, Batch Loss:     1.570338, Batch Acc: 0.584499, Tokens per Sec:    15471, Lr: 0.000028\n",
            "2023-01-19 13:55:18,697 - INFO - joeynmt.training - Epoch 301: total training loss 551.80\n",
            "2023-01-19 13:55:18,698 - INFO - joeynmt.training - EPOCH 302\n",
            "2023-01-19 13:55:20,876 - INFO - joeynmt.training - Epoch 302, Step:   104800, Batch Loss:     1.530253, Batch Acc: 0.584730, Tokens per Sec:    15694, Lr: 0.000028\n",
            "2023-01-19 13:55:28,516 - INFO - joeynmt.training - Epoch 302, Step:   104900, Batch Loss:     1.647214, Batch Acc: 0.583415, Tokens per Sec:    15857, Lr: 0.000028\n",
            "2023-01-19 13:55:37,579 - INFO - joeynmt.training - Epoch 302, Step:   105000, Batch Loss:     1.577254, Batch Acc: 0.586858, Tokens per Sec:    13462, Lr: 0.000028\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 143.57ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9740.79ex/s]\n",
            "2023-01-19 13:55:37,850 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=105000\n",
            "2023-01-19 13:55:37,850 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:55:43,190 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:55:43,190 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:55:43,190 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:55:43,191 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:55:43,194 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  10.11, loss:   2.77, ppl:  16.01, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.2911[sec], evaluation: 0.0449[sec]\n",
            "2023-01-19 13:55:43,194 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
            "2023-01-19 13:55:43,406 - INFO - joeynmt.helpers - delete RESULTS/model/61000.ckpt\n",
            "2023-01-19 13:55:43,420 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:55:43,424 - INFO - joeynmt.training - \tSource:     بئلهلیکله ، یوصیف قالخدێ ، همین گئجه کؤرپه ایله آناسێنێ گؤتوروب میصیره یوللاندی . \n",
            "2023-01-19 13:55:43,424 - INFO - joeynmt.training - \tReference:  پس ، یوسف برخاست و شب‌هنگام با کودک و مادر او عازم مصر شد . \n",
            "2023-01-19 13:55:43,424 - INFO - joeynmt.training - \tHypothesis: پس یوسف برخاست و در حالی که یوسف برخاست ، شب را برداشت و مادر خود را برداشت و در دمشق به راه افتاد .\n",
            "2023-01-19 13:55:43,425 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:55:43,427 - INFO - joeynmt.training - \tSource:     ایییرمی دؤرد آغساققال تاختدا اوتورانێن اؤنونده یئره قاپانێب ابدی وار اولانا سجده ائدیر وه تاجلارێنێ تاختێن اؤنونه آتێب دئییرلر : \n",
            "2023-01-19 13:55:43,427 - INFO - joeynmt.training - \tReference:  آن ۲۴ پیر در مقابل آن تخت‌نشین زانو می‌زنند و او را که همیشه و تا ابد زنده است ، پرستش می‌کنند . آنگاه تاج خود را پیش آن تخت می‌اندازند و می‌گویند : \n",
            "2023-01-19 13:55:43,427 - INFO - joeynmt.training - \tHypothesis: آنگاه آن چهار موجود زنده که بر تخت نشسته بود ، تخت ها و تخت ها که در مقابل تخت می نشستند ، پرستش می کردند و می گفتند : آن ها را به خاک می کشند .\n",
            "2023-01-19 13:55:43,427 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:55:43,429 - INFO - joeynmt.training - \tSource:      آللاها وه اونون پیغمبرینه ایمان گتیرهسینیز . آللاه یولوندا مالێنێز وه جانێنێزلا ووروشاسێنێز . بیلسهنیز ، بو سیزین اۆچۆن نه قدر خئیرلیدیر ! \n",
            "2023-01-19 13:55:43,430 - INFO - joeynmt.training - \tReference:  به خدا و فرستاده او بگروید و در راه خدا با مال و جانتان جهاد کنید . این گذشت و فداکاری‌ اگر بدانید ، برای شما بهتر است . \n",
            "2023-01-19 13:55:43,430 - INFO - joeynmt.training - \tHypothesis: و به خدا و پیامبر او ایمان آورید ، و در راه خدا با شما بجنگید ، و شما را در راه خدا جهاد کرده اید ، و اگر مؤمن باشید ، این برای شما بهتر است .\n",
            "2023-01-19 13:55:43,430 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:55:43,432 - INFO - joeynmt.training - \tSource:     اؤلچنده اؤلچوده دۆز اولون ، دۆزگۆن ترهزی ایله چکین . بو داها خئیرلی وه نتیجه ائ تباریله داها یاخشیدیر ! \n",
            "2023-01-19 13:55:43,432 - INFO - joeynmt.training - \tReference:  و چون پیمانه می‌کنید ، پیمانه را تمام دهید ، و با ترازوی درست بسنجید که این بهتر و خوش فرجام‌تر است . \n",
            "2023-01-19 13:55:43,432 - INFO - joeynmt.training - \tHypothesis: و به راستی که پیمانه را درست است ، و ترجیب و ترین و ترینین و بهتر است .\n",
            "2023-01-19 13:55:51,364 - INFO - joeynmt.training - Epoch 302, Step:   105100, Batch Loss:     1.647941, Batch Acc: 0.583324, Tokens per Sec:    14284, Lr: 0.000028\n",
            "2023-01-19 13:55:52,863 - INFO - joeynmt.training - Epoch 302: total training loss 550.41\n",
            "2023-01-19 13:55:52,863 - INFO - joeynmt.training - EPOCH 303\n",
            "2023-01-19 13:55:59,055 - INFO - joeynmt.training - Epoch 303, Step:   105200, Batch Loss:     1.569209, Batch Acc: 0.584067, Tokens per Sec:    15803, Lr: 0.000028\n",
            "2023-01-19 13:56:06,678 - INFO - joeynmt.training - Epoch 303, Step:   105300, Batch Loss:     1.571937, Batch Acc: 0.584522, Tokens per Sec:    15702, Lr: 0.000028\n",
            "2023-01-19 13:56:14,375 - INFO - joeynmt.training - Epoch 303, Step:   105400, Batch Loss:     1.536955, Batch Acc: 0.583528, Tokens per Sec:    15688, Lr: 0.000028\n",
            "2023-01-19 13:56:19,645 - INFO - joeynmt.training - Epoch 303: total training loss 553.41\n",
            "2023-01-19 13:56:19,645 - INFO - joeynmt.training - EPOCH 304\n",
            "2023-01-19 13:56:22,072 - INFO - joeynmt.training - Epoch 304, Step:   105500, Batch Loss:     1.505682, Batch Acc: 0.588642, Tokens per Sec:    16109, Lr: 0.000028\n",
            "2023-01-19 13:56:29,792 - INFO - joeynmt.training - Epoch 304, Step:   105600, Batch Loss:     1.615389, Batch Acc: 0.586712, Tokens per Sec:    15709, Lr: 0.000028\n",
            "2023-01-19 13:56:37,469 - INFO - joeynmt.training - Epoch 304, Step:   105700, Batch Loss:     1.696989, Batch Acc: 0.585038, Tokens per Sec:    15643, Lr: 0.000028\n",
            "2023-01-19 13:56:45,242 - INFO - joeynmt.training - Epoch 304, Step:   105800, Batch Loss:     1.392815, Batch Acc: 0.584081, Tokens per Sec:    15635, Lr: 0.000027\n",
            "2023-01-19 13:56:46,482 - INFO - joeynmt.training - Epoch 304: total training loss 550.46\n",
            "2023-01-19 13:56:46,483 - INFO - joeynmt.training - EPOCH 305\n",
            "2023-01-19 13:56:53,024 - INFO - joeynmt.training - Epoch 305, Step:   105900, Batch Loss:     1.574023, Batch Acc: 0.585944, Tokens per Sec:    15524, Lr: 0.000027\n",
            "2023-01-19 13:57:00,726 - INFO - joeynmt.training - Epoch 305, Step:   106000, Batch Loss:     1.478989, Batch Acc: 0.584368, Tokens per Sec:    15640, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.23ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9838.43ex/s]\n",
            "2023-01-19 13:57:00,996 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=106000\n",
            "2023-01-19 13:57:00,996 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:57:06,176 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:57:06,177 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:57:06,177 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:57:06,178 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:57:06,181 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  10.28, loss:   2.73, ppl:  15.30, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1362[sec], evaluation: 0.0413[sec]\n",
            "2023-01-19 13:57:06,181 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
            "2023-01-19 13:57:06,381 - INFO - joeynmt.helpers - delete RESULTS/model/103000.ckpt\n",
            "2023-01-19 13:57:06,394 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:57:06,398 - INFO - joeynmt.training - \tSource:     آللاها وه پیغمبره ایتاعت ائدین کی ، بلکه ، باغێشلانمێش اولاسێنێز ! \n",
            "2023-01-19 13:57:06,398 - INFO - joeynmt.training - \tReference:  خدا و رسول را فرمان برید ، باشد که مشمول رحمت قرار گیرید . \n",
            "2023-01-19 13:57:06,398 - INFO - joeynmt.training - \tHypothesis: و خدا را فرمان بر پیامبرید و فرمانبردارید که شما آمرزنده مهربان بودید .\n",
            "2023-01-19 13:57:06,398 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:57:06,400 - INFO - joeynmt.training - \tSource:     اونلارێن گؤزلری زلیلجهسینه یئره دیکیلهجک ، اؤزلرینی ده ذلت بۆرویهجکدیر . بو اونلارا وه د اولونموش همین قیامت گۆنۆدۆر ! \n",
            "2023-01-19 13:57:06,401 - INFO - joeynmt.training - \tReference:  دیدگانشان فرو افتاده ، غبار مذلت آنان را فرو گرفته است . این است همان روزی که به ایشان وعده داده می‌شد . \n",
            "2023-01-19 13:57:06,401 - INFO - joeynmt.training - \tHypothesis: چشمانشان فروخته شوند و بر آنان چیره شده است و آنان روز رستاخیز داوری خواهند شد .\n",
            "2023-01-19 13:57:06,401 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:57:06,403 - INFO - joeynmt.training - \tSource:     ایمهانلی بیر قادێنێن دول قادێن یاخینلاری وارسا ، قوی اونلارا یاردیم ائتسین . ایمهنلیلار جمیتی بئله یۆکۆن آلتێنا گیرمهسین کی ، حقیقی دول قادێنلارا یاردیم ائده بیلسین . \n",
            "2023-01-19 13:57:06,403 - INFO - joeynmt.training - \tReference:  اگر زنی ایماندار خویشاوندانی بیوه دارد ، باید به آنان کمک کند تا سربار جماعت نباشند . به این ترتیب ، جماعت می‌تواند به بیوه‌زنانی که واقعا بی‌کس و نیازمندند ، کمک کند . \n",
            "2023-01-19 13:57:06,403 - INFO - joeynmt.training - \tHypothesis: آنان با زنی از زنی هستند که شوهرش به جماعت بیندازند ، مبادا به جماعت ها داخل شوند و بیوه زنان در بندازند که خود را در بند کشند ، چنان که خود را در کمالد .\n",
            "2023-01-19 13:57:06,403 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:57:06,405 - INFO - joeynmt.training - \tSource:     مجدللی مریم وه او بیری مریم اورادا ، قبیرین قارشیسیندا اوتورموشدولار . \n",
            "2023-01-19 13:57:06,406 - INFO - joeynmt.training - \tReference:  اما مریم مجدلیه و آن مریم دیگر همچنان در آنجا در مقابل مقبره نشستند . \n",
            "2023-01-19 13:57:06,406 - INFO - joeynmt.training - \tHypothesis: مریم مجدلیه و مریم در آنجا نشسته بودند .\n",
            "2023-01-19 13:57:14,390 - INFO - joeynmt.training - Epoch 305, Step:   106100, Batch Loss:     1.549474, Batch Acc: 0.585248, Tokens per Sec:    14245, Lr: 0.000027\n",
            "2023-01-19 13:57:19,321 - INFO - joeynmt.training - Epoch 305: total training loss 550.18\n",
            "2023-01-19 13:57:19,322 - INFO - joeynmt.training - EPOCH 306\n",
            "2023-01-19 13:57:22,103 - INFO - joeynmt.training - Epoch 306, Step:   106200, Batch Loss:     1.551372, Batch Acc: 0.587562, Tokens per Sec:    15796, Lr: 0.000027\n",
            "2023-01-19 13:57:29,816 - INFO - joeynmt.training - Epoch 306, Step:   106300, Batch Loss:     1.607131, Batch Acc: 0.586693, Tokens per Sec:    15590, Lr: 0.000027\n",
            "2023-01-19 13:57:37,738 - INFO - joeynmt.training - Epoch 306, Step:   106400, Batch Loss:     1.478897, Batch Acc: 0.584496, Tokens per Sec:    15165, Lr: 0.000027\n",
            "2023-01-19 13:57:45,439 - INFO - joeynmt.training - Epoch 306, Step:   106500, Batch Loss:     1.547032, Batch Acc: 0.585876, Tokens per Sec:    15707, Lr: 0.000027\n",
            "2023-01-19 13:57:46,426 - INFO - joeynmt.training - Epoch 306: total training loss 550.55\n",
            "2023-01-19 13:57:46,427 - INFO - joeynmt.training - EPOCH 307\n",
            "2023-01-19 13:57:53,071 - INFO - joeynmt.training - Epoch 307, Step:   106600, Batch Loss:     1.591029, Batch Acc: 0.585981, Tokens per Sec:    15861, Lr: 0.000027\n",
            "2023-01-19 13:58:00,664 - INFO - joeynmt.training - Epoch 307, Step:   106700, Batch Loss:     1.643414, Batch Acc: 0.587192, Tokens per Sec:    15786, Lr: 0.000027\n",
            "2023-01-19 13:58:08,285 - INFO - joeynmt.training - Epoch 307, Step:   106800, Batch Loss:     1.555174, Batch Acc: 0.581879, Tokens per Sec:    15779, Lr: 0.000027\n",
            "2023-01-19 13:58:13,063 - INFO - joeynmt.training - Epoch 307: total training loss 553.82\n",
            "2023-01-19 13:58:13,063 - INFO - joeynmt.training - EPOCH 308\n",
            "2023-01-19 13:58:15,998 - INFO - joeynmt.training - Epoch 308, Step:   106900, Batch Loss:     1.628630, Batch Acc: 0.587939, Tokens per Sec:    15493, Lr: 0.000027\n",
            "2023-01-19 13:58:23,768 - INFO - joeynmt.training - Epoch 308, Step:   107000, Batch Loss:     1.572805, Batch Acc: 0.586052, Tokens per Sec:    15290, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.59ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9277.71ex/s]\n",
            "2023-01-19 13:58:24,053 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=107000\n",
            "2023-01-19 13:58:24,053 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:58:29,888 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:58:29,888 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:58:29,888 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:58:29,889 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:58:29,892 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.11, loss:   2.80, ppl:  16.51, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.7860[sec], evaluation: 0.0451[sec]\n",
            "2023-01-19 13:58:29,895 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:58:29,898 - INFO - joeynmt.training - \tSource:     اونلار : ائی ربیمیز ! بیزه شدتلی جزا وئرمهسیندن وه یا آزغێنلاشاراق هدینی آشماسێندان قورخوروق ! دئدیلر . \n",
            "2023-01-19 13:58:29,898 - INFO - joeynmt.training - \tReference:  آن دو گفتند : پروردگارا ، ما می‌ترسیم که او آسیبی به ما برساند یا آنکه سرکشی کند . \n",
            "2023-01-19 13:58:29,899 - INFO - joeynmt.training - \tHypothesis: گفتند : پروردگارا ، برای ما عذابی سخت نگوید که از خدا بترسد و از تقوا دشوار درنیده شود .\n",
            "2023-01-19 13:58:29,899 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:58:29,901 - INFO - joeynmt.training - \tSource:     دامدا اولان ائویندن نه ایسه گؤتورمک اۆچۆن آشاغێ ائنیب ائوینه گیرمهسین . \n",
            "2023-01-19 13:58:29,901 - INFO - joeynmt.training - \tReference:  کسی که بر بام است ، پایین نیاید و به خانه‌اش داخل نشود تا چیزی از آنجا بردارد . \n",
            "2023-01-19 13:58:29,901 - INFO - joeynmt.training - \tHypothesis: اما وقتی به خانهٔ من ، نرفت که نهم از خانهٔ من است ، داخل شد و نه برای خانهٔ خانهٔ من .\n",
            "2023-01-19 13:58:29,901 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:58:29,903 - INFO - joeynmt.training - \tSource:     ائی ایمان گتیرنلر ! بیر دسته ایله اۆز اۆزه گلدیکده مۆحکم اولون وه آللاهێ چوخ یادا سالێن کی ، نجات تاپاسێنێز ! \n",
            "2023-01-19 13:58:29,903 - INFO - joeynmt.training - \tReference:  ای کسانی که ایمان آورده‌اید ، چون با گروهی برخورد می‌کنید پایداری ورزید و خدا را بسیار یاد کنید ، باشد که رستگار شوید . \n",
            "2023-01-19 13:58:29,904 - INFO - joeynmt.training - \tHypothesis: ای کسانی که ایمان آورده اید ، گروهی از دسته ای به روی برتافت ، و خدا را یاد کنید و یاد کنید که رستگار شوید .\n",
            "2023-01-19 13:58:29,904 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:58:29,906 - INFO - joeynmt.training - \tSource:     بیلدیگینیز کیمی ، اولجه فیلیپیده ازیت چکمیش ، تحقیر اولونموشدوق . آمما بؤیوک زدیتلهیتلره باخمایاراق ، آللاهێن مۆژدسنی سیزه بیان ائتمک اۆچۆن آللاهیمیزدان جصارت آلدێق . \n",
            "2023-01-19 13:58:29,906 - INFO - joeynmt.training - \tReference:  بلکه چنان که می‌دانید ، هرچند نخست در فیلیپی مورد آزار و اهانت قرار گرفتیم ، به کمک خدای خود شهامت یافتیم تا با وجود مخالفت‌های بسیار ، بشارت خدا را به شما برسانیم . \n",
            "2023-01-19 13:58:29,906 - INFO - joeynmt.training - \tHypothesis: زیرا می دانید که پیش از این رو ، فیلیپس ، سختی هایی که به شما رسیده بودیم ، آزار می رسانیم ، اما اکنون از دیدن خدا ، با شهامت بسیار به وجود آمده ایم .\n",
            "2023-01-19 13:58:37,655 - INFO - joeynmt.training - Epoch 308, Step:   107100, Batch Loss:     1.591608, Batch Acc: 0.582914, Tokens per Sec:    15086, Lr: 0.000027\n",
            "2023-01-19 13:58:45,495 - INFO - joeynmt.training - Epoch 308, Step:   107200, Batch Loss:     1.701344, Batch Acc: 0.585406, Tokens per Sec:    15292, Lr: 0.000027\n",
            "2023-01-19 13:58:46,478 - INFO - joeynmt.training - Epoch 308: total training loss 552.82\n",
            "2023-01-19 13:58:46,478 - INFO - joeynmt.training - EPOCH 309\n",
            "2023-01-19 13:58:53,296 - INFO - joeynmt.training - Epoch 309, Step:   107300, Batch Loss:     1.591485, Batch Acc: 0.588631, Tokens per Sec:    15633, Lr: 0.000027\n",
            "2023-01-19 13:59:01,777 - INFO - joeynmt.training - Epoch 309, Step:   107400, Batch Loss:     1.547050, Batch Acc: 0.587612, Tokens per Sec:    14226, Lr: 0.000027\n",
            "2023-01-19 13:59:09,877 - INFO - joeynmt.training - Epoch 309, Step:   107500, Batch Loss:     1.558280, Batch Acc: 0.586429, Tokens per Sec:    14967, Lr: 0.000027\n",
            "2023-01-19 13:59:14,538 - INFO - joeynmt.training - Epoch 309: total training loss 548.87\n",
            "2023-01-19 13:59:14,539 - INFO - joeynmt.training - EPOCH 310\n",
            "2023-01-19 13:59:17,614 - INFO - joeynmt.training - Epoch 310, Step:   107600, Batch Loss:     1.594772, Batch Acc: 0.582521, Tokens per Sec:    15681, Lr: 0.000027\n",
            "2023-01-19 13:59:25,419 - INFO - joeynmt.training - Epoch 310, Step:   107700, Batch Loss:     1.588178, Batch Acc: 0.589146, Tokens per Sec:    15655, Lr: 0.000027\n",
            "2023-01-19 13:59:33,209 - INFO - joeynmt.training - Epoch 310, Step:   107800, Batch Loss:     1.433598, Batch Acc: 0.587951, Tokens per Sec:    15581, Lr: 0.000027\n",
            "2023-01-19 13:59:41,024 - INFO - joeynmt.training - Epoch 310, Step:   107900, Batch Loss:     1.521404, Batch Acc: 0.582760, Tokens per Sec:    15244, Lr: 0.000027\n",
            "2023-01-19 13:59:41,670 - INFO - joeynmt.training - Epoch 310: total training loss 549.77\n",
            "2023-01-19 13:59:41,670 - INFO - joeynmt.training - EPOCH 311\n",
            "2023-01-19 13:59:48,761 - INFO - joeynmt.training - Epoch 311, Step:   108000, Batch Loss:     1.582711, Batch Acc: 0.587878, Tokens per Sec:    15839, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.55ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10557.57ex/s]\n",
            "2023-01-19 13:59:49,035 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=108000\n",
            "2023-01-19 13:59:49,035 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 13:59:54,357 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 13:59:54,357 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 13:59:54,357 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 13:59:54,358 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 13:59:54,361 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.63, loss:   2.73, ppl:  15.29, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.2745[sec], evaluation: 0.0439[sec]\n",
            "2023-01-19 13:59:54,364 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 13:59:54,367 - INFO - joeynmt.training - \tSource:     حقیقتا ، او ، درگاهێمێزا یاخین اولاجاق وه اونون قاییدیب گلهجهگی یئر ده گؤزل اولاجاقدێر . \n",
            "2023-01-19 13:59:54,367 - INFO - joeynmt.training - \tReference:  و قطعا برای او در پیشگاه ما تقرب و فرجام نیکوست . \n",
            "2023-01-19 13:59:54,367 - INFO - joeynmt.training - \tHypothesis: در حقیقت ، او نزدیک خواهد بود و فرجامی نزدیک خواهد بود .\n",
            "2023-01-19 13:59:54,368 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 13:59:54,370 - INFO - joeynmt.training - \tSource:      اینکار ائدیلمیش کیمسهیه مۆکافات اولاراق وئریلن گؤزوموزون قاباغێندا اۆزۆب گئدیردی . \n",
            "2023-01-19 13:59:54,370 - INFO - joeynmt.training - \tReference:   کشتی‌ زیر نظر ما روان بود . این‌ پاداش کسی بود که مورد انکار واقع شده بود . \n",
            "2023-01-19 13:59:54,370 - INFO - joeynmt.training - \tHypothesis: و کسانی که کافر شده بودند ، آنچه پیش از آن یافته بود پاداش یافته بود ، به طور کامل می رفتند .\n",
            "2023-01-19 13:59:54,370 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 13:59:54,372 - INFO - joeynmt.training - \tSource:     گؤیلری وه یئری حاقق اولاراق یارادان اودور . اونون : اول ! دئیهجهگی گۆن درحال اولار . اونون سؤزو حاقدێر . سورون چالیناجاغی گۆن هؤکم اونوندور . غیبی وه آشکارێ بیلن ده اودور . او ، هکمت صاحبدر ، خبرداردێر ! \n",
            "2023-01-19 13:59:54,372 - INFO - joeynmt.training - \tReference:  و او کسی است که آسمانها و زمین را به حق آفرید ، و هر گاه که می‌گوید : باش ، بی‌درنگ موجود شود ؛ سخنش راست است ؛ و روزی که در صور دمیده شود ، فرمانروایی از آن اوست ؛ داننده غیب و شهود است ؛ و اوست حکیم آگاه . \n",
            "2023-01-19 13:59:54,373 - INFO - joeynmt.training - \tHypothesis: اوست کسی که آسمانها و زمین را آفرید . پس به حق آفرید . بگو : امروز پیش از آن اوست ، و کسی که در صور دمیده شود ، و در حقیقت ، فرمانروایی پاک است . آگاه باش که او دانای حکیم است .\n",
            "2023-01-19 13:59:54,373 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 13:59:54,375 - INFO - joeynmt.training - \tSource:     آیهلریمیزه ایمان گتیرنلر یانینا گلدیکده اونلارا دئ : سیزه سالام اولسون ! ربینیز اؤزو اؤزونه رحملی اولماغێ یازمیشدیر کی ، سیزلردن هر کس پیس ایش گؤرسه ، سونرا تؤؤبه ائدیب دۆزلسه . شبههسیز کی ، آللاه باغیشلایاندیر ، رحم ائدندیر ! \n",
            "2023-01-19 13:59:54,375 - INFO - joeynmt.training - \tReference:  و چون کسانی که به آیات ما ایمان دارند ، نزد تو آیند ، بگو : درود بر شما ، پروردگارتان رحمت را بر خود مقرر کرده که هر کس از شما به نادانی کار بدی کند و آنگاه به توبه و صلاح آید ، پس وی آمرزنده مهربان است . \n",
            "2023-01-19 13:59:54,375 - INFO - joeynmt.training - \tHypothesis: و چون آیات ما به سوی کسانی که ایمان آورده اند ، گفتند : به سلام بر شما گناهی برده ، برای شما مقرر شده است . هر که پیش از خود بر شما گناهی مرتکب شده است ، سپس به زودی خدا آمرزنده مهربان است .\n",
            "2023-01-19 14:00:02,292 - INFO - joeynmt.training - Epoch 311, Step:   108100, Batch Loss:     1.528666, Batch Acc: 0.589784, Tokens per Sec:    14689, Lr: 0.000027\n",
            "2023-01-19 14:00:10,077 - INFO - joeynmt.training - Epoch 311, Step:   108200, Batch Loss:     1.610440, Batch Acc: 0.584902, Tokens per Sec:    15397, Lr: 0.000027\n",
            "2023-01-19 14:00:14,448 - INFO - joeynmt.training - Epoch 311: total training loss 547.96\n",
            "2023-01-19 14:00:14,448 - INFO - joeynmt.training - EPOCH 312\n",
            "2023-01-19 14:00:17,885 - INFO - joeynmt.training - Epoch 312, Step:   108300, Batch Loss:     1.615777, Batch Acc: 0.590286, Tokens per Sec:    15109, Lr: 0.000027\n",
            "2023-01-19 14:00:25,663 - INFO - joeynmt.training - Epoch 312, Step:   108400, Batch Loss:     1.543811, Batch Acc: 0.585005, Tokens per Sec:    15358, Lr: 0.000027\n",
            "2023-01-19 14:00:33,626 - INFO - joeynmt.training - Epoch 312, Step:   108500, Batch Loss:     1.564176, Batch Acc: 0.586505, Tokens per Sec:    15052, Lr: 0.000027\n",
            "2023-01-19 14:00:41,336 - INFO - joeynmt.training - Epoch 312, Step:   108600, Batch Loss:     1.638832, Batch Acc: 0.584897, Tokens per Sec:    15614, Lr: 0.000027\n",
            "2023-01-19 14:00:41,919 - INFO - joeynmt.training - Epoch 312: total training loss 553.53\n",
            "2023-01-19 14:00:41,919 - INFO - joeynmt.training - EPOCH 313\n",
            "2023-01-19 14:00:49,296 - INFO - joeynmt.training - Epoch 313, Step:   108700, Batch Loss:     1.455282, Batch Acc: 0.587298, Tokens per Sec:    15264, Lr: 0.000027\n",
            "2023-01-19 14:00:57,427 - INFO - joeynmt.training - Epoch 313, Step:   108800, Batch Loss:     1.576336, Batch Acc: 0.584417, Tokens per Sec:    14777, Lr: 0.000027\n",
            "2023-01-19 14:01:05,256 - INFO - joeynmt.training - Epoch 313, Step:   108900, Batch Loss:     1.434424, Batch Acc: 0.589555, Tokens per Sec:    15225, Lr: 0.000027\n",
            "2023-01-19 14:01:09,707 - INFO - joeynmt.training - Epoch 313: total training loss 548.12\n",
            "2023-01-19 14:01:09,707 - INFO - joeynmt.training - EPOCH 314\n",
            "2023-01-19 14:01:13,273 - INFO - joeynmt.training - Epoch 314, Step:   109000, Batch Loss:     1.644232, Batch Acc: 0.589337, Tokens per Sec:    15185, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.45ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8873.45ex/s]\n",
            "2023-01-19 14:01:13,567 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=109000\n",
            "2023-01-19 14:01:13,567 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:01:18,675 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:01:18,676 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:01:18,676 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:01:18,677 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:01:18,680 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.75, loss:   2.73, ppl:  15.27, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0635[sec], evaluation: 0.0416[sec]\n",
            "2023-01-19 14:01:18,682 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:01:18,686 - INFO - joeynmt.training - \tSource:     ایسا اونا دئدی : منه توخونما ، چۆنکی من هله آتانێن یانینا قالخمامێشام . آمما قارداشلارێمێن یانینا گئدیب اونلارا سؤیله : اؤز آتامێن سیزین آتانێزێن ، اؤز آللاهێمێن سیزین آللاهینیزین یانینا قالخێرام . \n",
            "2023-01-19 14:01:18,686 - INFO - joeynmt.training - \tReference:  عیسی به او گفت : من هنوز نزد پدر بالا نرفته‌ام ، پس دیگر بر من میاویز ، بلکه نزد برادرانم برو و به آنان بگو : من نزد پدر خود و پدر شما و خدای خود و خدای شما بالا می‌روم . \n",
            "2023-01-19 14:01:18,686 - INFO - joeynmt.training - \tHypothesis: عیسی به او گفت : من دستگیر هستم ؛ زیرا هنوز نزد پدر هستم . اما من نزد پدر می روم ، اما پدرتان نزد پدرم بروم و به آنان گفت : من نزد پدرتان بروم .\n",
            "2023-01-19 14:01:18,686 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:01:18,688 - INFO - joeynmt.training - \tSource:     قارداشلێق سئوگیسی ایله بیر بیرینیزی سئوین . هؤرمت گؤسترمکده بیر بیرینیزی قاباقلایین . \n",
            "2023-01-19 14:01:18,688 - INFO - joeynmt.training - \tReference:  با محبت برادرانه یکدیگر را از صمیم دل دوست داشته باشید . در حرمت گذاشتن به یکدیگر پیشقدم شوید . \n",
            "2023-01-19 14:01:18,689 - INFO - joeynmt.training - \tHypothesis: همچون برادر خود را دوست داشته باشید . با یکدیگر یکدیگر یکدیگر محبت کنید ، با یکدیگر یکدیگر یکدیگر به یکدیگر محبت کنید .\n",
            "2023-01-19 14:01:18,689 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:01:18,691 - INFO - joeynmt.training - \tSource:     دئدیم : ائله بیل یاتمیشدین آییلدین هه ! \n",
            "2023-01-19 14:01:18,691 - INFO - joeynmt.training - \tReference:  گفتم‌ : خوابت‌ پرید ! \n",
            "2023-01-19 14:01:18,691 - INFO - joeynmt.training - \tHypothesis: گفت : ای آینه خوابیه بوده ام .\n",
            "2023-01-19 14:01:18,691 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:01:18,693 - INFO - joeynmt.training - \tSource:     بیر واخت قانون منده یوخ ایکن من ساغ ایدیم ، آمما او امر منه گلنده گۆناه دیریلدی ، \n",
            "2023-01-19 14:01:18,693 - INFO - joeynmt.training - \tReference:  در واقع ، من نیز پیش از شریعت زنده بودم . اما هنگامی که آن حکم آمد ، گناه زنده شد و من مردم . \n",
            "2023-01-19 14:01:18,693 - INFO - joeynmt.training - \tHypothesis: در یک زمان ، شریعت نیز در من نبود ، اما من گناهی را که به من سپرده بود ، گناه کرده است .\n",
            "2023-01-19 14:01:26,697 - INFO - joeynmt.training - Epoch 314, Step:   109100, Batch Loss:     1.612566, Batch Acc: 0.588093, Tokens per Sec:    14624, Lr: 0.000027\n",
            "2023-01-19 14:01:34,621 - INFO - joeynmt.training - Epoch 314, Step:   109200, Batch Loss:     1.528880, Batch Acc: 0.587277, Tokens per Sec:    15074, Lr: 0.000027\n",
            "2023-01-19 14:01:42,500 - INFO - joeynmt.training - Epoch 314, Step:   109300, Batch Loss:     1.403638, Batch Acc: 0.587237, Tokens per Sec:    15368, Lr: 0.000027\n",
            "2023-01-19 14:01:42,775 - INFO - joeynmt.training - Epoch 314: total training loss 546.61\n",
            "2023-01-19 14:01:42,775 - INFO - joeynmt.training - EPOCH 315\n",
            "2023-01-19 14:01:50,325 - INFO - joeynmt.training - Epoch 315, Step:   109400, Batch Loss:     1.667777, Batch Acc: 0.588778, Tokens per Sec:    15430, Lr: 0.000027\n",
            "2023-01-19 14:01:58,140 - INFO - joeynmt.training - Epoch 315, Step:   109500, Batch Loss:     1.607369, Batch Acc: 0.585225, Tokens per Sec:    15605, Lr: 0.000027\n",
            "2023-01-19 14:02:05,937 - INFO - joeynmt.training - Epoch 315, Step:   109600, Batch Loss:     1.597130, Batch Acc: 0.589498, Tokens per Sec:    15253, Lr: 0.000027\n",
            "2023-01-19 14:02:10,098 - INFO - joeynmt.training - Epoch 315: total training loss 548.73\n",
            "2023-01-19 14:02:10,099 - INFO - joeynmt.training - EPOCH 316\n",
            "2023-01-19 14:02:13,944 - INFO - joeynmt.training - Epoch 316, Step:   109700, Batch Loss:     1.522210, Batch Acc: 0.590795, Tokens per Sec:    14915, Lr: 0.000027\n",
            "2023-01-19 14:02:21,883 - INFO - joeynmt.training - Epoch 316, Step:   109800, Batch Loss:     1.680068, Batch Acc: 0.588160, Tokens per Sec:    15252, Lr: 0.000027\n",
            "2023-01-19 14:02:30,989 - INFO - joeynmt.training - Epoch 316, Step:   109900, Batch Loss:     1.510555, Batch Acc: 0.588057, Tokens per Sec:    13285, Lr: 0.000027\n",
            "2023-01-19 14:02:38,924 - INFO - joeynmt.training - Epoch 316, Step:   110000, Batch Loss:     1.654200, Batch Acc: 0.587492, Tokens per Sec:    15076, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 126.89ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9109.74ex/s]\n",
            "2023-01-19 14:02:39,238 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=110000\n",
            "2023-01-19 14:02:39,238 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:02:44,342 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:02:44,342 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:02:44,342 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:02:44,344 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:02:44,349 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.49, loss:   2.72, ppl:  15.12, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0496[sec], evaluation: 0.0521[sec]\n",
            "2023-01-19 14:02:44,352 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:02:44,355 - INFO - joeynmt.training - \tSource:     چۆنکی اگر کیمسه یانینیزا گلیب بیزیم وز ائتدیگیمیز ایسهآدان باشقا بیر ایسنانی تبلیغ ائدیرسه ، آلدیغینیزدان فرقلی بیر روح آلیرسینیزسا وه یاخود قبول ائتدیگینیزدن فرقلی بیر مۆژدنی قبول ائدیرسینیزسه ، سیز بونا نئجه ده ممنونایتله دؤزورسونوز . \n",
            "2023-01-19 14:02:44,355 - INFO - joeynmt.training - \tReference:  زیرا اگر کسی نزد شما بیاید و در مورد عیسایی غیر از آن که ما موعظه کردیم موعظه کند ، یا اگر روحی غیر از آنچه دریافت کردید بیاورد ، یا اگر بشارتی غیر از آنچه پذیرفتید اعلام کند ، شما به‌راحتی او را تحمل می‌کنید ؛ \n",
            "2023-01-19 14:02:44,356 - INFO - joeynmt.training - \tHypothesis: زیرا اگر کسی نزد شما آمده است ، پس از دیگران را ترغیب می کنیم و اگر دیگری را که دیگران را به شما داده ایم ، طبق یا دیگری را پذیرفتید ، یا دیگران را به دیگران خواهید پذیرفت . پس چگونه همان گونه که شما را به دیدار آن استوار می کنید .\n",
            "2023-01-19 14:02:44,356 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:02:44,358 - INFO - joeynmt.training - \tSource:     اونلارێ نلر ساردێ ، نلر ! \n",
            "2023-01-19 14:02:44,358 - INFO - joeynmt.training - \tReference:  پوشاند بر آن دو شهر ، از باران گوگردی‌ آنچه را پوشاند . \n",
            "2023-01-19 14:02:44,358 - INFO - joeynmt.training - \tHypothesis: و آنها را ندا درمی آیند .\n",
            "2023-01-19 14:02:44,358 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:02:44,360 - INFO - joeynmt.training - \tSource:     هانسێ بیرینیز قایغی چکمکله اؤمرونو بیر آنلێق بئله ، اوزادا بیلر ؟ \n",
            "2023-01-19 14:02:44,360 - INFO - joeynmt.training - \tReference:  کدام یک از شما می‌تواند با نگرانی حتی لحظه‌ای به عمر خود بیفزاید ؟ \n",
            "2023-01-19 14:02:44,360 - INFO - joeynmt.training - \tHypothesis: کدام یک از شما را با یکدیگر به همدیگر می تواند در ریاه دهید ؟\n",
            "2023-01-19 14:02:44,361 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:02:44,362 - INFO - joeynmt.training - \tSource:     او یئرین آداملارێ ایسنانی تانیدیقلاری زامان اترافداکێ هر یئره خبر یایدیلار . بۆتۆن خستهلری اونون یانینا گتیردیلر . \n",
            "2023-01-19 14:02:44,363 - INFO - joeynmt.training - \tReference:  اهالی آنجا وقتی عیسی را شناختند ، عده‌ای را به تمام نواحی آن اطراف فرستادند و مردم نیز همهٔ بیماران را نزد او آوردند\n",
            "2023-01-19 14:02:44,363 - INFO - joeynmt.training - \tHypothesis: وقتی مردم را می شناختند ، او را می شناسند . آنان خبر همهٔ مردم را به مکان اهالی شهر نزد او آوردند .\n",
            "2023-01-19 14:02:44,472 - INFO - joeynmt.training - Epoch 316: total training loss 548.23\n",
            "2023-01-19 14:02:44,473 - INFO - joeynmt.training - EPOCH 317\n",
            "2023-01-19 14:02:52,292 - INFO - joeynmt.training - Epoch 317, Step:   110100, Batch Loss:     1.533298, Batch Acc: 0.589196, Tokens per Sec:    15259, Lr: 0.000027\n",
            "2023-01-19 14:03:00,288 - INFO - joeynmt.training - Epoch 317, Step:   110200, Batch Loss:     1.586274, Batch Acc: 0.588507, Tokens per Sec:    15021, Lr: 0.000027\n",
            "2023-01-19 14:03:08,334 - INFO - joeynmt.training - Epoch 317, Step:   110300, Batch Loss:     1.560065, Batch Acc: 0.585566, Tokens per Sec:    15065, Lr: 0.000027\n",
            "2023-01-19 14:03:12,236 - INFO - joeynmt.training - Epoch 317: total training loss 546.35\n",
            "2023-01-19 14:03:12,236 - INFO - joeynmt.training - EPOCH 318\n",
            "2023-01-19 14:03:16,304 - INFO - joeynmt.training - Epoch 318, Step:   110400, Batch Loss:     1.499973, Batch Acc: 0.592520, Tokens per Sec:    15271, Lr: 0.000027\n",
            "2023-01-19 14:03:24,342 - INFO - joeynmt.training - Epoch 318, Step:   110500, Batch Loss:     1.602819, Batch Acc: 0.587391, Tokens per Sec:    14990, Lr: 0.000027\n",
            "2023-01-19 14:03:32,406 - INFO - joeynmt.training - Epoch 318, Step:   110600, Batch Loss:     1.507335, Batch Acc: 0.587909, Tokens per Sec:    14909, Lr: 0.000027\n",
            "2023-01-19 14:03:40,279 - INFO - joeynmt.training - Epoch 318: total training loss 546.44\n",
            "2023-01-19 14:03:40,279 - INFO - joeynmt.training - EPOCH 319\n",
            "2023-01-19 14:03:40,537 - INFO - joeynmt.training - Epoch 319, Step:   110700, Batch Loss:     1.586942, Batch Acc: 0.585477, Tokens per Sec:    14488, Lr: 0.000027\n",
            "2023-01-19 14:03:48,653 - INFO - joeynmt.training - Epoch 319, Step:   110800, Batch Loss:     1.624033, Batch Acc: 0.590260, Tokens per Sec:    14784, Lr: 0.000027\n",
            "2023-01-19 14:03:56,734 - INFO - joeynmt.training - Epoch 319, Step:   110900, Batch Loss:     1.627162, Batch Acc: 0.587429, Tokens per Sec:    14931, Lr: 0.000027\n",
            "2023-01-19 14:04:04,733 - INFO - joeynmt.training - Epoch 319, Step:   111000, Batch Loss:     1.534463, Batch Acc: 0.588947, Tokens per Sec:    15028, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.62ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9893.79ex/s]\n",
            "2023-01-19 14:04:05,028 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=111000\n",
            "2023-01-19 14:04:05,028 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:04:10,395 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:04:10,396 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:04:10,396 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:04:10,397 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:04:10,400 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.64, loss:   2.75, ppl:  15.65, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.3141[sec], evaluation: 0.0499[sec]\n",
            "2023-01-19 14:04:10,403 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:04:10,407 - INFO - joeynmt.training - \tSource:     آمما قادێن دوغوشلا خلاص اولاجاق ، بیر شرطله کی آغێل کاماللا اماندا ، محبتده وه مۆقدسلیکده یاشاماغا داوام ائتسین . \n",
            "2023-01-19 14:04:10,407 - INFO - joeynmt.training - \tReference:  با این حال ، زنان با به دنیا آوردن فرزند در امان می‌مانند ، اگر همچنان باایمان ، بامحبت ، مقدس و فهمیده باشند . \n",
            "2023-01-19 14:04:10,407 - INFO - joeynmt.training - \tHypothesis: اما وقتی زنی که در اتحاد با او خواهد بود ، نجات خواهد یافت که در محبت ، با خردمندان و محبت مقدس و در اتحاد با مقدسان زندگی کند .\n",
            "2023-01-19 14:04:10,408 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:04:10,410 - INFO - joeynmt.training - \tSource:     زاودایین اوغوللارێ یاقوب وه یهیا اونا یاخینلاشیب دئدیلر : مۆعلم ، ایستهییریک کی ، دیلهدیگیمیزی بیزیم اۆچۆن ائدهسن . \n",
            "2023-01-19 14:04:10,410 - INFO - joeynmt.training - \tReference:  یعقوب و یوحنا ، پسران زبدی نزد عیسی آمدند و گفتند : ای استاد ، خواهش ما این است که آنچه از تو درخواست می‌کنیم ، برای ما انجام دهی . \n",
            "2023-01-19 14:04:10,410 - INFO - joeynmt.training - \tHypothesis: هنگامی که زکریا به او گفتند : استاد ، یحیی به او گفتند : استاد ، می خواهیم که ما بخواهیم ، از ما خواهیم خواست .\n",
            "2023-01-19 14:04:10,410 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:04:10,413 - INFO - joeynmt.training - \tSource:     تایفاسینین باشچیلاری اونا ! بیز سنی آچێق آیدین آزمێش گؤروروک دئیه جاواب وئردیلر . \n",
            "2023-01-19 14:04:10,413 - INFO - joeynmt.training - \tReference:  سران قومش گفتند : واقعا ما تو را در گمراهی آشکاری می‌بینیم . \n",
            "2023-01-19 14:04:10,413 - INFO - joeynmt.training - \tHypothesis: و گفتند : قومش تو را ببینند ، و ما تو را گمراهی آشکاری کردیم .\n",
            "2023-01-19 14:04:10,413 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:04:10,415 - INFO - joeynmt.training - \tSource:      بیری فاریسئی ، دیگری ایسه وئرگیییغان اولان ایکی نفر دوعا ائتمک اۆچۆن مبدع گیردی . \n",
            "2023-01-19 14:04:10,416 - INFO - joeynmt.training - \tReference:   دو مرد برای دعا کردن به معبد رفتند ؛ یکی فریسی و دیگری خراجگیر . \n",
            "2023-01-19 14:04:10,416 - INFO - joeynmt.training - \tHypothesis: یکی از آن روز ، دیگری به دعا کردن به دعا کردن به دعا کردن برای معبد رفتن به معبد رفت .\n",
            "2023-01-19 14:04:14,187 - INFO - joeynmt.training - Epoch 319: total training loss 546.75\n",
            "2023-01-19 14:04:14,187 - INFO - joeynmt.training - EPOCH 320\n",
            "2023-01-19 14:04:18,490 - INFO - joeynmt.training - Epoch 320, Step:   111100, Batch Loss:     1.506380, Batch Acc: 0.588657, Tokens per Sec:    15159, Lr: 0.000027\n",
            "2023-01-19 14:04:26,443 - INFO - joeynmt.training - Epoch 320, Step:   111200, Batch Loss:     1.532275, Batch Acc: 0.589727, Tokens per Sec:    15135, Lr: 0.000027\n",
            "2023-01-19 14:04:34,263 - INFO - joeynmt.training - Epoch 320, Step:   111300, Batch Loss:     1.526433, Batch Acc: 0.590580, Tokens per Sec:    15359, Lr: 0.000027\n",
            "2023-01-19 14:04:41,721 - INFO - joeynmt.training - Epoch 320: total training loss 546.42\n",
            "2023-01-19 14:04:41,722 - INFO - joeynmt.training - EPOCH 321\n",
            "2023-01-19 14:04:42,121 - INFO - joeynmt.training - Epoch 321, Step:   111400, Batch Loss:     1.536087, Batch Acc: 0.587587, Tokens per Sec:    15484, Lr: 0.000027\n",
            "2023-01-19 14:04:49,967 - INFO - joeynmt.training - Epoch 321, Step:   111500, Batch Loss:     1.610738, Batch Acc: 0.588650, Tokens per Sec:    15271, Lr: 0.000027\n",
            "2023-01-19 14:04:57,774 - INFO - joeynmt.training - Epoch 321, Step:   111600, Batch Loss:     1.526671, Batch Acc: 0.589622, Tokens per Sec:    15508, Lr: 0.000027\n",
            "2023-01-19 14:05:05,704 - INFO - joeynmt.training - Epoch 321, Step:   111700, Batch Loss:     1.618880, Batch Acc: 0.587135, Tokens per Sec:    15417, Lr: 0.000027\n",
            "2023-01-19 14:05:08,989 - INFO - joeynmt.training - Epoch 321: total training loss 541.76\n",
            "2023-01-19 14:05:08,990 - INFO - joeynmt.training - EPOCH 322\n",
            "2023-01-19 14:05:13,626 - INFO - joeynmt.training - Epoch 322, Step:   111800, Batch Loss:     1.548919, Batch Acc: 0.594197, Tokens per Sec:    15464, Lr: 0.000027\n",
            "2023-01-19 14:05:21,510 - INFO - joeynmt.training - Epoch 322, Step:   111900, Batch Loss:     1.613330, Batch Acc: 0.587901, Tokens per Sec:    15469, Lr: 0.000027\n",
            "2023-01-19 14:05:29,502 - INFO - joeynmt.training - Epoch 322, Step:   112000, Batch Loss:     1.511173, Batch Acc: 0.591059, Tokens per Sec:    15130, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.54ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9530.37ex/s]\n",
            "2023-01-19 14:05:29,810 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=112000\n",
            "2023-01-19 14:05:29,810 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:05:35,768 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:05:35,768 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:05:35,768 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:05:35,769 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:05:35,772 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.41, loss:   2.78, ppl:  16.13, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.8994[sec], evaluation: 0.0544[sec]\n",
            "2023-01-19 14:05:35,775 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:05:35,778 - INFO - joeynmt.training - \tSource:     اوندا ایسهآدان سوروشدولار : آتان هارادادێر ؟ ایسا جاواب وئردی : سیز نه منی ، نه ده آتامێ تانییرسینیز . اگر منی تانیسایدینیز ، آتامێ دا تانییاردینیز . \n",
            "2023-01-19 14:05:35,779 - INFO - joeynmt.training - \tReference:  آنگاه از او پرسیدند : پدرت کجاست ؟ عیسی پاسخ داد : شما نه مرا می‌شناسید و نه پدر مرا . اگر مرا می‌شناختید ، پدرم را نیز می‌شناختید . \n",
            "2023-01-19 14:05:35,779 - INFO - joeynmt.training - \tHypothesis: پس از او پرسیدند : پدر ، کجاست ؟ عیسی پاسخ داد : شما مرا می شناسید . اگر پدر مرا می شناختید ، پدر مرا می شناسید .\n",
            "2023-01-19 14:05:35,779 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:05:35,782 - INFO - joeynmt.training - \tSource:     اود ایشینه مسول اولان باشقا بیر ملک ده قوربانگاهدان چێخێب ایتی اوراغێ اولان ملهیه اوجا سسله ندا ائتدی : ایتتی اوراغێنێ ایشه سال وه یئر اۆزوندکی اۆزۆم تنهییندن سالخیملاری کس یێغ ، چۆنکی اۆزۆملری یئتیشیب . \n",
            "2023-01-19 14:05:35,782 - INFO - joeynmt.training - \tReference:  فرشته‌ای دیگر نیز از مذبح بیرون آمد که بر آتش اقتدار داشت . او با صدای بلند به آن فرشته‌ای که داسی تیز در دست داشت ، گفت : داس تیز خود را بردار و خوشه‌های انگور را از تاک زمین جمع کن ؛ زیرا انگورهایش رسیده است . \n",
            "2023-01-19 14:05:35,783 - INFO - joeynmt.training - \tHypothesis: فرشتهٔ دیگری که در مذبح قربانی مقدس بود ، آنگاه فرشتهٔ دیگر بیرون آمد و به صدای کسی که از آن فرشته ای دیگر بود ، گفت : آن وحش را از خاموش کردن در مصیبت پرده و گاوها و انبار خود را در انجیر کرده اند ، چون دامه های زمین ، محصولات فاسقست .\n",
            "2023-01-19 14:05:35,783 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:05:35,785 - INFO - joeynmt.training - \tSource:     دوغروسو ، بیز بونونلا وه آتالارێنا اؤمورلری اوزانینجایا قدر گۆن گۆزران وئردیک مگر بیزیم تورپاغێنا گیریب اونو هر ترفدن اسکیلتدیگیمیزی گؤرمورلرمی ؟ بئله اولدوقدا ، غالب اولانلار اونلارمێدێر \n",
            "2023-01-19 14:05:35,785 - INFO - joeynmt.training - \tReference:   نه‌ بلکه اینها و پدرانشان را برخوردار کردیم تا عمرشان به درازا کشید . آیا نمی‌بینند که ما می‌آییم و زمین را از جوانب آن فرو می‌کاهیم ؟ آیا باز هم آنان پیروزند ؟ \n",
            "2023-01-19 14:05:35,785 - INFO - joeynmt.training - \tHypothesis: و این گونه ، ما را از پدران و پدرانشان به جای او پلیده دادیم تا در آن روز ، او را از هر جا که در خاکسترده ، به سرایندازی آن را می بینند ؟ آیا بر ایشان غالب آمده اند ؟\n",
            "2023-01-19 14:05:35,785 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:05:35,787 - INFO - joeynmt.training - \tSource:     بدنین ده سویوق دی . \n",
            "2023-01-19 14:05:35,787 - INFO - joeynmt.training - \tReference:  بدنت هم سرد است . \n",
            "2023-01-19 14:05:35,788 - INFO - joeynmt.training - \tHypothesis: بدنش است .\n",
            "2023-01-19 14:05:42,778 - INFO - joeynmt.training - Epoch 322: total training loss 542.94\n",
            "2023-01-19 14:05:42,778 - INFO - joeynmt.training - EPOCH 323\n",
            "2023-01-19 14:05:43,773 - INFO - joeynmt.training - Epoch 323, Step:   112100, Batch Loss:     1.498831, Batch Acc: 0.588243, Tokens per Sec:    14613, Lr: 0.000027\n",
            "2023-01-19 14:05:51,713 - INFO - joeynmt.training - Epoch 323, Step:   112200, Batch Loss:     1.519389, Batch Acc: 0.588798, Tokens per Sec:    15210, Lr: 0.000027\n",
            "2023-01-19 14:06:00,906 - INFO - joeynmt.training - Epoch 323, Step:   112300, Batch Loss:     1.549988, Batch Acc: 0.589800, Tokens per Sec:    12964, Lr: 0.000027\n",
            "2023-01-19 14:06:08,998 - INFO - joeynmt.training - Epoch 323, Step:   112400, Batch Loss:     1.536955, Batch Acc: 0.587780, Tokens per Sec:    14958, Lr: 0.000027\n",
            "2023-01-19 14:06:11,983 - INFO - joeynmt.training - Epoch 323: total training loss 543.84\n",
            "2023-01-19 14:06:11,988 - INFO - joeynmt.training - EPOCH 324\n",
            "2023-01-19 14:06:17,145 - INFO - joeynmt.training - Epoch 324, Step:   112500, Batch Loss:     1.497063, Batch Acc: 0.595041, Tokens per Sec:    15065, Lr: 0.000027\n",
            "2023-01-19 14:06:25,276 - INFO - joeynmt.training - Epoch 324, Step:   112600, Batch Loss:     1.466254, Batch Acc: 0.590231, Tokens per Sec:    14791, Lr: 0.000027\n",
            "2023-01-19 14:06:33,473 - INFO - joeynmt.training - Epoch 324, Step:   112700, Batch Loss:     1.520616, Batch Acc: 0.588906, Tokens per Sec:    14705, Lr: 0.000027\n",
            "2023-01-19 14:06:40,307 - INFO - joeynmt.training - Epoch 324: total training loss 542.64\n",
            "2023-01-19 14:06:40,307 - INFO - joeynmt.training - EPOCH 325\n",
            "2023-01-19 14:06:41,602 - INFO - joeynmt.training - Epoch 325, Step:   112800, Batch Loss:     1.629688, Batch Acc: 0.589183, Tokens per Sec:    15348, Lr: 0.000027\n",
            "2023-01-19 14:06:49,739 - INFO - joeynmt.training - Epoch 325, Step:   112900, Batch Loss:     1.468832, Batch Acc: 0.590235, Tokens per Sec:    14831, Lr: 0.000027\n",
            "2023-01-19 14:06:57,968 - INFO - joeynmt.training - Epoch 325, Step:   113000, Batch Loss:     1.544693, Batch Acc: 0.590788, Tokens per Sec:    14748, Lr: 0.000027\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.70ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9664.43ex/s]\n",
            "2023-01-19 14:06:58,250 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=113000\n",
            "2023-01-19 14:06:58,250 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:07:02,666 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:07:02,666 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:07:02,666 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:07:02,667 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:07:02,670 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.49, loss:   2.76, ppl:  15.80, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3663[sec], evaluation: 0.0415[sec]\n",
            "2023-01-19 14:07:02,673 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:07:02,676 - INFO - joeynmt.training - \tSource:     دامدا اولان ائویندن نه ایسه گؤتورمک اۆچۆن آشاغێ ائنیب ائوینه گیرمهسین . \n",
            "2023-01-19 14:07:02,677 - INFO - joeynmt.training - \tReference:  کسی که بر بام است ، پایین نیاید و به خانه‌اش داخل نشود تا چیزی از آنجا بردارد . \n",
            "2023-01-19 14:07:02,677 - INFO - joeynmt.training - \tHypothesis: اما وقتی به خانهٔ من ، نقره برداشت برداشت تا خانهٔ من بیاید ، داخل شد .\n",
            "2023-01-19 14:07:02,677 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:07:02,679 - INFO - joeynmt.training - \tSource:     دا هئچ زاد . \n",
            "2023-01-19 14:07:02,679 - INFO - joeynmt.training - \tReference:  دیگه هیچی . \n",
            "2023-01-19 14:07:02,679 - INFO - joeynmt.training - \tHypothesis: دیگر هیچ زیان نمی شود .\n",
            "2023-01-19 14:07:02,679 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:07:02,681 - INFO - joeynmt.training - \tSource:     نئچه نئچه مملکت اؤز ربینین وه اونون پیغمبرینین امریندن چێخدێ . بیز ده اونلارا شدتلی جزا وئریب گؤرونمهمیش مۆدهش ازابا دۆچار ائتدیک . \n",
            "2023-01-19 14:07:02,682 - INFO - joeynmt.training - \tReference:  و چه بسیار شهرها که از فرمان پروردگار خود و پیامبرانش سر پیچیدند و از آنها حسابی سخت کشیدیم و آنان را به عذابی بس‌ زشت عذاب کردیم . \n",
            "2023-01-19 14:07:02,682 - INFO - joeynmt.training - \tHypothesis: و چه بسیار شهرهای پروردگارش را از پیش خود بیرون آوردیم و به فرمان او مجادله دادیم ، و ما عذابی دردناک را به آنان دادیم .\n",
            "2023-01-19 14:07:02,682 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:07:02,684 - INFO - joeynmt.training - \tSource:     ایسا اللهرینی قادێنێن اۆستۆنه قویاندا او درحال دیکهلیب آللاهێ ایزتلندیرمهیه باشلادێ . \n",
            "2023-01-19 14:07:02,684 - INFO - joeynmt.training - \tReference:  سپس دست‌هایش را بر او گذاشت و همان دم ، زن راست ایستاد و شروع به تمجید خدا کرد . \n",
            "2023-01-19 14:07:02,684 - INFO - joeynmt.training - \tHypothesis: عیسی دست هایش را در بیابان گذاشت و در آنجا دمشوار شد .\n",
            "2023-01-19 14:07:10,795 - INFO - joeynmt.training - Epoch 325, Step:   113100, Batch Loss:     1.716171, Batch Acc: 0.588703, Tokens per Sec:    14438, Lr: 0.000027\n",
            "2023-01-19 14:07:13,359 - INFO - joeynmt.training - Epoch 325: total training loss 543.88\n",
            "2023-01-19 14:07:13,360 - INFO - joeynmt.training - EPOCH 326\n",
            "2023-01-19 14:07:18,887 - INFO - joeynmt.training - Epoch 326, Step:   113200, Batch Loss:     1.528654, Batch Acc: 0.592649, Tokens per Sec:    14774, Lr: 0.000027\n",
            "2023-01-19 14:07:26,984 - INFO - joeynmt.training - Epoch 326, Step:   113300, Batch Loss:     1.534037, Batch Acc: 0.591851, Tokens per Sec:    14906, Lr: 0.000027\n",
            "2023-01-19 14:07:35,213 - INFO - joeynmt.training - Epoch 326, Step:   113400, Batch Loss:     1.545087, Batch Acc: 0.585057, Tokens per Sec:    14704, Lr: 0.000027\n",
            "2023-01-19 14:07:41,674 - INFO - joeynmt.training - Epoch 326: total training loss 544.51\n",
            "2023-01-19 14:07:41,674 - INFO - joeynmt.training - EPOCH 327\n",
            "2023-01-19 14:07:43,164 - INFO - joeynmt.training - Epoch 327, Step:   113500, Batch Loss:     1.485292, Batch Acc: 0.588988, Tokens per Sec:    15586, Lr: 0.000027\n",
            "2023-01-19 14:07:51,047 - INFO - joeynmt.training - Epoch 327, Step:   113600, Batch Loss:     1.429894, Batch Acc: 0.591996, Tokens per Sec:    15200, Lr: 0.000027\n",
            "2023-01-19 14:07:59,015 - INFO - joeynmt.training - Epoch 327, Step:   113700, Batch Loss:     1.517231, Batch Acc: 0.587898, Tokens per Sec:    15082, Lr: 0.000027\n",
            "2023-01-19 14:08:06,814 - INFO - joeynmt.training - Epoch 327, Step:   113800, Batch Loss:     1.456752, Batch Acc: 0.587826, Tokens per Sec:    15471, Lr: 0.000027\n",
            "2023-01-19 14:08:09,244 - INFO - joeynmt.training - Epoch 327: total training loss 543.96\n",
            "2023-01-19 14:08:09,244 - INFO - joeynmt.training - EPOCH 328\n",
            "2023-01-19 14:08:14,717 - INFO - joeynmt.training - Epoch 328, Step:   113900, Batch Loss:     1.524224, Batch Acc: 0.592339, Tokens per Sec:    15175, Lr: 0.000027\n",
            "2023-01-19 14:08:22,499 - INFO - joeynmt.training - Epoch 328, Step:   114000, Batch Loss:     1.524071, Batch Acc: 0.592434, Tokens per Sec:    15410, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.14ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10779.22ex/s]\n",
            "2023-01-19 14:08:22,763 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=114000\n",
            "2023-01-19 14:08:22,763 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:08:27,704 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:08:27,704 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:08:27,704 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:08:27,705 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:08:27,708 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.95, loss:   2.85, ppl:  17.21, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.8950[sec], evaluation: 0.0424[sec]\n",
            "2023-01-19 14:08:27,723 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:08:27,726 - INFO - joeynmt.training - \tSource:      سن اؤز ربینین هؤکمونه صبر ائت ! شبههسیز کی ، سن بیزیم گؤزوموزون قاباغێنداسان . قالخدێقدا ربینی همد سنا ایله زیکر ائت ! \n",
            "2023-01-19 14:08:27,726 - INFO - joeynmt.training - \tReference:  و در برابر دستور پروردگارت شکیبایی پیشه کن که تو خود در حمایت مایی و هنگامی که از خواب‌ بر می‌خیزی به نیایش پروردگارت تسبیح گوی . \n",
            "2023-01-19 14:08:27,727 - INFO - joeynmt.training - \tHypothesis: و تو به شکیباییی کن که در حقیقت پیش پروردگارت بر ما خوانده شو ، و چون تو را پیش از غرقه ای ، یاد کن .\n",
            "2023-01-19 14:08:27,727 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:08:27,729 - INFO - joeynmt.training - \tSource:     اونو جبرایل ائندیردی : \n",
            "2023-01-19 14:08:27,729 - INFO - joeynmt.training - \tReference:   روح الامین آن را بر دلت نازل کرد ، \n",
            "2023-01-19 14:08:27,729 - INFO - joeynmt.training - \tHypothesis: و آن را به صورت جهاد فرو فرستاد :\n",
            "2023-01-19 14:08:27,729 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:08:27,731 - INFO - joeynmt.training - \tSource:      حقیقتا ، من سیزین ان اوجا ربینیزم ! \n",
            "2023-01-19 14:08:27,731 - INFO - joeynmt.training - \tReference:  و گفت : پروردگار بزرگتر شما منم ! \n",
            "2023-01-19 14:08:27,731 - INFO - joeynmt.training - \tHypothesis: من همان پروردگار شما که منین شما را برترم .\n",
            "2023-01-19 14:08:27,732 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:08:27,733 - INFO - joeynmt.training - \tSource:     موسا بئله دئمیشدی : آللاهینیز رب سیزین اۆچۆن اؤز سویداشلارینیز ایچریسیندن منیم کیمی بیر پیغمبر یئتیرهجک . سیزه نه سؤیلهیهجکسه ، اونا قولاق آسێن . \n",
            "2023-01-19 14:08:27,734 - INFO - joeynmt.training - \tReference:  موسی گفته است : یهوه خدایتان پیامبری همانند من از میان برادرانتان برای شما مبعوث خواهد کرد . شما باید به هر چه او به شما می‌گوید ، گوش دهید . \n",
            "2023-01-19 14:08:27,734 - INFO - joeynmt.training - \tHypothesis: موسی گفت : یهوه خدا را برای شما خواهد آورد ، همان طور که به یهوه می گوید ، شما پیامبری را که به شما گفته خواهد کرد . اما به شما گوش دهید .\n",
            "2023-01-19 14:08:35,521 - INFO - joeynmt.training - Epoch 328, Step:   114100, Batch Loss:     1.651171, Batch Acc: 0.588342, Tokens per Sec:    15164, Lr: 0.000026\n",
            "2023-01-19 14:08:41,515 - INFO - joeynmt.training - Epoch 328: total training loss 541.96\n",
            "2023-01-19 14:08:41,516 - INFO - joeynmt.training - EPOCH 329\n",
            "2023-01-19 14:08:43,238 - INFO - joeynmt.training - Epoch 329, Step:   114200, Batch Loss:     1.572735, Batch Acc: 0.591922, Tokens per Sec:    15558, Lr: 0.000026\n",
            "2023-01-19 14:08:50,924 - INFO - joeynmt.training - Epoch 329, Step:   114300, Batch Loss:     1.624731, Batch Acc: 0.589999, Tokens per Sec:    15545, Lr: 0.000026\n",
            "2023-01-19 14:08:58,732 - INFO - joeynmt.training - Epoch 329, Step:   114400, Batch Loss:     1.654657, Batch Acc: 0.591988, Tokens per Sec:    15330, Lr: 0.000026\n",
            "2023-01-19 14:09:06,607 - INFO - joeynmt.training - Epoch 329, Step:   114500, Batch Loss:     1.584993, Batch Acc: 0.590332, Tokens per Sec:    15397, Lr: 0.000026\n",
            "2023-01-19 14:09:08,758 - INFO - joeynmt.training - Epoch 329: total training loss 543.63\n",
            "2023-01-19 14:09:08,759 - INFO - joeynmt.training - EPOCH 330\n",
            "2023-01-19 14:09:14,386 - INFO - joeynmt.training - Epoch 330, Step:   114600, Batch Loss:     1.588502, Batch Acc: 0.591234, Tokens per Sec:    15766, Lr: 0.000026\n",
            "2023-01-19 14:09:23,111 - INFO - joeynmt.training - Epoch 330, Step:   114700, Batch Loss:     1.541749, Batch Acc: 0.592111, Tokens per Sec:    13796, Lr: 0.000026\n",
            "2023-01-19 14:09:30,804 - INFO - joeynmt.training - Epoch 330, Step:   114800, Batch Loss:     1.554789, Batch Acc: 0.589948, Tokens per Sec:    15641, Lr: 0.000026\n",
            "2023-01-19 14:09:36,652 - INFO - joeynmt.training - Epoch 330: total training loss 542.96\n",
            "2023-01-19 14:09:36,653 - INFO - joeynmt.training - EPOCH 331\n",
            "2023-01-19 14:09:38,576 - INFO - joeynmt.training - Epoch 331, Step:   114900, Batch Loss:     1.494256, Batch Acc: 0.592335, Tokens per Sec:    15176, Lr: 0.000026\n",
            "2023-01-19 14:09:46,274 - INFO - joeynmt.training - Epoch 331, Step:   115000, Batch Loss:     1.486651, Batch Acc: 0.588317, Tokens per Sec:    15768, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.00ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10451.06ex/s]\n",
            "2023-01-19 14:09:46,538 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=115000\n",
            "2023-01-19 14:09:46,538 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:09:51,183 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:09:51,183 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:09:51,183 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:09:51,184 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:09:51,188 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.30, loss:   2.70, ppl:  14.90, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5983[sec], evaluation: 0.0438[sec]\n",
            "2023-01-19 14:09:51,190 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:09:51,194 - INFO - joeynmt.training - \tSource:      آمین ! آللاهیمیزا ابدی اولاراق آلقێش ، عزت ، هکمت ، شۆکۆر ، شرف ، قدرت وه غۆۆت اولسون ! آمین ! \n",
            "2023-01-19 14:09:51,194 - INFO - joeynmt.training - \tReference:  گفتند : آمین ! سپاس ، جلال ، حکمت ، شکرگزاری ، حرمت ، قدرت و توانایی همیشه و تا ابد از آن خدایمان باد ! آمین . \n",
            "2023-01-19 14:09:51,194 - INFO - joeynmt.training - \tHypothesis: آمین ، خدا تا ابد جلال و قدرت ما ، سپاس و سپاسگزار ، سپاس و سپاس بر قدرت و سپاسگزار .\n",
            "2023-01-19 14:09:51,194 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:09:51,196 - INFO - joeynmt.training - \tSource:     سور چالیناجاق ، آللاهێن گؤیلرده وه یئرده اولان ایستهدیگی کیمسهلردن باشقا ، درحال هامێ یێخیلیب اؤلهجک . سونرا بیر داها چالێنان کیمی اونلار قالخێب مۆنتزر اولاجاقلار ! \n",
            "2023-01-19 14:09:51,197 - INFO - joeynmt.training - \tReference:  و در صور دمیده می‌شود ، پس هر که در آسمانها و هر که در زمین است بیهوش درمی‌افتد ، مگر کسی که خدا بخواهد ؛ سپس بار دیگر در آن دمیده می‌شود و بناگاه آنان بر پای ایستاده می‌نگرند . \n",
            "2023-01-19 14:09:51,197 - INFO - joeynmt.training - \tHypothesis: و در صور دمیده شود ، و کسانی که در آسمانها و زمین است میرزند ، سپس آنان را در هر جا که می کشتند ، بناگاه آنان به جایگاه بازگشتند .\n",
            "2023-01-19 14:09:51,197 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:09:51,199 - INFO - joeynmt.training - \tSource:     ائلهجه ده تزه شرابێ کؤهنه تولوقلارا دولدورمازلار . یوخسا تولوقلار پارتلایار ، شراب تؤکۆلر ، تولوقلار دا زای اولار . تزه شراب تزه تولوقلارا دولدورولار ؛ اوندا هر ایکیسی قورونار . \n",
            "2023-01-19 14:09:51,199 - INFO - joeynmt.training - \tReference:  همچنین کسی شراب تازه را در مشک کهنه نمی‌ریزد ؛ زیرا مشک می‌ترکد و شراب می‌ریزد و مشک ضایع می‌شود . شراب تازه را در مشک نو می‌ریزند و به این ترتیب ، هم شراب و هم مشک حفظ می‌شود . \n",
            "2023-01-19 14:09:51,199 - INFO - joeynmt.training - \tHypothesis: به همین ترتیب ، شراب تازه مشک است کهنه نمی ریزد . مشک را می ریزد ، شراب مشک نیز می ریزد . شراب مشک ، مشک ، شراب تازه ها و مشک ، شراب تازه در آن تعلف خواهد بود .\n",
            "2023-01-19 14:09:51,199 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:09:51,201 - INFO - joeynmt.training - \tSource:     مۆتقلر ایسه قورخوسوز خطرسیز بیر یئرده \n",
            "2023-01-19 14:09:51,201 - INFO - joeynmt.training - \tReference:  به راستی پرهیزگاران در جایگاهی آسوده اند ، \n",
            "2023-01-19 14:09:51,201 - INFO - joeynmt.training - \tHypothesis: و پرهیزگاران در جایگاهی است .\n",
            "2023-01-19 14:09:58,905 - INFO - joeynmt.training - Epoch 331, Step:   115100, Batch Loss:     1.448853, Batch Acc: 0.591606, Tokens per Sec:    15146, Lr: 0.000026\n",
            "2023-01-19 14:10:06,627 - INFO - joeynmt.training - Epoch 331, Step:   115200, Batch Loss:     1.655605, Batch Acc: 0.591602, Tokens per Sec:    15557, Lr: 0.000026\n",
            "2023-01-19 14:10:08,518 - INFO - joeynmt.training - Epoch 331: total training loss 541.57\n",
            "2023-01-19 14:10:08,518 - INFO - joeynmt.training - EPOCH 332\n",
            "2023-01-19 14:10:14,410 - INFO - joeynmt.training - Epoch 332, Step:   115300, Batch Loss:     1.532257, Batch Acc: 0.594324, Tokens per Sec:    15623, Lr: 0.000026\n",
            "2023-01-19 14:10:22,125 - INFO - joeynmt.training - Epoch 332, Step:   115400, Batch Loss:     1.616006, Batch Acc: 0.591254, Tokens per Sec:    15678, Lr: 0.000026\n",
            "2023-01-19 14:10:29,870 - INFO - joeynmt.training - Epoch 332, Step:   115500, Batch Loss:     1.535092, Batch Acc: 0.588077, Tokens per Sec:    15468, Lr: 0.000026\n",
            "2023-01-19 14:10:35,604 - INFO - joeynmt.training - Epoch 332: total training loss 542.55\n",
            "2023-01-19 14:10:35,605 - INFO - joeynmt.training - EPOCH 333\n",
            "2023-01-19 14:10:37,691 - INFO - joeynmt.training - Epoch 333, Step:   115600, Batch Loss:     1.624095, Batch Acc: 0.593449, Tokens per Sec:    15879, Lr: 0.000026\n",
            "2023-01-19 14:10:45,302 - INFO - joeynmt.training - Epoch 333, Step:   115700, Batch Loss:     1.554201, Batch Acc: 0.594510, Tokens per Sec:    15955, Lr: 0.000026\n",
            "2023-01-19 14:10:52,956 - INFO - joeynmt.training - Epoch 333, Step:   115800, Batch Loss:     1.541846, Batch Acc: 0.590073, Tokens per Sec:    15902, Lr: 0.000026\n",
            "2023-01-19 14:11:00,541 - INFO - joeynmt.training - Epoch 333, Step:   115900, Batch Loss:     1.509558, Batch Acc: 0.585538, Tokens per Sec:    15849, Lr: 0.000026\n",
            "2023-01-19 14:11:02,059 - INFO - joeynmt.training - Epoch 333: total training loss 539.24\n",
            "2023-01-19 14:11:02,060 - INFO - joeynmt.training - EPOCH 334\n",
            "2023-01-19 14:11:08,235 - INFO - joeynmt.training - Epoch 334, Step:   116000, Batch Loss:     1.506526, Batch Acc: 0.592250, Tokens per Sec:    15873, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 148.30ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10802.77ex/s]\n",
            "2023-01-19 14:11:08,510 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=116000\n",
            "2023-01-19 14:11:08,511 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:11:13,034 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:11:13,035 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:11:13,035 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:11:13,036 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:11:13,039 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.25, loss:   2.73, ppl:  15.34, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4792[sec], evaluation: 0.0406[sec]\n",
            "2023-01-19 14:11:13,041 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:11:13,044 - INFO - joeynmt.training - \tSource:     حقیقتا ، او ، درگاهێمێزا یاخین اولاجاق وه اونون قاییدیب گلهجهگی یئر ده گؤزل اولاجاقدێر . \n",
            "2023-01-19 14:11:13,045 - INFO - joeynmt.training - \tReference:  و قطعا برای او در پیشگاه ما تقرب و فرجام نیکوست . \n",
            "2023-01-19 14:11:13,045 - INFO - joeynmt.training - \tHypothesis: در حقیقت ، او نزدیک خواهد بود و فرجامی نزدیک خواهد بود .\n",
            "2023-01-19 14:11:13,045 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:11:13,047 - INFO - joeynmt.training - \tSource:     آللاهێن کلامێ یاییلیر ، یئروسلیمده شاگردلرین سایی خئیلی آرتێر وه بیر چوخ کاهنلر ده بو ایمانا تابع اولوردو . \n",
            "2023-01-19 14:11:13,047 - INFO - joeynmt.training - \tReference:  پس کلام خدا همچنان گسترش می‌یافت و در اورشلیم بر شمار شاگردان همواره افزوده می‌شد و گروهی کثیر از کاهنان نیز ایمان آوردند . \n",
            "2023-01-19 14:11:13,047 - INFO - joeynmt.training - \tHypothesis: کلام خدا در اورشلیم ، شاگردانش در اورشلیم می شدند و عده ای کفیر نیز با این که پیام ایمان می آورند ، عقایهٔ سران کاهنان نیز رفتار می کنند .\n",
            "2023-01-19 14:11:13,047 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:11:13,049 - INFO - joeynmt.training - \tSource:     بیز اونلاردان اول نئچه نئچه نسیللری محو ائتدیک . ایندی هئچ اونلاردان بیرینی گؤرۆر ، یاخود اونلاردان بیر سس سمیر ائشیدیرسنمی \n",
            "2023-01-19 14:11:13,049 - INFO - joeynmt.training - \tReference:  و چه بسیار نسلها که پیش از آنان هلاک کردیم . آیا کسی از آنان را می‌یابی یا صدایی از ایشان می‌شنوی ؟ \n",
            "2023-01-19 14:11:13,050 - INFO - joeynmt.training - \tHypothesis: و چه بسیار نسلها را هلاک کردیم که ما از ایشان هلاک کردیم ، پس چون کاری از آنها نمی بینی ، یا از آنها نمی شنوی ؟\n",
            "2023-01-19 14:11:13,050 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:11:13,052 - INFO - joeynmt.training - \tSource:     پادشاه دا جاواب وئریب اونلارا دئیهجک : سیزه دوغروسونو دئییرم : سیز بو ان کیچیک قارداشلاریمدان بیرینه ائتدیگینیزی منه ائتمیش اولدونوز . \n",
            "2023-01-19 14:11:13,052 - INFO - joeynmt.training - \tReference:  پادشاه در جواب خواهد گفت : به‌راستی به شما می‌گویم ، هر آنچه برای یکی از کوچک‌ترین برادران من کردید ، برای من کردید . \n",
            "2023-01-19 14:11:13,052 - INFO - joeynmt.training - \tHypothesis: پادشاه در جواب گفت : به راستی به شما می گویم ، به راستی به شما می گویم که این برادران ، به من گفتید که به من بیازید .\n",
            "2023-01-19 14:11:20,702 - INFO - joeynmt.training - Epoch 334, Step:   116100, Batch Loss:     1.529540, Batch Acc: 0.592711, Tokens per Sec:    15040, Lr: 0.000026\n",
            "2023-01-19 14:11:28,299 - INFO - joeynmt.training - Epoch 334, Step:   116200, Batch Loss:     1.515275, Batch Acc: 0.588862, Tokens per Sec:    15842, Lr: 0.000026\n",
            "2023-01-19 14:11:33,585 - INFO - joeynmt.training - Epoch 334: total training loss 540.57\n",
            "2023-01-19 14:11:33,586 - INFO - joeynmt.training - EPOCH 335\n",
            "2023-01-19 14:11:36,076 - INFO - joeynmt.training - Epoch 335, Step:   116300, Batch Loss:     1.566618, Batch Acc: 0.594200, Tokens per Sec:    15182, Lr: 0.000026\n",
            "2023-01-19 14:11:43,814 - INFO - joeynmt.training - Epoch 335, Step:   116400, Batch Loss:     1.740912, Batch Acc: 0.590966, Tokens per Sec:    15510, Lr: 0.000026\n",
            "2023-01-19 14:11:51,445 - INFO - joeynmt.training - Epoch 335, Step:   116500, Batch Loss:     1.530408, Batch Acc: 0.592682, Tokens per Sec:    15949, Lr: 0.000026\n",
            "2023-01-19 14:11:59,114 - INFO - joeynmt.training - Epoch 335, Step:   116600, Batch Loss:     1.554410, Batch Acc: 0.591761, Tokens per Sec:    15879, Lr: 0.000026\n",
            "2023-01-19 14:12:00,378 - INFO - joeynmt.training - Epoch 335: total training loss 539.39\n",
            "2023-01-19 14:12:00,378 - INFO - joeynmt.training - EPOCH 336\n",
            "2023-01-19 14:12:07,095 - INFO - joeynmt.training - Epoch 336, Step:   116700, Batch Loss:     1.565901, Batch Acc: 0.594844, Tokens per Sec:    15160, Lr: 0.000026\n",
            "2023-01-19 14:12:14,945 - INFO - joeynmt.training - Epoch 336, Step:   116800, Batch Loss:     1.520716, Batch Acc: 0.592649, Tokens per Sec:    15389, Lr: 0.000026\n",
            "2023-01-19 14:12:22,803 - INFO - joeynmt.training - Epoch 336, Step:   116900, Batch Loss:     1.611442, Batch Acc: 0.590929, Tokens per Sec:    15379, Lr: 0.000026\n",
            "2023-01-19 14:12:27,879 - INFO - joeynmt.training - Epoch 336: total training loss 537.90\n",
            "2023-01-19 14:12:27,879 - INFO - joeynmt.training - EPOCH 337\n",
            "2023-01-19 14:12:30,886 - INFO - joeynmt.training - Epoch 337, Step:   117000, Batch Loss:     1.541753, Batch Acc: 0.592371, Tokens per Sec:    15096, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 123.52ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10006.19ex/s]\n",
            "2023-01-19 14:12:31,161 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=117000\n",
            "2023-01-19 14:12:31,162 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:12:36,256 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:12:36,257 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:12:36,257 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:12:36,258 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:12:36,261 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.14, loss:   2.71, ppl:  15.07, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0479[sec], evaluation: 0.0439[sec]\n",
            "2023-01-19 14:12:36,264 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:12:36,267 - INFO - joeynmt.training - \tSource:     سن آنجاق قور آنا تابع اولوب ، رحماندان گؤرمهدن قورخان کیمسنی قورخودا بیلرسن . بئلهسینه باغیشلاناجاغی وه چوخ گؤزل بیر مۆکافاتا نائل اولاجاغێ ایله مۆژده وئر ! \n",
            "2023-01-19 14:12:36,267 - INFO - joeynmt.training - \tReference:  بیم دادن تو ، تنها کسی را سودمند است که کتاب حق را پیروی کند و از خدای‌ رحمان در نهان بترسد . چنین کسی را به آمرزش و پاداشی پر ارزش مژده ده . \n",
            "2023-01-19 14:12:36,267 - INFO - joeynmt.training - \tHypothesis: و تو جز اینکه قرآن را پیروی کن و از رحمت خدای بخشایشگر ، هر کس از رحمتش بترسد و عذابی دردناک را می دهی و به آنان خواهد داد .\n",
            "2023-01-19 14:12:36,267 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:12:36,269 - INFO - joeynmt.training - \tSource:     سنی ده آلملره آنجاق بیر رحمت اولاراق گؤندردیک \n",
            "2023-01-19 14:12:36,269 - INFO - joeynmt.training - \tReference:  و تو را جز رحمتی برای جهانیان نفرستادیم . \n",
            "2023-01-19 14:12:36,270 - INFO - joeynmt.training - \tHypothesis: و تو را جز یک از تو را به سوی قومی نفرستادیم .\n",
            "2023-01-19 14:12:36,270 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:12:36,272 - INFO - joeynmt.training - \tSource:     سیز بو دۆنیانین گئدیشینه وه هاوادا اولان حاکمیتین ، ینی ایندی آللاها ایتاعتسیز اینسانلاردا فعالیت گؤستهرن روحون هؤکمدارێنا اویاراق بو گۆناهلارێن ایچینده هیات سۆرۆردۆنۆز . \n",
            "2023-01-19 14:12:36,272 - INFO - joeynmt.training - \tReference:  شما در گذشته در طریق این دنیا گام برمی‌داشتید و از آن حاکمی که بر هوا اقتدار دارد ، پیروی می‌کردید . این هوا همان روحی است که اکنون در سرکشان عمل می‌کند . \n",
            "2023-01-19 14:12:36,272 - INFO - joeynmt.training - \tHypothesis: شما از این دنیا ، به خاطر حفظ اعمال نامشروع جنسی و قدرت ها و قدرت روح القدس مطیع باشید که تحسین را با قدرت روحانی به خدا نشان می دهد ، به خاطر روحانی که از طریق روحیهٔ خود بیرون می کشند ، به حیات می انجامد .\n",
            "2023-01-19 14:12:36,272 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:12:36,274 - INFO - joeynmt.training - \tSource:     اوندا بۆتۆن توپلانتێ ساکیتلشدی وه بارنابا ایله پاولا قولاق آسماغا باشلادێلار . بارنابا ایله پاول آللاهێن اونلار واسطهسیله باشقا ملتلر آراسێندا گؤستردیگی علامت وه خارقهلرین هامیسینی دانێشدێ . \n",
            "2023-01-19 14:12:36,274 - INFO - joeynmt.training - \tReference:  آنگاه تمام آن جمع ساکت شدند و به سخنان برنابا و پولس در مورد بسیاری نشانه‌ها و معجزات که خدا از طریق آنان در میان قوم‌ها به ظهور رسانده بود ، گوش فرادادند . \n",
            "2023-01-19 14:12:36,275 - INFO - joeynmt.training - \tHypothesis: سپس تمامی گرد هم آمد و برنابا را به سخنان خود رساندند و با پولس برنابا و برنابا به علاوهٔ خدا و برنابا برایشان معجزات و معجزاتشان معجزاتشان را حفظ کرد .\n",
            "2023-01-19 14:12:44,220 - INFO - joeynmt.training - Epoch 337, Step:   117100, Batch Loss:     1.566101, Batch Acc: 0.594720, Tokens per Sec:    14697, Lr: 0.000026\n",
            "2023-01-19 14:12:52,640 - INFO - joeynmt.training - Epoch 337, Step:   117200, Batch Loss:     1.558267, Batch Acc: 0.593260, Tokens per Sec:    14189, Lr: 0.000026\n",
            "2023-01-19 14:13:00,325 - INFO - joeynmt.training - Epoch 337, Step:   117300, Batch Loss:     1.597010, Batch Acc: 0.589179, Tokens per Sec:    15716, Lr: 0.000026\n",
            "2023-01-19 14:13:01,216 - INFO - joeynmt.training - Epoch 337: total training loss 539.77\n",
            "2023-01-19 14:13:01,216 - INFO - joeynmt.training - EPOCH 338\n",
            "2023-01-19 14:13:08,131 - INFO - joeynmt.training - Epoch 338, Step:   117400, Batch Loss:     1.532115, Batch Acc: 0.591199, Tokens per Sec:    15642, Lr: 0.000026\n",
            "2023-01-19 14:13:15,782 - INFO - joeynmt.training - Epoch 338, Step:   117500, Batch Loss:     1.584181, Batch Acc: 0.592055, Tokens per Sec:    15609, Lr: 0.000026\n",
            "2023-01-19 14:13:23,481 - INFO - joeynmt.training - Epoch 338, Step:   117600, Batch Loss:     1.650570, Batch Acc: 0.593151, Tokens per Sec:    15739, Lr: 0.000026\n",
            "2023-01-19 14:13:28,065 - INFO - joeynmt.training - Epoch 338: total training loss 539.14\n",
            "2023-01-19 14:13:28,065 - INFO - joeynmt.training - EPOCH 339\n",
            "2023-01-19 14:13:31,232 - INFO - joeynmt.training - Epoch 339, Step:   117700, Batch Loss:     1.443509, Batch Acc: 0.593660, Tokens per Sec:    15821, Lr: 0.000026\n",
            "2023-01-19 14:13:38,963 - INFO - joeynmt.training - Epoch 339, Step:   117800, Batch Loss:     1.608166, Batch Acc: 0.591445, Tokens per Sec:    15528, Lr: 0.000026\n",
            "2023-01-19 14:13:46,562 - INFO - joeynmt.training - Epoch 339, Step:   117900, Batch Loss:     1.600728, Batch Acc: 0.592628, Tokens per Sec:    15871, Lr: 0.000026\n",
            "2023-01-19 14:13:54,278 - INFO - joeynmt.training - Epoch 339, Step:   118000, Batch Loss:     1.514019, Batch Acc: 0.593893, Tokens per Sec:    15882, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 139.12ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10461.96ex/s]\n",
            "2023-01-19 14:13:54,541 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=118000\n",
            "2023-01-19 14:13:54,541 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:13:59,315 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:13:59,315 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:13:59,315 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:13:59,316 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:13:59,319 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.58, loss:   2.73, ppl:  15.38, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7270[sec], evaluation: 0.0426[sec]\n",
            "2023-01-19 14:13:59,322 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:13:59,325 - INFO - joeynmt.training - \tSource:     ائی ایمان گتیرنلر ! آللاهدان قورخون وه دوغرو اولانلارلا اولون ! \n",
            "2023-01-19 14:13:59,325 - INFO - joeynmt.training - \tReference:  ای کسانی که ایمان آورده‌اید ، از خدا پروا کنید و با راستان باشید . \n",
            "2023-01-19 14:13:59,325 - INFO - joeynmt.training - \tHypothesis: ای کسانی که ایمان آورده اید ، از خدا پروا دارید و کسانی که هدایت می شوند ، هدایت یافته اند .\n",
            "2023-01-19 14:13:59,325 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:13:59,327 - INFO - joeynmt.training - \tSource:     آخێردا ائلهدیگینی ده ائلهدین . سن نانکورون بیریسن ! \n",
            "2023-01-19 14:13:59,328 - INFO - joeynmt.training - \tReference:  و سرانجام‌ کار خود را کردی ، و تو از ناسپاسانی . \n",
            "2023-01-19 14:13:59,328 - INFO - joeynmt.training - \tHypothesis: پس در آن صورتش می گویند : تو یکسپولد !\n",
            "2023-01-19 14:13:59,328 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:13:59,330 - INFO - joeynmt.training - \tSource:     قادێن ساکیتلیک وه تام بیر تابئلیک ایچینده اؤیرنسین ؛ \n",
            "2023-01-19 14:13:59,330 - INFO - joeynmt.training - \tReference:  زن باید در سکوت و با اطاعت کامل تعلیم بگیرد . \n",
            "2023-01-19 14:13:59,330 - INFO - joeynmt.training - \tHypothesis: و زنی که بی شکنی و مصیبت بود ، در آن سرزمینی که در آن ساکنند ،\n",
            "2023-01-19 14:13:59,330 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:13:59,332 - INFO - joeynmt.training - \tSource:     اؤلچنده اؤلچوده دۆز اولون ، دۆزگۆن ترهزی ایله چکین . بو داها خئیرلی وه نتیجه ائ تباریله داها یاخشیدیر ! \n",
            "2023-01-19 14:13:59,332 - INFO - joeynmt.training - \tReference:  و چون پیمانه می‌کنید ، پیمانه را تمام دهید ، و با ترازوی درست بسنجید که این بهتر و خوش فرجام‌تر است . \n",
            "2023-01-19 14:13:59,332 - INFO - joeynmt.training - \tHypothesis: و به راستی اگر در راهی راست و درستی سفش تر است ، و به راستی بهتر و پایدارتر است .\n",
            "2023-01-19 14:13:59,827 - INFO - joeynmt.training - Epoch 339: total training loss 536.49\n",
            "2023-01-19 14:13:59,828 - INFO - joeynmt.training - EPOCH 340\n",
            "2023-01-19 14:14:07,074 - INFO - joeynmt.training - Epoch 340, Step:   118100, Batch Loss:     1.625838, Batch Acc: 0.596227, Tokens per Sec:    15728, Lr: 0.000026\n",
            "2023-01-19 14:14:14,757 - INFO - joeynmt.training - Epoch 340, Step:   118200, Batch Loss:     1.548097, Batch Acc: 0.595379, Tokens per Sec:    15644, Lr: 0.000026\n",
            "2023-01-19 14:14:22,430 - INFO - joeynmt.training - Epoch 340, Step:   118300, Batch Loss:     1.584253, Batch Acc: 0.590075, Tokens per Sec:    15707, Lr: 0.000026\n",
            "2023-01-19 14:14:26,649 - INFO - joeynmt.training - Epoch 340: total training loss 537.64\n",
            "2023-01-19 14:14:26,649 - INFO - joeynmt.training - EPOCH 341\n",
            "2023-01-19 14:14:30,179 - INFO - joeynmt.training - Epoch 341, Step:   118400, Batch Loss:     1.560955, Batch Acc: 0.597736, Tokens per Sec:    15445, Lr: 0.000026\n",
            "2023-01-19 14:14:37,941 - INFO - joeynmt.training - Epoch 341, Step:   118500, Batch Loss:     1.569669, Batch Acc: 0.592975, Tokens per Sec:    15459, Lr: 0.000026\n",
            "2023-01-19 14:14:45,626 - INFO - joeynmt.training - Epoch 341, Step:   118600, Batch Loss:     1.519021, Batch Acc: 0.593103, Tokens per Sec:    15728, Lr: 0.000026\n",
            "2023-01-19 14:14:53,328 - INFO - joeynmt.training - Epoch 341, Step:   118700, Batch Loss:     1.557947, Batch Acc: 0.591356, Tokens per Sec:    15797, Lr: 0.000026\n",
            "2023-01-19 14:14:53,550 - INFO - joeynmt.training - Epoch 341: total training loss 538.68\n",
            "2023-01-19 14:14:53,550 - INFO - joeynmt.training - EPOCH 342\n",
            "2023-01-19 14:15:01,014 - INFO - joeynmt.training - Epoch 342, Step:   118800, Batch Loss:     1.587911, Batch Acc: 0.591710, Tokens per Sec:    15685, Lr: 0.000026\n",
            "2023-01-19 14:15:08,793 - INFO - joeynmt.training - Epoch 342, Step:   118900, Batch Loss:     1.512065, Batch Acc: 0.594214, Tokens per Sec:    15436, Lr: 0.000026\n",
            "2023-01-19 14:15:16,488 - INFO - joeynmt.training - Epoch 342, Step:   119000, Batch Loss:     1.485990, Batch Acc: 0.593017, Tokens per Sec:    15613, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 79.82ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10775.74ex/s]\n",
            "2023-01-19 14:15:16,767 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=119000\n",
            "2023-01-19 14:15:16,767 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:15:22,384 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:15:22,384 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:15:22,384 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:15:22,385 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:15:22,388 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  11.08, loss:   2.67, ppl:  14.40, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.5675[sec], evaluation: 0.0452[sec]\n",
            "2023-01-19 14:15:22,388 - INFO - joeynmt.training - Hooray! New best validation result [bleu]!\n",
            "2023-01-19 14:15:22,612 - INFO - joeynmt.helpers - delete RESULTS/model/56000.ckpt\n",
            "2023-01-19 14:15:22,626 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:15:22,629 - INFO - joeynmt.training - \tSource:     اگر حاقسێزلێق ائتمیشمسه ، اؤلوم جزاسێنا لایق بیر ایش توتموشامسا ، اؤلومدن قاچمێرام . یوخ ، اگر بونلارێن اتحاملاری اساسسێزدێرسا ، منی اونلارێن الینه تسلیم ائتمک اۆچۆن هئچ کیمین صلاحیتی یوخدور . من قئیسرین محکمهسینه مۆراجعت ائتمک ایستهییرم . \n",
            "2023-01-19 14:15:22,629 - INFO - joeynmt.training - \tReference:  اگر به‌راستی خطاکارم و جرمی سزاوار مرگ مرتکب شده‌ام ، از مرگ نمی‌گریزم . اما اگر اتهاماتی که این مردان به من نسبت می‌دهند ، بی‌اساس است ، هیچ کس حق ندارد برای خشنودی آنان مرا به دستشان تسلیم کند . من از قیصر درخواست فرجام می‌کنم ! \n",
            "2023-01-19 14:15:22,629 - INFO - joeynmt.training - \tHypothesis: اگر به بدکاری مرتکب اعمال نامشروع جنسی نکرده ام ، اگر مرگی من هستم ، باید اگر بگویم ، باید اگر این حال از اتهاماتمند ، مرا بی شرمسارت به دست من سپرده است . اگر چنین افرادی مرا به دست ندهند ، از دست من سپرده ام .\n",
            "2023-01-19 14:15:22,630 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:15:22,632 - INFO - joeynmt.training - \tSource:     لاکین آللاه بیزه اولان محبتینی بونونلا سۆبوت ائدیر کی ، بیز هله گۆناهکار اولا اولا مصیح بیزیم اوغروموزدا اؤلدو . \n",
            "2023-01-19 14:15:22,632 - INFO - joeynmt.training - \tReference:  اما خدا ارزش محبت خود را این گونه به ما ثابت می‌کند که وقتی هنوز گناهکار بودیم ، مسیح جان خود را برای ما داد . \n",
            "2023-01-19 14:15:22,632 - INFO - joeynmt.training - \tHypothesis: اما خدا محبت را که از ما به ما عطا کرده است ، به خاطر محبت کرده است که هنوز در ما مرده است ، اما او مرد .\n",
            "2023-01-19 14:15:22,632 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:15:22,634 - INFO - joeynmt.training - \tSource:     سونرا دا همین گۆن نئ متلر بارهسینده مۆتلق سورغو سوعال اولوناجاقسینیز ! \n",
            "2023-01-19 14:15:22,634 - INFO - joeynmt.training - \tReference:  سپس در همان روز است که از نعمت روی زمین‌ پرسیده خواهید شد . \n",
            "2023-01-19 14:15:22,635 - INFO - joeynmt.training - \tHypothesis: سپس در آن روز ، قطعا در باره آن صورت خواهید شد .\n",
            "2023-01-19 14:15:22,635 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:15:22,637 - INFO - joeynmt.training - \tSource:     محض اوندا معؤ مینلر امتهاهانا چکیلمیش وه مۆحکم سارسێلمێشدێلار . \n",
            "2023-01-19 14:15:22,637 - INFO - joeynmt.training - \tReference:  آنجا بود که‌ مؤمنان در آزمایش قرار گرفتند و سخت تکان خوردند . \n",
            "2023-01-19 14:15:22,637 - INFO - joeynmt.training - \tHypothesis: پس مؤمنانی را که ایمان آورده بودند ، آزمایشیشیند ، و سفینه ها آزمایشیشگر بودند .\n",
            "2023-01-19 14:15:26,594 - INFO - joeynmt.training - Epoch 342: total training loss 537.21\n",
            "2023-01-19 14:15:26,595 - INFO - joeynmt.training - EPOCH 343\n",
            "2023-01-19 14:15:30,361 - INFO - joeynmt.training - Epoch 343, Step:   119100, Batch Loss:     1.625031, Batch Acc: 0.596480, Tokens per Sec:    15300, Lr: 0.000026\n",
            "2023-01-19 14:15:38,346 - INFO - joeynmt.training - Epoch 343, Step:   119200, Batch Loss:     1.573482, Batch Acc: 0.594367, Tokens per Sec:    15036, Lr: 0.000026\n",
            "2023-01-19 14:15:46,033 - INFO - joeynmt.training - Epoch 343, Step:   119300, Batch Loss:     1.632015, Batch Acc: 0.592449, Tokens per Sec:    15642, Lr: 0.000026\n",
            "2023-01-19 14:15:53,692 - INFO - joeynmt.training - Epoch 343, Step:   119400, Batch Loss:     1.604039, Batch Acc: 0.593447, Tokens per Sec:    15696, Lr: 0.000026\n",
            "2023-01-19 14:15:53,841 - INFO - joeynmt.training - Epoch 343: total training loss 541.30\n",
            "2023-01-19 14:15:53,842 - INFO - joeynmt.training - EPOCH 344\n",
            "2023-01-19 14:16:01,391 - INFO - joeynmt.training - Epoch 344, Step:   119500, Batch Loss:     1.551119, Batch Acc: 0.594378, Tokens per Sec:    15655, Lr: 0.000026\n",
            "2023-01-19 14:16:09,183 - INFO - joeynmt.training - Epoch 344, Step:   119600, Batch Loss:     1.453283, Batch Acc: 0.594691, Tokens per Sec:    15229, Lr: 0.000026\n",
            "2023-01-19 14:16:18,006 - INFO - joeynmt.training - Epoch 344, Step:   119700, Batch Loss:     1.426857, Batch Acc: 0.593903, Tokens per Sec:    13843, Lr: 0.000026\n",
            "2023-01-19 14:16:21,842 - INFO - joeynmt.training - Epoch 344: total training loss 535.44\n",
            "2023-01-19 14:16:21,843 - INFO - joeynmt.training - EPOCH 345\n",
            "2023-01-19 14:16:25,636 - INFO - joeynmt.training - Epoch 345, Step:   119800, Batch Loss:     1.477631, Batch Acc: 0.599260, Tokens per Sec:    15902, Lr: 0.000026\n",
            "2023-01-19 14:16:33,213 - INFO - joeynmt.training - Epoch 345, Step:   119900, Batch Loss:     1.516991, Batch Acc: 0.596017, Tokens per Sec:    15844, Lr: 0.000026\n",
            "2023-01-19 14:16:41,107 - INFO - joeynmt.training - Epoch 345, Step:   120000, Batch Loss:     1.667266, Batch Acc: 0.592137, Tokens per Sec:    15438, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.32ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10276.98ex/s]\n",
            "2023-01-19 14:16:41,399 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=120000\n",
            "2023-01-19 14:16:41,399 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:16:46,373 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:16:46,374 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:16:46,374 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:16:46,375 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:16:46,378 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.75, loss:   2.74, ppl:  15.42, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9261[sec], evaluation: 0.0455[sec]\n",
            "2023-01-19 14:16:46,383 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:16:46,386 - INFO - joeynmt.training - \tSource:     ائی اوشاقلار ، سیزه یازدیم ، چۆنکی آتانێ تانییرسینیز . ائی آتالار ، سیزه یازدیم ، چۆنکی باشلانغێجدان بریع وار اولانێ تانییرسینیز . ائی گنجلر ، سیزه یازدیم ، چۆنکی گۆجلۆسۆنۆز ، آللاهێن کلامێ سیزده قالێر ، شر اولانا دا غالب گلمیسینیز . \n",
            "2023-01-19 14:16:46,386 - INFO - joeynmt.training - \tReference:  ای پدران ، به شما می‌نویسم ؛ زیرا کسی را که از آغاز بوده است ، می‌شناسید . ای مردان جوان ، به شما می‌نویسم ؛ زیرا قوی هستید و کلام خدا در شما می‌ماند و بر آن شریر غالب آمده‌اید . \n",
            "2023-01-19 14:16:46,386 - INFO - joeynmt.training - \tHypothesis: ای فرزندان عزیز ، به شما نوشتم ؛ زیرا پدر ، شما می شناسم . ای پدر ، شما که آغاز و آن را می شناسید ؛ زیرا کلام خداست ، می شناسید ؛ زیرا کلام خدا که آن را می شناسید ، می نویسم ، بر شما غالب آید .\n",
            "2023-01-19 14:16:46,387 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:16:46,389 - INFO - joeynmt.training - \tSource:     بئله کی بۆتۆن مۆقدسلرله بیرلیکده مسیحین محبتنینین نه قدر گئنیش ، اوزون ، اوجا وه درین اولدوغونو باشا دۆشمهیه ، \n",
            "2023-01-19 14:16:46,389 - INFO - joeynmt.training - \tReference:  تا همراه با تمام مقدسان بتوانید عرض ، طول ، بلندی و عمق حقیقت را به‌خوبی دریابید\n",
            "2023-01-19 14:16:46,389 - INFO - joeynmt.training - \tHypothesis: تا بتوانید تمامی محبت مسیح ، هم تاکستانه ، چه کمالد و چه کمتر و چه کمتر از همهٔ مقدس است ،\n",
            "2023-01-19 14:16:46,389 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:16:46,391 - INFO - joeynmt.training - \tSource:     آمما اونلار ایسهآدان تکیدله خاهش ائدیب دئدیلر : بیزیمله قال ، چۆنکی آخشام دۆشۆر وه گۆن باتماق اۆزرهدیر . او دا اونلارلا قالماق اۆچۆن ایچری گیردی . \n",
            "2023-01-19 14:16:46,391 - INFO - joeynmt.training - \tReference:  اما آنان به او اصرار ورزیدند که بماند و گفتند : با ما بمان ؛ زیرا چیزی به پایان روز نمانده و نزدیک غروب است . سپس با آنان به خانه رفت تا نزدشان بماند . \n",
            "2023-01-19 14:16:46,391 - INFO - joeynmt.training - \tHypothesis: اما آنان به احساس به احسالی گفتند : ما با ما بیش از غروب و غروب می شویم ؛ زیرا با آنان داخل شود .\n",
            "2023-01-19 14:16:46,392 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:16:46,394 - INFO - joeynmt.training - \tSource:     بئلهلیکله ، زینداندا یهیانین بوینونو ووردوردو . \n",
            "2023-01-19 14:16:46,394 - INFO - joeynmt.training - \tReference:  پس فرستاد تا در زندان سر یحیی را از تنش جدا کنند . \n",
            "2023-01-19 14:16:46,394 - INFO - joeynmt.training - \tHypothesis: پس در زندانی از یحیی دستگیر خود را در تنشید .\n",
            "2023-01-19 14:16:53,940 - INFO - joeynmt.training - Epoch 345: total training loss 534.86\n",
            "2023-01-19 14:16:53,941 - INFO - joeynmt.training - EPOCH 346\n",
            "2023-01-19 14:16:54,166 - INFO - joeynmt.training - Epoch 346, Step:   120100, Batch Loss:     1.564843, Batch Acc: 0.608915, Tokens per Sec:    15833, Lr: 0.000026\n",
            "2023-01-19 14:17:01,932 - INFO - joeynmt.training - Epoch 346, Step:   120200, Batch Loss:     1.566585, Batch Acc: 0.596228, Tokens per Sec:    15358, Lr: 0.000026\n",
            "2023-01-19 14:17:09,632 - INFO - joeynmt.training - Epoch 346, Step:   120300, Batch Loss:     1.503565, Batch Acc: 0.594653, Tokens per Sec:    15671, Lr: 0.000026\n",
            "2023-01-19 14:17:17,497 - INFO - joeynmt.training - Epoch 346, Step:   120400, Batch Loss:     1.598672, Batch Acc: 0.593438, Tokens per Sec:    15466, Lr: 0.000026\n",
            "2023-01-19 14:17:21,080 - INFO - joeynmt.training - Epoch 346: total training loss 537.70\n",
            "2023-01-19 14:17:21,081 - INFO - joeynmt.training - EPOCH 347\n",
            "2023-01-19 14:17:25,214 - INFO - joeynmt.training - Epoch 347, Step:   120500, Batch Loss:     1.467261, Batch Acc: 0.595589, Tokens per Sec:    15525, Lr: 0.000026\n",
            "2023-01-19 14:17:32,936 - INFO - joeynmt.training - Epoch 347, Step:   120600, Batch Loss:     1.613356, Batch Acc: 0.593346, Tokens per Sec:    15625, Lr: 0.000026\n",
            "2023-01-19 14:17:40,728 - INFO - joeynmt.training - Epoch 347, Step:   120700, Batch Loss:     1.554953, Batch Acc: 0.595920, Tokens per Sec:    15580, Lr: 0.000026\n",
            "2023-01-19 14:17:48,103 - INFO - joeynmt.training - Epoch 347: total training loss 535.36\n",
            "2023-01-19 14:17:48,103 - INFO - joeynmt.training - EPOCH 348\n",
            "2023-01-19 14:17:48,590 - INFO - joeynmt.training - Epoch 348, Step:   120800, Batch Loss:     1.459136, Batch Acc: 0.604532, Tokens per Sec:    15593, Lr: 0.000026\n",
            "2023-01-19 14:17:56,351 - INFO - joeynmt.training - Epoch 348, Step:   120900, Batch Loss:     1.575140, Batch Acc: 0.598717, Tokens per Sec:    15650, Lr: 0.000026\n",
            "2023-01-19 14:18:04,117 - INFO - joeynmt.training - Epoch 348, Step:   121000, Batch Loss:     1.467472, Batch Acc: 0.593195, Tokens per Sec:    15563, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.13ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9627.07ex/s] \n",
            "2023-01-19 14:18:04,393 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=121000\n",
            "2023-01-19 14:18:04,394 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:18:09,924 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:18:09,924 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:18:09,924 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:18:09,925 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:18:09,928 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.55, loss:   2.77, ppl:  15.93, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.4813[sec], evaluation: 0.0453[sec]\n",
            "2023-01-19 14:18:09,931 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:18:09,934 - INFO - joeynmt.training - \tSource:     وه قارداشێمێز ، مسیحین مۆژدسنی یایماقدا آللاهێن امکداشێ اولان تیموتئیی سیزی اماندا مۆحکملندیرمک وه روحلاندێرماق اۆچۆن گؤندردیک . \n",
            "2023-01-19 14:18:09,934 - INFO - joeynmt.training - \tReference:  آنگاه تیموتائوس را که برادر ما و خادم خداست و در مورد مسیح بشارت می‌دهد ، فرستادیم تا برای تقویت ایمانتان ، شما را استوار و دلگرم سازد\n",
            "2023-01-19 14:18:09,934 - INFO - joeynmt.training - \tHypothesis: و ما ما این است که بشارت را به تیموتائوس فرستادیم تا شما را به تیموتائوس از خدمت به تیموتائوس در خدمت به مسیحْ عیسی مسیح بگذریم .\n",
            "2023-01-19 14:18:09,934 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:18:09,937 - INFO - joeynmt.training - \tSource:     سن مصیحده مالک اولدوغوموز هر خئیرلی شئیی درک ائتدیکجه ایمانینی باشقالاری ایله پایلاشماقدا فعال اولماغێن اۆچۆن دوعا ائدیرم . \n",
            "2023-01-19 14:18:09,937 - INFO - joeynmt.training - \tReference:  دعایم این است ، همین ایمان که تو و هم‌ایمانانت از آن برخوردارید ، تو را به درک تمامی برکاتی برساند که از طریق مسیح نصیبمان شده است . \n",
            "2023-01-19 14:18:09,937 - INFO - joeynmt.training - \tHypothesis: حال که تو در اتحاد با مسیح هستی ، هر چیز را داریم ، با همان ایمان وفهٔ دیگران مطیع خود به افزونی می کنم .\n",
            "2023-01-19 14:18:09,937 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:18:09,939 - INFO - joeynmt.training - \tSource:     بونا گؤره اگر بیر اۆزۆ اعذاب چکیرسه ، باشقالاری دا اونونلا بیرگه اعذاب چکیر . اۆزۆلردن بیری شرفه چاتێرسا ، باشقالاری دا اونونلا بیرگه سئوینیر . \n",
            "2023-01-19 14:18:09,939 - INFO - joeynmt.training - \tReference:  اگر عضوی دردمند شود ، تمام اعضای دیگر با آن درد خواهند کشید . اگر عضوی نیز عزت یابد ، تمام اعضای دیگر با او شادی خواهند کرد . \n",
            "2023-01-19 14:18:09,939 - INFO - joeynmt.training - \tHypothesis: پس اگر یکی از این که رنج بکشد ، دیگری را رنج می کشد ، یکی از آن ها سختی ها رنج می کشد و دیگری با او بسیار شادمانند .\n",
            "2023-01-19 14:18:09,939 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:18:09,941 - INFO - joeynmt.training - \tSource:     ترفکئشلیک ائتمهدن اؤز امهلینه گؤره هر کسی مۆحاکمه ائدن آللاهێ آتا دئیه چاغێردێغێنێز اۆچۆن بو غربت دۆنیاداکی واختینیزی آللاه قورخوسوندا کئچیرین . \n",
            "2023-01-19 14:18:09,941 - INFO - joeynmt.training - \tReference:  اگر آن پدر را می‌خوانید که هر کس را مطابق اعمالش بی‌طرفانه قضاوت می‌کند ، پس رفتار شما مادامی که ساکن موقت هستید ، با خداترسی همراه باشد ؛ \n",
            "2023-01-19 14:18:09,942 - INFO - joeynmt.training - \tHypothesis: در واقع ، خدا را از طریق احوال بیهودهٔ خود نگاه می دارد تا هر کسی که مطابق اعمالش فرا می خواند ، خدا را به دنیا تعیین کنم ؛ زیرا خدا از این رو ، ترسان را به دنیا می رسانید .\n",
            "2023-01-19 14:18:18,108 - INFO - joeynmt.training - Epoch 348, Step:   121100, Batch Loss:     1.571506, Batch Acc: 0.592563, Tokens per Sec:    14391, Lr: 0.000026\n",
            "2023-01-19 14:18:21,290 - INFO - joeynmt.training - Epoch 348: total training loss 532.63\n",
            "2023-01-19 14:18:21,290 - INFO - joeynmt.training - EPOCH 349\n",
            "2023-01-19 14:18:26,061 - INFO - joeynmt.training - Epoch 349, Step:   121200, Batch Loss:     1.449578, Batch Acc: 0.599467, Tokens per Sec:    15181, Lr: 0.000026\n",
            "2023-01-19 14:18:33,961 - INFO - joeynmt.training - Epoch 349, Step:   121300, Batch Loss:     1.545243, Batch Acc: 0.596147, Tokens per Sec:    15403, Lr: 0.000026\n",
            "2023-01-19 14:18:41,916 - INFO - joeynmt.training - Epoch 349, Step:   121400, Batch Loss:     1.500343, Batch Acc: 0.595067, Tokens per Sec:    14963, Lr: 0.000026\n",
            "2023-01-19 14:18:48,963 - INFO - joeynmt.training - Epoch 349: total training loss 534.36\n",
            "2023-01-19 14:18:48,963 - INFO - joeynmt.training - EPOCH 350\n",
            "2023-01-19 14:18:49,914 - INFO - joeynmt.training - Epoch 350, Step:   121500, Batch Loss:     1.648510, Batch Acc: 0.600638, Tokens per Sec:    15497, Lr: 0.000026\n",
            "2023-01-19 14:18:57,734 - INFO - joeynmt.training - Epoch 350, Step:   121600, Batch Loss:     1.517905, Batch Acc: 0.596781, Tokens per Sec:    15461, Lr: 0.000026\n",
            "2023-01-19 14:19:05,428 - INFO - joeynmt.training - Epoch 350, Step:   121700, Batch Loss:     1.441898, Batch Acc: 0.592823, Tokens per Sec:    15672, Lr: 0.000026\n",
            "2023-01-19 14:19:13,228 - INFO - joeynmt.training - Epoch 350, Step:   121800, Batch Loss:     1.543135, Batch Acc: 0.593033, Tokens per Sec:    15454, Lr: 0.000026\n",
            "2023-01-19 14:19:16,140 - INFO - joeynmt.training - Epoch 350: total training loss 537.46\n",
            "2023-01-19 14:19:16,140 - INFO - joeynmt.training - EPOCH 351\n",
            "2023-01-19 14:19:20,975 - INFO - joeynmt.training - Epoch 351, Step:   121900, Batch Loss:     1.541572, Batch Acc: 0.594373, Tokens per Sec:    15845, Lr: 0.000026\n",
            "2023-01-19 14:19:28,829 - INFO - joeynmt.training - Epoch 351, Step:   122000, Batch Loss:     1.534563, Batch Acc: 0.596956, Tokens per Sec:    15265, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.48ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10046.73ex/s]\n",
            "2023-01-19 14:19:29,100 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=122000\n",
            "2023-01-19 14:19:29,101 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:19:33,658 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:19:33,658 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:19:33,658 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:19:33,659 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:19:33,662 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.93, loss:   2.70, ppl:  14.81, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5124[sec], evaluation: 0.0416[sec]\n",
            "2023-01-19 14:19:33,665 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:19:33,668 - INFO - joeynmt.training - \tSource:     بئله بیریسی ایله اؤیونهجهیهم ، آمما اؤزومله الاقدار اولاراق زیفلیگیمدن باشقا بیر شئیله اؤیونمهیهجهیهم . \n",
            "2023-01-19 14:19:33,668 - INFO - joeynmt.training - \tReference:  من به چنین شخصی فخر می‌کنم ، اما به خود فخر نخواهم کرد ، مگر در مورد ضعف‌هایم . \n",
            "2023-01-19 14:19:33,668 - INFO - joeynmt.training - \tHypothesis: اما من به هیچ وجه از خود فخر خواهم کرد ، اما هیچ کس را با خود فخر کنم که به خود افکارم ، فخر کنم .\n",
            "2023-01-19 14:19:33,668 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:19:33,670 - INFO - joeynmt.training - \tSource:     هامیسیندان سونرا بو قادێن دا اؤلدو . \n",
            "2023-01-19 14:19:33,671 - INFO - joeynmt.training - \tReference:  در آخر ، آن زن نیز مرد . \n",
            "2023-01-19 14:19:33,672 - INFO - joeynmt.training - \tHypothesis: بعد از این که زن ، مرگ نیز مرده بود .\n",
            "2023-01-19 14:19:33,672 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:19:33,674 - INFO - joeynmt.training - \tSource:     اونلار ایسنانین دیریلمهسیندن سونرا قبیرلردن چێخێب مۆقدس شهره گیردیلر وه بیر چوخ اینسانا گؤروندولر . \n",
            "2023-01-19 14:19:33,674 - INFO - joeynmt.training - \tReference:  و بسیاری از مردم آن‌ها را دیدند . \n",
            "2023-01-19 14:19:33,674 - INFO - joeynmt.training - \tHypothesis: آنان پس از آن ، عیسی را از مقبره بیرون شهر داخل شدند و بسیاری از مقدسان را دیدند .\n",
            "2023-01-19 14:19:33,675 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:19:33,676 - INFO - joeynmt.training - \tSource:     شنبه گۆنۆ آنجاق اونون بارهسینده اختلافدا اولانلارا واجب ائدیلدی . حقیقتا ، ربین قیامت گۆنۆ اختلافدا اولدوقلاری مسهلر بارهسینده اونلارێن آراسێندا هؤکم وئرهجکدیر . \n",
            "2023-01-19 14:19:33,677 - INFO - joeynmt.training - \tReference:   بزرگداشت‌ شنبه ، بر کسانی که در باره آن اختلاف کردند مقرر گردید ، و قطعا پروردگارت روز رستاخیز میان آنها در باره چیزی که در مورد آن اختلاف می‌کردند ، داوری خواهد کرد . \n",
            "2023-01-19 14:19:33,677 - INFO - joeynmt.training - \tHypothesis: جز در روزهای سبت آنکه در باره آن اختلاف می کردند ، در باره کسانی که به اختلاف می کردند ، در روز قیامت قیامت در آنچه در آن اختلاف می کردند ، داوری می کنند .\n",
            "2023-01-19 14:19:42,147 - INFO - joeynmt.training - Epoch 351, Step:   122100, Batch Loss:     1.626633, Batch Acc: 0.593799, Tokens per Sec:    13651, Lr: 0.000026\n",
            "2023-01-19 14:19:49,285 - INFO - joeynmt.training - Epoch 351: total training loss 535.88\n",
            "2023-01-19 14:19:49,285 - INFO - joeynmt.training - EPOCH 352\n",
            "2023-01-19 14:19:50,387 - INFO - joeynmt.training - Epoch 352, Step:   122200, Batch Loss:     1.530066, Batch Acc: 0.599387, Tokens per Sec:    15416, Lr: 0.000026\n",
            "2023-01-19 14:19:58,110 - INFO - joeynmt.training - Epoch 352, Step:   122300, Batch Loss:     1.562521, Batch Acc: 0.596517, Tokens per Sec:    15630, Lr: 0.000026\n",
            "2023-01-19 14:20:05,880 - INFO - joeynmt.training - Epoch 352, Step:   122400, Batch Loss:     1.401904, Batch Acc: 0.594312, Tokens per Sec:    15605, Lr: 0.000026\n",
            "2023-01-19 14:20:13,682 - INFO - joeynmt.training - Epoch 352, Step:   122500, Batch Loss:     1.508519, Batch Acc: 0.596263, Tokens per Sec:    15565, Lr: 0.000026\n",
            "2023-01-19 14:20:16,275 - INFO - joeynmt.training - Epoch 352: total training loss 532.36\n",
            "2023-01-19 14:20:16,276 - INFO - joeynmt.training - EPOCH 353\n",
            "2023-01-19 14:20:21,397 - INFO - joeynmt.training - Epoch 353, Step:   122600, Batch Loss:     1.468202, Batch Acc: 0.603304, Tokens per Sec:    15722, Lr: 0.000026\n",
            "2023-01-19 14:20:28,964 - INFO - joeynmt.training - Epoch 353, Step:   122700, Batch Loss:     1.568439, Batch Acc: 0.597451, Tokens per Sec:    15958, Lr: 0.000026\n",
            "2023-01-19 14:20:36,648 - INFO - joeynmt.training - Epoch 353, Step:   122800, Batch Loss:     1.489958, Batch Acc: 0.593822, Tokens per Sec:    15606, Lr: 0.000026\n",
            "2023-01-19 14:20:42,889 - INFO - joeynmt.training - Epoch 353: total training loss 533.05\n",
            "2023-01-19 14:20:42,889 - INFO - joeynmt.training - EPOCH 354\n",
            "2023-01-19 14:20:44,401 - INFO - joeynmt.training - Epoch 354, Step:   122900, Batch Loss:     1.640852, Batch Acc: 0.589192, Tokens per Sec:    15182, Lr: 0.000026\n",
            "2023-01-19 14:20:52,112 - INFO - joeynmt.training - Epoch 354, Step:   123000, Batch Loss:     1.479149, Batch Acc: 0.596892, Tokens per Sec:    15721, Lr: 0.000026\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.15ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10550.09ex/s]\n",
            "2023-01-19 14:20:52,392 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=123000\n",
            "2023-01-19 14:20:52,392 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:20:56,964 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:20:56,964 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:20:56,964 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:20:56,966 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:20:56,970 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.51, loss:   2.69, ppl:  14.70, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5271[sec], evaluation: 0.0440[sec]\n",
            "2023-01-19 14:20:56,974 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:20:56,977 - INFO - joeynmt.training - \tSource:     بئلهجه ده رب مۆژدنی ائلان ائدنلره مۆژدن دولانماغێ امر ائتمیشدیر . \n",
            "2023-01-19 14:20:56,977 - INFO - joeynmt.training - \tReference:  به همین‌سان ، سرور در خصوص کسانی که بشارت را اعلام می‌کنند ، فرمان داد که روزی خود را از طریق بشارت دریافت کنند . \n",
            "2023-01-19 14:20:56,977 - INFO - joeynmt.training - \tHypothesis: پس ، کسانی که به یهوه بشارت داده ، اعلام کرد که به آنان فرمان داده اند ، به آنان حکم کرد که به بشارت داده اند .\n",
            "2023-01-19 14:20:56,978 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:20:56,980 - INFO - joeynmt.training - \tSource:     بۆتۆن خالق بو ، داوودون اوغلو دئییلمی ؟ دئیه مات قالدێ . \n",
            "2023-01-19 14:20:56,981 - INFO - joeynmt.training - \tReference:  تمام جمعیت شگفت‌زده شدند و گفتند : آیا ممکن است که او پسر داوود باشد ؟ \n",
            "2023-01-19 14:20:56,981 - INFO - joeynmt.training - \tHypothesis: همهٔ مردم این قوم داوود نیستند و می گفتند : ای پسر داوود ، خوابیده است ؟\n",
            "2023-01-19 14:20:56,981 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:20:56,983 - INFO - joeynmt.training - \tSource:     ایسا اونا بئله جاواب وئردی : آی اینسان ، منی کیم اۆزهرینیزه حاکم یا وکیل قویوب ؟ \n",
            "2023-01-19 14:20:56,984 - INFO - joeynmt.training - \tReference:  عیسی گفت : ای مرد ، چه کسی مرا میان شما دو نفر ، داور یا میانجی قرار داده است ؟ \n",
            "2023-01-19 14:20:56,984 - INFO - joeynmt.training - \tHypothesis: عیسی در جواب گفت : انسان به چه کسی مرا برخلافی بریده است ؟\n",
            "2023-01-19 14:20:56,984 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:20:56,986 - INFO - joeynmt.training - \tSource:      اونلارێ رازێ قالاجاقلاری بیر یئره داخل ائدهجکدیر . حقیقتا ، آللاه بیلندیر ، هلیمدیر ! \n",
            "2023-01-19 14:20:56,986 - INFO - joeynmt.training - \tReference:  آنان را به جایگاهی که آن را می‌پسندند درخواهد آورد ، و شک نیست که خداوند دانایی بردبار است . \n",
            "2023-01-19 14:20:56,986 - INFO - joeynmt.training - \tHypothesis: و آنان را خشنودی آنان را دنبال می کند ، و قطعا خدا دانای سنجیده کار است .\n",
            "2023-01-19 14:21:04,690 - INFO - joeynmt.training - Epoch 354, Step:   123100, Batch Loss:     1.505550, Batch Acc: 0.596890, Tokens per Sec:    15436, Lr: 0.000025\n",
            "2023-01-19 14:21:12,428 - INFO - joeynmt.training - Epoch 354, Step:   123200, Batch Loss:     1.494633, Batch Acc: 0.594813, Tokens per Sec:    15356, Lr: 0.000025\n",
            "2023-01-19 14:21:14,614 - INFO - joeynmt.training - Epoch 354: total training loss 532.17\n",
            "2023-01-19 14:21:14,615 - INFO - joeynmt.training - EPOCH 355\n",
            "2023-01-19 14:21:20,206 - INFO - joeynmt.training - Epoch 355, Step:   123300, Batch Loss:     1.390959, Batch Acc: 0.598041, Tokens per Sec:    15629, Lr: 0.000025\n",
            "2023-01-19 14:21:27,855 - INFO - joeynmt.training - Epoch 355, Step:   123400, Batch Loss:     1.627255, Batch Acc: 0.596479, Tokens per Sec:    15750, Lr: 0.000025\n",
            "2023-01-19 14:21:35,528 - INFO - joeynmt.training - Epoch 355, Step:   123500, Batch Loss:     1.511736, Batch Acc: 0.596967, Tokens per Sec:    15935, Lr: 0.000025\n",
            "2023-01-19 14:21:41,484 - INFO - joeynmt.training - Epoch 355: total training loss 533.14\n",
            "2023-01-19 14:21:41,484 - INFO - joeynmt.training - EPOCH 356\n",
            "2023-01-19 14:21:43,355 - INFO - joeynmt.training - Epoch 356, Step:   123600, Batch Loss:     1.454662, Batch Acc: 0.598530, Tokens per Sec:    15573, Lr: 0.000025\n",
            "2023-01-19 14:21:51,059 - INFO - joeynmt.training - Epoch 356, Step:   123700, Batch Loss:     1.567393, Batch Acc: 0.596658, Tokens per Sec:    15606, Lr: 0.000025\n",
            "2023-01-19 14:21:58,612 - INFO - joeynmt.training - Epoch 356, Step:   123800, Batch Loss:     1.574032, Batch Acc: 0.596678, Tokens per Sec:    15807, Lr: 0.000025\n",
            "2023-01-19 14:22:06,215 - INFO - joeynmt.training - Epoch 356, Step:   123900, Batch Loss:     1.486644, Batch Acc: 0.598032, Tokens per Sec:    15971, Lr: 0.000025\n",
            "2023-01-19 14:22:08,200 - INFO - joeynmt.training - Epoch 356: total training loss 534.01\n",
            "2023-01-19 14:22:08,200 - INFO - joeynmt.training - EPOCH 357\n",
            "2023-01-19 14:22:13,983 - INFO - joeynmt.training - Epoch 357, Step:   124000, Batch Loss:     1.552061, Batch Acc: 0.600726, Tokens per Sec:    15677, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.86ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 4996.60ex/s]\n",
            "2023-01-19 14:22:14,403 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=124000\n",
            "2023-01-19 14:22:14,403 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:22:19,676 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:22:19,676 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:22:19,677 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:22:19,677 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:22:19,680 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  10.03, loss:   2.75, ppl:  15.70, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.2269[sec], evaluation: 0.0425[sec]\n",
            "2023-01-19 14:22:19,683 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:22:19,686 - INFO - joeynmt.training - \tSource:     اونلارێن خرجلهدیکلرینین قبول اولونماسێنا مانع اولان یالنیز آللاهێ وه اونون پیغمبرینی اینکار ائتمهلری ، نامازا تنبل تنبل گلمهلری وه ایستهمهیه ایستهمهیه خرجلهمهلریدیر . \n",
            "2023-01-19 14:22:19,687 - INFO - joeynmt.training - \tReference:  و هیچ چیز مانع پذیرفته شدن انفاقهای آنان نشد جز اینکه به خدا و پیامبرش کفر ورزیدند ، و جز با حال‌ کسالت نماز به جا نمی‌آورند ، و جز با کراهت انفاق نمی‌کنند . \n",
            "2023-01-19 14:22:19,687 - INFO - joeynmt.training - \tHypothesis: و آنچه را که از ایشان مجادله می کنند ، برای آنان خدا و فرستاده اش کفر ورزد ، و بر نماز برپا دارند و بر نماز برپا دارند ، و هراس ورزد بر خود اذن آنها ، و هر که بخواهد بر خود گواهی دهند .\n",
            "2023-01-19 14:22:19,687 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:22:19,689 - INFO - joeynmt.training - \tSource:     بونو گۆنبهگۆن تکرار ائتدی . بو ، پاولو چوخ بئزدیردی . نهایت ، او چئوریلیب روحا دئدی : ایسا مسیحین آدێ ایله سنه امر ائدیرم ، بو قێزدان چێخ ! روح درحال قێزدان چێخدێ . \n",
            "2023-01-19 14:22:19,689 - INFO - joeynmt.training - \tReference:  او روزهای بسیار به این کار ادامه داد . سرانجام پولس به ستوه آمد و برگشت و خطاب به آن روح گفت : به نام عیسی مسیح به تو فرمان می‌دهم که از او بیرون آیی . روح همان دم بیرون آمد . \n",
            "2023-01-19 14:22:19,689 - INFO - joeynmt.training - \tHypothesis: آن روز این را به تفرقه کرد . بسیاری از این رو ، پولس را با سخنانی که فورا از روح بیرون می آمد ، به او گفت : روح ، روحیهٔ او از او بیرون می آید . او روحی بیرون را از این امر بیرون می کرد .\n",
            "2023-01-19 14:22:19,689 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:22:19,691 - INFO - joeynmt.training - \tSource:      بویورار : ائلهدیر ، آمما سنه آیهلریمیز گلدی ، سن ایسه اونلارێ اونوتدون . بو گۆن ائلهجه ده سن اونودولاجاقسان ! \n",
            "2023-01-19 14:22:19,692 - INFO - joeynmt.training - \tReference:  می‌فرماید : همان طور که نشانه‌های ما بر تو آمد و آن را به فراموشی سپردی ، امروز همان گونه فراموش می‌شوی . \n",
            "2023-01-19 14:22:19,692 - INFO - joeynmt.training - \tHypothesis: فرمود : آن کس که آیات ما را به تو آمد ، و آنان را در روزگار خود به تو رسد ، امروز او را خواهی کرد .\n",
            "2023-01-19 14:22:19,692 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:22:19,694 - INFO - joeynmt.training - \tSource:     آند اولسون شهادت وئرهنه وه شهادت وئریلهنه کی ، \n",
            "2023-01-19 14:22:19,694 - INFO - joeynmt.training - \tReference:  و به گواه و مورد گواهی ، \n",
            "2023-01-19 14:22:19,694 - INFO - joeynmt.training - \tHypothesis: و به شهادت داده شد که :\n",
            "2023-01-19 14:22:27,385 - INFO - joeynmt.training - Epoch 357, Step:   124100, Batch Loss:     1.626474, Batch Acc: 0.598378, Tokens per Sec:    14978, Lr: 0.000025\n",
            "2023-01-19 14:22:35,106 - INFO - joeynmt.training - Epoch 357, Step:   124200, Batch Loss:     1.518647, Batch Acc: 0.594826, Tokens per Sec:    15591, Lr: 0.000025\n",
            "2023-01-19 14:22:40,707 - INFO - joeynmt.training - Epoch 357: total training loss 529.73\n",
            "2023-01-19 14:22:40,708 - INFO - joeynmt.training - EPOCH 358\n",
            "2023-01-19 14:22:42,884 - INFO - joeynmt.training - Epoch 358, Step:   124300, Batch Loss:     1.563019, Batch Acc: 0.597011, Tokens per Sec:    15160, Lr: 0.000025\n",
            "2023-01-19 14:22:50,689 - INFO - joeynmt.training - Epoch 358, Step:   124400, Batch Loss:     1.572036, Batch Acc: 0.598938, Tokens per Sec:    15415, Lr: 0.000025\n",
            "2023-01-19 14:22:58,407 - INFO - joeynmt.training - Epoch 358, Step:   124500, Batch Loss:     1.581521, Batch Acc: 0.597425, Tokens per Sec:    15598, Lr: 0.000025\n",
            "2023-01-19 14:23:06,012 - INFO - joeynmt.training - Epoch 358, Step:   124600, Batch Loss:     1.558763, Batch Acc: 0.596005, Tokens per Sec:    15828, Lr: 0.000025\n",
            "2023-01-19 14:23:08,332 - INFO - joeynmt.training - Epoch 358: total training loss 535.31\n",
            "2023-01-19 14:23:08,332 - INFO - joeynmt.training - EPOCH 359\n",
            "2023-01-19 14:23:14,718 - INFO - joeynmt.training - Epoch 359, Step:   124700, Batch Loss:     1.484042, Batch Acc: 0.597087, Tokens per Sec:    14744, Lr: 0.000025\n",
            "2023-01-19 14:23:22,432 - INFO - joeynmt.training - Epoch 359, Step:   124800, Batch Loss:     1.433337, Batch Acc: 0.599130, Tokens per Sec:    15433, Lr: 0.000025\n",
            "2023-01-19 14:23:30,112 - INFO - joeynmt.training - Epoch 359, Step:   124900, Batch Loss:     1.556539, Batch Acc: 0.598573, Tokens per Sec:    15896, Lr: 0.000025\n",
            "2023-01-19 14:23:35,488 - INFO - joeynmt.training - Epoch 359: total training loss 530.96\n",
            "2023-01-19 14:23:35,489 - INFO - joeynmt.training - EPOCH 360\n",
            "2023-01-19 14:23:37,801 - INFO - joeynmt.training - Epoch 360, Step:   125000, Batch Loss:     1.525127, Batch Acc: 0.599157, Tokens per Sec:    15699, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 63.52ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10120.57ex/s]\n",
            "2023-01-19 14:23:38,097 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=125000\n",
            "2023-01-19 14:23:38,097 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:23:43,796 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:23:43,797 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:23:43,797 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:23:43,798 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:23:43,801 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.19, loss:   2.85, ppl:  17.37, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.6528[sec], evaluation: 0.0430[sec]\n",
            "2023-01-19 14:23:43,803 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:23:43,806 - INFO - joeynmt.training - \tSource:     سن مصیحده مالک اولدوغوموز هر خئیرلی شئیی درک ائتدیکجه ایمانینی باشقالاری ایله پایلاشماقدا فعال اولماغێن اۆچۆن دوعا ائدیرم . \n",
            "2023-01-19 14:23:43,807 - INFO - joeynmt.training - \tReference:  دعایم این است ، همین ایمان که تو و هم‌ایمانانت از آن برخوردارید ، تو را به درک تمامی برکاتی برساند که از طریق مسیح نصیبمان شده است . \n",
            "2023-01-19 14:23:43,807 - INFO - joeynmt.training - \tHypothesis: تو در اتحاد با مسیح هستی که در اتحاد با مسیح هستی ، ایمان خود را به درستی رهایی دادیم تا با همان ایمانی که با مسیحْ عیسی عمل کرده اند ، برای همیشه دعای تقویت کنیم .\n",
            "2023-01-19 14:23:43,807 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:23:43,809 - INFO - joeynmt.training - \tSource:     بئلهجه اونو باغدان کنارا چێخارێب اؤلدوردولر . بئلهلیکله ، باغ صاحبی اونلارا نه ائدهجک ؟ \n",
            "2023-01-19 14:23:43,809 - INFO - joeynmt.training - \tReference:  پس او را گرفتند ، از تاکستان بیرون انداختند و کشتند . با این وصف ، مالک تاکستان با آن باغبانان چه خواهد کرد ؟ \n",
            "2023-01-19 14:23:43,809 - INFO - joeynmt.training - \tHypothesis: پس او را از باغبانان بیرون انداختند و تا تاکستان را بکشند ، پس چگونه می توانند ؟\n",
            "2023-01-19 14:23:43,809 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:23:43,811 - INFO - joeynmt.training - \tSource:     ائی آغێلسێز آدام ، املسیز ایمانین پوچ اولدوغونا سۆبوت ایستهییرسنمی ؟ \n",
            "2023-01-19 14:23:43,811 - INFO - joeynmt.training - \tReference:  ای نادان ، آیا می‌خواهی بدانی که چرا ایمان ، بدون عمل بی‌فایده است ؟ \n",
            "2023-01-19 14:23:43,812 - INFO - joeynmt.training - \tHypothesis: ای خردمندان ، آیا می خواهی که خردمنش را به جا آوردن اعمال نامشروع کرده ای ؟\n",
            "2023-01-19 14:23:43,812 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:23:43,814 - INFO - joeynmt.training - \tSource:     گئدین وه من قوربان دئییل ، مرحمت ایستهییرم سؤزونون مناسێنێ اؤیرهنین . چۆنکی من سالهلری دئییل ، گۆناهکارلاری چاغێرماق اۆچۆن گلمیشم . \n",
            "2023-01-19 14:23:43,814 - INFO - joeynmt.training - \tReference:  بروید و مفهوم این گفته را درک کنید : رحمت می‌خواهم ، نه قربانی . چون آمده‌ام تا گناهکاران را فراخوانم ، نه درستکاران را . \n",
            "2023-01-19 14:23:43,814 - INFO - joeynmt.training - \tHypothesis: بروید و من از آنچه می خواهم ، از رحمت رحمت و هنوز به آن تعلیم می دهم ؛ زیرا من گناهکاران را فراخوانده ام تا گناهکاران را فراخوانده ام .\n",
            "2023-01-19 14:23:51,536 - INFO - joeynmt.training - Epoch 360, Step:   125100, Batch Loss:     1.539703, Batch Acc: 0.600038, Tokens per Sec:    15060, Lr: 0.000025\n",
            "2023-01-19 14:23:59,295 - INFO - joeynmt.training - Epoch 360, Step:   125200, Batch Loss:     1.441155, Batch Acc: 0.595628, Tokens per Sec:    15517, Lr: 0.000025\n",
            "2023-01-19 14:24:06,974 - INFO - joeynmt.training - Epoch 360, Step:   125300, Batch Loss:     1.467462, Batch Acc: 0.598764, Tokens per Sec:    15724, Lr: 0.000025\n",
            "2023-01-19 14:24:08,423 - INFO - joeynmt.training - Epoch 360: total training loss 530.66\n",
            "2023-01-19 14:24:08,424 - INFO - joeynmt.training - EPOCH 361\n",
            "2023-01-19 14:24:14,722 - INFO - joeynmt.training - Epoch 361, Step:   125400, Batch Loss:     1.448465, Batch Acc: 0.601156, Tokens per Sec:    15692, Lr: 0.000025\n",
            "2023-01-19 14:24:22,460 - INFO - joeynmt.training - Epoch 361, Step:   125500, Batch Loss:     1.556012, Batch Acc: 0.597982, Tokens per Sec:    15600, Lr: 0.000025\n",
            "2023-01-19 14:24:30,135 - INFO - joeynmt.training - Epoch 361, Step:   125600, Batch Loss:     1.501242, Batch Acc: 0.597539, Tokens per Sec:    15801, Lr: 0.000025\n",
            "2023-01-19 14:24:35,178 - INFO - joeynmt.training - Epoch 361: total training loss 529.10\n",
            "2023-01-19 14:24:35,178 - INFO - joeynmt.training - EPOCH 362\n",
            "2023-01-19 14:24:37,970 - INFO - joeynmt.training - Epoch 362, Step:   125700, Batch Loss:     1.515233, Batch Acc: 0.596205, Tokens per Sec:    15462, Lr: 0.000025\n",
            "2023-01-19 14:24:45,689 - INFO - joeynmt.training - Epoch 362, Step:   125800, Batch Loss:     1.449246, Batch Acc: 0.599872, Tokens per Sec:    15814, Lr: 0.000025\n",
            "2023-01-19 14:24:53,307 - INFO - joeynmt.training - Epoch 362, Step:   125900, Batch Loss:     1.511823, Batch Acc: 0.598306, Tokens per Sec:    15903, Lr: 0.000025\n",
            "2023-01-19 14:25:00,944 - INFO - joeynmt.training - Epoch 362, Step:   126000, Batch Loss:     1.453288, Batch Acc: 0.597783, Tokens per Sec:    15830, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.50ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10219.70ex/s]\n",
            "2023-01-19 14:25:01,215 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=126000\n",
            "2023-01-19 14:25:01,215 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:25:05,843 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:25:05,844 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:25:05,844 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:25:05,845 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:25:05,848 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.99, loss:   2.70, ppl:  14.89, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5815[sec], evaluation: 0.0432[sec]\n",
            "2023-01-19 14:25:05,851 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:25:05,854 - INFO - joeynmt.training - \tSource:     نورونوز وار ایکن نورا ایمان ائدین کی ، نور اؤؤلادلارێ اولاسێنێز . ایسا بو سؤزلری سؤیلهیندن سونرا گئدیب اونلاردان گیزلندی . \n",
            "2023-01-19 14:25:05,854 - INFO - joeynmt.training - \tReference:  تا زمانی که نور را دارید ، به نور ایمان بورزید تا پسران نور شوید . عیسی پس از گفتن این سخنان آنجا را ترک کرد و خود را از آنان پنهان ساخت . \n",
            "2023-01-19 14:25:05,855 - INFO - joeynmt.training - \tHypothesis: نور را روشن کنید تا پسران نور را درخش کنید ؛ زیرا به این ترتیب ، کلامی که از این سخنان پیروی کنید ، از ایشان خواهد رفت .\n",
            "2023-01-19 14:25:05,855 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:25:05,857 - INFO - joeynmt.training - \tSource:     کۆلکلری رحمتی اؤنونده مۆژدهچی اولاراق گؤندهرن ده اودور . بیز گؤیدن ترتمیز سو ائندیردیک کی ، \n",
            "2023-01-19 14:25:05,857 - INFO - joeynmt.training - \tReference:  و اوست آن کس که بادها را نویدی پیشاپیش رحمت خویش باران‌ فرستاد و از آسمان ، آبی پاک فرود آوردیم ، \n",
            "2023-01-19 14:25:05,857 - INFO - joeynmt.training - \tHypothesis: اوست آن کس که باد باد مژده می آید ، و ما از آسمانی فرستاده ای از آسمان فرود آوردیم تا از آسمان آبی فرود آوردیم .\n",
            "2023-01-19 14:25:05,857 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:25:05,859 - INFO - joeynmt.training - \tSource:     مگر اونلار آللاهێن مخلوقاتێ اولجه نئجه یاراتدیغینی ، سونرا دا اونو یئنیدن دیریلدهجهیینی بیلمیرلرمی ؟ حقیقتا ، بو ، آللاه اۆچۆن آساندێر ! \n",
            "2023-01-19 14:25:05,859 - INFO - joeynmt.training - \tReference:  آیا ندیده‌اند که خدا چگونه آفرینش را آغاز می‌کند سپس آن را باز می‌گرداند ؟ در حقیقت ، این کار بر خدا آسان است . \n",
            "2023-01-19 14:25:05,859 - INFO - joeynmt.training - \tHypothesis: آیا نمی دانند که خدا و آنچه را که آفرینش را آغاز کرده و باز آن را زنده می گرداند ؟ قطعا این است خدا آسان است .\n",
            "2023-01-19 14:25:05,860 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:25:05,862 - INFO - joeynmt.training - \tSource:     وه یا ازابێ گؤردوگو زامان : کاش بیر دفه ده قاییدا بیلهیدیم ؛ یاخشی ایشلر گؤرنلردن اولاردێم ! دئمهسین ! \n",
            "2023-01-19 14:25:05,862 - INFO - joeynmt.training - \tReference:  یا چون عذاب را ببیند ، بگوید : کاش مرا برگشتی بود تا از نیکوکاران می‌شدم . \n",
            "2023-01-19 14:25:05,862 - INFO - joeynmt.training - \tHypothesis: و چون عذاب را دید که : کاشی عذاب بر من واجب شده است ، پس به کار شایسته رسیده است .\n",
            "2023-01-19 14:25:06,737 - INFO - joeynmt.training - Epoch 362: total training loss 526.90\n",
            "2023-01-19 14:25:06,737 - INFO - joeynmt.training - EPOCH 363\n",
            "2023-01-19 14:25:13,613 - INFO - joeynmt.training - Epoch 363, Step:   126100, Batch Loss:     1.526043, Batch Acc: 0.603974, Tokens per Sec:    15735, Lr: 0.000025\n",
            "2023-01-19 14:25:21,380 - INFO - joeynmt.training - Epoch 363, Step:   126200, Batch Loss:     1.540616, Batch Acc: 0.597157, Tokens per Sec:    15535, Lr: 0.000025\n",
            "2023-01-19 14:25:29,005 - INFO - joeynmt.training - Epoch 363, Step:   126300, Batch Loss:     1.480846, Batch Acc: 0.596870, Tokens per Sec:    15662, Lr: 0.000025\n",
            "2023-01-19 14:25:33,659 - INFO - joeynmt.training - Epoch 363: total training loss 531.31\n",
            "2023-01-19 14:25:33,659 - INFO - joeynmt.training - EPOCH 364\n",
            "2023-01-19 14:25:36,773 - INFO - joeynmt.training - Epoch 364, Step:   126400, Batch Loss:     1.574171, Batch Acc: 0.600291, Tokens per Sec:    15694, Lr: 0.000025\n",
            "2023-01-19 14:25:44,493 - INFO - joeynmt.training - Epoch 364, Step:   126500, Batch Loss:     1.397979, Batch Acc: 0.599275, Tokens per Sec:    15870, Lr: 0.000025\n",
            "2023-01-19 14:25:52,193 - INFO - joeynmt.training - Epoch 364, Step:   126600, Batch Loss:     1.620129, Batch Acc: 0.597356, Tokens per Sec:    15770, Lr: 0.000025\n",
            "2023-01-19 14:25:59,839 - INFO - joeynmt.training - Epoch 364, Step:   126700, Batch Loss:     1.606768, Batch Acc: 0.597698, Tokens per Sec:    15794, Lr: 0.000025\n",
            "2023-01-19 14:26:00,291 - INFO - joeynmt.training - Epoch 364: total training loss 525.14\n",
            "2023-01-19 14:26:00,291 - INFO - joeynmt.training - EPOCH 365\n",
            "2023-01-19 14:26:07,825 - INFO - joeynmt.training - Epoch 365, Step:   126800, Batch Loss:     1.582032, Batch Acc: 0.599264, Tokens per Sec:    15104, Lr: 0.000025\n",
            "2023-01-19 14:26:15,523 - INFO - joeynmt.training - Epoch 365, Step:   126900, Batch Loss:     1.558848, Batch Acc: 0.599793, Tokens per Sec:    15818, Lr: 0.000025\n",
            "2023-01-19 14:26:23,243 - INFO - joeynmt.training - Epoch 365, Step:   127000, Batch Loss:     1.675498, Batch Acc: 0.598087, Tokens per Sec:    15659, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.04ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8671.59ex/s]\n",
            "2023-01-19 14:26:23,537 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=127000\n",
            "2023-01-19 14:26:23,538 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:26:28,568 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:26:28,568 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:26:28,569 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:26:28,570 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:26:28,572 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.65, loss:   2.71, ppl:  14.96, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9825[sec], evaluation: 0.0445[sec]\n",
            "2023-01-19 14:26:28,575 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:26:28,579 - INFO - joeynmt.training - \tSource:     سیزدن کیم سۆبوت ائدر کی ، من بیر گۆناه ائتمیشم ؟ من حقیقتی سؤیلهییرهمسه ، نیه منه اینانمیرسینیز ؟ \n",
            "2023-01-19 14:26:28,579 - INFO - joeynmt.training - \tReference:  کدام یک از شما می‌تواند مرا به گناه محکوم کند ؟ پس اگر من حقیقت را می‌گویم ، چرا سخن مرا باور نمی‌کنید ؟ \n",
            "2023-01-19 14:26:28,579 - INFO - joeynmt.training - \tHypothesis: و هر که سلیمان را که به شما می رسم ، گناهی مرتکب گناه می شوم ؟ چرا به من ایمان نمی آورید ؟\n",
            "2023-01-19 14:26:28,579 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:26:28,581 - INFO - joeynmt.training - \tSource:      آللاها ، پیغمبره ایمان گتیردیک ، ایتاعت ائتدیک ! دئیر ، بوندان سونرا ایسه اونلاردان بیر دسته اۆز چئویرر . بئلهلری معؤ مین دئییللر . \n",
            "2023-01-19 14:26:28,581 - INFO - joeynmt.training - \tReference:  و می‌گویند : به خدا و پیامبر او گرویدیم و اطاعت کردیم . آنگاه دسته‌ای از ایشان پس از این اقرار روی برمی‌گردانند ، و آنان مؤمن نیستند . \n",
            "2023-01-19 14:26:28,582 - INFO - joeynmt.training - \tHypothesis: و به خدا و پیامبر او ایمان آوردیم ، می گوید : پس از این گروه ، از آنان روی برتاب ، و از آنان روی برتابند .\n",
            "2023-01-19 14:26:28,582 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:26:28,584 - INFO - joeynmt.training - \tSource:     کی واختیندا پیس بیتیشیب أتی نین یاریقی گؤزه ویریردی\n",
            "2023-01-19 14:26:28,584 - INFO - joeynmt.training - \tReference:  كه بد جوش خورده بود و گوشت سرخ از لای شیارهای صورتش برق میزد \n",
            "2023-01-19 14:26:28,584 - INFO - joeynmt.training - \tHypothesis: كه موعكه یته یك و كوكك می كرد\n",
            "2023-01-19 14:26:28,584 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:26:28,586 - INFO - joeynmt.training - \tSource:     سونرا ایسا دئدی : آللاهێن پادشاهلێغێ تورپاغا توخوم سپن اکینچیه بنزهییر . \n",
            "2023-01-19 14:26:28,586 - INFO - joeynmt.training - \tReference:  پس از آن چنین افزود : پادشاهی خدا را می‌توان این گونه تشبیه کرد ؛ مردی بذر بر زمین می‌پاشد ، \n",
            "2023-01-19 14:26:28,586 - INFO - joeynmt.training - \tHypothesis: سپس عیسی گفت : پادشاهی خدا مانند کاشتهٔ خاک خوب است .\n",
            "2023-01-19 14:26:32,687 - INFO - joeynmt.training - Epoch 365: total training loss 530.23\n",
            "2023-01-19 14:26:32,688 - INFO - joeynmt.training - EPOCH 366\n",
            "2023-01-19 14:26:36,740 - INFO - joeynmt.training - Epoch 366, Step:   127100, Batch Loss:     1.422166, Batch Acc: 0.600600, Tokens per Sec:    14322, Lr: 0.000025\n",
            "2023-01-19 14:26:45,298 - INFO - joeynmt.training - Epoch 366, Step:   127200, Batch Loss:     1.514724, Batch Acc: 0.601487, Tokens per Sec:    14068, Lr: 0.000025\n",
            "2023-01-19 14:26:53,048 - INFO - joeynmt.training - Epoch 366, Step:   127300, Batch Loss:     1.658894, Batch Acc: 0.596347, Tokens per Sec:    15623, Lr: 0.000025\n",
            "2023-01-19 14:27:00,730 - INFO - joeynmt.training - Epoch 366: total training loss 525.63\n",
            "2023-01-19 14:27:00,731 - INFO - joeynmt.training - EPOCH 367\n",
            "2023-01-19 14:27:00,811 - INFO - joeynmt.training - Epoch 367, Step:   127400, Batch Loss:     1.500997, Batch Acc: 0.613712, Tokens per Sec:    15038, Lr: 0.000025\n",
            "2023-01-19 14:27:08,581 - INFO - joeynmt.training - Epoch 367, Step:   127500, Batch Loss:     1.564115, Batch Acc: 0.598412, Tokens per Sec:    15563, Lr: 0.000025\n",
            "2023-01-19 14:27:16,289 - INFO - joeynmt.training - Epoch 367, Step:   127600, Batch Loss:     1.502568, Batch Acc: 0.596586, Tokens per Sec:    15621, Lr: 0.000025\n",
            "2023-01-19 14:27:24,007 - INFO - joeynmt.training - Epoch 367, Step:   127700, Batch Loss:     1.532218, Batch Acc: 0.598481, Tokens per Sec:    15683, Lr: 0.000025\n",
            "2023-01-19 14:27:27,624 - INFO - joeynmt.training - Epoch 367: total training loss 529.24\n",
            "2023-01-19 14:27:27,624 - INFO - joeynmt.training - EPOCH 368\n",
            "2023-01-19 14:27:31,772 - INFO - joeynmt.training - Epoch 368, Step:   127800, Batch Loss:     1.514220, Batch Acc: 0.597717, Tokens per Sec:    15691, Lr: 0.000025\n",
            "2023-01-19 14:27:39,591 - INFO - joeynmt.training - Epoch 368, Step:   127900, Batch Loss:     1.606205, Batch Acc: 0.600062, Tokens per Sec:    15648, Lr: 0.000025\n",
            "2023-01-19 14:27:47,613 - INFO - joeynmt.training - Epoch 368, Step:   128000, Batch Loss:     1.450347, Batch Acc: 0.598992, Tokens per Sec:    14888, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.51ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10391.75ex/s]\n",
            "2023-01-19 14:27:47,900 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=128000\n",
            "2023-01-19 14:27:47,900 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:27:53,034 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:27:53,034 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:27:53,034 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:27:53,035 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:27:53,038 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.12, loss:   2.82, ppl:  16.82, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0859[sec], evaluation: 0.0440[sec]\n",
            "2023-01-19 14:27:53,041 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:27:53,044 - INFO - joeynmt.training - \tSource:      سن اؤز ربینین هؤکمونه صبر ائت ! شبههسیز کی ، سن بیزیم گؤزوموزون قاباغێنداسان . قالخدێقدا ربینی همد سنا ایله زیکر ائت ! \n",
            "2023-01-19 14:27:53,044 - INFO - joeynmt.training - \tReference:  و در برابر دستور پروردگارت شکیبایی پیشه کن که تو خود در حمایت مایی و هنگامی که از خواب‌ بر می‌خیزی به نیایش پروردگارت تسبیح گوی . \n",
            "2023-01-19 14:27:53,045 - INFO - joeynmt.training - \tHypothesis: و تو صبر کن که پروردگارت شکیبایی کن ، زیرا که تو چشمه ای . پس چون تو را پیش از بر تخت پروردگارت و زنهار ، یاد کن .\n",
            "2023-01-19 14:27:53,045 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:27:53,047 - INFO - joeynmt.training - \tSource:      آللاه : دینی دوغرو دۆرۆست توتون ، اوندا آیریلیغا دۆشمهیین ! دئیه نوحا تؤؤسیه ائتدیگینی ، سنه وحی بویوردوغونو ، ابراهیمه ، موسایا ، وه ایسهآیا تؤؤسیه ائتدیگینی دینده سیزین اۆچۆن ده قانونی ائتدی . سنین ده وط ائتدیگین مۆشرکلره آغێر گلدی . آللاه ایستهدیگی کیمسنی اؤزونه سئچر وه تؤؤبه ائدیب اونا طرف قاییدان کیمسنی ده دوغرو یولا یؤنلدر ! \n",
            "2023-01-19 14:27:53,047 - INFO - joeynmt.training - \tReference:  از احکام‌ دین ، آنچه را که به نوح در باره آن سفارش کرد ، برای شما تشریع کرد و آنچه را به تو وحی کردیم و آنچه را که در باره آن به ابراهیم و موسی و عیسی سفارش نمودیم که : دین را برپا دارید و در آن تفرقه‌اندازی مکنید . بر مشرکان آنچه که ایشان را به سوی آن فرا می‌خوانی ، گران می‌آید . خدا هر که را بخواهد ، به سوی خود برمی‌گزیند ، و هر که را که از در توبه درآید ، به سوی خود راه می‌نماید . \n",
            "2023-01-19 14:27:53,047 - INFO - joeynmt.training - \tHypothesis: و خدا می گویند : راست می فرستید : پس تو را برمی گزید و به راستی ابراهیم و موسی و ابراهیم را به تو وحی کرده است . و به راستی که موسی و موسی و موسی و عیسی و دینش را به جا آورده ای ، و هر که را بخواهد به سوی تو وحی کرده ای ، به سوی دانای نهفت .\n",
            "2023-01-19 14:27:53,047 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:27:53,049 - INFO - joeynmt.training - \tSource:     آنانا وحی اولوناجاق شئیی وحی ائتدیگیمیز زامان . \n",
            "2023-01-19 14:27:53,049 - INFO - joeynmt.training - \tReference:  هنگامی که به مادرت آنچه را که باید وحی می‌شد وحی کردیم : \n",
            "2023-01-19 14:27:53,050 - INFO - joeynmt.training - \tHypothesis: و چون به او وحی می کردیم ، وحی کن که آنگاه که او به جای او وحی می کردیم .\n",
            "2023-01-19 14:27:53,050 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:27:53,052 - INFO - joeynmt.training - \tSource:     شاگردلر ایسنانین دئدیکلرینی اونلارا تکرارلایاندا او آداملار شاگردلره امکان وئردی . \n",
            "2023-01-19 14:27:53,052 - INFO - joeynmt.training - \tReference:  شاگردان درست همان طور که عیسی گفته بود ، پاسخ دادند و آنان نیز اجازه دادند که آن را ببرند . \n",
            "2023-01-19 14:27:53,052 - INFO - joeynmt.training - \tHypothesis: شاگردانش به آنان گفتند : در خلوت را در خلوت ، شاگردانش به آنان داد .\n",
            "2023-01-19 14:28:00,436 - INFO - joeynmt.training - Epoch 368: total training loss 527.67\n",
            "2023-01-19 14:28:00,436 - INFO - joeynmt.training - EPOCH 369\n",
            "2023-01-19 14:28:00,911 - INFO - joeynmt.training - Epoch 369, Step:   128100, Batch Loss:     1.435975, Batch Acc: 0.611453, Tokens per Sec:    15090, Lr: 0.000025\n",
            "2023-01-19 14:28:08,939 - INFO - joeynmt.training - Epoch 369, Step:   128200, Batch Loss:     1.349825, Batch Acc: 0.603185, Tokens per Sec:    15035, Lr: 0.000025\n",
            "2023-01-19 14:28:16,765 - INFO - joeynmt.training - Epoch 369, Step:   128300, Batch Loss:     1.430663, Batch Acc: 0.599639, Tokens per Sec:    15419, Lr: 0.000025\n",
            "2023-01-19 14:28:24,588 - INFO - joeynmt.training - Epoch 369, Step:   128400, Batch Loss:     1.415228, Batch Acc: 0.598136, Tokens per Sec:    15403, Lr: 0.000025\n",
            "2023-01-19 14:28:27,934 - INFO - joeynmt.training - Epoch 369: total training loss 527.93\n",
            "2023-01-19 14:28:27,934 - INFO - joeynmt.training - EPOCH 370\n",
            "2023-01-19 14:28:32,411 - INFO - joeynmt.training - Epoch 370, Step:   128500, Batch Loss:     1.492084, Batch Acc: 0.602071, Tokens per Sec:    15335, Lr: 0.000025\n",
            "2023-01-19 14:28:40,184 - INFO - joeynmt.training - Epoch 370, Step:   128600, Batch Loss:     1.566009, Batch Acc: 0.600768, Tokens per Sec:    15556, Lr: 0.000025\n",
            "2023-01-19 14:28:48,101 - INFO - joeynmt.training - Epoch 370, Step:   128700, Batch Loss:     1.510226, Batch Acc: 0.597282, Tokens per Sec:    15264, Lr: 0.000025\n",
            "2023-01-19 14:28:55,326 - INFO - joeynmt.training - Epoch 370: total training loss 528.12\n",
            "2023-01-19 14:28:55,327 - INFO - joeynmt.training - EPOCH 371\n",
            "2023-01-19 14:28:56,108 - INFO - joeynmt.training - Epoch 371, Step:   128800, Batch Loss:     1.582243, Batch Acc: 0.590541, Tokens per Sec:    15520, Lr: 0.000025\n",
            "2023-01-19 14:29:03,782 - INFO - joeynmt.training - Epoch 371, Step:   128900, Batch Loss:     1.507911, Batch Acc: 0.599527, Tokens per Sec:    15749, Lr: 0.000025\n",
            "2023-01-19 14:29:11,541 - INFO - joeynmt.training - Epoch 371, Step:   129000, Batch Loss:     1.626916, Batch Acc: 0.602002, Tokens per Sec:    15595, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 139.81ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9989.84ex/s]\n",
            "2023-01-19 14:29:11,839 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=129000\n",
            "2023-01-19 14:29:11,839 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:29:17,420 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:29:17,421 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:29:17,421 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:29:17,422 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:29:17,425 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.76, loss:   2.75, ppl:  15.59, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.5321[sec], evaluation: 0.0458[sec]\n",
            "2023-01-19 14:29:17,428 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:29:17,432 - INFO - joeynmt.training - \tSource:     ائی ایمان گتیرنلر ! اؤزوندن باشقاسێنێ اؤزونوزه سیرداش توتمایین . اونلار سیزین بارهنیزده فیتنه فصاد تؤرتمکدن ال چکمزلر ، سیزین ازییته دۆشمهیینیزی ایستهییرلر . حقیقتا ، اونلارێن سیزه قارشێ اولان اداوتی آغێزلارێندان چێخان سؤزلردن آشکار اولور . آمما اۆرکلرینده گیزلتدیکلری ایسه داها بؤیوکدور . اگر دۆشۆنۆب درک ائدیرسینیزسه ، آیهلری آرتێق سیزه اضاح ائتدیک . \n",
            "2023-01-19 14:29:17,432 - INFO - joeynmt.training - \tReference:  ای کسانی که ایمان آورده‌اید ، از غیر خودتان ، دوست و همراز مگیرید . آنان‌ از هیچ نابکاری در حق شما کوتاهی نمی‌ورزند . آرزو دارند که در رنج بیفتید . دشمنی از لحن و سخنشان آشکار است ؛ و آنچه سینه‌هایشان نهان می‌دارد ، بزرگتر است . در حقیقت ، ما نشانه‌ها ی دشمنی آنان‌ را برای شما بیان کردیم ، اگر تعقل کنید . \n",
            "2023-01-19 14:29:17,432 - INFO - joeynmt.training - \tHypothesis: ای کسانی که ایمان آورده اید ، به خود خود بر ضد دیگری مخالفت مکنید ، و در آن ، در تقوا با خود خود بر شما حکایت مکنید ، و در این ، با همدیگر از خودتان بر ضد شما بیابد ؛ و اگر می خواهند در دلهایتان تقوا پیشه کرد ، قطعا در دلهایتان برای شما روشن تر است .\n",
            "2023-01-19 14:29:17,432 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:29:17,434 - INFO - joeynmt.training - \tSource:      ادبسیزلیک ائتدیکلری زامان : آتالاریمیزی بئله گؤردۆک . بونو بیزه آللاه امر ائتمیشدیر ، دئییرلر . دئ : آللاه ادبسیزلیک امر ائتمز . آللاها قارشێ بیلمهدیگینیز شئییمی دئییرسینیز ؟ \n",
            "2023-01-19 14:29:17,434 - INFO - joeynmt.training - \tReference:  و چون کار زشتی کنند ، می‌گویند : پدران خود را بر آن یافتیم و خدا ما را بدان فرمان داده است . بگو : قطعا خدا به کار زشت فرمان نمی‌دهد ، آیا چیزی را که نمی‌دانید به خدا نسبت می‌دهید ؟ \n",
            "2023-01-19 14:29:17,435 - INFO - joeynmt.training - \tHypothesis: و چون به بی انتها می گفتند : پدران ما را دیدیم ، به ما فرمان داده است . این را خدا به ما حکم کرده است . بگو : آیا به این فرمان را به خدا نمی گویید که خدا به چیزی نمی گویید ؟\n",
            "2023-01-19 14:29:17,435 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:29:17,437 - INFO - joeynmt.training - \tSource:     ائلهجه ده سمود ، لوت قؤومو ، ایکه اهلی بو فرقهلرین\n",
            "2023-01-19 14:29:17,437 - INFO - joeynmt.training - \tReference:  و ثمود و قوم لوط و اصحاب ایکه نیز به تکذیب پرداختند آنها دسته‌های مخالف بودند . \n",
            "2023-01-19 14:29:17,437 - INFO - joeynmt.training - \tHypothesis: و و قوم من و قوم لوط را ،\n",
            "2023-01-19 14:29:17,437 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:29:17,439 - INFO - joeynmt.training - \tSource:     بس بونلار بارهده نه دئیه بیلهریک ؟ اگر آللاه بیزیملهدیرسه ، کیم بیزه قارشێ دورا بیلر ؟ \n",
            "2023-01-19 14:29:17,439 - INFO - joeynmt.training - \tReference:  پس در مورد این امور چه می‌توان گفت ؟ اگر خدا با ماست ، چه کسی می‌تواند بر ضد ما باشد ؟ \n",
            "2023-01-19 14:29:17,439 - INFO - joeynmt.training - \tHypothesis: پس چه حقی که می توانیم به خدا چه حقی بگوییم ؟ اگر چه کسی با ما بر ما افتد ، چه کسی به ما افتخار می کند ؟\n",
            "2023-01-19 14:29:25,288 - INFO - joeynmt.training - Epoch 371, Step:   129100, Batch Loss:     1.536412, Batch Acc: 0.597937, Tokens per Sec:    14742, Lr: 0.000025\n",
            "2023-01-19 14:29:28,242 - INFO - joeynmt.training - Epoch 371: total training loss 525.42\n",
            "2023-01-19 14:29:28,243 - INFO - joeynmt.training - EPOCH 372\n",
            "2023-01-19 14:29:33,095 - INFO - joeynmt.training - Epoch 372, Step:   129200, Batch Loss:     1.471731, Batch Acc: 0.600111, Tokens per Sec:    15608, Lr: 0.000025\n",
            "2023-01-19 14:29:40,867 - INFO - joeynmt.training - Epoch 372, Step:   129300, Batch Loss:     1.460257, Batch Acc: 0.601251, Tokens per Sec:    15404, Lr: 0.000025\n",
            "2023-01-19 14:29:48,564 - INFO - joeynmt.training - Epoch 372, Step:   129400, Batch Loss:     1.461558, Batch Acc: 0.597906, Tokens per Sec:    15609, Lr: 0.000025\n",
            "2023-01-19 14:29:55,289 - INFO - joeynmt.training - Epoch 372: total training loss 528.96\n",
            "2023-01-19 14:29:55,289 - INFO - joeynmt.training - EPOCH 373\n",
            "2023-01-19 14:29:56,412 - INFO - joeynmt.training - Epoch 373, Step:   129500, Batch Loss:     1.582544, Batch Acc: 0.602927, Tokens per Sec:    15096, Lr: 0.000025\n",
            "2023-01-19 14:30:04,010 - INFO - joeynmt.training - Epoch 373, Step:   129600, Batch Loss:     1.489648, Batch Acc: 0.602380, Tokens per Sec:    15926, Lr: 0.000025\n",
            "2023-01-19 14:30:12,824 - INFO - joeynmt.training - Epoch 373, Step:   129700, Batch Loss:     1.501447, Batch Acc: 0.601558, Tokens per Sec:    13591, Lr: 0.000025\n",
            "2023-01-19 14:30:20,707 - INFO - joeynmt.training - Epoch 373, Step:   129800, Batch Loss:     1.467869, Batch Acc: 0.600369, Tokens per Sec:    15215, Lr: 0.000025\n",
            "2023-01-19 14:30:23,409 - INFO - joeynmt.training - Epoch 373: total training loss 527.22\n",
            "2023-01-19 14:30:23,409 - INFO - joeynmt.training - EPOCH 374\n",
            "2023-01-19 14:30:28,490 - INFO - joeynmt.training - Epoch 374, Step:   129900, Batch Loss:     1.426611, Batch Acc: 0.605621, Tokens per Sec:    15548, Lr: 0.000025\n",
            "2023-01-19 14:30:36,121 - INFO - joeynmt.training - Epoch 374, Step:   130000, Batch Loss:     1.442380, Batch Acc: 0.600012, Tokens per Sec:    15906, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 136.47ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10208.67ex/s]\n",
            "2023-01-19 14:30:36,387 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=130000\n",
            "2023-01-19 14:30:36,388 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:30:41,444 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:30:41,444 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:30:41,444 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:30:41,446 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:30:41,451 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.10, loss:   2.83, ppl:  17.00, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0097[sec], evaluation: 0.0449[sec]\n",
            "2023-01-19 14:30:41,454 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:30:41,457 - INFO - joeynmt.training - \tSource:     بو آدام ائتدیگی حاقسێزلێغێن حاققێنا بیر تارلا ساتێن آلدێ ، سونرا دا اورادان کللهمایاللاق آشێب قارنێ یێرتیلدی وه باغێرساقلارێ تؤکۆلدۆ . \n",
            "2023-01-19 14:30:41,457 - INFO - joeynmt.training - \tReference:   همین شخص ، با مزدی که بابت عمل شریرانهٔ خود گرفت ، مزرعه‌ای خرید و با سر در همان مزرعه سقوط کرد و شکمش پاره شد و تمام امعا و احشایش بیرون ریخت . \n",
            "2023-01-19 14:30:41,457 - INFO - joeynmt.training - \tHypothesis: پس چون این مرد سختی را به بدرقه کرد ، به یکی از آن انجام داد ، آنجا را می داد و از آنجا بیرون ریختند و می توانستند کتان را می ریزد و بر آن سومسد .\n",
            "2023-01-19 14:30:41,457 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:30:41,460 - INFO - joeynmt.training - \tSource:     بو ، اونا گؤرهدیر کی ، ربین مملهکتلرین اهالیسینی ، اونلار قافل اولا اولا ظۆلمله محو ائتمهیی اؤزونه روا بیلمز ! \n",
            "2023-01-19 14:30:41,460 - INFO - joeynmt.training - \tReference:  این اتمام حجت‌ بدان سبب است که پروردگار تو هیچ گاه شهرها را به ستم نابوده نکرده ، در حالی که مردم آن غافل باشند . \n",
            "2023-01-19 14:30:41,460 - INFO - joeynmt.training - \tHypothesis: این بدان سبب است که پروردگار تو هر که ستم کرده اند ، در صورت ستمگران یارزد .\n",
            "2023-01-19 14:30:41,461 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:30:41,463 - INFO - joeynmt.training - \tSource:     کیمی باغیشلاسانیز ، من ده باغیشلایارام . اگر من بیر شئی باغیشلامیشامسا ، مسیحین هۆزوروندا سیزین خئیرینیز اۆچۆن باغێشلامێشام کی ، \n",
            "2023-01-19 14:30:41,463 - INFO - joeynmt.training - \tReference:  اگر شما کسی را ببخشید ، من نیز او را می‌بخشم . در واقع ، اگر خطایی را بخشیده‌ام به خاطر شما و در حضور مسیح بوده است\n",
            "2023-01-19 14:30:41,463 - INFO - joeynmt.training - \tHypothesis: اگر ببخشید ، من نیز ببخشم ، اگر من از سوی شماستم ، قطعا در حضور مسیح حاضر هستم ، گناهان شما را بر او ببخشم .\n",
            "2023-01-19 14:30:41,463 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:30:41,465 - INFO - joeynmt.training - \tSource:     اگر اونو عرب اولمایانلاردان بیرینه نازل ائتسیدیک ؛ \n",
            "2023-01-19 14:30:41,465 - INFO - joeynmt.training - \tReference:  و اگر آن را بر برخی از غیر عرب زبانان نازل می‌کردیم ، \n",
            "2023-01-19 14:30:41,465 - INFO - joeynmt.training - \tHypothesis: و اگر آن را از طغیان نازل کردیم ،\n",
            "2023-01-19 14:30:49,074 - INFO - joeynmt.training - Epoch 374, Step:   130100, Batch Loss:     1.477709, Batch Acc: 0.599475, Tokens per Sec:    15221, Lr: 0.000025\n",
            "2023-01-19 14:30:55,531 - INFO - joeynmt.training - Epoch 374: total training loss 525.97\n",
            "2023-01-19 14:30:55,531 - INFO - joeynmt.training - EPOCH 375\n",
            "2023-01-19 14:30:57,011 - INFO - joeynmt.training - Epoch 375, Step:   130200, Batch Loss:     1.412912, Batch Acc: 0.604418, Tokens per Sec:    14783, Lr: 0.000025\n",
            "2023-01-19 14:31:04,930 - INFO - joeynmt.training - Epoch 375, Step:   130300, Batch Loss:     1.451323, Batch Acc: 0.601305, Tokens per Sec:    15306, Lr: 0.000025\n",
            "2023-01-19 14:31:12,769 - INFO - joeynmt.training - Epoch 375, Step:   130400, Batch Loss:     1.569547, Batch Acc: 0.601564, Tokens per Sec:    15624, Lr: 0.000025\n",
            "2023-01-19 14:31:20,570 - INFO - joeynmt.training - Epoch 375, Step:   130500, Batch Loss:     1.515953, Batch Acc: 0.599781, Tokens per Sec:    15461, Lr: 0.000025\n",
            "2023-01-19 14:31:22,817 - INFO - joeynmt.training - Epoch 375: total training loss 523.45\n",
            "2023-01-19 14:31:22,818 - INFO - joeynmt.training - EPOCH 376\n",
            "2023-01-19 14:31:28,366 - INFO - joeynmt.training - Epoch 376, Step:   130600, Batch Loss:     1.453167, Batch Acc: 0.607272, Tokens per Sec:    15558, Lr: 0.000025\n",
            "2023-01-19 14:31:36,126 - INFO - joeynmt.training - Epoch 376, Step:   130700, Batch Loss:     1.539185, Batch Acc: 0.601025, Tokens per Sec:    15447, Lr: 0.000025\n",
            "2023-01-19 14:31:43,952 - INFO - joeynmt.training - Epoch 376, Step:   130800, Batch Loss:     1.476238, Batch Acc: 0.601300, Tokens per Sec:    15555, Lr: 0.000025\n",
            "2023-01-19 14:31:49,963 - INFO - joeynmt.training - Epoch 376: total training loss 526.92\n",
            "2023-01-19 14:31:49,963 - INFO - joeynmt.training - EPOCH 377\n",
            "2023-01-19 14:31:51,774 - INFO - joeynmt.training - Epoch 377, Step:   130900, Batch Loss:     1.425148, Batch Acc: 0.606415, Tokens per Sec:    15584, Lr: 0.000025\n",
            "2023-01-19 14:31:59,595 - INFO - joeynmt.training - Epoch 377, Step:   131000, Batch Loss:     1.502818, Batch Acc: 0.603241, Tokens per Sec:    15301, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.33ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10497.10ex/s]\n",
            "2023-01-19 14:31:59,862 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=131000\n",
            "2023-01-19 14:31:59,862 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:32:04,933 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:32:04,934 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:32:04,934 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:32:04,935 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:32:04,938 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.70, loss:   2.80, ppl:  16.47, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0258[sec], evaluation: 0.0426[sec]\n",
            "2023-01-19 14:32:04,940 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:32:04,944 - INFO - joeynmt.training - \tSource:     بو اینسانلار اۆچۆن ائله بیر موعزدیر کی ، اونونلا هم قورخسونلار ، هم ده آللاهێن تک بیر تانرێ اولدوغونو بیلسینلر ، هم ده آغێل صاحبلری دۆشۆنۆب ابرت آلسێنلار ! \n",
            "2023-01-19 14:32:04,944 - INFO - joeynmt.training - \tReference:  این قرآن‌ ابلاغی برای مردم است تا به وسیله آن هدایت شوند و بدان بیم یابند و بدانند که او معبودی یگانه است ، و تا صاحبان خرد پند گیرند . \n",
            "2023-01-19 14:32:04,944 - INFO - joeynmt.training - \tHypothesis: این برای برای مردمی که برای او خداست که آنها را می ترسند ، و می ترسند که خدایانی را معبودی است . آری ، خردمندان عبرت گیرند .\n",
            "2023-01-19 14:32:04,944 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:32:04,946 - INFO - joeynmt.training - \tSource:     بوندان سونرا پئتئر جاواب وئرهرک اونا دئدی : باخ بیز هر شئیی قویوب سنین آردێنجا گلمیشیک ، اوهزینده نییمیز اولاجاق ؟ \n",
            "2023-01-19 14:32:04,947 - INFO - joeynmt.training - \tReference:  پطرس در مقابل به او گفت : ما همه چیز را رها کرده‌ایم و از تو پیروی می‌کنیم ؛ پس چه چیز عاید ما خواهد شد ؟ \n",
            "2023-01-19 14:32:04,947 - INFO - joeynmt.training - \tHypothesis: پس پطرس در جواب به او گفت : آیا همه چیز را رها کنیم و به ما تو خواهیم آمد ؟\n",
            "2023-01-19 14:32:04,947 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:32:04,949 - INFO - joeynmt.training - \tSource:     پیس امللر ائتدیکدن سونرا تؤؤبه ائدیب ایمان گتیرنلره گلدیکده ایسه ، ربین تؤؤبهدن سونرا باغیشلایاندیر ، رحم ائدندیر ! \n",
            "2023-01-19 14:32:04,949 - INFO - joeynmt.training - \tReference:  و لی‌ کسانی که مرتکب گناهان شدند ، آنگاه توبه کردند و ایمان آوردند ، قطعا پروردگار تو پس از آن آمرزنده مهربان خواهد بود . \n",
            "2023-01-19 14:32:04,949 - INFO - joeynmt.training - \tHypothesis: و کسانی که مرتکب شده اند ، سپس به کسانی که توبه کرده اند ، سپس توبه کند ، آمرزنده مهربان است .\n",
            "2023-01-19 14:32:04,949 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:32:04,951 - INFO - joeynmt.training - \tSource:     بیز ایسه قۆۆتلی بیر جاماآتێق ! \n",
            "2023-01-19 14:32:04,951 - INFO - joeynmt.training - \tReference:  و لی‌ ما همگی به حال آماده‌باش درآمده‌ایم . \n",
            "2023-01-19 14:32:04,951 - INFO - joeynmt.training - \tHypothesis: و گروهی را از نیرومند بر گروهی می کشیم .\n",
            "2023-01-19 14:32:12,835 - INFO - joeynmt.training - Epoch 377, Step:   131100, Batch Loss:     1.495527, Batch Acc: 0.599096, Tokens per Sec:    14882, Lr: 0.000025\n",
            "2023-01-19 14:32:20,601 - INFO - joeynmt.training - Epoch 377, Step:   131200, Batch Loss:     1.535114, Batch Acc: 0.601771, Tokens per Sec:    15591, Lr: 0.000025\n",
            "2023-01-19 14:32:22,585 - INFO - joeynmt.training - Epoch 377: total training loss 525.24\n",
            "2023-01-19 14:32:22,586 - INFO - joeynmt.training - EPOCH 378\n",
            "2023-01-19 14:32:28,489 - INFO - joeynmt.training - Epoch 378, Step:   131300, Batch Loss:     1.515452, Batch Acc: 0.600456, Tokens per Sec:    15323, Lr: 0.000025\n",
            "2023-01-19 14:32:36,318 - INFO - joeynmt.training - Epoch 378, Step:   131400, Batch Loss:     1.555281, Batch Acc: 0.602966, Tokens per Sec:    15290, Lr: 0.000025\n",
            "2023-01-19 14:32:44,170 - INFO - joeynmt.training - Epoch 378, Step:   131500, Batch Loss:     1.527537, Batch Acc: 0.600604, Tokens per Sec:    15227, Lr: 0.000025\n",
            "2023-01-19 14:32:50,016 - INFO - joeynmt.training - Epoch 378: total training loss 527.05\n",
            "2023-01-19 14:32:50,017 - INFO - joeynmt.training - EPOCH 379\n",
            "2023-01-19 14:32:52,083 - INFO - joeynmt.training - Epoch 379, Step:   131600, Batch Loss:     1.532270, Batch Acc: 0.600878, Tokens per Sec:    15556, Lr: 0.000025\n",
            "2023-01-19 14:32:59,945 - INFO - joeynmt.training - Epoch 379, Step:   131700, Batch Loss:     1.424425, Batch Acc: 0.602995, Tokens per Sec:    15358, Lr: 0.000025\n",
            "2023-01-19 14:33:07,792 - INFO - joeynmt.training - Epoch 379, Step:   131800, Batch Loss:     1.429071, Batch Acc: 0.600728, Tokens per Sec:    15381, Lr: 0.000025\n",
            "2023-01-19 14:33:15,708 - INFO - joeynmt.training - Epoch 379, Step:   131900, Batch Loss:     1.495576, Batch Acc: 0.602657, Tokens per Sec:    15213, Lr: 0.000025\n",
            "2023-01-19 14:33:17,455 - INFO - joeynmt.training - Epoch 379: total training loss 524.46\n",
            "2023-01-19 14:33:17,455 - INFO - joeynmt.training - EPOCH 380\n",
            "2023-01-19 14:33:23,621 - INFO - joeynmt.training - Epoch 380, Step:   132000, Batch Loss:     1.535010, Batch Acc: 0.603500, Tokens per Sec:    15238, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.67ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10662.11ex/s]\n",
            "2023-01-19 14:33:23,880 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=132000\n",
            "2023-01-19 14:33:23,880 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:33:28,461 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:33:28,462 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:33:28,462 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:33:28,463 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:33:28,466 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.77, loss:   2.70, ppl:  14.95, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.5343[sec], evaluation: 0.0435[sec]\n",
            "2023-01-19 14:33:28,468 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:33:28,472 - INFO - joeynmt.training - \tSource:     دئدیلر : یهودیلرین آنادان اولموش پادشاهێ هارادادێر ؟ شرقده اونون اولدوزونو گؤردۆک وه اونا سجده قێلماغا گلدیک . \n",
            "2023-01-19 14:33:28,472 - INFO - joeynmt.training - \tReference:  و می‌گفتند : آن پادشاه یهودیان که متولد شده است ، کجاست ؟ زیرا ما ستارهٔ او را زمانی که در مشرق بودیم ، دیدیم و آمده‌ایم تا در برابر او سر تعظیم فرود آوریم . \n",
            "2023-01-19 14:33:28,472 - INFO - joeynmt.training - \tHypothesis: گفتند : آیا از کجا بود ؟ پس پادشاه یهودیان ، از کجا یافته ایم ؟ به او افتادیم و به او گفتیم : ببینیم و به او نشان دادیم و به او تعظیمت کن .\n",
            "2023-01-19 14:33:28,472 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:33:28,474 - INFO - joeynmt.training - \tSource:     آنجاق باشچێ کاهنلر خالقێ قێزێشدێردێلار کی ، ایسنانین یوخ ، بارابانێن آزاد اولماسێنێ خاهش ائتسینلر . \n",
            "2023-01-19 14:33:28,474 - INFO - joeynmt.training - \tReference:  اما سران کاهنان مردم را تحریک کردند که باراباس را در عوض ، برایشان آزاد کند . \n",
            "2023-01-19 14:33:28,474 - INFO - joeynmt.training - \tHypothesis: اما سران کاهنان ، سران کاهنان کاهنان را ترک کردند تا توسط او را آزاد کنند . باراباس کنند .\n",
            "2023-01-19 14:33:28,475 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:33:28,476 - INFO - joeynmt.training - \tSource:     اونلار ، سانکی یاقوت وه مرجاندێرلار . \n",
            "2023-01-19 14:33:28,476 - INFO - joeynmt.training - \tReference:  گویی که آنها یاقوت و مرجانند . \n",
            "2023-01-19 14:33:28,477 - INFO - joeynmt.training - \tHypothesis: و سیاه و مرجان را مرجان می کنند .\n",
            "2023-01-19 14:33:28,477 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:33:28,479 - INFO - joeynmt.training - \tSource:     هرمآغاسی اونا دئدی : اگر منه یول گؤستهرن یوخدورسا ، بونو نئجه باشا دۆشمک اولار ؟ سونرا فیلیپدن خاهش ائتدی کی ، آرابایا مینیب اونون یانیندا اوتورسون . \n",
            "2023-01-19 14:33:28,479 - INFO - joeynmt.training - \tReference:  او گفت : واقعا چگونه بدانم ، اگر کسی مرا راهنمایی نکند ؟ پس ، از فیلیپس خواهش کرد که سوار ارابه شود و کنار او بنشیند . \n",
            "2023-01-19 14:33:28,480 - INFO - joeynmt.training - \tHypothesis: آنگاه خالی به او گفت : اگر مرا راهی به من ثابت کرده است ، پس از آن که می خواست ، فیلیپس را تصمیم گرفته است تا از او بنشینند و از او بنشینند .\n",
            "2023-01-19 14:33:36,843 - INFO - joeynmt.training - Epoch 380, Step:   132100, Batch Loss:     1.535908, Batch Acc: 0.600700, Tokens per Sec:    13890, Lr: 0.000025\n",
            "2023-01-19 14:33:45,248 - INFO - joeynmt.training - Epoch 380, Step:   132200, Batch Loss:     1.591975, Batch Acc: 0.604134, Tokens per Sec:    14373, Lr: 0.000025\n",
            "2023-01-19 14:33:50,746 - INFO - joeynmt.training - Epoch 380: total training loss 524.21\n",
            "2023-01-19 14:33:50,746 - INFO - joeynmt.training - EPOCH 381\n",
            "2023-01-19 14:33:53,080 - INFO - joeynmt.training - Epoch 381, Step:   132300, Batch Loss:     1.547540, Batch Acc: 0.603336, Tokens per Sec:    15954, Lr: 0.000025\n",
            "2023-01-19 14:34:00,892 - INFO - joeynmt.training - Epoch 381, Step:   132400, Batch Loss:     1.627410, Batch Acc: 0.604220, Tokens per Sec:    15409, Lr: 0.000025\n",
            "2023-01-19 14:34:08,683 - INFO - joeynmt.training - Epoch 381, Step:   132500, Batch Loss:     1.450729, Batch Acc: 0.601158, Tokens per Sec:    15589, Lr: 0.000025\n",
            "2023-01-19 14:34:16,496 - INFO - joeynmt.training - Epoch 381, Step:   132600, Batch Loss:     1.569897, Batch Acc: 0.599837, Tokens per Sec:    15402, Lr: 0.000025\n",
            "2023-01-19 14:34:17,889 - INFO - joeynmt.training - Epoch 381: total training loss 523.14\n",
            "2023-01-19 14:34:17,890 - INFO - joeynmt.training - EPOCH 382\n",
            "2023-01-19 14:34:24,357 - INFO - joeynmt.training - Epoch 382, Step:   132700, Batch Loss:     1.519837, Batch Acc: 0.602429, Tokens per Sec:    15531, Lr: 0.000025\n",
            "2023-01-19 14:34:32,171 - INFO - joeynmt.training - Epoch 382, Step:   132800, Batch Loss:     1.461791, Batch Acc: 0.598821, Tokens per Sec:    15308, Lr: 0.000025\n",
            "2023-01-19 14:34:40,020 - INFO - joeynmt.training - Epoch 382, Step:   132900, Batch Loss:     1.434188, Batch Acc: 0.602402, Tokens per Sec:    15519, Lr: 0.000025\n",
            "2023-01-19 14:34:45,045 - INFO - joeynmt.training - Epoch 382: total training loss 523.36\n",
            "2023-01-19 14:34:45,046 - INFO - joeynmt.training - EPOCH 383\n",
            "2023-01-19 14:34:47,906 - INFO - joeynmt.training - Epoch 383, Step:   133000, Batch Loss:     1.489971, Batch Acc: 0.606019, Tokens per Sec:    15444, Lr: 0.000025\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.67ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9209.58ex/s] \n",
            "2023-01-19 14:34:48,191 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=133000\n",
            "2023-01-19 14:34:48,192 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:34:53,341 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:34:53,342 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:34:53,342 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:34:53,343 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:34:53,346 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  10.04, loss:   2.67, ppl:  14.44, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0992[sec], evaluation: 0.0454[sec]\n",
            "2023-01-19 14:34:53,349 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:34:53,353 - INFO - joeynmt.training - \tSource:     بیر زامان ملکلره : آدمه سجده ائدین ! دئیه بویورموشدوق . ایبلیسدن باشقا هامێسێ سجده ائتدی . او ، بویون قاچێرتدێ . \n",
            "2023-01-19 14:34:53,353 - INFO - joeynmt.training - \tReference:  و یاد کن‌ هنگامی را که به فرشتگان گفتیم : برای آدم سجده کنید . پس ، جز ابلیس که سر باز زد همه‌ سجده کردند . \n",
            "2023-01-19 14:34:53,353 - INFO - joeynmt.training - \tHypothesis: و چون فرشتگان گفت : به سوی آدم سجده کنید ، پس همه را سجده کردند و همه جز ابلیس سجده کردند . همه همه از این گران می شد .\n",
            "2023-01-19 14:34:53,353 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:34:53,355 - INFO - joeynmt.training - \tSource:     آداملار سیزه باخ ، او اورادادێر یاخود باخ ، بورادادێر دئیهجک . گئدیب اونلارێن آرخاسێنجا دۆشمهیین ! \n",
            "2023-01-19 14:34:53,355 - INFO - joeynmt.training - \tReference:  مردم به شما خواهند گفت : آنجا را ببین ! یا اینجا را ببین ! بیرون مروید یا آنان را دنبال مکنید . \n",
            "2023-01-19 14:34:53,356 - INFO - joeynmt.training - \tHypothesis: مردم به شما می اندیشند ، یا اینجاست ! بگویید : اینجاست ! به سوی آنان بروید و به سلام مگویید .\n",
            "2023-01-19 14:34:53,356 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:34:53,358 - INFO - joeynmt.training - \tSource:     فیر اون دئدی : من سیزه ایزین وئرمهدن اول سیز اونا ایمان گتیردینیز ؟ بو ، شبههسیز کی ، اهالیسینی چێخارتماق مقصدله شهرده قوردوغونوز بیر هییلهدیر . بیلهجکسینیز ! \n",
            "2023-01-19 14:34:53,358 - INFO - joeynmt.training - \tReference:  فرعون گفت : آیا پیش از آنکه به شما رخصت دهم ، به او ایمان آوردید ؟ قطعا این نیرنگی است که در شهر به راه انداخته‌اید تا مردمش را از آن بیرون کنید . پس به زودی خواهید دانست . \n",
            "2023-01-19 14:34:53,358 - INFO - joeynmt.training - \tHypothesis: فرعون گفت : آیا پیش از من نداید که پیش از آنکه به شما ایمان آورده اید ؟ این قطعا در نیرنگ او از مجادله می کنند ، و نیرنگی از این به نیرنگ می آوردید .\n",
            "2023-01-19 14:34:53,358 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:34:53,360 - INFO - joeynmt.training - \tSource:     آند ایچیرم بو شهره \n",
            "2023-01-19 14:34:53,360 - INFO - joeynmt.training - \tReference:  سوگند به این شهر ، \n",
            "2023-01-19 14:34:53,360 - INFO - joeynmt.training - \tHypothesis: و سوگند به شهری که آن شهر می خورند ،\n",
            "2023-01-19 14:35:01,239 - INFO - joeynmt.training - Epoch 383, Step:   133100, Batch Loss:     1.419866, Batch Acc: 0.604531, Tokens per Sec:    14795, Lr: 0.000025\n",
            "2023-01-19 14:35:09,056 - INFO - joeynmt.training - Epoch 383, Step:   133200, Batch Loss:     1.536558, Batch Acc: 0.600475, Tokens per Sec:    15453, Lr: 0.000025\n",
            "2023-01-19 14:35:16,828 - INFO - joeynmt.training - Epoch 383, Step:   133300, Batch Loss:     1.629730, Batch Acc: 0.601796, Tokens per Sec:    15386, Lr: 0.000024\n",
            "2023-01-19 14:35:17,816 - INFO - joeynmt.training - Epoch 383: total training loss 523.98\n",
            "2023-01-19 14:35:17,816 - INFO - joeynmt.training - EPOCH 384\n",
            "2023-01-19 14:35:24,765 - INFO - joeynmt.training - Epoch 384, Step:   133400, Batch Loss:     1.535618, Batch Acc: 0.602463, Tokens per Sec:    15342, Lr: 0.000024\n",
            "2023-01-19 14:35:32,672 - INFO - joeynmt.training - Epoch 384, Step:   133500, Batch Loss:     1.477375, Batch Acc: 0.600257, Tokens per Sec:    15277, Lr: 0.000024\n",
            "2023-01-19 14:35:40,534 - INFO - joeynmt.training - Epoch 384, Step:   133600, Batch Loss:     1.595394, Batch Acc: 0.601354, Tokens per Sec:    15198, Lr: 0.000024\n",
            "2023-01-19 14:35:45,375 - INFO - joeynmt.training - Epoch 384: total training loss 526.49\n",
            "2023-01-19 14:35:45,375 - INFO - joeynmt.training - EPOCH 385\n",
            "2023-01-19 14:35:48,468 - INFO - joeynmt.training - Epoch 385, Step:   133700, Batch Loss:     1.470299, Batch Acc: 0.599131, Tokens per Sec:    15488, Lr: 0.000024\n",
            "2023-01-19 14:35:56,380 - INFO - joeynmt.training - Epoch 385, Step:   133800, Batch Loss:     1.481260, Batch Acc: 0.604358, Tokens per Sec:    15343, Lr: 0.000024\n",
            "2023-01-19 14:36:04,285 - INFO - joeynmt.training - Epoch 385, Step:   133900, Batch Loss:     1.546971, Batch Acc: 0.602511, Tokens per Sec:    14966, Lr: 0.000024\n",
            "2023-01-19 14:36:12,118 - INFO - joeynmt.training - Epoch 385, Step:   134000, Batch Loss:     1.431961, Batch Acc: 0.601573, Tokens per Sec:    15407, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.57ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10172.55ex/s]\n",
            "2023-01-19 14:36:12,385 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=134000\n",
            "2023-01-19 14:36:12,385 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:36:17,557 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:36:17,557 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:36:17,557 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:36:17,558 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:36:17,561 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.90, loss:   2.75, ppl:  15.67, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1259[sec], evaluation: 0.0420[sec]\n",
            "2023-01-19 14:36:17,563 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:36:17,567 - INFO - joeynmt.training - \tSource:     مگر آللاهێن نئجه بیر مسل چکدیگینی گؤرمورسنمی ؟ خوش بیر سؤز کؤکو یئرده مۆحکم اولوب بوداقلارێ گؤیه اوجالان گؤزل بیر آغاج کیمیدیر . \n",
            "2023-01-19 14:36:17,567 - INFO - joeynmt.training - \tReference:  آیا ندیدی خدا چگونه مثل زده : سخنی پاک که مانند درختی پاک است که ریشه‌اش استوار و شاخه‌اش در آسمان است ؟ \n",
            "2023-01-19 14:36:17,567 - INFO - joeynmt.training - \tHypothesis: آیا ندیده ای که خدا چگونه است می بینی ؟ باید در حالی که در آن سراویی استوار قرار گیری و در آن درختی با شاخه های بسیار به آسمان فرو می آورد .\n",
            "2023-01-19 14:36:17,567 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:36:17,569 - INFO - joeynmt.training - \tSource:     مریم اوغلو ایسا دئدی : یا آللاه ، ائی بیزیم ربیمیز ! بیزه گؤیدن بیر صفره نازل ائت کی ، او بیزیم هم بیرینجیمیز ، هم ده آخێرێنجێمێز اۆچۆن بیر بایرام وه سندن بیر معؤ جۆزه اولسون . بیزه روزی وئر کی ، سن روزی وئرنلرین ان یاخشیسیسان ! \n",
            "2023-01-19 14:36:17,569 - INFO - joeynmt.training - \tReference:  عیسی پسر مریم گفت : بار الها ، پروردگارا ، از آسمان ، خوانی بر ما فرو فرست تا عیدی برای اول و آخر ما باشد و نشانه‌ای از جانب تو . و ما را روزی ده که تو بهترین روزی‌دهندگانی . \n",
            "2023-01-19 14:36:17,569 - INFO - joeynmt.training - \tHypothesis: عیسی پسر مریم گفت : ای خدا ، پروردگارا ، تا از آسمان بر ما نازل کن که نخستین بار دیگر و نخستین بار و نخستین بار و بنگر ! و نخستین بار و یعقوب و یعقوب و انجیر برای ما روزی ده .\n",
            "2023-01-19 14:36:17,570 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:36:17,572 - INFO - joeynmt.training - \tSource:     هیرود اؤز اسگرلری ایله اونو تحقیر ائدیب اله سالدێ . اونون اینینه بیر گؤزل پالتار گئییندیریب پیلاتین یانینا قایتاردی . \n",
            "2023-01-19 14:36:17,572 - INFO - joeynmt.training - \tReference:  همچنین هیرودیس و سربازانش به وی بی‌حرمتی کردند و هیرودیس برای تمسخر او ، ردایی فاخر بر او پوشاند و سپس او را نزد پیلاتس بازگرداند . \n",
            "2023-01-19 14:36:17,572 - INFO - joeynmt.training - \tHypothesis: سپس او را دست هیرودیس انداخت و به او اهانت آمیز انداخت . آنگاه پیلاتس که لباسش را به او بازگردد و از پیلاتس بازگرداند .\n",
            "2023-01-19 14:36:17,572 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:36:17,574 - INFO - joeynmt.training - \tSource:     آخێ بو اتیری باها قییمته ساتێب پولونو یوخسوللارا وئرمک اولاردێ . \n",
            "2023-01-19 14:36:17,574 - INFO - joeynmt.training - \tReference:  زیرا می‌شد آن را به مبلغی زیاد فروخت و پولش را به فقیران داد . \n",
            "2023-01-19 14:36:17,574 - INFO - joeynmt.training - \tHypothesis: زیرا این رو ، وقتی این روغن معطر را بهای رهایی داد ، فقیران را بهای رهایی داده شود .\n",
            "2023-01-19 14:36:18,419 - INFO - joeynmt.training - Epoch 385: total training loss 524.47\n",
            "2023-01-19 14:36:18,420 - INFO - joeynmt.training - EPOCH 386\n",
            "2023-01-19 14:36:25,362 - INFO - joeynmt.training - Epoch 386, Step:   134100, Batch Loss:     1.441227, Batch Acc: 0.608795, Tokens per Sec:    15466, Lr: 0.000024\n",
            "2023-01-19 14:36:33,112 - INFO - joeynmt.training - Epoch 386, Step:   134200, Batch Loss:     1.485349, Batch Acc: 0.602034, Tokens per Sec:    15506, Lr: 0.000024\n",
            "2023-01-19 14:36:40,769 - INFO - joeynmt.training - Epoch 386, Step:   134300, Batch Loss:     1.421464, Batch Acc: 0.599932, Tokens per Sec:    15650, Lr: 0.000024\n",
            "2023-01-19 14:36:45,431 - INFO - joeynmt.training - Epoch 386: total training loss 524.90\n",
            "2023-01-19 14:36:45,431 - INFO - joeynmt.training - EPOCH 387\n",
            "2023-01-19 14:36:48,634 - INFO - joeynmt.training - Epoch 387, Step:   134400, Batch Loss:     1.490853, Batch Acc: 0.606099, Tokens per Sec:    15330, Lr: 0.000024\n",
            "2023-01-19 14:36:56,386 - INFO - joeynmt.training - Epoch 387, Step:   134500, Batch Loss:     1.421941, Batch Acc: 0.604239, Tokens per Sec:    15635, Lr: 0.000024\n",
            "2023-01-19 14:37:04,116 - INFO - joeynmt.training - Epoch 387, Step:   134600, Batch Loss:     1.419849, Batch Acc: 0.606808, Tokens per Sec:    15632, Lr: 0.000024\n",
            "2023-01-19 14:37:13,114 - INFO - joeynmt.training - Epoch 387, Step:   134700, Batch Loss:     1.550483, Batch Acc: 0.602825, Tokens per Sec:    13439, Lr: 0.000024\n",
            "2023-01-19 14:37:13,678 - INFO - joeynmt.training - Epoch 387: total training loss 522.59\n",
            "2023-01-19 14:37:13,678 - INFO - joeynmt.training - EPOCH 388\n",
            "2023-01-19 14:37:20,893 - INFO - joeynmt.training - Epoch 388, Step:   134800, Batch Loss:     1.422987, Batch Acc: 0.602770, Tokens per Sec:    15496, Lr: 0.000024\n",
            "2023-01-19 14:37:28,698 - INFO - joeynmt.training - Epoch 388, Step:   134900, Batch Loss:     1.600230, Batch Acc: 0.603606, Tokens per Sec:    15291, Lr: 0.000024\n",
            "2023-01-19 14:37:36,478 - INFO - joeynmt.training - Epoch 388, Step:   135000, Batch Loss:     1.573316, Batch Acc: 0.604092, Tokens per Sec:    15687, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.13ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10597.38ex/s]\n",
            "2023-01-19 14:37:36,744 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=135000\n",
            "2023-01-19 14:37:36,744 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:37:42,202 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:37:42,202 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:37:42,202 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:37:42,203 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:37:42,206 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.72, loss:   2.79, ppl:  16.36, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.4092[sec], evaluation: 0.0449[sec]\n",
            "2023-01-19 14:37:42,209 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:37:42,212 - INFO - joeynmt.training - \tSource:     کیم تاپماسا ، قادێنێ ایله یاخینلێق ائتمزدن اول ایکی آی سراسر اوروج توتمالێ ، بونا دا گۆجۆ چاتماسا ، آلتمێش یوخسولو یئدیردیب دویدورمالیدیر . بو سیزین آللاه وه اونون پیغمبرینه ایمان گتیرمنیز اۆچۆندۆر . بونلار آللاهێن هدلریدیر . کافرلری شدتلی بیر اعذاب گؤزلهییر ! \n",
            "2023-01-19 14:37:42,213 - INFO - joeynmt.training - \tReference:  و آن کس که بر آزادکردن بنده‌ دسترسی ندارد ، باید پیش از تماس با زن خود دو ماه پیاپی روزه بدارد ؛ و هر که نتواند ، باید شصت بینوا را خوراک بدهد . این حکم‌ برای آن است که به خدا و فرستاده او ایمان بیاورید ، و این است حدود خدا . و کافران را عذابی پردرد خواهد بود . \n",
            "2023-01-19 14:37:42,213 - INFO - joeynmt.training - \tHypothesis: و اگر کسی زنی نیست که به دو ساحل نر و موجب به دو تنگدستی و به قدر مخالفت کند ، قطعا به قدرت خدا و آن دو است که وی را جز فرستاده اش و زاغ تر است یا آنکه ستم کرده باشد ، این است که خدا آنان را به عذابی دردناک خواهند داشت .\n",
            "2023-01-19 14:37:42,213 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:37:42,215 - INFO - joeynmt.training - \tSource:     یوخسا اونلار دۆزگۆن ایش گؤرموشدۆلر ائله ایسه بیز ده دۆزگۆن ایش گؤروروک ! \n",
            "2023-01-19 14:37:42,215 - INFO - joeynmt.training - \tReference:  یا در کاری ابرام ورزیده‌اند ؟ ما نیز ابرام می‌ورزیم . \n",
            "2023-01-19 14:37:42,215 - INFO - joeynmt.training - \tHypothesis: آیا آنان را بر آنچه درست درست یافته اند انجام داده ایم ، و ما کار می کنیم ؟\n",
            "2023-01-19 14:37:42,215 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:37:42,217 - INFO - joeynmt.training - \tSource:     تفو سیزه ده ، آللاهدان باشقا ابادت ائتدیگینیز بۆتلره ده ! اجابا ، آنلامێرسێنێز ؟ \n",
            "2023-01-19 14:37:42,218 - INFO - joeynmt.training - \tReference:  اف بر شما و بر آنچه غیر از خدا می‌پرستید . مگر نمی‌اندیشید ؟ \n",
            "2023-01-19 14:37:42,218 - INFO - joeynmt.training - \tHypothesis: ولی شما و غیر از خدا می پرستید که جز خدا را بپرستید ؟ آیا نمی دانید ؟\n",
            "2023-01-19 14:37:42,218 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:37:42,220 - INFO - joeynmt.training - \tSource:     کی آمبولانسین سسی اوجالدی . \n",
            "2023-01-19 14:37:42,220 - INFO - joeynmt.training - \tReference:  كه صداي آمبولانس بلند شد . \n",
            "2023-01-19 14:37:42,220 - INFO - joeynmt.training - \tHypothesis: كه بام بلند مشتون می زد .\n",
            "2023-01-19 14:37:46,570 - INFO - joeynmt.training - Epoch 388: total training loss 523.16\n",
            "2023-01-19 14:37:46,570 - INFO - joeynmt.training - EPOCH 389\n",
            "2023-01-19 14:37:50,102 - INFO - joeynmt.training - Epoch 389, Step:   135100, Batch Loss:     1.373830, Batch Acc: 0.605108, Tokens per Sec:    15292, Lr: 0.000024\n",
            "2023-01-19 14:37:57,843 - INFO - joeynmt.training - Epoch 389, Step:   135200, Batch Loss:     1.508766, Batch Acc: 0.605227, Tokens per Sec:    15507, Lr: 0.000024\n",
            "2023-01-19 14:38:05,704 - INFO - joeynmt.training - Epoch 389, Step:   135300, Batch Loss:     1.404212, Batch Acc: 0.603439, Tokens per Sec:    15365, Lr: 0.000024\n",
            "2023-01-19 14:38:13,438 - INFO - joeynmt.training - Epoch 389, Step:   135400, Batch Loss:     1.490515, Batch Acc: 0.602084, Tokens per Sec:    15736, Lr: 0.000024\n",
            "2023-01-19 14:38:13,715 - INFO - joeynmt.training - Epoch 389: total training loss 522.41\n",
            "2023-01-19 14:38:13,716 - INFO - joeynmt.training - EPOCH 390\n",
            "2023-01-19 14:38:21,373 - INFO - joeynmt.training - Epoch 390, Step:   135500, Batch Loss:     1.390737, Batch Acc: 0.606719, Tokens per Sec:    15415, Lr: 0.000024\n",
            "2023-01-19 14:38:29,234 - INFO - joeynmt.training - Epoch 390, Step:   135600, Batch Loss:     1.517550, Batch Acc: 0.604342, Tokens per Sec:    15293, Lr: 0.000024\n",
            "2023-01-19 14:38:37,003 - INFO - joeynmt.training - Epoch 390, Step:   135700, Batch Loss:     1.489699, Batch Acc: 0.603114, Tokens per Sec:    15559, Lr: 0.000024\n",
            "2023-01-19 14:38:40,978 - INFO - joeynmt.training - Epoch 390: total training loss 521.14\n",
            "2023-01-19 14:38:40,979 - INFO - joeynmt.training - EPOCH 391\n",
            "2023-01-19 14:38:44,748 - INFO - joeynmt.training - Epoch 391, Step:   135800, Batch Loss:     1.494771, Batch Acc: 0.607258, Tokens per Sec:    15869, Lr: 0.000024\n",
            "2023-01-19 14:38:52,425 - INFO - joeynmt.training - Epoch 391, Step:   135900, Batch Loss:     1.523073, Batch Acc: 0.606861, Tokens per Sec:    15678, Lr: 0.000024\n",
            "2023-01-19 14:39:00,129 - INFO - joeynmt.training - Epoch 391, Step:   136000, Batch Loss:     1.481716, Batch Acc: 0.603718, Tokens per Sec:    15600, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.19ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10185.92ex/s]\n",
            "2023-01-19 14:39:00,408 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=136000\n",
            "2023-01-19 14:39:00,409 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:39:05,908 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:39:05,908 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:39:05,908 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:39:05,909 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:39:05,913 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   7.71, loss:   2.89, ppl:  17.94, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.4503[sec], evaluation: 0.0464[sec]\n",
            "2023-01-19 14:39:05,916 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:39:05,920 - INFO - joeynmt.training - \tSource:     اونایسیمین بیر مۆدت سندن آیریلماسی بلکه ده بوندان اؤترو اولدو کی ، سن اونو همیشهلیک ، \n",
            "2023-01-19 14:39:05,920 - INFO - joeynmt.training - \tReference:  شاید دلیل این که انیسیموس مدتی کوتاه از تو جدا شد ، واقعا این بوده است که او را برای همیشه بازیابی ، \n",
            "2023-01-19 14:39:05,920 - INFO - joeynmt.training - \tHypothesis: و به او این اندازه تا مدتی از تو باشد ، شاید بتوانی آن را برای خود تقوابی او باشد ،\n",
            "2023-01-19 14:39:05,920 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:39:05,923 - INFO - joeynmt.training - \tSource:     او واختدان ایسا وز ائدیب بئله دئمهیه باشلادێ : تؤؤبه ائدین ! چۆنکی سماوی پادشاهلێق یاخینلاشیب . \n",
            "2023-01-19 14:39:05,923 - INFO - joeynmt.training - \tReference:  از آن زمان عیسی موعظه را آغاز کرد . او می‌گفت : توبه کنید ؛ زیرا پادشاهی آسمان‌ها نزدیک شده است . \n",
            "2023-01-19 14:39:05,923 - INFO - joeynmt.training - \tHypothesis: در آن زمان ، عیسی از او چنین گفت : توبه کنید ؛ زیرا به پادشاهی آسمان ها نزدیک شده است .\n",
            "2023-01-19 14:39:05,923 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:39:05,925 - INFO - joeynmt.training - \tSource:     بیز تورپاقدان یارانانین سورهتینی نئجه گزدیریکسه ، سماوی اولانێن سورهتینی ده ائله گزدیرهجهییک . \n",
            "2023-01-19 14:39:05,925 - INFO - joeynmt.training - \tReference:  همان طور که ما شکل انسان خاکی را به خود گرفته‌ایم ، در آینده شکل او را نیز که از آسمان آمد ، به خود خواهیم گرفت . \n",
            "2023-01-19 14:39:05,926 - INFO - joeynmt.training - \tHypothesis: و ما که از خاک آفریده ایم ، پس اگر کسی از آسمانی صور دشوده شود ، قطعا در راه راه های آسمانی راه می رود .\n",
            "2023-01-19 14:39:05,926 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:39:05,928 - INFO - joeynmt.training - \tSource:     آند اولسون کی ، سیز ایلک یارادیلیش بیلیرسینیز . ائله ایسه هئچ دۆشۆنمۆرسۆنۆز \n",
            "2023-01-19 14:39:05,928 - INFO - joeynmt.training - \tReference:  و قطعا پدیدار شدن نخستین خود را شناختید ؛ پس چرا سر عبرت گرفتن ندارید ؟ \n",
            "2023-01-19 14:39:05,928 - INFO - joeynmt.training - \tHypothesis: و قطعا شما می دانید که هیچ سالی را نمی دانید .\n",
            "2023-01-19 14:39:13,705 - INFO - joeynmt.training - Epoch 391, Step:   136100, Batch Loss:     1.565294, Batch Acc: 0.603411, Tokens per Sec:    14808, Lr: 0.000024\n",
            "2023-01-19 14:39:13,739 - INFO - joeynmt.training - Epoch 391: total training loss 522.76\n",
            "2023-01-19 14:39:13,739 - INFO - joeynmt.training - EPOCH 392\n",
            "2023-01-19 14:39:21,485 - INFO - joeynmt.training - Epoch 392, Step:   136200, Batch Loss:     1.534428, Batch Acc: 0.608197, Tokens per Sec:    15566, Lr: 0.000024\n",
            "2023-01-19 14:39:29,443 - INFO - joeynmt.training - Epoch 392, Step:   136300, Batch Loss:     1.526427, Batch Acc: 0.604625, Tokens per Sec:    15158, Lr: 0.000024\n",
            "2023-01-19 14:39:37,323 - INFO - joeynmt.training - Epoch 392, Step:   136400, Batch Loss:     1.506665, Batch Acc: 0.603814, Tokens per Sec:    15259, Lr: 0.000024\n",
            "2023-01-19 14:39:41,078 - INFO - joeynmt.training - Epoch 392: total training loss 521.18\n",
            "2023-01-19 14:39:41,078 - INFO - joeynmt.training - EPOCH 393\n",
            "2023-01-19 14:39:45,095 - INFO - joeynmt.training - Epoch 393, Step:   136500, Batch Loss:     1.425161, Batch Acc: 0.605205, Tokens per Sec:    15481, Lr: 0.000024\n",
            "2023-01-19 14:39:52,901 - INFO - joeynmt.training - Epoch 393, Step:   136600, Batch Loss:     1.586853, Batch Acc: 0.605509, Tokens per Sec:    15560, Lr: 0.000024\n",
            "2023-01-19 14:40:00,750 - INFO - joeynmt.training - Epoch 393, Step:   136700, Batch Loss:     1.533449, Batch Acc: 0.604756, Tokens per Sec:    15437, Lr: 0.000024\n",
            "2023-01-19 14:40:08,323 - INFO - joeynmt.training - Epoch 393: total training loss 520.78\n",
            "2023-01-19 14:40:08,324 - INFO - joeynmt.training - EPOCH 394\n",
            "2023-01-19 14:40:08,669 - INFO - joeynmt.training - Epoch 394, Step:   136800, Batch Loss:     1.424940, Batch Acc: 0.600123, Tokens per Sec:    14169, Lr: 0.000024\n",
            "2023-01-19 14:40:16,357 - INFO - joeynmt.training - Epoch 394, Step:   136900, Batch Loss:     1.502594, Batch Acc: 0.605282, Tokens per Sec:    15747, Lr: 0.000024\n",
            "2023-01-19 14:40:24,000 - INFO - joeynmt.training - Epoch 394, Step:   137000, Batch Loss:     1.634637, Batch Acc: 0.605277, Tokens per Sec:    15841, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 130.45ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9597.61ex/s] \n",
            "2023-01-19 14:40:24,279 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=137000\n",
            "2023-01-19 14:40:24,279 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:40:29,546 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:40:29,546 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:40:29,546 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:40:29,547 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:40:29,550 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.01, loss:   2.83, ppl:  16.98, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.2190[sec], evaluation: 0.0440[sec]\n",
            "2023-01-19 14:40:29,553 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:40:29,556 - INFO - joeynmt.training - \tSource:     درحال سیناقاوقلاردا ایسنانین آللاهێن اوغلو اولدوغونو وز ائتمهیه باشلادێ . \n",
            "2023-01-19 14:40:29,557 - INFO - joeynmt.training - \tReference:  و فورا در کنیسه‌ها به موعظهٔ این پیام پرداخت که عیسی همان پسر خداست . \n",
            "2023-01-19 14:40:29,557 - INFO - joeynmt.training - \tHypothesis: در آن زمان ، سعی کردند که عیسی پسر خدا بود و عیسی در مورد عیسی پسر خدا بود .\n",
            "2023-01-19 14:40:29,557 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:40:29,559 - INFO - joeynmt.training - \tSource:     بئواختا قالیرسان . \n",
            "2023-01-19 14:40:29,559 - INFO - joeynmt.training - \tReference:  دیر میرسی . \n",
            "2023-01-19 14:40:29,559 - INFO - joeynmt.training - \tHypothesis: بلکه در افسر در افس می شوی .\n",
            "2023-01-19 14:40:29,559 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:40:29,561 - INFO - joeynmt.training - \tSource:     دئ : ربیم عدالتی امر ائتدی . هر سجده اۆزۆنۆزۆ توتون . دینی یالنیز اونا مخصوص ائدهرک ابادت ائدین . سیزی یاراتدیغی کیمی ، یئنه اونون هۆزورونا قاییداجاقسینیز ! \n",
            "2023-01-19 14:40:29,561 - INFO - joeynmt.training - \tReference:  بگو : پروردگارم به دادگری فرمان داده است ، و اینکه‌ در هر مسجدی روی خود را مستقیم به سوی قبله‌ کنید ، و در حالی که دین خود را برای او خالص گردانیده‌اید وی را بخوانید ، همان گونه که شما را پدید آورد به سوی او برمی‌گردید . \n",
            "2023-01-19 14:40:29,561 - INFO - joeynmt.training - \tHypothesis: بگو : پروردگارم را فرمان داد که هر چه دین را به سجده بپرستید ، پس او را در حالی که دین خود را می خوانید ، و شما را آفریدگار خواهد کرد . و به سوی او بازگردانیده خواهید شد .\n",
            "2023-01-19 14:40:29,562 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:40:29,563 - INFO - joeynmt.training - \tSource:      سن اؤز ربینین هؤکمونه صبر ائت ! شبههسیز کی ، سن بیزیم گؤزوموزون قاباغێنداسان . قالخدێقدا ربینی همد سنا ایله زیکر ائت ! \n",
            "2023-01-19 14:40:29,564 - INFO - joeynmt.training - \tReference:  و در برابر دستور پروردگارت شکیبایی پیشه کن که تو خود در حمایت مایی و هنگامی که از خواب‌ بر می‌خیزی به نیایش پروردگارت تسبیح گوی . \n",
            "2023-01-19 14:40:29,564 - INFO - joeynmt.training - \tHypothesis: و تو صبر کن که در آن شکیبای کن ، زیرا که تو چشمه ای . پس چون تو را پیش از برمی داری ، زنهار ، یاد کن .\n",
            "2023-01-19 14:40:37,253 - INFO - joeynmt.training - Epoch 394, Step:   137100, Batch Loss:     1.519264, Batch Acc: 0.604048, Tokens per Sec:    15042, Lr: 0.000024\n",
            "2023-01-19 14:40:41,419 - INFO - joeynmt.training - Epoch 394: total training loss 520.53\n",
            "2023-01-19 14:40:41,419 - INFO - joeynmt.training - EPOCH 395\n",
            "2023-01-19 14:40:46,083 - INFO - joeynmt.training - Epoch 395, Step:   137200, Batch Loss:     1.471710, Batch Acc: 0.604153, Tokens per Sec:    14396, Lr: 0.000024\n",
            "2023-01-19 14:40:53,759 - INFO - joeynmt.training - Epoch 395, Step:   137300, Batch Loss:     1.592776, Batch Acc: 0.604409, Tokens per Sec:    15774, Lr: 0.000024\n",
            "2023-01-19 14:41:01,382 - INFO - joeynmt.training - Epoch 395, Step:   137400, Batch Loss:     1.569306, Batch Acc: 0.604061, Tokens per Sec:    15840, Lr: 0.000024\n",
            "2023-01-19 14:41:08,505 - INFO - joeynmt.training - Epoch 395: total training loss 520.68\n",
            "2023-01-19 14:41:08,506 - INFO - joeynmt.training - EPOCH 396\n",
            "2023-01-19 14:41:09,143 - INFO - joeynmt.training - Epoch 396, Step:   137500, Batch Loss:     1.519871, Batch Acc: 0.605297, Tokens per Sec:    15648, Lr: 0.000024\n",
            "2023-01-19 14:41:16,740 - INFO - joeynmt.training - Epoch 396, Step:   137600, Batch Loss:     1.450318, Batch Acc: 0.605460, Tokens per Sec:    15997, Lr: 0.000024\n",
            "2023-01-19 14:41:24,406 - INFO - joeynmt.training - Epoch 396, Step:   137700, Batch Loss:     1.540911, Batch Acc: 0.609203, Tokens per Sec:    15894, Lr: 0.000024\n",
            "2023-01-19 14:41:32,029 - INFO - joeynmt.training - Epoch 396, Step:   137800, Batch Loss:     1.607784, Batch Acc: 0.604585, Tokens per Sec:    15736, Lr: 0.000024\n",
            "2023-01-19 14:41:35,014 - INFO - joeynmt.training - Epoch 396: total training loss 515.62\n",
            "2023-01-19 14:41:35,015 - INFO - joeynmt.training - EPOCH 397\n",
            "2023-01-19 14:41:39,912 - INFO - joeynmt.training - Epoch 397, Step:   137900, Batch Loss:     1.411017, Batch Acc: 0.608022, Tokens per Sec:    15412, Lr: 0.000024\n",
            "2023-01-19 14:41:47,608 - INFO - joeynmt.training - Epoch 397, Step:   138000, Batch Loss:     1.448925, Batch Acc: 0.606247, Tokens per Sec:    15675, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 141.50ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9303.19ex/s]\n",
            "2023-01-19 14:41:47,908 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=138000\n",
            "2023-01-19 14:41:47,908 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:41:53,292 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:41:53,292 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:41:53,293 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:41:53,294 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:41:53,297 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.74, loss:   2.73, ppl:  15.32, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.3363[sec], evaluation: 0.0441[sec]\n",
            "2023-01-19 14:41:53,299 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:41:53,303 - INFO - joeynmt.training - \tSource:     یهودا قبیلهسیندن 12 ، 000 مۆهورلنمیش نفر ؛ رووئن قبیلهسیندن 12 ، 000 نفر ؛ قاد قبیلهسیندن 12 ، 000 نفر ؛ \n",
            "2023-01-19 14:41:53,303 - INFO - joeynmt.training - \tReference:  از طایفهٔ یهودا ۱۲ ۰۰۰ نفر مهر شدند ؛ از طایفهٔ رئوبین ۱۲ ۰۰۰ نفر ؛ از طایفهٔ جاد ۱۲ ۰۰۰ نفر ؛ \n",
            "2023-01-19 14:41:53,303 - INFO - joeynmt.training - \tHypothesis: یهودا ۰۰۰ نفر ؛ از طایفهٔ یساکار ۱۲ ۰۰۰ نفر ؛ از طایفهٔ طایفهٔ ی ۱۲ ۰۰۰ نفر ؛\n",
            "2023-01-19 14:41:53,303 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:41:53,305 - INFO - joeynmt.training - \tSource:     او بؤیوک اژداها ابلیس وه شیطان دئییلن ، بۆتۆن دۆنیانی آلدادان قدیم ایلان یئر اۆزۆنه آتێلدێ ، ملکلری ده اونونلا بیرگه آتێلدێ . \n",
            "2023-01-19 14:41:53,306 - INFO - joeynmt.training - \tReference:  پس آن اژدهای بزرگ به پایین افکنده شد ؛ یعنی همان مار کهن که ابلیس و شیطان خوانده می‌شود و تمام ساکنان زمین را گمراه می‌کند . او به همراه فرشتگانش به زمین افکنده شد . \n",
            "2023-01-19 14:41:53,306 - INFO - joeynmt.training - \tHypothesis: او اژدها بسیار ، ابلیس و شیطان است که تمام دنیا را از زیر پایین افکنده بود و فرشتگان را در زمین فرو افکند . فرشتگان نیز در آنجا افکند و فریادی به زندان افکند .\n",
            "2023-01-19 14:41:53,306 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:41:53,308 - INFO - joeynmt.training - \tSource:     آمما سندن بیر شکایتیم وار : اوهلکی محبتندن ال چکمیسن . \n",
            "2023-01-19 14:41:53,308 - INFO - joeynmt.training - \tReference:  با این حال ، این عیب را در تو می‌بینم که محبت نخستین خود را از دست داده‌ای . \n",
            "2023-01-19 14:41:53,308 - INFO - joeynmt.training - \tHypothesis: اما تو از تو شکایت می کنم که محبتی به خود می کشم .\n",
            "2023-01-19 14:41:53,308 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:41:53,310 - INFO - joeynmt.training - \tSource:     آخایا والصی قاللیو اولان زامان یهودیلر بیرلهشرک پاولا حۆجوم ائتدیلر وه اونو والینین هؤکم کۆرسۆسۆ قارشیسینا چێخارتدێلار . \n",
            "2023-01-19 14:41:53,310 - INFO - joeynmt.training - \tReference:  اما زمانی که گالیو ، استاندار اخائیه بود ، یهودیان همگی بر ضد پولس برخاستند و او را به مقابل مسند داوری کشیدند . \n",
            "2023-01-19 14:41:53,311 - INFO - joeynmt.training - \tHypothesis: وقتی یهودیان با اهالی تصمیموتائوس و قسمه پولس را به قسم والی تمجید کردند ، او را به حضور او بردند .\n",
            "2023-01-19 14:42:01,058 - INFO - joeynmt.training - Epoch 397, Step:   138100, Batch Loss:     1.506352, Batch Acc: 0.604292, Tokens per Sec:    14905, Lr: 0.000024\n",
            "2023-01-19 14:42:07,691 - INFO - joeynmt.training - Epoch 397: total training loss 518.99\n",
            "2023-01-19 14:42:07,691 - INFO - joeynmt.training - EPOCH 398\n",
            "2023-01-19 14:42:08,830 - INFO - joeynmt.training - Epoch 398, Step:   138200, Batch Loss:     1.542041, Batch Acc: 0.600506, Tokens per Sec:    14937, Lr: 0.000024\n",
            "2023-01-19 14:42:16,524 - INFO - joeynmt.training - Epoch 398, Step:   138300, Batch Loss:     1.493751, Batch Acc: 0.607613, Tokens per Sec:    15588, Lr: 0.000024\n",
            "2023-01-19 14:42:24,129 - INFO - joeynmt.training - Epoch 398, Step:   138400, Batch Loss:     1.496451, Batch Acc: 0.605799, Tokens per Sec:    16014, Lr: 0.000024\n",
            "2023-01-19 14:42:31,846 - INFO - joeynmt.training - Epoch 398, Step:   138500, Batch Loss:     1.505692, Batch Acc: 0.604104, Tokens per Sec:    15624, Lr: 0.000024\n",
            "2023-01-19 14:42:34,482 - INFO - joeynmt.training - Epoch 398: total training loss 517.97\n",
            "2023-01-19 14:42:34,482 - INFO - joeynmt.training - EPOCH 399\n",
            "2023-01-19 14:42:39,709 - INFO - joeynmt.training - Epoch 399, Step:   138600, Batch Loss:     1.500538, Batch Acc: 0.608675, Tokens per Sec:    15494, Lr: 0.000024\n",
            "2023-01-19 14:42:47,457 - INFO - joeynmt.training - Epoch 399, Step:   138700, Batch Loss:     1.526009, Batch Acc: 0.607002, Tokens per Sec:    15497, Lr: 0.000024\n",
            "2023-01-19 14:42:55,071 - INFO - joeynmt.training - Epoch 399, Step:   138800, Batch Loss:     1.487875, Batch Acc: 0.604302, Tokens per Sec:    15727, Lr: 0.000024\n",
            "2023-01-19 14:43:01,280 - INFO - joeynmt.training - Epoch 399: total training loss 520.22\n",
            "2023-01-19 14:43:01,280 - INFO - joeynmt.training - EPOCH 400\n",
            "2023-01-19 14:43:02,661 - INFO - joeynmt.training - Epoch 400, Step:   138900, Batch Loss:     1.544835, Batch Acc: 0.608200, Tokens per Sec:    16255, Lr: 0.000024\n",
            "2023-01-19 14:43:10,383 - INFO - joeynmt.training - Epoch 400, Step:   139000, Batch Loss:     1.456751, Batch Acc: 0.607658, Tokens per Sec:    15663, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.07ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10474.39ex/s]\n",
            "2023-01-19 14:43:10,648 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=139000\n",
            "2023-01-19 14:43:10,649 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:43:16,112 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:43:16,113 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:43:16,113 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:43:16,114 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:43:16,117 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.06, loss:   2.85, ppl:  17.26, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.4165[sec], evaluation: 0.0440[sec]\n",
            "2023-01-19 14:43:16,120 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:43:16,123 - INFO - joeynmt.training - \tSource:      هم مۆژده وئرندیر وه هم ده قورخودان . اونلارێن اکسریتیتی اۆز دؤندهریب دینلهمز . \n",
            "2023-01-19 14:43:16,123 - INFO - joeynmt.training - \tReference:  بشارتگر و هشداردهنده است . و لی‌ بیشتر آنان رویگردان شدند ، در نتیجه چیزی را نمی‌شنوند . \n",
            "2023-01-19 14:43:16,123 - INFO - joeynmt.training - \tHypothesis: و بشارتگر و بیشترشان را بیم دهنده ای که بیشترشان از آن رویگردانیده است .\n",
            "2023-01-19 14:43:16,123 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:43:16,125 - INFO - joeynmt.training - \tSource:     گرک آختارام . \n",
            "2023-01-19 14:43:16,125 - INFO - joeynmt.training - \tReference:  باید بگردم‌ . \n",
            "2023-01-19 14:43:16,125 - INFO - joeynmt.training - \tHypothesis: باید پیدا به دنبال من باشیم .\n",
            "2023-01-19 14:43:16,126 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:43:16,127 - INFO - joeynmt.training - \tSource:      دئ : اگر اینسانلار وه جینلر بیر یئره یێغیشیب بو قور آنا بنزر بیر شئی گتیرمک اۆچۆن بیر بیرینه کؤمک ائتسهلر ، یئنه ده اونا بنزرینی گتیره بیلمزلر . \n",
            "2023-01-19 14:43:16,128 - INFO - joeynmt.training - \tReference:  بگو : اگر انس و جن گرد آیند تا نظیر این قرآن را بیاورند ، مانند آن را نخواهند آورد ، هر چند برخی از آنها پشتیبان برخی دیگر باشند . \n",
            "2023-01-19 14:43:16,128 - INFO - joeynmt.training - \tHypothesis: بگو : اگر مردم و انبوه ، برای هم جمعی همچون رهنمودی که رهن یا مثل مثل آورده ایم ، برای او یا کمی بیاورند .\n",
            "2023-01-19 14:43:16,128 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:43:16,130 - INFO - joeynmt.training - \tSource:     سونرا اونا دئدیلر : بس سن کیمسن ؟ بیزی گؤندهرنلره نه جاواب وئرک ؟ اؤزون بارهده نه دئییرسن ؟ \n",
            "2023-01-19 14:43:16,130 - INFO - joeynmt.training - \tReference:  آنگاه به او گفتند : پس به ما بگو تو کیستی تا بتوانیم برای آنان که ما را فرستاده‌اند ، جوابی ببریم ؛ در مورد خود چه می‌گویی ؟ \n",
            "2023-01-19 14:43:16,130 - INFO - joeynmt.training - \tHypothesis: سپس به او گفتند : تو کیستی ؟ پاسخ دادند : ما را به ما می گویی ؟\n",
            "2023-01-19 14:43:23,731 - INFO - joeynmt.training - Epoch 400, Step:   139100, Batch Loss:     1.486014, Batch Acc: 0.609048, Tokens per Sec:    15208, Lr: 0.000024\n",
            "2023-01-19 14:43:31,369 - INFO - joeynmt.training - Epoch 400, Step:   139200, Batch Loss:     1.431292, Batch Acc: 0.604120, Tokens per Sec:    15784, Lr: 0.000024\n",
            "2023-01-19 14:43:33,694 - INFO - joeynmt.training - Epoch 400: total training loss 518.43\n",
            "2023-01-19 14:43:33,694 - INFO - joeynmt.training - EPOCH 401\n",
            "2023-01-19 14:43:39,068 - INFO - joeynmt.training - Epoch 401, Step:   139300, Batch Loss:     1.560416, Batch Acc: 0.607202, Tokens per Sec:    15743, Lr: 0.000024\n",
            "2023-01-19 14:43:46,777 - INFO - joeynmt.training - Epoch 401, Step:   139400, Batch Loss:     1.449567, Batch Acc: 0.605571, Tokens per Sec:    15701, Lr: 0.000024\n",
            "2023-01-19 14:43:54,320 - INFO - joeynmt.training - Epoch 401, Step:   139500, Batch Loss:     1.466468, Batch Acc: 0.605247, Tokens per Sec:    15894, Lr: 0.000024\n",
            "2023-01-19 14:44:00,302 - INFO - joeynmt.training - Epoch 401: total training loss 520.04\n",
            "2023-01-19 14:44:00,302 - INFO - joeynmt.training - EPOCH 402\n",
            "2023-01-19 14:44:01,930 - INFO - joeynmt.training - Epoch 402, Step:   139600, Batch Loss:     1.338123, Batch Acc: 0.612221, Tokens per Sec:    15531, Lr: 0.000024\n",
            "2023-01-19 14:44:09,605 - INFO - joeynmt.training - Epoch 402, Step:   139700, Batch Loss:     1.614716, Batch Acc: 0.605648, Tokens per Sec:    16003, Lr: 0.000024\n",
            "2023-01-19 14:44:18,464 - INFO - joeynmt.training - Epoch 402, Step:   139800, Batch Loss:     1.496001, Batch Acc: 0.606653, Tokens per Sec:    13824, Lr: 0.000024\n",
            "2023-01-19 14:44:26,025 - INFO - joeynmt.training - Epoch 402, Step:   139900, Batch Loss:     1.529119, Batch Acc: 0.604855, Tokens per Sec:    15804, Lr: 0.000024\n",
            "2023-01-19 14:44:28,020 - INFO - joeynmt.training - Epoch 402: total training loss 516.78\n",
            "2023-01-19 14:44:28,020 - INFO - joeynmt.training - EPOCH 403\n",
            "2023-01-19 14:44:33,542 - INFO - joeynmt.training - Epoch 403, Step:   140000, Batch Loss:     1.557362, Batch Acc: 0.605053, Tokens per Sec:    16007, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 136.33ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10813.32ex/s]\n",
            "2023-01-19 14:44:33,799 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=140000\n",
            "2023-01-19 14:44:33,799 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:44:38,475 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:44:38,475 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:44:38,475 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:44:38,476 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:44:38,479 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.91, loss:   2.71, ppl:  15.04, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6285[sec], evaluation: 0.0444[sec]\n",
            "2023-01-19 14:44:38,493 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:44:38,496 - INFO - joeynmt.training - \tSource:     اونلار گؤیلردکیلرین تصویری وه کؤلگهسی اولان یئرده ابادت ائدیر ، نئجه کی موسا ابادت چادێرێنێ قورماق اۆزره اولاندا آللاه اونو خبردار ائتمیشدی : داغدا سنه گؤستریلن نمونهیه گؤره بونلارێن هامیسینی احتیاتلا دۆزلت . \n",
            "2023-01-19 14:44:38,497 - INFO - joeynmt.training - \tReference:  این کاهنان به خدمت مقدس مشغولند که مظهر و سایهٔ چیزهای آسمانی است ؛ موسی نیز هنگامی که می‌خواست خیمه را بسازد به او این حکم ال هی داده شد : دقت کن و همه چیز را مطابق نمونه‌ای بساز که در کوه به تو نشان داده شد . \n",
            "2023-01-19 14:44:38,497 - INFO - joeynmt.training - \tHypothesis: و آنچه در آسمانها و زمین است از آنچه در زمین است می پرستند ، تا آنچه را که در زمین است می پرستند و می پرستند : اینه را به تو نشان دهی ، به زودی خدا را به تمام آنچه انجام می دهد ، به خاطر تو نشان می دهد .\n",
            "2023-01-19 14:44:38,497 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:44:38,499 - INFO - joeynmt.training - \tSource:     کیم من آللاهێ سئویرم دئییر ، آمما اؤز قارداشێنا نفرت ائدیرسه ، یالانچیدیر . آخێ گؤردوگو قارداشێنێ سئومهین گؤرمهدیگی آللاهێ سئوه بیلمز . \n",
            "2023-01-19 14:44:38,499 - INFO - joeynmt.training - \tReference:  اگر کسی بگوید : خدا را دوست دارم ، اما از برادر خود نفرت داشته باشد دروغگوست ؛ زیرا هر که برادر خود را که دیده است دوست نداشته باشد ، نمی‌تواند خدایی را که ندیده است دوست داشته باشد . \n",
            "2023-01-19 14:44:38,499 - INFO - joeynmt.training - \tHypothesis: کسی که به خدا محبت می کند ، اما اگر خدا را دوست دارد ، از برادر خود نفرت دارد . اما کسی را که برادر خود را دوست می دارد ، از او دوست دارد .\n",
            "2023-01-19 14:44:38,499 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:44:38,501 - INFO - joeynmt.training - \tSource:     ائی گۆرزهلر نسلی ، سیز پیس اولدوغونوز حالدا نئجه یاخشی شئیلر سؤیلهیه بیلرسینیز ؟ چۆنکی اۆرک دولولوغوندان آغێز دانێشار . \n",
            "2023-01-19 14:44:38,502 - INFO - joeynmt.training - \tReference:  ای افعی‌زادگان ! چگونه می‌توانید سخن نیکو بگویید ، در حالی که خود شریرید ؟ زیرا زبان از آنچه دل از آن پر است ، سخن می‌گوید . \n",
            "2023-01-19 14:44:38,502 - INFO - joeynmt.training - \tHypothesis: ای شما ، چه در مورد شما چه نسلی که شریرانه می کنید ، با این حال سخن می گوید ؟ زیرا روح در دل خود پرجلال است .\n",
            "2023-01-19 14:44:38,502 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:44:38,504 - INFO - joeynmt.training - \tSource:     اونلار اونون اۆچۆن بئله بیر هییه قورماق ایستهدیلر ، بیز ایسه اونلارێ چوخ صفیل بیر وزیته سالدێق \n",
            "2023-01-19 14:44:38,504 - INFO - joeynmt.training - \tReference:  پس خواستند به او نیرنگی زنند ؛ و لی‌ ما آنان را پست گردانیدیم . \n",
            "2023-01-19 14:44:38,504 - INFO - joeynmt.training - \tHypothesis: و چون چنین برای او کوشاهی می خواستند ، و لی آنان را در کمال زحت وضعیت آنها را بر سر سفری فرو ریختیم .\n",
            "2023-01-19 14:44:46,125 - INFO - joeynmt.training - Epoch 403, Step:   140100, Batch Loss:     1.466132, Batch Acc: 0.606619, Tokens per Sec:    15101, Lr: 0.000024\n",
            "2023-01-19 14:44:53,711 - INFO - joeynmt.training - Epoch 403, Step:   140200, Batch Loss:     1.636040, Batch Acc: 0.603867, Tokens per Sec:    15902, Lr: 0.000024\n",
            "2023-01-19 14:44:59,531 - INFO - joeynmt.training - Epoch 403: total training loss 520.96\n",
            "2023-01-19 14:44:59,531 - INFO - joeynmt.training - EPOCH 404\n",
            "2023-01-19 14:45:01,364 - INFO - joeynmt.training - Epoch 404, Step:   140300, Batch Loss:     1.599011, Batch Acc: 0.604542, Tokens per Sec:    15767, Lr: 0.000024\n",
            "2023-01-19 14:45:09,041 - INFO - joeynmt.training - Epoch 404, Step:   140400, Batch Loss:     1.481674, Batch Acc: 0.608885, Tokens per Sec:    15858, Lr: 0.000024\n",
            "2023-01-19 14:45:16,765 - INFO - joeynmt.training - Epoch 404, Step:   140500, Batch Loss:     1.587209, Batch Acc: 0.608607, Tokens per Sec:    15547, Lr: 0.000024\n",
            "2023-01-19 14:45:24,410 - INFO - joeynmt.training - Epoch 404, Step:   140600, Batch Loss:     1.435491, Batch Acc: 0.601407, Tokens per Sec:    15915, Lr: 0.000024\n",
            "2023-01-19 14:45:26,189 - INFO - joeynmt.training - Epoch 404: total training loss 516.98\n",
            "2023-01-19 14:45:26,190 - INFO - joeynmt.training - EPOCH 405\n",
            "2023-01-19 14:45:32,076 - INFO - joeynmt.training - Epoch 405, Step:   140700, Batch Loss:     1.546680, Batch Acc: 0.610774, Tokens per Sec:    15855, Lr: 0.000024\n",
            "2023-01-19 14:45:39,634 - INFO - joeynmt.training - Epoch 405, Step:   140800, Batch Loss:     1.480888, Batch Acc: 0.609980, Tokens per Sec:    15874, Lr: 0.000024\n",
            "2023-01-19 14:45:47,234 - INFO - joeynmt.training - Epoch 405, Step:   140900, Batch Loss:     1.512977, Batch Acc: 0.601112, Tokens per Sec:    15884, Lr: 0.000024\n",
            "2023-01-19 14:45:52,694 - INFO - joeynmt.training - Epoch 405: total training loss 519.30\n",
            "2023-01-19 14:45:52,694 - INFO - joeynmt.training - EPOCH 406\n",
            "2023-01-19 14:45:54,820 - INFO - joeynmt.training - Epoch 406, Step:   141000, Batch Loss:     1.465639, Batch Acc: 0.601755, Tokens per Sec:    15608, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.07ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10447.34ex/s]\n",
            "2023-01-19 14:45:55,092 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=141000\n",
            "2023-01-19 14:45:55,092 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:46:00,228 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:46:00,228 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:46:00,228 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:46:00,229 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:46:00,232 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.32, loss:   2.80, ppl:  16.45, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0892[sec], evaluation: 0.0428[sec]\n",
            "2023-01-19 14:46:00,235 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:46:00,238 - INFO - joeynmt.training - \tSource:     بو ، آیهلریمیزی اینکار ائتدیکلرینه وه : سۆر سۆمۆک ، چۆرۆگۆب اووخالانمێش تورپاق اولدوغوموز حالدا ، بیز دیریلدیلیب یئنی بیر مخلوقمو اولاجاغێق ؟ دئدیکلرینه گؤره اونلارێن جزاسێدێر . \n",
            "2023-01-19 14:46:00,238 - INFO - joeynmt.training - \tReference:  جزای آنها این است ، چرا که آیات ما را انکار کردند و گفتند : آیا وقتی ما استخوان و خاک شدیم باز در آفرینشی جدید برانگیخته خواهیم شد ؟ \n",
            "2023-01-19 14:46:00,238 - INFO - joeynmt.training - \tHypothesis: و این کیفر بهان کفر ورزیدند و می گویند : آیا آنچه را که در آن ناخوش است برانگیخته ایم ، برانگیخته ایم ، و به زودی در آن صورت ، گفتیم : کیفر آنچه را که سرمست ناشای است ؟\n",
            "2023-01-19 14:46:00,238 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:46:00,240 - INFO - joeynmt.training - \tSource:     آللاه بو قدرتله مصیحده ائله فعالیت گؤستردی کی ، اونو اؤلولر آراسێندان دیریلدیب سمادا اؤز ساغێندا اوتورتدو\n",
            "2023-01-19 14:46:00,241 - INFO - joeynmt.training - \tReference:  که در خصوص مسیح به کار برد ، یعنی زمانی که او را از میان مردگان برخیزاند و در جایگاه‌های آسمانی بر دست راست خود نشاند ؛ \n",
            "2023-01-19 14:46:00,241 - INFO - joeynmt.training - \tHypothesis: خدا این قدرت را به خاطر مسیح در اتحاد با مسیحْ عیسی آشکار ساخت تا او را از مردگان برخیزانید و در میان مردگان برخیزانید .\n",
            "2023-01-19 14:46:00,241 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:46:00,243 - INFO - joeynmt.training - \tSource:     بیز موسایا دوققوز آشکار معؤ جۆزه وئردیک . اسراعل اوغوللاریندان سوروش : اونلارێن یانینا گلدیکده ، فیر اون اونا : یا موسا ! منه ائله گلیر کی ، سن اووسونلانمێشان ، دئمیشدی . \n",
            "2023-01-19 14:46:00,243 - INFO - joeynmt.training - \tReference:  و در حقیقت ، ما به موسی نه نشانه آشکار دادیم . پس ، از فرزندان اسرائیل بپرس آنگاه که نزد آنان آمد ، و فرعون به او گفت : ای موسی‌ ، من جدا تو را افسون‌شده می‌پندارم . \n",
            "2023-01-19 14:46:00,243 - INFO - joeynmt.training - \tHypothesis: و به موسی گفتیم : ای فرزندان اسرائیل ، برگردانیدیم . پس چون از فرزندان اسرائیلیان گفت : ای موسی ، پروردگار ما به او گفت : ای موسی ، تو به من خبر ده که تو همان کسی است که به او می رسد .\n",
            "2023-01-19 14:46:00,243 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:46:00,245 - INFO - joeynmt.training - \tSource:     اونون اؤؤلادلارێنێ آزارا سالێب اؤلدورهجهیم . بۆتۆن جمیتلر ده بیلهجک کی ، اینسانین داخلینی وه اۆرهیینی آراشدێران منم وه هر بیرینیزه امللرینیزه گؤره اوز وئرهجهیهم . \n",
            "2023-01-19 14:46:00,245 - INFO - joeynmt.training - \tReference:  فرزندان او را با بلای مرگبار خواهم کشت . به این ترتیب ، همهٔ جماعت‌ها خواهند دانست ، من آن کسی هستم که عمیق‌ترین افکار و دل‌ها را می‌کاود . من به هر یک از شما مطابق اعمالتان عوض خواهم داد . \n",
            "2023-01-19 14:46:00,245 - INFO - joeynmt.training - \tHypothesis: فرزندان او را شفا خواهم داد و حتی اگر جماعت را به دست آورد ، دل های او نیز خواهم نمود و دل خود را به هر یک از شما در دل خود قضاوت خواهم داد و از هر گونه اعمالش به انجام خواهم داد .\n",
            "2023-01-19 14:46:07,899 - INFO - joeynmt.training - Epoch 406, Step:   141100, Batch Loss:     1.469789, Batch Acc: 0.608325, Tokens per Sec:    15285, Lr: 0.000024\n",
            "2023-01-19 14:46:16,581 - INFO - joeynmt.training - Epoch 406, Step:   141200, Batch Loss:     1.547914, Batch Acc: 0.605172, Tokens per Sec:    14013, Lr: 0.000024\n",
            "2023-01-19 14:46:24,072 - INFO - joeynmt.training - Epoch 406, Step:   141300, Batch Loss:     1.501922, Batch Acc: 0.608800, Tokens per Sec:    16078, Lr: 0.000024\n",
            "2023-01-19 14:46:25,562 - INFO - joeynmt.training - Epoch 406: total training loss 517.62\n",
            "2023-01-19 14:46:25,562 - INFO - joeynmt.training - EPOCH 407\n",
            "2023-01-19 14:46:31,618 - INFO - joeynmt.training - Epoch 407, Step:   141400, Batch Loss:     1.569860, Batch Acc: 0.609121, Tokens per Sec:    15784, Lr: 0.000024\n",
            "2023-01-19 14:46:39,206 - INFO - joeynmt.training - Epoch 407, Step:   141500, Batch Loss:     1.453806, Batch Acc: 0.606835, Tokens per Sec:    15934, Lr: 0.000024\n",
            "2023-01-19 14:46:46,899 - INFO - joeynmt.training - Epoch 407, Step:   141600, Batch Loss:     1.362124, Batch Acc: 0.609244, Tokens per Sec:    15627, Lr: 0.000024\n",
            "2023-01-19 14:46:52,118 - INFO - joeynmt.training - Epoch 407: total training loss 517.94\n",
            "2023-01-19 14:46:52,118 - INFO - joeynmt.training - EPOCH 408\n",
            "2023-01-19 14:46:54,454 - INFO - joeynmt.training - Epoch 408, Step:   141700, Batch Loss:     1.495078, Batch Acc: 0.606244, Tokens per Sec:    16125, Lr: 0.000024\n",
            "2023-01-19 14:47:01,957 - INFO - joeynmt.training - Epoch 408, Step:   141800, Batch Loss:     1.480086, Batch Acc: 0.608393, Tokens per Sec:    16012, Lr: 0.000024\n",
            "2023-01-19 14:47:09,593 - INFO - joeynmt.training - Epoch 408, Step:   141900, Batch Loss:     1.521063, Batch Acc: 0.608717, Tokens per Sec:    15969, Lr: 0.000024\n",
            "2023-01-19 14:47:17,239 - INFO - joeynmt.training - Epoch 408, Step:   142000, Batch Loss:     1.349687, Batch Acc: 0.606967, Tokens per Sec:    15838, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.66ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10770.27ex/s]\n",
            "2023-01-19 14:47:17,499 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=142000\n",
            "2023-01-19 14:47:17,499 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:47:22,771 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:47:22,771 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:47:22,772 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:47:22,773 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:47:22,776 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  10.52, loss:   2.74, ppl:  15.54, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.2229[sec], evaluation: 0.0457[sec]\n",
            "2023-01-19 14:47:22,974 - INFO - joeynmt.helpers - delete RESULTS/model/105000.ckpt\n",
            "2023-01-19 14:47:22,999 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:47:23,003 - INFO - joeynmt.training - \tSource:     ائی ایمان گتیرنلر ! معؤ مین قادێنلار سیزین یانینیزا مۆحاجر کیمی گلدیکلری زامان اونلارێ امتهاهانا چکین . آللاه اونلارێن ایمانینی چوخ گؤزل بیلیر . اگر بونلارێن معؤ مین اولدوقلارینی بیلسهنیز ، آرتێق اونلارێ کافرلرین یانینا قایتارمایین . نه بونلار اونلارا ، نه ده اونلار بونلارا هالالدێر . اونلارێن خرجلهدیکلرینی اؤزلرینه قایتاریب وئرین . بونلارێن مئهرلرینی اؤزلرینه وئردیگینیز تقدیرده اونلارلا ائولنمهیینیزدن سیزه هئچ بیر گۆناه گلمز . کافر قادێنلارێ اؤز کبینینیز آلتێندا ساخلامایین . وئردیگینیز مئهری ایستهگین . صرف ائتدیکلری مئهری ایستهسینلر . آللاهێن هؤکمو بودور . او سیزین آرانێزدا هؤکم ائدر . آللاه بیلندیر ، هکمت صاحبدر ! \n",
            "2023-01-19 14:47:23,003 - INFO - joeynmt.training - \tReference:  ای کسانی که ایمان آورده‌اید ، چون زنان با ایمان مهاجر ، نزد شما آیند آنان را بیازمایید . خدا به ایمان آنان داناتر است . پس اگر آنان را باایمان تشخیص دادید ، دیگر ایشان را به سوی کافران بازنگردانید : نه آن زنان بر ایشان حلالند و نه آن مردان‌ بر این زنان حلال . و هر چه خرج این زنان‌ کرده‌اند به شوهران‌ آنها بدهید ، و بر شما گناهی نیست که در صورتی که مهرشان را به آنان بدهید با ایشان ازدواج کنید ، و به پیوندهای قبلی کافران متمسک نشوید و پایبند نباشید و آنچه را شما برای زنان مرتد و فراری خود که به کفار پناهنده شده‌اند خرج کرده‌اید ، از کافران‌ مطالبه کنید ، و آنها هم باید آنچه را خرج کرده‌اند از شما مطالبه کنند . این حکم خداست که‌ میان شما داوری می‌کند ، و خدا دانای حکیم است . \n",
            "2023-01-19 14:47:23,003 - INFO - joeynmt.training - \tHypothesis: ای کسانی که ایمان آورده اید ، چون خدا آنان را به وسیله شما بیمابند ، اموالی برای شما بپرسند . و اگر خدا آنان را از ایمان آورده اند ، آنان را به تنایشیند که به شما ایمان آورده اند ، به طوریش که کفر ورزیده اند ، به میان شما دو تن بهره منزود ، و خدا به آنچه انجام می دادند به آنان بازگرداند به سوی خود بازگردانیده\n",
            "2023-01-19 14:47:23,004 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:47:23,006 - INFO - joeynmt.training - \tSource:     بونو اونا ایمان ائدنلرین آلاجاقلاری روح بارهده سؤیلهدی . روح ایسه هله یوخ ایدی ، چۆنکی ایسا هله ایزتلنمهمیشدی . \n",
            "2023-01-19 14:47:23,006 - INFO - joeynmt.training - \tReference:  اما او این را در مورد روحی گفت که به‌زودی به کسانی داده می‌شد که به او ایمان می‌آوردند ؛ زیرا تا آن زمان روح عطا نشده بود ، چون عیسی هنوز جلال نیافته بود . \n",
            "2023-01-19 14:47:23,006 - INFO - joeynmt.training - \tHypothesis: اما آنانی که این را شنیدند ، گفته می شود : روحیهٔ ایمان داشت ؛ زیرا روحی که از او آگاه شده بود ، هنوز ناپذیر نبوده بود .\n",
            "2023-01-19 14:47:23,006 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:47:23,008 - INFO - joeynmt.training - \tSource:     ائی ایمان گتیرنلر ! بیر دسته ایله اۆز اۆزه گلدیکده مۆحکم اولون وه آللاهێ چوخ یادا سالێن کی ، نجات تاپاسێنێز ! \n",
            "2023-01-19 14:47:23,008 - INFO - joeynmt.training - \tReference:  ای کسانی که ایمان آورده‌اید ، چون با گروهی برخورد می‌کنید پایداری ورزید و خدا را بسیار یاد کنید ، باشد که رستگار شوید . \n",
            "2023-01-19 14:47:23,009 - INFO - joeynmt.training - \tHypothesis: ای کسانی که ایمان آورده اید ، گروهی از آنان روی برتاب و به یاد آورید و خدا را یاد کنید ، باشد که رستگار شوید .\n",
            "2023-01-19 14:47:23,009 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:47:23,011 - INFO - joeynmt.training - \tSource:     آللاه امین آمانلێق یوردونا چاغێرێر وه ایستهدیگینی دوغرو یولا سالێر ! \n",
            "2023-01-19 14:47:23,011 - INFO - joeynmt.training - \tReference:  و خدا شما را به سرای سلامت فرا می‌خواند ، و هر که را بخواهد به راه راست هدایت می‌کند . \n",
            "2023-01-19 14:47:23,011 - INFO - joeynmt.training - \tHypothesis: خدا هر که را بخواهد رحمتی به او می خواند ، و هر که را بخواهد هدایت کند .\n",
            "2023-01-19 14:47:24,308 - INFO - joeynmt.training - Epoch 408: total training loss 514.61\n",
            "2023-01-19 14:47:24,308 - INFO - joeynmt.training - EPOCH 409\n",
            "2023-01-19 14:47:30,903 - INFO - joeynmt.training - Epoch 409, Step:   142100, Batch Loss:     1.379947, Batch Acc: 0.606979, Tokens per Sec:    15425, Lr: 0.000024\n",
            "2023-01-19 14:47:38,461 - INFO - joeynmt.training - Epoch 409, Step:   142200, Batch Loss:     1.472402, Batch Acc: 0.607494, Tokens per Sec:    16212, Lr: 0.000024\n",
            "2023-01-19 14:47:46,081 - INFO - joeynmt.training - Epoch 409, Step:   142300, Batch Loss:     1.456295, Batch Acc: 0.606756, Tokens per Sec:    15770, Lr: 0.000024\n",
            "2023-01-19 14:47:51,896 - INFO - joeynmt.training - Epoch 409: total training loss 513.16\n",
            "2023-01-19 14:47:51,896 - INFO - joeynmt.training - EPOCH 410\n",
            "2023-01-19 14:47:54,912 - INFO - joeynmt.training - Epoch 410, Step:   142400, Batch Loss:     1.490343, Batch Acc: 0.609944, Tokens per Sec:    15109, Lr: 0.000024\n",
            "2023-01-19 14:48:02,556 - INFO - joeynmt.training - Epoch 410, Step:   142500, Batch Loss:     1.453852, Batch Acc: 0.605773, Tokens per Sec:    15899, Lr: 0.000024\n",
            "2023-01-19 14:48:10,343 - INFO - joeynmt.training - Epoch 410, Step:   142600, Batch Loss:     1.511091, Batch Acc: 0.605739, Tokens per Sec:    15760, Lr: 0.000024\n",
            "2023-01-19 14:48:18,116 - INFO - joeynmt.training - Epoch 410, Step:   142700, Batch Loss:     1.456136, Batch Acc: 0.609709, Tokens per Sec:    15681, Lr: 0.000024\n",
            "2023-01-19 14:48:18,686 - INFO - joeynmt.training - Epoch 410: total training loss 511.43\n",
            "2023-01-19 14:48:18,686 - INFO - joeynmt.training - EPOCH 411\n",
            "2023-01-19 14:48:25,722 - INFO - joeynmt.training - Epoch 411, Step:   142800, Batch Loss:     1.454511, Batch Acc: 0.610808, Tokens per Sec:    16056, Lr: 0.000024\n",
            "2023-01-19 14:48:33,320 - INFO - joeynmt.training - Epoch 411, Step:   142900, Batch Loss:     1.557337, Batch Acc: 0.608130, Tokens per Sec:    15850, Lr: 0.000024\n",
            "2023-01-19 14:48:41,014 - INFO - joeynmt.training - Epoch 411, Step:   143000, Batch Loss:     1.481935, Batch Acc: 0.607739, Tokens per Sec:    15811, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.43ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9521.68ex/s] \n",
            "2023-01-19 14:48:41,299 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=143000\n",
            "2023-01-19 14:48:41,299 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:48:46,363 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:48:46,364 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:48:46,364 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:48:46,365 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:48:46,368 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.19, loss:   2.85, ppl:  17.35, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0167[sec], evaluation: 0.0442[sec]\n",
            "2023-01-19 14:48:46,371 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:48:46,374 - INFO - joeynmt.training - \tSource:     منه چاتدێرێلاندا کی بو کیشیگه قارشێ بیر سوعی قصد قورولوب ، درحال اونو سنین یانینا گؤندردیم . اونو اتهام ائدنلرین ده شکایتلرینی سنه بیلدیرمهلرینی امر ائتدیم . \n",
            "2023-01-19 14:48:46,374 - INFO - joeynmt.training - \tReference:  اما چون بر من فاش شد که توطئه‌ای علیه او در کار است ، فورا او را نزد شما فرستادم و به متهم‌کنندگانش حکم کردم تا شکایتی را که علیه او دارند ، به شما بگویند . \n",
            "2023-01-19 14:48:46,374 - INFO - joeynmt.training - \tHypothesis: پس به من گفتم که مردی مجسمهٔ آن مرد را به سوی مسپاس برسد ، تا او را بر شاخه های خود بنوشم . پس ، او را به شکرگزاران رساندم و به تو حکم کردم .\n",
            "2023-01-19 14:48:46,374 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:48:46,376 - INFO - joeynmt.training - \tSource:     بونا امین اولاراق بیلیرم : ساغ قالاجاغام وه هامینیزلا بیرلیکده قالماقدا داوام ائدهجهیم کی ، اماندا هم ایرهلیلهیهسینیز ، هم ده سئوینهسینیز . \n",
            "2023-01-19 14:48:46,377 - INFO - joeynmt.training - \tReference:  پس ، از آنجا که به این امر اطمینان دارم ، می‌دانم که برای پیشرفت و شادی شما در ایمان ، در جسم خواهم ماند و با همهٔ شما به سر خواهم برد . \n",
            "2023-01-19 14:48:46,377 - INFO - joeynmt.training - \tHypothesis: از این رو ، اطمینان دارم که من به دست راست من در نگفتم و با تمامی تواناییم که باید در بندم و همدی کنید .\n",
            "2023-01-19 14:48:46,377 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:48:46,379 - INFO - joeynmt.training - \tSource:     اوندا نه دئیه بیلهریک ؟ اسراعل آختاردێغێنێ الده ائتمهدی ، سئچیلمیشلر الده ائتدیلر ؛ دیگرلری ایسه اینادکار اولدو . \n",
            "2023-01-19 14:48:46,379 - INFO - joeynmt.training - \tReference:  پس چه بگوییم ؟ اسرائیل به آنچه مشتاقانه در پی آن بود ، دست نیافت ، بلکه برگزیدگان به آن دست یافتند . سایرین سختدل گردیدند ، \n",
            "2023-01-19 14:48:46,379 - INFO - joeynmt.training - \tHypothesis: پس چه بگوییم ؟ آنان تا به دنبال اسرائیل برویم و حتی یکی از آنان برگزیده نشدند .\n",
            "2023-01-19 14:48:46,379 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:48:46,381 - INFO - joeynmt.training - \tSource:     او بؤیوک اژداها ابلیس وه شیطان دئییلن ، بۆتۆن دۆنیانی آلدادان قدیم ایلان یئر اۆزۆنه آتێلدێ ، ملکلری ده اونونلا بیرگه آتێلدێ . \n",
            "2023-01-19 14:48:46,381 - INFO - joeynmt.training - \tReference:  پس آن اژدهای بزرگ به پایین افکنده شد ؛ یعنی همان مار کهن که ابلیس و شیطان خوانده می‌شود و تمام ساکنان زمین را گمراه می‌کند . او به همراه فرشتگانش به زمین افکنده شد . \n",
            "2023-01-19 14:48:46,382 - INFO - joeynmt.training - \tHypothesis: او اژدها بسیار ، ابلیس و شیطان است که تمام دنیا و شیطان را از تمام زمین پایین افکنده بود و فرشتگانش را در بیابان افکند .\n",
            "2023-01-19 14:48:50,613 - INFO - joeynmt.training - Epoch 411: total training loss 513.35\n",
            "2023-01-19 14:48:50,614 - INFO - joeynmt.training - EPOCH 412\n",
            "2023-01-19 14:48:54,175 - INFO - joeynmt.training - Epoch 412, Step:   143100, Batch Loss:     1.510113, Batch Acc: 0.610301, Tokens per Sec:    15641, Lr: 0.000024\n",
            "2023-01-19 14:49:01,798 - INFO - joeynmt.training - Epoch 412, Step:   143200, Batch Loss:     1.464354, Batch Acc: 0.607068, Tokens per Sec:    15721, Lr: 0.000024\n",
            "2023-01-19 14:49:09,420 - INFO - joeynmt.training - Epoch 412, Step:   143300, Batch Loss:     1.462007, Batch Acc: 0.609227, Tokens per Sec:    15920, Lr: 0.000024\n",
            "2023-01-19 14:49:17,137 - INFO - joeynmt.training - Epoch 412, Step:   143400, Batch Loss:     1.474254, Batch Acc: 0.607788, Tokens per Sec:    15675, Lr: 0.000024\n",
            "2023-01-19 14:49:17,307 - INFO - joeynmt.training - Epoch 412: total training loss 515.71\n",
            "2023-01-19 14:49:17,307 - INFO - joeynmt.training - EPOCH 413\n",
            "2023-01-19 14:49:24,818 - INFO - joeynmt.training - Epoch 413, Step:   143500, Batch Loss:     1.499484, Batch Acc: 0.611617, Tokens per Sec:    15944, Lr: 0.000024\n",
            "2023-01-19 14:49:32,534 - INFO - joeynmt.training - Epoch 413, Step:   143600, Batch Loss:     1.472950, Batch Acc: 0.606161, Tokens per Sec:    15624, Lr: 0.000024\n",
            "2023-01-19 14:49:40,158 - INFO - joeynmt.training - Epoch 413, Step:   143700, Batch Loss:     1.482402, Batch Acc: 0.605950, Tokens per Sec:    15770, Lr: 0.000024\n",
            "2023-01-19 14:49:43,999 - INFO - joeynmt.training - Epoch 413: total training loss 515.72\n",
            "2023-01-19 14:49:44,000 - INFO - joeynmt.training - EPOCH 414\n",
            "2023-01-19 14:49:48,941 - INFO - joeynmt.training - Epoch 414, Step:   143800, Batch Loss:     1.412710, Batch Acc: 0.614646, Tokens per Sec:    12252, Lr: 0.000024\n",
            "2023-01-19 14:49:56,523 - INFO - joeynmt.training - Epoch 414, Step:   143900, Batch Loss:     1.567210, Batch Acc: 0.609218, Tokens per Sec:    15956, Lr: 0.000024\n",
            "2023-01-19 14:50:04,145 - INFO - joeynmt.training - Epoch 414, Step:   144000, Batch Loss:     1.475407, Batch Acc: 0.605116, Tokens per Sec:    15688, Lr: 0.000024\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.83ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10031.37ex/s]\n",
            "2023-01-19 14:50:04,414 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=144000\n",
            "2023-01-19 14:50:04,414 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:50:09,249 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:50:09,249 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:50:09,249 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:50:09,250 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:50:09,253 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.24, loss:   2.72, ppl:  15.24, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7882[sec], evaluation: 0.0429[sec]\n",
            "2023-01-19 14:50:09,256 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:50:09,259 - INFO - joeynmt.training - \tSource:     محبت آتانێن امرلرینه گؤره هیات سۆرمهییمیزی طلب ائدیر . اولدن بریع ائشیتدیگینیز کیمی امر بودور ؛ گرک بو امره گؤره هیات سۆرهسینیز . \n",
            "2023-01-19 14:50:09,259 - INFO - joeynmt.training - \tReference:  محبت به این معنی است که مطابق احکام او گام برداریم . چنان که از آغاز شنیده‌اید ، آن حکم این است که همچنان در محبت گام بردارید ؛ \n",
            "2023-01-19 14:50:09,259 - INFO - joeynmt.training - \tHypothesis: محبتی که از طریق محبت به جا آوریم ، این است که از پیش از این رو ، چنین است ؛ همچون اکنون نیز شنیده اید ؛ زیرا این احکام را به جا آوردید .\n",
            "2023-01-19 14:50:09,259 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:50:09,261 - INFO - joeynmt.training - \tSource:     بئلهجه بشر اوغلو گلمهدی کی ، اونا خدمت ائتسینلر ، گلدی کی ، اؤزو خدمت ائتسین وه چوخلارێنێ ساتێن آلماق اۆچۆن اؤز جانێنێ فدیه وئرسین . \n",
            "2023-01-19 14:50:09,262 - INFO - joeynmt.training - \tReference:  به همین گونه ، پسر انسان نیامد تا به او خدمت شود ، بلکه آمد تا خدمت کند و جان خود را همچون بهای رهایی در عوض بسیاری بدهد . \n",
            "2023-01-19 14:50:09,262 - INFO - joeynmt.training - \tHypothesis: پس پسر انسان آمد تا خدمت کند تا او را به او خدمت کند و جان خود را به بار دهد و جان خود را بدهد .\n",
            "2023-01-19 14:50:09,262 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:50:09,264 - INFO - joeynmt.training - \tSource:     ایمان واسطهسیله پادشاهێن قزهبیندن قورخماییب میصیردن چێخدێ ، چۆنکی او ، گؤزه گؤرونمهینی سانکی گؤرموش کیمی مۆحکم دایاندی . \n",
            "2023-01-19 14:50:09,264 - INFO - joeynmt.training - \tReference:  با ایمان بود که موسی مصر را بدون ترس از خشم پادشاه ترک کرد ؛ زیرا ثابت‌قدم بود ، چنان که گویی آن نادیده را می‌دید . \n",
            "2023-01-19 14:50:09,264 - INFO - joeynmt.training - \tHypothesis: از ایمان بود که او از طریق مصر ایمان ، عده ای از دریافت کردن ترس به تمنا کرد ؛ زیرا چشمان خود را دیده بود و سامری همچون شیر داده بود .\n",
            "2023-01-19 14:50:09,264 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:50:09,266 - INFO - joeynmt.training - \tSource:      نامازێ گۆندۆزۆن ایکی باشێندا وه گئجهنین به زی ساعاتلارێندا قێل . حقیقتا ، یاخشی امللر پیس ایشلهری یویوب آپارار . بو ، یادا سالانلارا اؤیود نصیحتدیر . \n",
            "2023-01-19 14:50:09,267 - INFO - joeynmt.training - \tReference:  و در دو طرف روز اول و آخر آن‌ و نخستین ساعات شب نماز را برپا دار ، زیرا خوبیها بدیها را از میان می‌برد . این برای پندگیرندگان ، پندی است . \n",
            "2023-01-19 14:50:09,267 - INFO - joeynmt.training - \tHypothesis: و نماز را در روز و دو باغ درمی آورد ، و گمانهای شایسته کنند که کارهای شایسته انجام داده اند . قطعا به یاد آوردن کارهای شایسته کرده اند ، این تذکر ده .\n",
            "2023-01-19 14:50:16,854 - INFO - joeynmt.training - Epoch 414: total training loss 514.59\n",
            "2023-01-19 14:50:16,854 - INFO - joeynmt.training - EPOCH 415\n",
            "2023-01-19 14:50:17,006 - INFO - joeynmt.training - Epoch 415, Step:   144100, Batch Loss:     1.451597, Batch Acc: 0.621743, Tokens per Sec:    16208, Lr: 0.000024\n",
            "2023-01-19 14:50:24,692 - INFO - joeynmt.training - Epoch 415, Step:   144200, Batch Loss:     1.478577, Batch Acc: 0.609948, Tokens per Sec:    15684, Lr: 0.000024\n",
            "2023-01-19 14:50:32,453 - INFO - joeynmt.training - Epoch 415, Step:   144300, Batch Loss:     1.529114, Batch Acc: 0.606468, Tokens per Sec:    15576, Lr: 0.000024\n",
            "2023-01-19 14:50:40,168 - INFO - joeynmt.training - Epoch 415, Step:   144400, Batch Loss:     1.495147, Batch Acc: 0.607141, Tokens per Sec:    15672, Lr: 0.000024\n",
            "2023-01-19 14:50:43,710 - INFO - joeynmt.training - Epoch 415: total training loss 514.70\n",
            "2023-01-19 14:50:43,711 - INFO - joeynmt.training - EPOCH 416\n",
            "2023-01-19 14:50:47,849 - INFO - joeynmt.training - Epoch 416, Step:   144500, Batch Loss:     1.485299, Batch Acc: 0.608363, Tokens per Sec:    15656, Lr: 0.000024\n",
            "2023-01-19 14:50:55,515 - INFO - joeynmt.training - Epoch 416, Step:   144600, Batch Loss:     1.475175, Batch Acc: 0.610396, Tokens per Sec:    15737, Lr: 0.000024\n",
            "2023-01-19 14:51:03,076 - INFO - joeynmt.training - Epoch 416, Step:   144700, Batch Loss:     1.478268, Batch Acc: 0.608763, Tokens per Sec:    15795, Lr: 0.000024\n",
            "2023-01-19 14:51:10,318 - INFO - joeynmt.training - Epoch 416: total training loss 514.73\n",
            "2023-01-19 14:51:10,318 - INFO - joeynmt.training - EPOCH 417\n",
            "2023-01-19 14:51:10,784 - INFO - joeynmt.training - Epoch 417, Step:   144800, Batch Loss:     1.519733, Batch Acc: 0.611727, Tokens per Sec:    15511, Lr: 0.000024\n",
            "2023-01-19 14:51:18,468 - INFO - joeynmt.training - Epoch 417, Step:   144900, Batch Loss:     1.423171, Batch Acc: 0.607659, Tokens per Sec:    15639, Lr: 0.000023\n",
            "2023-01-19 14:51:27,251 - INFO - joeynmt.training - Epoch 417, Step:   145000, Batch Loss:     1.401943, Batch Acc: 0.609348, Tokens per Sec:    13659, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 116.93ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10058.04ex/s]\n",
            "2023-01-19 14:51:27,535 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=145000\n",
            "2023-01-19 14:51:27,535 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:51:31,980 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:51:31,981 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:51:31,981 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:51:31,982 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:51:31,985 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.50, loss:   2.79, ppl:  16.27, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3961[sec], evaluation: 0.0466[sec]\n",
            "2023-01-19 14:51:31,988 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:51:31,991 - INFO - joeynmt.training - \tSource:     هر بیرینیز آللاهێ تانیمآیان بۆتپرستلر کیمی شهوت وه احترآسلا دئییل ، مۆقدسلیک وه هؤرمتله آرواد آلماغێ اؤیرهنین ، \n",
            "2023-01-19 14:51:31,992 - INFO - joeynmt.training - \tReference:  هر یک از شما باید بداند که برای قدوسیت و نیکنامی ، چگونه بر بدن خود تسلط داشته باشد\n",
            "2023-01-19 14:51:31,992 - INFO - joeynmt.training - \tHypothesis: هر یک از شما خدا را می شناسم ، به خاطر شریرانهٔ شکل و شکل و شکوه ، به مقدسان بی حرمت نیست ، بلکه زنی از مقدسان به رفتار بی شرمانه تعلیم می دهد ،\n",
            "2023-01-19 14:51:31,992 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:51:31,994 - INFO - joeynmt.training - \tSource:     وه ذکات وئرمهیی قاداغان ائدرلر . \n",
            "2023-01-19 14:51:31,994 - INFO - joeynmt.training - \tReference:  و از دادن‌ زکات و وسایل و مایحتاج خانه‌ خودداری می‌ورزند . \n",
            "2023-01-19 14:51:31,994 - INFO - joeynmt.training - \tHypothesis: و زکات را از آن باز می دارند و آنان را از آنچه می کنند .\n",
            "2023-01-19 14:51:31,994 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:51:31,996 - INFO - joeynmt.training - \tSource:     بۆتۆن رحبرلرینیزی وه مۆقدسلرین هامیسینی سالاملایین . ایتالییادان اولانلار سیزه سالام گؤندهریرلر . \n",
            "2023-01-19 14:51:31,997 - INFO - joeynmt.training - \tReference:  سلام مرا به تمام کسانی که هدایت را در میان شما بر عهده دارند و به تمام مقدسان برسانید . برادران در ایتالیا به شما سلام می‌رسانند . \n",
            "2023-01-19 14:51:31,997 - INFO - joeynmt.training - \tHypothesis: همهٔ کسانی که به شما وقایف مقدسان را به شما سلام می رسانند ، به آنان سلام می رسانند .\n",
            "2023-01-19 14:51:31,997 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:51:31,999 - INFO - joeynmt.training - \tSource:     بو واخت او گئدیب اؤزوندن داها بئتر یئددی باشقا روحو گؤتورر . اونلار دا اورایا گیریب مسکونلاشار . اوندا بو آدامێن آخێرێ اوهلکیندن داها پیس اولار . \n",
            "2023-01-19 14:51:31,999 - INFO - joeynmt.training - \tReference:  آنگاه می‌رود و هفت روح شریرتر از خود را می‌آورد و با آنان به آن خانه داخل گشته و در آن ساکن می‌شوند . در نتیجه ، عاقبت آن شخص از ابتدایش بدتر می‌شود . \n",
            "2023-01-19 14:51:31,999 - INFO - joeynmt.training - \tHypothesis: آنگاه رود اراده رفت و هفت روح به آن هفت روح ، به کوهی رفت . آنان به آن سوی رود و در آنجا زمان حاضر می شود .\n",
            "2023-01-19 14:51:39,723 - INFO - joeynmt.training - Epoch 417, Step:   145100, Batch Loss:     1.509496, Batch Acc: 0.610438, Tokens per Sec:    14986, Lr: 0.000023\n",
            "2023-01-19 14:51:43,034 - INFO - joeynmt.training - Epoch 417: total training loss 515.81\n",
            "2023-01-19 14:51:43,035 - INFO - joeynmt.training - EPOCH 418\n",
            "2023-01-19 14:51:47,356 - INFO - joeynmt.training - Epoch 418, Step:   145200, Batch Loss:     1.433108, Batch Acc: 0.609102, Tokens per Sec:    16042, Lr: 0.000023\n",
            "2023-01-19 14:51:55,009 - INFO - joeynmt.training - Epoch 418, Step:   145300, Batch Loss:     1.451687, Batch Acc: 0.611233, Tokens per Sec:    15743, Lr: 0.000023\n",
            "2023-01-19 14:52:02,754 - INFO - joeynmt.training - Epoch 418, Step:   145400, Batch Loss:     1.507463, Batch Acc: 0.609959, Tokens per Sec:    15648, Lr: 0.000023\n",
            "2023-01-19 14:52:09,848 - INFO - joeynmt.training - Epoch 418: total training loss 513.82\n",
            "2023-01-19 14:52:09,849 - INFO - joeynmt.training - EPOCH 419\n",
            "2023-01-19 14:52:10,539 - INFO - joeynmt.training - Epoch 419, Step:   145500, Batch Loss:     1.312199, Batch Acc: 0.613564, Tokens per Sec:    15500, Lr: 0.000023\n",
            "2023-01-19 14:52:18,257 - INFO - joeynmt.training - Epoch 419, Step:   145600, Batch Loss:     1.543756, Batch Acc: 0.611473, Tokens per Sec:    15483, Lr: 0.000023\n",
            "2023-01-19 14:52:25,947 - INFO - joeynmt.training - Epoch 419, Step:   145700, Batch Loss:     1.544075, Batch Acc: 0.612024, Tokens per Sec:    15581, Lr: 0.000023\n",
            "2023-01-19 14:52:33,601 - INFO - joeynmt.training - Epoch 419, Step:   145800, Batch Loss:     1.400862, Batch Acc: 0.608129, Tokens per Sec:    15572, Lr: 0.000023\n",
            "2023-01-19 14:52:36,833 - INFO - joeynmt.training - Epoch 419: total training loss 517.49\n",
            "2023-01-19 14:52:36,833 - INFO - joeynmt.training - EPOCH 420\n",
            "2023-01-19 14:52:41,198 - INFO - joeynmt.training - Epoch 420, Step:   145900, Batch Loss:     1.450569, Batch Acc: 0.613736, Tokens per Sec:    15905, Lr: 0.000023\n",
            "2023-01-19 14:52:48,798 - INFO - joeynmt.training - Epoch 420, Step:   146000, Batch Loss:     1.521665, Batch Acc: 0.607910, Tokens per Sec:    15964, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.76ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9916.05ex/s] \n",
            "2023-01-19 14:52:49,069 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=146000\n",
            "2023-01-19 14:52:49,070 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:52:54,580 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:52:54,581 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:52:54,581 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:52:54,582 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:52:54,585 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.58, loss:   2.81, ppl:  16.54, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.4651[sec], evaluation: 0.0425[sec]\n",
            "2023-01-19 14:52:54,588 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:52:54,591 - INFO - joeynmt.training - \tSource:     او ، سفرهدن قالخێب اۆست پالتارێنێ بیر یانا قویدو وه بیر دسمال گؤتوروب بئلینه باغلادێ . \n",
            "2023-01-19 14:52:54,591 - INFO - joeynmt.training - \tReference:  از سر سفرهٔ شام برخاست و ردای خود را به کناری گذاشت . دستمالی نیز گرفت و به کمر بست . \n",
            "2023-01-19 14:52:54,591 - INFO - joeynmt.training - \tHypothesis: پس او بر سر سفره نشست و ردای خود را برداشت و یکی از قایق سوم را گرفت و بستر بسته شد .\n",
            "2023-01-19 14:52:54,591 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:52:54,593 - INFO - joeynmt.training - \tSource:     ائلهجه ده سارماشان باغلار . \n",
            "2023-01-19 14:52:54,593 - INFO - joeynmt.training - \tReference:  و باغهای در هم پیچیده و انبوه . \n",
            "2023-01-19 14:52:54,593 - INFO - joeynmt.training - \tHypothesis: و همین طور که سارد می کند .\n",
            "2023-01-19 14:52:54,594 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:52:54,596 - INFO - joeynmt.training - \tSource:     اگر رب او گۆنلری قێسالتماسایدی ، هئچ بیر اینسان خلاص اولا بیلمزدی . آمما رب سئچیلمیشلره اؤز سئچدیگی اینسانلارا گؤره او گۆنلری قێسالدێب . \n",
            "2023-01-19 14:52:54,596 - INFO - joeynmt.training - \tReference:  در واقع ، اگر یهوه آن روزها را کوتاه نمی‌کرد ، هیچ کس نجات نمی‌یافت . اما به خاطر برگزیدگانی که خود برگزیده ، آن روزها را کوتاه کرده است . \n",
            "2023-01-19 14:52:54,596 - INFO - joeynmt.training - \tHypothesis: یهوه می تواند روزها را در دست داشته باشد ، اما هیچ انسانی نجات نخواهد یافت . اما اگر یهوه برگزیده می شود ، روزه برای انسان ها برگزیده شده است .\n",
            "2023-01-19 14:52:54,596 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:52:54,598 - INFO - joeynmt.training - \tSource:     کۆفر ائدنلر ده ، سیزین مسجیدالحراما داخل اولماغێنێزا مانئچیلیک تؤرهدنلر ده ، قوربانلێقلاری اؤز یئرینه گئدیب چاتماغا قویمایانلار دا محض اونلاردێر . اگر تانیمادیغینیز معؤ مین کیشی وه قادێنلارێ بیلمهدن آیاق آلتێنا آلێب ازمک وه بوندان دولایی سیزه گۆناه گلمک احتمالێ اولماسایدی . آمما آللاه ایستهدیگینی اؤز مرحمتینه قوووشدورسون دئیه . اگر اونلار بیر بیریندن سئچیلیب آیریلمیش اولسایدیلار ، بیز اونلاردان کافر اولانلارێ شدتلی بیر ازابا دۆچار ائدردیک ! \n",
            "2023-01-19 14:52:54,598 - INFO - joeynmt.training - \tReference:  آنها بودند که کفر ورزیدند و شما را از مسجد الحرام بازداشتند و نگذاشتند قربانی شما که بازداشته شده بود به محلش برسد ، و اگر در مکه‌ مردان و زنان با ایمانی نبودند که ممکن بود بی‌آنکه آنان را بشناسید ، ندانسته پایمالشان کنید و تاوانشان بر شما بماند فرمان حمله به مکه می‌دادیم‌ تا خدا هر که را بخواهد در جوار رحمت خویش درآورد . اگر کافر و مؤمن‌ از هم متمایز می‌شدند ، قطعا کافران را به عذاب دردناکی معذب می‌داشتیم . \n",
            "2023-01-19 14:52:54,598 - INFO - joeynmt.training - \tHypothesis: و کسانی که کفر ورزیده اند ، برای شما درآیند ، و می گویند : اگر از جانب خود به درای ناچیزی ، و اگر از خانه های خود به جا آورم ، آنان از جهان وجدانی که کفر ورزیده اند ، به طور پس اگر از ایشان که کفر ورزد ، گناهی را از عذابی دردناک بگیرند .\n",
            "2023-01-19 14:53:02,169 - INFO - joeynmt.training - Epoch 420, Step:   146100, Batch Loss:     1.454265, Batch Acc: 0.611629, Tokens per Sec:    15316, Lr: 0.000023\n",
            "2023-01-19 14:53:09,140 - INFO - joeynmt.training - Epoch 420: total training loss 514.07\n",
            "2023-01-19 14:53:09,140 - INFO - joeynmt.training - EPOCH 421\n",
            "2023-01-19 14:53:09,832 - INFO - joeynmt.training - Epoch 421, Step:   146200, Batch Loss:     1.452974, Batch Acc: 0.611096, Tokens per Sec:    15908, Lr: 0.000023\n",
            "2023-01-19 14:53:18,081 - INFO - joeynmt.training - Epoch 421, Step:   146300, Batch Loss:     1.432050, Batch Acc: 0.611700, Tokens per Sec:    14633, Lr: 0.000023\n",
            "2023-01-19 14:53:26,586 - INFO - joeynmt.training - Epoch 421, Step:   146400, Batch Loss:     1.391567, Batch Acc: 0.608516, Tokens per Sec:    14233, Lr: 0.000023\n",
            "2023-01-19 14:53:34,375 - INFO - joeynmt.training - Epoch 421, Step:   146500, Batch Loss:     1.448808, Batch Acc: 0.608090, Tokens per Sec:    15556, Lr: 0.000023\n",
            "2023-01-19 14:53:37,541 - INFO - joeynmt.training - Epoch 421: total training loss 512.28\n",
            "2023-01-19 14:53:37,542 - INFO - joeynmt.training - EPOCH 422\n",
            "2023-01-19 14:53:42,183 - INFO - joeynmt.training - Epoch 422, Step:   146600, Batch Loss:     1.505043, Batch Acc: 0.608234, Tokens per Sec:    15960, Lr: 0.000023\n",
            "2023-01-19 14:53:49,955 - INFO - joeynmt.training - Epoch 422, Step:   146700, Batch Loss:     1.569012, Batch Acc: 0.609819, Tokens per Sec:    15479, Lr: 0.000023\n",
            "2023-01-19 14:53:57,842 - INFO - joeynmt.training - Epoch 422, Step:   146800, Batch Loss:     1.451509, Batch Acc: 0.610256, Tokens per Sec:    15369, Lr: 0.000023\n",
            "2023-01-19 14:54:04,648 - INFO - joeynmt.training - Epoch 422: total training loss 512.80\n",
            "2023-01-19 14:54:04,648 - INFO - joeynmt.training - EPOCH 423\n",
            "2023-01-19 14:54:05,663 - INFO - joeynmt.training - Epoch 423, Step:   146900, Batch Loss:     1.458821, Batch Acc: 0.616210, Tokens per Sec:    15331, Lr: 0.000023\n",
            "2023-01-19 14:54:13,334 - INFO - joeynmt.training - Epoch 423, Step:   147000, Batch Loss:     1.306597, Batch Acc: 0.607890, Tokens per Sec:    15734, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 78.19ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9191.60ex/s]\n",
            "2023-01-19 14:54:13,631 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=147000\n",
            "2023-01-19 14:54:13,631 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:54:18,361 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:54:18,361 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:54:18,362 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:54:18,363 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:54:18,366 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.64, loss:   2.83, ppl:  16.87, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.6856[sec], evaluation: 0.0413[sec]\n",
            "2023-01-19 14:54:18,368 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:54:18,372 - INFO - joeynmt.training - \tSource:     اونلار بیر بیریندن حال اهوال توتماغا باشلایاجاقلار . \n",
            "2023-01-19 14:54:18,372 - INFO - joeynmt.training - \tReference:  و برخی‌شان رو به برخی کنند و از هم پرسند ، \n",
            "2023-01-19 14:54:18,372 - INFO - joeynmt.training - \tHypothesis: و از یکی از یکدیگر جدا گردند .\n",
            "2023-01-19 14:54:18,372 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:54:18,374 - INFO - joeynmt.training - \tSource:     اوندان اول موسانێن رحبر وه مرحمت اولان کیتابی وار ایدی . بو عرب دیلینده تصدیق ائدن ، زالیملاری قورخوتماق وه یاخشی امل صاحبلرینه مۆژده وئرمک اۆچۆن اولان بیر کیتابدیر ! \n",
            "2023-01-19 14:54:18,374 - INFO - joeynmt.training - \tReference:  و حال آنکه‌ پیش از آن ، کتاب موسی ، راهبر و مایه‌ رحمتی بود ؛ و این قرآن‌ کتابی است به زبان عربی که تصدیق‌کننده آن‌ است ، تا کسانی را که ستم کرده‌اند هشدار دهد و برای نیکوکاران مژده‌ای باشد . \n",
            "2023-01-19 14:54:18,375 - INFO - joeynmt.training - \tHypothesis: کتابی پیش از آن موسی ، رحمت و رحمتی بود . این است آنچه را که به زبان عنوان کتاب و تصدیق می کند به آن ، و کسانی که تقوا و بشارتگر و مستحقه دهان کتابی .\n",
            "2023-01-19 14:54:18,375 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:54:18,377 - INFO - joeynmt.training - \tSource:     او کسلر کی ، مملهکتلرده تۆغیان ائدیر ، \n",
            "2023-01-19 14:54:18,377 - INFO - joeynmt.training - \tReference:  همانان که در شهرها سر به طغیان برداشتند ، \n",
            "2023-01-19 14:54:18,377 - INFO - joeynmt.training - \tHypothesis: همانان که با تقریب می کنند ،\n",
            "2023-01-19 14:54:18,377 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:54:18,379 - INFO - joeynmt.training - \tSource:     واختلێ واختێندا اونلارا یئمک وئرمک اۆچۆن آغاسێنێن اؤز نؤکرلری اۆزهرینه قویدوغو صادق وه آغێللێ قول کیمدیر ؟ \n",
            "2023-01-19 14:54:18,379 - INFO - joeynmt.training - \tReference:   به‌راستی آن غلام امین و دانا کیست که اربابش او را بر خادمان خانهٔ خود گماشت تا به‌موقع خوراک آنان را بدهد ؟ \n",
            "2023-01-19 14:54:18,379 - INFO - joeynmt.training - \tHypothesis: کیست روز مواد خود را به آنان بدهد تا در مقابل آن تعقل کند و غلامانش را بر خادمان خود بپرستد ؟\n",
            "2023-01-19 14:54:26,006 - INFO - joeynmt.training - Epoch 423, Step:   147100, Batch Loss:     1.460650, Batch Acc: 0.612223, Tokens per Sec:    15011, Lr: 0.000023\n",
            "2023-01-19 14:54:33,625 - INFO - joeynmt.training - Epoch 423, Step:   147200, Batch Loss:     1.401679, Batch Acc: 0.607719, Tokens per Sec:    15906, Lr: 0.000023\n",
            "2023-01-19 14:54:36,413 - INFO - joeynmt.training - Epoch 423: total training loss 514.63\n",
            "2023-01-19 14:54:36,413 - INFO - joeynmt.training - EPOCH 424\n",
            "2023-01-19 14:54:41,281 - INFO - joeynmt.training - Epoch 424, Step:   147300, Batch Loss:     1.438492, Batch Acc: 0.610720, Tokens per Sec:    16067, Lr: 0.000023\n",
            "2023-01-19 14:54:48,927 - INFO - joeynmt.training - Epoch 424, Step:   147400, Batch Loss:     1.562393, Batch Acc: 0.610569, Tokens per Sec:    15866, Lr: 0.000023\n",
            "2023-01-19 14:54:57,263 - INFO - joeynmt.training - Epoch 424, Step:   147500, Batch Loss:     1.489607, Batch Acc: 0.607916, Tokens per Sec:    14371, Lr: 0.000023\n",
            "2023-01-19 14:55:04,039 - INFO - joeynmt.training - Epoch 424: total training loss 512.75\n",
            "2023-01-19 14:55:04,039 - INFO - joeynmt.training - EPOCH 425\n",
            "2023-01-19 14:55:05,267 - INFO - joeynmt.training - Epoch 425, Step:   147600, Batch Loss:     1.536905, Batch Acc: 0.608899, Tokens per Sec:    16040, Lr: 0.000023\n",
            "2023-01-19 14:55:13,005 - INFO - joeynmt.training - Epoch 425, Step:   147700, Batch Loss:     1.449757, Batch Acc: 0.612265, Tokens per Sec:    15604, Lr: 0.000023\n",
            "2023-01-19 14:55:20,645 - INFO - joeynmt.training - Epoch 425, Step:   147800, Batch Loss:     1.427891, Batch Acc: 0.609327, Tokens per Sec:    15802, Lr: 0.000023\n",
            "2023-01-19 14:55:28,314 - INFO - joeynmt.training - Epoch 425, Step:   147900, Batch Loss:     1.370346, Batch Acc: 0.609227, Tokens per Sec:    15737, Lr: 0.000023\n",
            "2023-01-19 14:55:30,812 - INFO - joeynmt.training - Epoch 425: total training loss 511.89\n",
            "2023-01-19 14:55:30,813 - INFO - joeynmt.training - EPOCH 426\n",
            "2023-01-19 14:55:36,048 - INFO - joeynmt.training - Epoch 426, Step:   148000, Batch Loss:     1.442524, Batch Acc: 0.615834, Tokens per Sec:    15551, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 129.51ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10374.46ex/s]\n",
            "2023-01-19 14:55:36,314 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=148000\n",
            "2023-01-19 14:55:36,314 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:55:41,375 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:55:41,376 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:55:41,376 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:55:41,377 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:55:41,380 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.83, loss:   2.74, ppl:  15.43, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0160[sec], evaluation: 0.0427[sec]\n",
            "2023-01-19 14:55:41,383 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:55:41,386 - INFO - joeynmt.training - \tSource:     بئلهلیکله ، زینداندا یهیانین بوینونو ووردوردو . \n",
            "2023-01-19 14:55:41,386 - INFO - joeynmt.training - \tReference:  پس فرستاد تا در زندان سر یحیی را از تنش جدا کنند . \n",
            "2023-01-19 14:55:41,386 - INFO - joeynmt.training - \tHypothesis: پس در زندانی از زن یحیی دستگیر خود را در تنشید .\n",
            "2023-01-19 14:55:41,386 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:55:41,388 - INFO - joeynmt.training - \tSource:      ایکتی گۆن سونرا پاسخا بایرامی اولاجاغێنێ بیلیرسینیز . بشر اوغلو چارمێخا چکیلمک اۆچۆن تسلیم ائدیلهجک . \n",
            "2023-01-19 14:55:41,389 - INFO - joeynmt.training - \tReference:   شما می‌دانید که دو روز دیگر عید پسح است و پسر انسان به دست دشمنان تحویل داده خواهد شد تا بر تیر اعدام شود . \n",
            "2023-01-19 14:55:41,389 - INFO - joeynmt.training - \tHypothesis: اما هنگام عید پسح ، عید پسح ، پسر انسان را به تیر میخکوب خواهند کرد . او به تیر شکنجهٔ پسر انسان خیانت خواهد شد .\n",
            "2023-01-19 14:55:41,389 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:55:41,391 - INFO - joeynmt.training - \tSource:     کناردان اینسانین داخلینه گیرن هئچ بیر شئی اینسانی موردار ائده بیلمز . آمما اینسانی موردار ائدن شئیلر داخلیندن چێخانلاردێر . \n",
            "2023-01-19 14:55:41,391 - INFO - joeynmt.training - \tReference:  هیچ چیز نمی‌تواند از بیرون به انسان وارد شود و او را نجس کند ، بلکه آنچه از انسان بیرون می‌آید ، او را نجس می‌سازد . \n",
            "2023-01-19 14:55:41,391 - INFO - joeynmt.training - \tHypothesis: هیچ انسانی نمی تواند به کنار راه وارد شود ، هیچ انسانی نمی تواند به خود داخل شود . اما انسانی ناپاک از اعمال درآیند ، اما از انسان ها درهٔ هنوز در آتش می رود .\n",
            "2023-01-19 14:55:41,391 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:55:41,394 - INFO - joeynmt.training - \tSource:      نه اۆچۆن شاگردلرین آغساققاللاردان قالان آدت انهنهنی پوزور وه چؤرک یئدیکلری زامان اللهرینی یومورلار ؟ \n",
            "2023-01-19 14:55:41,394 - INFO - joeynmt.training - \tReference:   چرا شاگردان تو سنت گذشتگان را زیر پا می‌گذارند ؟ مثلا ، پیش از خوردن غذا دست‌هایشان را آب نمی‌کشند ! \n",
            "2023-01-19 14:55:41,394 - INFO - joeynmt.training - \tHypothesis: چرا تکهٔ باقی ماندگانی که از شاگردانت باقی مانده بودند ، نان و عجیب های پیش روسته بودند ، دست هایش را خوردند ؟\n",
            "2023-01-19 14:55:49,024 - INFO - joeynmt.training - Epoch 426, Step:   148100, Batch Loss:     1.611281, Batch Acc: 0.608140, Tokens per Sec:    15332, Lr: 0.000023\n",
            "2023-01-19 14:55:56,604 - INFO - joeynmt.training - Epoch 426, Step:   148200, Batch Loss:     1.452540, Batch Acc: 0.609285, Tokens per Sec:    15837, Lr: 0.000023\n",
            "2023-01-19 14:56:02,704 - INFO - joeynmt.training - Epoch 426: total training loss 512.46\n",
            "2023-01-19 14:56:02,704 - INFO - joeynmt.training - EPOCH 427\n",
            "2023-01-19 14:56:04,227 - INFO - joeynmt.training - Epoch 427, Step:   148300, Batch Loss:     1.388623, Batch Acc: 0.613069, Tokens per Sec:    15825, Lr: 0.000023\n",
            "2023-01-19 14:56:11,862 - INFO - joeynmt.training - Epoch 427, Step:   148400, Batch Loss:     1.506652, Batch Acc: 0.611879, Tokens per Sec:    15820, Lr: 0.000023\n",
            "2023-01-19 14:56:19,481 - INFO - joeynmt.training - Epoch 427, Step:   148500, Batch Loss:     1.555428, Batch Acc: 0.607905, Tokens per Sec:    15843, Lr: 0.000023\n",
            "2023-01-19 14:56:27,135 - INFO - joeynmt.training - Epoch 427, Step:   148600, Batch Loss:     1.466815, Batch Acc: 0.607770, Tokens per Sec:    15644, Lr: 0.000023\n",
            "2023-01-19 14:56:29,359 - INFO - joeynmt.training - Epoch 427: total training loss 512.13\n",
            "2023-01-19 14:56:29,360 - INFO - joeynmt.training - EPOCH 428\n",
            "2023-01-19 14:56:34,706 - INFO - joeynmt.training - Epoch 428, Step:   148700, Batch Loss:     1.405502, Batch Acc: 0.611886, Tokens per Sec:    15782, Lr: 0.000023\n",
            "2023-01-19 14:56:42,316 - INFO - joeynmt.training - Epoch 428, Step:   148800, Batch Loss:     1.511922, Batch Acc: 0.612198, Tokens per Sec:    15936, Lr: 0.000023\n",
            "2023-01-19 14:56:49,942 - INFO - joeynmt.training - Epoch 428, Step:   148900, Batch Loss:     1.430297, Batch Acc: 0.609416, Tokens per Sec:    15855, Lr: 0.000023\n",
            "2023-01-19 14:56:57,088 - INFO - joeynmt.training - Epoch 428: total training loss 512.91\n",
            "2023-01-19 14:56:57,089 - INFO - joeynmt.training - EPOCH 429\n",
            "2023-01-19 14:56:58,790 - INFO - joeynmt.training - Epoch 429, Step:   149000, Batch Loss:     1.535298, Batch Acc: 0.616838, Tokens per Sec:    15625, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 140.32ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8912.71ex/s]\n",
            "2023-01-19 14:56:59,086 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=149000\n",
            "2023-01-19 14:56:59,086 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:57:04,007 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:57:04,007 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:57:04,007 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:57:04,008 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:57:04,011 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.40, loss:   2.79, ppl:  16.29, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.8736[sec], evaluation: 0.0442[sec]\n",
            "2023-01-19 14:57:04,014 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:57:04,017 - INFO - joeynmt.training - \tSource:     نه بختیاردیر اۆرهیی تمیز اولانلار ! چۆنکی اونلار آللاهێ گؤرهجک . \n",
            "2023-01-19 14:57:04,017 - INFO - joeynmt.training - \tReference:   خوشا به حال آنان که دلی پاک دارند ؛ زیرا خدا را خواهند دید . \n",
            "2023-01-19 14:57:04,018 - INFO - joeynmt.training - \tHypothesis: خوشا به حال آنان که دل خود را پاک می گرداند ؛ زیرا خدا را می بیند .\n",
            "2023-01-19 14:57:04,018 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:57:04,020 - INFO - joeynmt.training - \tSource:      بیز سنی حاقق ایله مۆژده وئرمهیه وه قورخوتماغا گوندردیک . جهنم اهلی بارهسینده ایسه سن سورغو سوعالا توتولمایاجاقسان . \n",
            "2023-01-19 14:57:04,020 - INFO - joeynmt.training - \tReference:  ما تو را بحق فرستادیم ، تا بشارتگر و بیم‌دهنده باشی ، و لی‌ درباره دوزخیان ، از تو پرسشی نخواهد شد . \n",
            "2023-01-19 14:57:04,020 - INFO - joeynmt.training - \tHypothesis: و ما تو را به حق بشارت دهنده و بیم ده تا پندی ، و لی از اهل دوزخ را بدان بیمناک نخواهی کرد .\n",
            "2023-01-19 14:57:04,020 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:57:04,022 - INFO - joeynmt.training - \tSource:     اونو اله سالاندان سونرا خالاتێ اینیندن چێخارێب اؤز پالتارێنێ گئییندیردیلر . سونرا اونو چارمێخا چکمهیه آپاردێلار . \n",
            "2023-01-19 14:57:04,022 - INFO - joeynmt.training - \tReference:  سرانجام ، پس از تمسخر کردن او ، ردا را از تنش درآوردند و لباسش را بر تنش کردند و او را از آنجا بردند تا به تیر میخکوب کنند . \n",
            "2023-01-19 14:57:04,023 - INFO - joeynmt.training - \tHypothesis: پس از آن ، دستش را درازخش کرد و از لباسش را بر تیرهم کردند . سپس او را به تیر میخکوب کردند .\n",
            "2023-01-19 14:57:04,023 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:57:04,025 - INFO - joeynmt.training - \tSource:     ایسا مسیحین دوغولماسێ بئله اولدو . آناسێ مریم یوصفه نیشانلانمیشدی . آمما بیرلیکده اولمالاریندان اول مریمین مۆقدس روحدان حامله اولدوغو آشکار اولدو . \n",
            "2023-01-19 14:57:04,025 - INFO - joeynmt.training - \tReference:  اما تولد عیسی مسیح به این صورت بود : زمانی که مریم ، مادر عیسی ، نامزد یوسف بود ، پیش از آن که به خانهٔ شوهر برود ، معلوم شد که به وسیلهٔ روح‌القدس باردار شده است . \n",
            "2023-01-19 14:57:04,025 - INFO - joeynmt.training - \tHypothesis: به علاوه ، عیسی مسیح ، مریم مصرار شد و مریم مصر بود . اما مریم با این که از روح القدس بود ، از طریق روح القدس آشکار شد .\n",
            "2023-01-19 14:57:11,756 - INFO - joeynmt.training - Epoch 429, Step:   149100, Batch Loss:     1.402699, Batch Acc: 0.613549, Tokens per Sec:    15081, Lr: 0.000023\n",
            "2023-01-19 14:57:19,395 - INFO - joeynmt.training - Epoch 429, Step:   149200, Batch Loss:     1.451951, Batch Acc: 0.612312, Tokens per Sec:    15750, Lr: 0.000023\n",
            "2023-01-19 14:57:26,998 - INFO - joeynmt.training - Epoch 429, Step:   149300, Batch Loss:     1.465806, Batch Acc: 0.610642, Tokens per Sec:    15864, Lr: 0.000023\n",
            "2023-01-19 14:57:29,053 - INFO - joeynmt.training - Epoch 429: total training loss 510.62\n",
            "2023-01-19 14:57:29,054 - INFO - joeynmt.training - EPOCH 430\n",
            "2023-01-19 14:57:34,774 - INFO - joeynmt.training - Epoch 430, Step:   149400, Batch Loss:     1.378616, Batch Acc: 0.612949, Tokens per Sec:    15683, Lr: 0.000023\n",
            "2023-01-19 14:57:42,498 - INFO - joeynmt.training - Epoch 430, Step:   149500, Batch Loss:     1.496743, Batch Acc: 0.613417, Tokens per Sec:    15684, Lr: 0.000023\n",
            "2023-01-19 14:57:50,145 - INFO - joeynmt.training - Epoch 430, Step:   149600, Batch Loss:     1.579381, Batch Acc: 0.610817, Tokens per Sec:    15820, Lr: 0.000023\n",
            "2023-01-19 14:57:55,717 - INFO - joeynmt.training - Epoch 430: total training loss 507.46\n",
            "2023-01-19 14:57:55,718 - INFO - joeynmt.training - EPOCH 431\n",
            "2023-01-19 14:57:57,899 - INFO - joeynmt.training - Epoch 431, Step:   149700, Batch Loss:     1.524811, Batch Acc: 0.616955, Tokens per Sec:    15497, Lr: 0.000023\n",
            "2023-01-19 14:58:05,601 - INFO - joeynmt.training - Epoch 431, Step:   149800, Batch Loss:     1.590681, Batch Acc: 0.609649, Tokens per Sec:    15638, Lr: 0.000023\n",
            "2023-01-19 14:58:13,391 - INFO - joeynmt.training - Epoch 431, Step:   149900, Batch Loss:     1.449130, Batch Acc: 0.611619, Tokens per Sec:    15346, Lr: 0.000023\n",
            "2023-01-19 14:58:21,158 - INFO - joeynmt.training - Epoch 431, Step:   150000, Batch Loss:     1.440671, Batch Acc: 0.614786, Tokens per Sec:    15623, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 126.74ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10645.86ex/s]\n",
            "2023-01-19 14:58:21,435 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=150000\n",
            "2023-01-19 14:58:21,435 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:58:27,145 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:58:27,146 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:58:27,146 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:58:27,147 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:58:27,150 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.19, loss:   2.86, ppl:  17.52, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.6513[sec], evaluation: 0.0552[sec]\n",
            "2023-01-19 14:58:27,153 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:58:27,156 - INFO - joeynmt.training - \tSource:     هامێ سیناقوق رعیسی سوستئنی توتوب هؤکم کۆرسۆسۆنۆن قارشیسیندا دؤیدو . قاللیو ایسه بو حادثهلره احمیت وئرمهدی . \n",
            "2023-01-19 14:58:27,157 - INFO - joeynmt.training - \tReference:  پس همگی ، سوستنیس ، مسئول کنیسه را گرفتند و او را در مقابل مسند داوری زدند . اما گالیو به این امور کاملا بی‌اعتنا بود . \n",
            "2023-01-19 14:58:27,157 - INFO - joeynmt.training - \tHypothesis: اما همگی در کنیسه ای در کنیسهٔ خود ایستاد و با صدای بلند به مقابل مسند داوری نشست .\n",
            "2023-01-19 14:58:27,157 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:58:27,159 - INFO - joeynmt.training - \tSource:     بیلین وه آگاه اولون کی ، گؤیلرده وه یئرده نه وارسا ، آللاهێندێر . او ، حقیقتا ، سیزین نه امل صاحبی اولدوغونوزو بیلیر . آللاه اونون هۆزورونا قایتاریلاجاقلاری گۆن اونلارا نه ائتدیکلرینی بیلدیرهجکدیر . آللاه هر شئیی بیلندیر ! \n",
            "2023-01-19 14:58:27,159 - INFO - joeynmt.training - \tReference:  هش‌دار که آنچه در آسمانها و زمین است از آن خداست . به یقین آنچه را که بر آنید می‌داند ، و روزی که به سوی او بازگردانیده می‌شوند آنان را از حقیقت‌ آنچه انجام داده‌اند خبر می‌دهد ، و خدا به هر چیزی داناست . \n",
            "2023-01-19 14:58:27,160 - INFO - joeynmt.training - \tHypothesis: آگاه باشید که آنچه در آسمانها و زمین است از آن خداست ، و او هر چه را بخواهد می داند ، و اوست آنچه را که در آن اوست ، و به سوی او بازگردانیده می شوند ، و خدا به آنچه انجام می دهد آگاه خواهد کرد .\n",
            "2023-01-19 14:58:27,160 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:58:27,162 - INFO - joeynmt.training - \tSource:     او ، عالی شورانێن قرارێ وه امهلی ایله رازێ اولمامێشدێ . بو آدام یهودئیانین آریمآتئیا شهریندن ایدی وه او دا آللاهێن پادشاهلیغینی گؤزلهییردی . \n",
            "2023-01-19 14:58:27,162 - INFO - joeynmt.training - \tReference:   او اهل رامه ، یکی از شهرهای یهودیان بود و انتظار پادشاهی خدا را می‌کشید . \n",
            "2023-01-19 14:58:27,162 - INFO - joeynmt.training - \tHypothesis: او در میان یهودیه و بیت عنیا در یهودیه و بیت عنیا بود . او از این رو ، او در یهودی و پادشاهی خدا نیز در انتظارش بود .\n",
            "2023-01-19 14:58:27,162 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:58:27,164 - INFO - joeynmt.training - \tSource:     اورادا هر مئیوهدن ایکی نؤؤ واردێر . \n",
            "2023-01-19 14:58:27,164 - INFO - joeynmt.training - \tReference:  در آن دو باغ‌ از هر میوه‌ای دو گونه است . \n",
            "2023-01-19 14:58:27,165 - INFO - joeynmt.training - \tHypothesis: در آنجا دو دو باغستان است .\n",
            "2023-01-19 14:58:28,912 - INFO - joeynmt.training - Epoch 431: total training loss 510.37\n",
            "2023-01-19 14:58:28,913 - INFO - joeynmt.training - EPOCH 432\n",
            "2023-01-19 14:58:36,119 - INFO - joeynmt.training - Epoch 432, Step:   150100, Batch Loss:     1.346465, Batch Acc: 0.612313, Tokens per Sec:    13461, Lr: 0.000023\n",
            "2023-01-19 14:58:43,783 - INFO - joeynmt.training - Epoch 432, Step:   150200, Batch Loss:     1.398937, Batch Acc: 0.612754, Tokens per Sec:    15698, Lr: 0.000023\n",
            "2023-01-19 14:58:51,501 - INFO - joeynmt.training - Epoch 432, Step:   150300, Batch Loss:     1.440243, Batch Acc: 0.610981, Tokens per Sec:    15686, Lr: 0.000023\n",
            "2023-01-19 14:58:56,697 - INFO - joeynmt.training - Epoch 432: total training loss 508.78\n",
            "2023-01-19 14:58:56,697 - INFO - joeynmt.training - EPOCH 433\n",
            "2023-01-19 14:58:59,388 - INFO - joeynmt.training - Epoch 433, Step:   150400, Batch Loss:     1.414771, Batch Acc: 0.616156, Tokens per Sec:    15195, Lr: 0.000023\n",
            "2023-01-19 14:59:07,116 - INFO - joeynmt.training - Epoch 433, Step:   150500, Batch Loss:     1.457860, Batch Acc: 0.613634, Tokens per Sec:    15539, Lr: 0.000023\n",
            "2023-01-19 14:59:14,874 - INFO - joeynmt.training - Epoch 433, Step:   150600, Batch Loss:     1.558284, Batch Acc: 0.614794, Tokens per Sec:    15277, Lr: 0.000023\n",
            "2023-01-19 14:59:22,630 - INFO - joeynmt.training - Epoch 433, Step:   150700, Batch Loss:     1.437046, Batch Acc: 0.608353, Tokens per Sec:    15610, Lr: 0.000023\n",
            "2023-01-19 14:59:23,921 - INFO - joeynmt.training - Epoch 433: total training loss 510.76\n",
            "2023-01-19 14:59:23,921 - INFO - joeynmt.training - EPOCH 434\n",
            "2023-01-19 14:59:30,454 - INFO - joeynmt.training - Epoch 434, Step:   150800, Batch Loss:     1.444391, Batch Acc: 0.613542, Tokens per Sec:    15742, Lr: 0.000023\n",
            "2023-01-19 14:59:38,187 - INFO - joeynmt.training - Epoch 434, Step:   150900, Batch Loss:     1.416291, Batch Acc: 0.612038, Tokens per Sec:    15620, Lr: 0.000023\n",
            "2023-01-19 14:59:45,851 - INFO - joeynmt.training - Epoch 434, Step:   151000, Batch Loss:     1.463653, Batch Acc: 0.614618, Tokens per Sec:    15629, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 140.45ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10632.62ex/s]\n",
            "2023-01-19 14:59:46,146 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=151000\n",
            "2023-01-19 14:59:46,146 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 14:59:51,247 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 14:59:51,247 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 14:59:51,247 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 14:59:51,248 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 14:59:51,251 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.02, loss:   2.68, ppl:  14.64, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0509[sec], evaluation: 0.0463[sec]\n",
            "2023-01-19 14:59:51,254 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 14:59:51,258 - INFO - joeynmt.training - \tSource:     بونا گؤره اگر بیر اۆزۆ اعذاب چکیرسه ، باشقالاری دا اونونلا بیرگه اعذاب چکیر . اۆزۆلردن بیری شرفه چاتێرسا ، باشقالاری دا اونونلا بیرگه سئوینیر . \n",
            "2023-01-19 14:59:51,258 - INFO - joeynmt.training - \tReference:  اگر عضوی دردمند شود ، تمام اعضای دیگر با آن درد خواهند کشید . اگر عضوی نیز عزت یابد ، تمام اعضای دیگر با او شادی خواهند کرد . \n",
            "2023-01-19 14:59:51,258 - INFO - joeynmt.training - \tHypothesis: پس اگر مزدی که در رنج بکشد ، دیگری سختگیرند و با او رنجی متحمل می کنند . اما اگر یکی از آن شریر را به همراه او می رساند ، دیگری با او شادی بسیار به همراه او می رساند .\n",
            "2023-01-19 14:59:51,258 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 14:59:51,260 - INFO - joeynmt.training - \tSource:     قادێن باجاردێغێنێ ائتدی . منیم بدهنیمه دفن اۆچۆن اولجهدن یاغ چکدی . \n",
            "2023-01-19 14:59:51,261 - INFO - joeynmt.training - \tReference:  این زن آنچه در توان داشت ، انجام داد و مرا با ریختن این روغن معطر بر بدن من ، پیشاپیش برای تدفین آماده کرد . \n",
            "2023-01-19 14:59:51,261 - INFO - joeynmt.training - \tHypothesis: زنی که آن زن را شک کرد ، به سبب بدن من آمد .\n",
            "2023-01-19 14:59:51,261 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 14:59:51,263 - INFO - joeynmt.training - \tSource:      پاک صحیفهلری اونلارا اوخویان ، آللاه ترهفیندن گؤندهریلمیش پیغمبردیر . \n",
            "2023-01-19 14:59:51,263 - INFO - joeynmt.training - \tReference:  فرستاده‌ای از جانب خدا که بر آنان‌ صحیفه‌هایی پاک را تلاوت کند ، \n",
            "2023-01-19 14:59:51,263 - INFO - joeynmt.training - \tHypothesis: و به پاکی فرستاده ای که آنان را به سوی آنان فرستاده شده است ، و پیامبر او فرستاده شده است .\n",
            "2023-01-19 14:59:51,263 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 14:59:51,265 - INFO - joeynmt.training - \tSource:     اوندا بۆتۆن توپلانتێ ساکیتلشدی وه بارنابا ایله پاولا قولاق آسماغا باشلادێلار . بارنابا ایله پاول آللاهێن اونلار واسطهسیله باشقا ملتلر آراسێندا گؤستردیگی علامت وه خارقهلرین هامیسینی دانێشدێ . \n",
            "2023-01-19 14:59:51,266 - INFO - joeynmt.training - \tReference:  آنگاه تمام آن جمع ساکت شدند و به سخنان برنابا و پولس در مورد بسیاری نشانه‌ها و معجزات که خدا از طریق آنان در میان قوم‌ها به ظهور رسانده بود ، گوش فرادادند . \n",
            "2023-01-19 14:59:51,266 - INFO - joeynmt.training - \tHypothesis: سپس تمامی گرد هم آمد و برنابا را به سخنان خود رساندند و پولس به سخنان خود و برنابا و پولس برایشان معجزات نکردند . این معجزات را برای آنان تعقیب کرده بود و این معجزات توف کرده بودند .\n",
            "2023-01-19 14:59:56,212 - INFO - joeynmt.training - Epoch 434: total training loss 508.18\n",
            "2023-01-19 14:59:56,213 - INFO - joeynmt.training - EPOCH 435\n",
            "2023-01-19 14:59:59,093 - INFO - joeynmt.training - Epoch 435, Step:   151100, Batch Loss:     1.579206, Batch Acc: 0.613062, Tokens per Sec:    15465, Lr: 0.000023\n",
            "2023-01-19 15:00:06,860 - INFO - joeynmt.training - Epoch 435, Step:   151200, Batch Loss:     1.556124, Batch Acc: 0.616788, Tokens per Sec:    15403, Lr: 0.000023\n",
            "2023-01-19 15:00:14,620 - INFO - joeynmt.training - Epoch 435, Step:   151300, Batch Loss:     1.439354, Batch Acc: 0.610287, Tokens per Sec:    15494, Lr: 0.000023\n",
            "2023-01-19 15:00:22,401 - INFO - joeynmt.training - Epoch 435, Step:   151400, Batch Loss:     1.452270, Batch Acc: 0.609642, Tokens per Sec:    15573, Lr: 0.000023\n",
            "2023-01-19 15:00:23,356 - INFO - joeynmt.training - Epoch 435: total training loss 510.55\n",
            "2023-01-19 15:00:23,357 - INFO - joeynmt.training - EPOCH 436\n",
            "2023-01-19 15:00:30,212 - INFO - joeynmt.training - Epoch 436, Step:   151500, Batch Loss:     1.516637, Batch Acc: 0.615118, Tokens per Sec:    15511, Lr: 0.000023\n",
            "2023-01-19 15:00:37,953 - INFO - joeynmt.training - Epoch 436, Step:   151600, Batch Loss:     1.448786, Batch Acc: 0.614320, Tokens per Sec:    15669, Lr: 0.000023\n",
            "2023-01-19 15:00:46,830 - INFO - joeynmt.training - Epoch 436, Step:   151700, Batch Loss:     1.520145, Batch Acc: 0.611001, Tokens per Sec:    13502, Lr: 0.000023\n",
            "2023-01-19 15:00:51,518 - INFO - joeynmt.training - Epoch 436: total training loss 508.88\n",
            "2023-01-19 15:00:51,518 - INFO - joeynmt.training - EPOCH 437\n",
            "2023-01-19 15:00:54,638 - INFO - joeynmt.training - Epoch 437, Step:   151800, Batch Loss:     1.413096, Batch Acc: 0.612784, Tokens per Sec:    15842, Lr: 0.000023\n",
            "2023-01-19 15:01:02,395 - INFO - joeynmt.training - Epoch 437, Step:   151900, Batch Loss:     1.448518, Batch Acc: 0.611497, Tokens per Sec:    15706, Lr: 0.000023\n",
            "2023-01-19 15:01:10,154 - INFO - joeynmt.training - Epoch 437, Step:   152000, Batch Loss:     1.473116, Batch Acc: 0.614868, Tokens per Sec:    15459, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.65ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10794.19ex/s]\n",
            "2023-01-19 15:01:10,433 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=152000\n",
            "2023-01-19 15:01:10,433 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:01:15,202 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:01:15,202 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:01:15,202 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:01:15,203 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:01:15,206 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.62, loss:   2.83, ppl:  16.89, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7228[sec], evaluation: 0.0425[sec]\n",
            "2023-01-19 15:01:15,209 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:01:15,212 - INFO - joeynmt.training - \tSource:     نیه ؟ سیزی سئومهدیگیمه گؤرهمی ؟ آللاه بیلیر کی ، سئویرم . \n",
            "2023-01-19 15:01:15,213 - INFO - joeynmt.training - \tReference:  چرا از شما چیزی نپذیرفته‌ام ؟ آیا به این دلیل است که شما را دوست ندارم ؟ خدا می‌داند که دوستتان دارم . \n",
            "2023-01-19 15:01:15,213 - INFO - joeynmt.training - \tHypothesis: چرا به همین دلیل ، به شما محبت نمی کنم ؟ خدا دوست می دارم که محبت می کند .\n",
            "2023-01-19 15:01:15,213 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:01:15,215 - INFO - joeynmt.training - \tSource:      یا موسا ! سنی اؤز جاماآتێندان آییریب تلسدیرن نه ایدی ؟ \n",
            "2023-01-19 15:01:15,215 - INFO - joeynmt.training - \tReference:   و ای موسی ، چه چیز تو را دور از قوم خودت ، به شتاب واداشته است ؟ \n",
            "2023-01-19 15:01:15,215 - INFO - joeynmt.training - \tHypothesis: ای موسی ، تو را از قوم خود بر قوم خود می زدند و چرا تو را از قوم خود تباه کرد ؟\n",
            "2023-01-19 15:01:15,215 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:01:15,217 - INFO - joeynmt.training - \tSource:     ربینین آیهلرینه اینانلار ؛ \n",
            "2023-01-19 15:01:15,217 - INFO - joeynmt.training - \tReference:  و کسانی که به نشانه‌های پروردگارشان ایمان می‌آورند ، \n",
            "2023-01-19 15:01:15,217 - INFO - joeynmt.training - \tHypothesis: و کسانی که به آیات پروردگارشان ایمان ندارند ،\n",
            "2023-01-19 15:01:15,218 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:01:15,220 - INFO - joeynmt.training - \tSource:     ائلهجه ده سیزدن دوغرو دۆز اولماق ایستهینلر اۆچۆن . \n",
            "2023-01-19 15:01:15,220 - INFO - joeynmt.training - \tReference:  برای هر یک از شما که خواهد به راه راست رود . \n",
            "2023-01-19 15:01:15,220 - INFO - joeynmt.training - \tHypothesis: پس اگر راست می خواهند برای شما بس است .\n",
            "2023-01-19 15:01:23,025 - INFO - joeynmt.training - Epoch 437, Step:   152100, Batch Loss:     1.401432, Batch Acc: 0.614974, Tokens per Sec:    15036, Lr: 0.000023\n",
            "2023-01-19 15:01:23,507 - INFO - joeynmt.training - Epoch 437: total training loss 505.37\n",
            "2023-01-19 15:01:23,507 - INFO - joeynmt.training - EPOCH 438\n",
            "2023-01-19 15:01:30,782 - INFO - joeynmt.training - Epoch 438, Step:   152200, Batch Loss:     1.422149, Batch Acc: 0.616731, Tokens per Sec:    15630, Lr: 0.000023\n",
            "2023-01-19 15:01:38,602 - INFO - joeynmt.training - Epoch 438, Step:   152300, Batch Loss:     1.637631, Batch Acc: 0.612799, Tokens per Sec:    15414, Lr: 0.000023\n",
            "2023-01-19 15:01:46,385 - INFO - joeynmt.training - Epoch 438, Step:   152400, Batch Loss:     1.401066, Batch Acc: 0.609739, Tokens per Sec:    15406, Lr: 0.000023\n",
            "2023-01-19 15:01:50,679 - INFO - joeynmt.training - Epoch 438: total training loss 510.05\n",
            "2023-01-19 15:01:50,679 - INFO - joeynmt.training - EPOCH 439\n",
            "2023-01-19 15:01:54,190 - INFO - joeynmt.training - Epoch 439, Step:   152500, Batch Loss:     1.533383, Batch Acc: 0.614048, Tokens per Sec:    15345, Lr: 0.000023\n",
            "2023-01-19 15:02:02,751 - INFO - joeynmt.training - Epoch 439, Step:   152600, Batch Loss:     1.473374, Batch Acc: 0.612652, Tokens per Sec:    14092, Lr: 0.000023\n",
            "2023-01-19 15:02:10,874 - INFO - joeynmt.training - Epoch 439, Step:   152700, Batch Loss:     1.431141, Batch Acc: 0.612612, Tokens per Sec:    14794, Lr: 0.000023\n",
            "2023-01-19 15:02:18,551 - INFO - joeynmt.training - Epoch 439, Step:   152800, Batch Loss:     1.507254, Batch Acc: 0.612154, Tokens per Sec:    15696, Lr: 0.000023\n",
            "2023-01-19 15:02:18,888 - INFO - joeynmt.training - Epoch 439: total training loss 510.45\n",
            "2023-01-19 15:02:18,888 - INFO - joeynmt.training - EPOCH 440\n",
            "2023-01-19 15:02:26,251 - INFO - joeynmt.training - Epoch 440, Step:   152900, Batch Loss:     1.478732, Batch Acc: 0.614597, Tokens per Sec:    15800, Lr: 0.000023\n",
            "2023-01-19 15:02:33,969 - INFO - joeynmt.training - Epoch 440, Step:   153000, Batch Loss:     1.430894, Batch Acc: 0.614975, Tokens per Sec:    15582, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 145.94ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9966.83ex/s]\n",
            "2023-01-19 15:02:34,263 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=153000\n",
            "2023-01-19 15:02:34,263 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:02:39,736 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:02:39,736 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:02:39,737 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:02:39,738 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:02:39,740 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.78, loss:   2.81, ppl:  16.69, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.4250[sec], evaluation: 0.0443[sec]\n",
            "2023-01-19 15:02:39,743 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:02:39,746 - INFO - joeynmt.training - \tSource:     آنجاق سؤزونوزده بلی نیز بلی ، خئیر اینیز خئیر اولسون . بوندان قالانێ شر اولانداندێر . \n",
            "2023-01-19 15:02:39,747 - INFO - joeynmt.training - \tReference:  بله‌تان فقط بله و نه‌تان فقط نه باشد ؛ چون سخنی بیش از این ، از آن شریر است . \n",
            "2023-01-19 15:02:39,747 - INFO - joeynmt.training - \tHypothesis: اما اگر سخنی غیر از این سخنان را در حدی والی نیست ، بلکه از آن آتشی که در آن باقی مانده است .\n",
            "2023-01-19 15:02:39,747 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:02:39,749 - INFO - joeynmt.training - \tSource:      بویوردو : سنین آرخانێ قارداشێنلا مۆحکملندیرجهجک ، ایکینیزه ده دلیل وئرهجهییک . اونلار آیهلریمیز سایهسینده سیزه هئچ بیر پیسلیک ائده بیلمهیهجکلر . سیز ده ، سیزه تابع اولانلار دا مۆتلق غالب گلهجکسینیز ! \n",
            "2023-01-19 15:02:39,749 - INFO - joeynmt.training - \tReference:  فرمود : به زودی بازویت را به وسیله‌ برادرت نیرومند خواهیم کرد و برای شما هر دو ، تسلطی قرار خواهیم داد که با وجود آیات ما ، به شما دست نخواهند یافت شما و هر که شما را پیروی کند چیره خواهید بود . \n",
            "2023-01-19 15:02:39,749 - INFO - joeynmt.training - \tHypothesis: فرمود : تو را به برادرت استوار می فرستیم ، و دو تن را به سوی خود خواهیم داد . به زودی آیات ما را به شما آسیبی نخواهند گرفت . و کسانی که آیات ما را پیروی می کنند ، از شما پیروی خواهند کرد .\n",
            "2023-01-19 15:02:39,749 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:02:39,751 - INFO - joeynmt.training - \tSource:     یهودیلر : اۆزئیر آللاهێن اوغلودور ، خاچپرستلر ده : مصیح آللاهێن اوغلودور ، دئدیلر . اونلارێن آغزێندا گزن بو سؤزلر داها اؤنجه کۆفر ائدنلرین سؤزلرینه بنزهییر . آللاه اونلارێ اؤلدورسون ! نئجه ده دؤندهریلیرلر ! \n",
            "2023-01-19 15:02:39,752 - INFO - joeynmt.training - \tReference:  و یهود گفتند : عزیر ، پسر خداست . و نصاری گفتند : مسیح ، پسر خداست . این سخنی است باطل‌ که به زبان می‌آورند ، و به گفتار کسانی که پیش از این کافر شده‌اند شباهت دارد . خدا آنان را بکشد ؛ چگونه از حق‌ بازگردانده می‌شوند ؟ \n",
            "2023-01-19 15:02:39,752 - INFO - joeynmt.training - \tHypothesis: یهودیان گفتند : ای کافی است که خدا یعنی پسران خدایی است و او نیز گفت : پسر خداست . این گفته ها را به سخنان کفرآمیزد و به سخنان کفران رسیده است . پس چگونه از آن کفر می ورزند . پس خدا از آنچه به سوی کفر کفر می روند .\n",
            "2023-01-19 15:02:39,752 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:02:39,754 - INFO - joeynmt.training - \tSource:     فیلیپ اونا دئدی : یا رب ، آتانێ بیزه گؤستر ، بو بیزیم اۆچۆن کفایتدیر . \n",
            "2023-01-19 15:02:39,754 - INFO - joeynmt.training - \tReference:  فیلیپس به او گفت : سرور ، پدر را به ما نشان بده و این برای ما کافی است . \n",
            "2023-01-19 15:02:39,754 - INFO - joeynmt.training - \tHypothesis: فیلیپس به او گفت : ای سرور ، این نان برای ماست .\n",
            "2023-01-19 15:02:47,583 - INFO - joeynmt.training - Epoch 440, Step:   153100, Batch Loss:     1.525657, Batch Acc: 0.613164, Tokens per Sec:    14732, Lr: 0.000023\n",
            "2023-01-19 15:02:51,706 - INFO - joeynmt.training - Epoch 440: total training loss 510.15\n",
            "2023-01-19 15:02:51,707 - INFO - joeynmt.training - EPOCH 441\n",
            "2023-01-19 15:02:55,347 - INFO - joeynmt.training - Epoch 441, Step:   153200, Batch Loss:     1.382504, Batch Acc: 0.614474, Tokens per Sec:    15869, Lr: 0.000023\n",
            "2023-01-19 15:03:03,207 - INFO - joeynmt.training - Epoch 441, Step:   153300, Batch Loss:     1.438093, Batch Acc: 0.616788, Tokens per Sec:    15290, Lr: 0.000023\n",
            "2023-01-19 15:03:10,985 - INFO - joeynmt.training - Epoch 441, Step:   153400, Batch Loss:     1.449922, Batch Acc: 0.614438, Tokens per Sec:    15489, Lr: 0.000023\n",
            "2023-01-19 15:03:18,731 - INFO - joeynmt.training - Epoch 441, Step:   153500, Batch Loss:     1.465745, Batch Acc: 0.609953, Tokens per Sec:    15662, Lr: 0.000023\n",
            "2023-01-19 15:03:18,778 - INFO - joeynmt.training - Epoch 441: total training loss 506.72\n",
            "2023-01-19 15:03:18,779 - INFO - joeynmt.training - EPOCH 442\n",
            "2023-01-19 15:03:26,544 - INFO - joeynmt.training - Epoch 442, Step:   153600, Batch Loss:     1.364450, Batch Acc: 0.617169, Tokens per Sec:    15459, Lr: 0.000023\n",
            "2023-01-19 15:03:34,409 - INFO - joeynmt.training - Epoch 442, Step:   153700, Batch Loss:     1.475042, Batch Acc: 0.614076, Tokens per Sec:    15236, Lr: 0.000023\n",
            "2023-01-19 15:03:42,270 - INFO - joeynmt.training - Epoch 442, Step:   153800, Batch Loss:     1.388046, Batch Acc: 0.612052, Tokens per Sec:    15535, Lr: 0.000023\n",
            "2023-01-19 15:03:46,019 - INFO - joeynmt.training - Epoch 442: total training loss 506.95\n",
            "2023-01-19 15:03:46,020 - INFO - joeynmt.training - EPOCH 443\n",
            "2023-01-19 15:03:50,050 - INFO - joeynmt.training - Epoch 443, Step:   153900, Batch Loss:     1.401121, Batch Acc: 0.611983, Tokens per Sec:    15789, Lr: 0.000023\n",
            "2023-01-19 15:03:57,786 - INFO - joeynmt.training - Epoch 443, Step:   154000, Batch Loss:     1.386382, Batch Acc: 0.615840, Tokens per Sec:    15575, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.80ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9357.75ex/s]\n",
            "2023-01-19 15:03:58,075 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=154000\n",
            "2023-01-19 15:03:58,075 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:04:03,704 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:04:03,705 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:04:03,705 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:04:03,706 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:04:03,709 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.80, loss:   2.82, ppl:  16.80, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.5744[sec], evaluation: 0.0478[sec]\n",
            "2023-01-19 15:04:03,712 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:04:03,716 - INFO - joeynmt.training - \tSource:      بیر نفر سنی تویا دوت ائدیرسه ، یوخاری باشدا اوتورما . بلکه ائو یییهسینین سندن ده هؤرمتلی بیر قوناغێ وار . \n",
            "2023-01-19 15:04:03,716 - INFO - joeynmt.training - \tReference:   وقتی کسی تو را به جشن عروسی دعوت می‌کند ، در صدر مجلس منشین . شاید شخصی مهم‌تر از تو را نیز دعوت کرده باشد . \n",
            "2023-01-19 15:04:03,716 - INFO - joeynmt.training - \tHypothesis: یک نفر تو را به تو نشان می دهم ، اما نه به خانهٔ تو ، بلکه باید به خانهٔ تو نزدیک شو . شاید حرمت گذارد و تو را از مهمانانش بیرون بده .\n",
            "2023-01-19 15:04:03,717 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:04:03,719 - INFO - joeynmt.training - \tSource:     هر کس اۆچۆن ائتدیگی امللره گؤره درهجهلر واردێر . ربین اونلارێن نه ائتدیکلریندن قافل دئییلدیر ! \n",
            "2023-01-19 15:04:03,719 - INFO - joeynmt.training - \tReference:  و برای هر یک از این دو گروه‌ ، از آنچه انجام داده‌اند ، در جزا مراتبی خواهد بود ، و پروردگارت از آنچه می‌کنند غافل نیست . \n",
            "2023-01-19 15:04:03,719 - INFO - joeynmt.training - \tHypothesis: و هر کس به سزای آنچه انجام داده اند برایشان بهتر است ؛ نه آنچه به دست می آوردند .\n",
            "2023-01-19 15:04:03,719 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:04:03,721 - INFO - joeynmt.training - \tSource:     ایسا آناسێنێ وه اونون یانیندا اؤز سئویملی شاگردینین دایاندیغینی گؤردۆکده آناسێنا دئدی : آنا ، بو سنین اوغلوندور ! \n",
            "2023-01-19 15:04:03,722 - INFO - joeynmt.training - \tReference:  مادر عیسی در کنار شاگردی که عیسی دوستش می‌داشت ، ایستاده بود . وقتی عیسی آنان را دید به مادرش گفت : ای مادر ، او از این پس پسر توست ! \n",
            "2023-01-19 15:04:03,722 - INFO - joeynmt.training - \tHypothesis: عیسی پس از آن که نزد او و مادرش آمد ، دید و به او گفت : مادر خود را به مادر خود پسرت ، پسر تو .\n",
            "2023-01-19 15:04:03,722 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:04:03,724 - INFO - joeynmt.training - \tSource:     اونلارێن هامێسێ قیامت گۆنۆ اونون هۆزورونا گلهجک . \n",
            "2023-01-19 15:04:03,724 - INFO - joeynmt.training - \tReference:  و روز قیامت همه آنها تنها ، به سوی او خواهند آمد . \n",
            "2023-01-19 15:04:03,724 - INFO - joeynmt.training - \tHypothesis: همه در روز قیامت نزد او آینده به سوی او خواهد آمد .\n",
            "2023-01-19 15:04:11,768 - INFO - joeynmt.training - Epoch 443, Step:   154100, Batch Loss:     1.425282, Batch Acc: 0.618028, Tokens per Sec:    14262, Lr: 0.000023\n",
            "2023-01-19 15:04:20,727 - INFO - joeynmt.training - Epoch 443: total training loss 508.26\n",
            "2023-01-19 15:04:20,727 - INFO - joeynmt.training - EPOCH 444\n",
            "2023-01-19 15:04:20,977 - INFO - joeynmt.training - Epoch 444, Step:   154200, Batch Loss:     1.394275, Batch Acc: 0.623565, Tokens per Sec:    16078, Lr: 0.000023\n",
            "2023-01-19 15:04:28,876 - INFO - joeynmt.training - Epoch 444, Step:   154300, Batch Loss:     1.393869, Batch Acc: 0.617165, Tokens per Sec:    15152, Lr: 0.000023\n",
            "2023-01-19 15:04:36,920 - INFO - joeynmt.training - Epoch 444, Step:   154400, Batch Loss:     1.450309, Batch Acc: 0.613266, Tokens per Sec:    14892, Lr: 0.000023\n",
            "2023-01-19 15:04:44,718 - INFO - joeynmt.training - Epoch 444, Step:   154500, Batch Loss:     1.401727, Batch Acc: 0.611035, Tokens per Sec:    15479, Lr: 0.000023\n",
            "2023-01-19 15:04:48,309 - INFO - joeynmt.training - Epoch 444: total training loss 507.91\n",
            "2023-01-19 15:04:48,310 - INFO - joeynmt.training - EPOCH 445\n",
            "2023-01-19 15:04:52,560 - INFO - joeynmt.training - Epoch 445, Step:   154600, Batch Loss:     1.460354, Batch Acc: 0.613569, Tokens per Sec:    15486, Lr: 0.000023\n",
            "2023-01-19 15:05:00,283 - INFO - joeynmt.training - Epoch 445, Step:   154700, Batch Loss:     1.481152, Batch Acc: 0.617003, Tokens per Sec:    15862, Lr: 0.000023\n",
            "2023-01-19 15:05:08,119 - INFO - joeynmt.training - Epoch 445, Step:   154800, Batch Loss:     1.536902, Batch Acc: 0.617179, Tokens per Sec:    15307, Lr: 0.000023\n",
            "2023-01-19 15:05:15,433 - INFO - joeynmt.training - Epoch 445: total training loss 505.47\n",
            "2023-01-19 15:05:15,433 - INFO - joeynmt.training - EPOCH 446\n",
            "2023-01-19 15:05:15,993 - INFO - joeynmt.training - Epoch 446, Step:   154900, Batch Loss:     1.506733, Batch Acc: 0.610604, Tokens per Sec:    15279, Lr: 0.000023\n",
            "2023-01-19 15:05:23,784 - INFO - joeynmt.training - Epoch 446, Step:   155000, Batch Loss:     1.571008, Batch Acc: 0.616160, Tokens per Sec:    15484, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 112.74ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10769.38ex/s]\n",
            "2023-01-19 15:05:24,047 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=155000\n",
            "2023-01-19 15:05:24,048 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:05:29,141 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:05:29,141 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:05:29,141 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:05:29,142 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:05:29,146 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.05, loss:   2.92, ppl:  18.62, acc:   0.41, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0479[sec], evaluation: 0.0427[sec]\n",
            "2023-01-19 15:05:29,148 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:05:29,152 - INFO - joeynmt.training - \tSource:     کی واختیندا پیس بیتیشیب أتی نین یاریقی گؤزه ویریردی\n",
            "2023-01-19 15:05:29,152 - INFO - joeynmt.training - \tReference:  كه بد جوش خورده بود و گوشت سرخ از لای شیارهای صورتش برق میزد \n",
            "2023-01-19 15:05:29,153 - INFO - joeynmt.training - \tHypothesis: كه موعكه یككه یكك و كوكك می كرد\n",
            "2023-01-19 15:05:29,161 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:05:29,163 - INFO - joeynmt.training - \tSource:     وه اونو اسراعل اؤؤلادێنا پیغمبر گؤندهرهجک . من ، حقیقتا ، ربینیزدن سیزه معؤ جۆزه گتیرمیشم . سیزین اۆچۆن پالچێقدان قوشا بنزر بیر سورت دۆزلهدیب اونا اۆفۆرهم ، او دا آللاهێن ایزنیله قوش اولار . آنادانگلمه کورلارێ ، جۆجام خستهلیگینه توتولانلارێ ساغالدار وه آللاهێن ایزنیله اؤلولهری دیریلدرم . من ائولرینیزده یئدیگینیز وه یێغیب ساخلادێغێنێز شئیلری ده سیزه خبر وئرهرم . اگر معؤ مینسینیزسه ، بوندا بیر دلیل واردێر . \n",
            "2023-01-19 15:05:29,163 - INFO - joeynmt.training - \tReference:  و او را به عنوان‌ پیامبری به سوی بنی اسرائیل می‌فرستد ، که او به آنان می‌گوید : در حقیقت ، من از جانب پروردگارتان برایتان معجزه‌ای آورده‌ام : من از گل برای شما چیزی‌ به شکل پرنده می‌سازم ، آنگاه در آن می‌دمم ، پس به اذن خدا پرنده‌ای می شود ؛ و به اذن خدا نابینای مادرزاد و پیس را بهبود می‌بخشم ؛ و مردگان را زنده می‌گردانم ؛ و شما را از آنچه می‌خورید و در خانه هایتان ذخیره می‌کنید ، خبر می‌دهم ؛ مسلما در این معجزات‌ ، برای شما اگر مؤمن باشید عبرت است . \n",
            "2023-01-19 15:05:29,164 - INFO - joeynmt.training - \tHypothesis: و به راستی او را برمی فرستاد ، و من از پروردگار خودتان برای شما نشانه ای آورده ام . و برای شما از گلو آفریده است ، و اگر بادی از گلی سینه های خود را برمی انگیخته است ، به راستی آن را از زمره ای که خدا می پوشیده شده است و برای شما پدید آوردن و از آنچه رام می پوستگانه می پوستندارند .\n",
            "2023-01-19 15:05:29,164 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:05:29,166 - INFO - joeynmt.training - \tSource:     اونلار دوروب : ربیمیز گؤیلرین وه یئرین ربیدیر . بیز اوندان باشقا هئچ بیر تانرێیا ابادت ائتمهیهجهییک . اکس تقدیرده ، دانێشماقدا هدی آشمێش اولارێق ! دئدیکلری زامان اونلارێن اۆرکلرینه غۆۆت وئرمیشدیک . \n",
            "2023-01-19 15:05:29,166 - INFO - joeynmt.training - \tReference:  و دلهایشان را استوار گردانیدیم آنگاه که به قصد مخالفت با شرک‌ برخاستند و گفتند : پروردگار ما پروردگار آسمانها و زمین است . جز او هرگز معبودی را نخواهیم خواند ، که در این صورت قطعا ناصواب گفته‌ایم . \n",
            "2023-01-19 15:05:29,166 - INFO - joeynmt.training - \tHypothesis: و گفته شد : پروردگار آسمانها و زمین ، پروردگار آسمانها و زمین را بپرستیم . جز او را بپرستیم ، و در دلهایشان سنگینی که در دلهایشان بودیم . و چون بر دلهایشان مهر نهاده بود .\n",
            "2023-01-19 15:05:29,166 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:05:29,168 - INFO - joeynmt.training - \tSource:     آرتێق منیمله اونلار آراسێندا سن هؤکم وئر ، منی وه منیمله بیرلیکده اولان معؤ مینلری قورتار ! \n",
            "2023-01-19 15:05:29,168 - INFO - joeynmt.training - \tReference:  میان من و آنان فیصله ده ، و من و هر کس از مؤمنان را که با من است نجات بخش . \n",
            "2023-01-19 15:05:29,169 - INFO - joeynmt.training - \tHypothesis: میان من و میان آنان داوری کن و با من در باره مؤمنان ، با من در باره مؤمنان ،\n",
            "2023-01-19 15:05:38,211 - INFO - joeynmt.training - Epoch 446, Step:   155100, Batch Loss:     1.463609, Batch Acc: 0.613518, Tokens per Sec:    13078, Lr: 0.000023\n",
            "2023-01-19 15:05:45,905 - INFO - joeynmt.training - Epoch 446, Step:   155200, Batch Loss:     1.504531, Batch Acc: 0.613938, Tokens per Sec:    15707, Lr: 0.000023\n",
            "2023-01-19 15:05:48,965 - INFO - joeynmt.training - Epoch 446: total training loss 502.96\n",
            "2023-01-19 15:05:48,966 - INFO - joeynmt.training - EPOCH 447\n",
            "2023-01-19 15:05:53,683 - INFO - joeynmt.training - Epoch 447, Step:   155300, Batch Loss:     1.483359, Batch Acc: 0.617572, Tokens per Sec:    15619, Lr: 0.000023\n",
            "2023-01-19 15:06:01,446 - INFO - joeynmt.training - Epoch 447, Step:   155400, Batch Loss:     1.538497, Batch Acc: 0.617610, Tokens per Sec:    15548, Lr: 0.000023\n",
            "2023-01-19 15:06:09,179 - INFO - joeynmt.training - Epoch 447, Step:   155500, Batch Loss:     1.327005, Batch Acc: 0.613029, Tokens per Sec:    15666, Lr: 0.000023\n",
            "2023-01-19 15:06:15,897 - INFO - joeynmt.training - Epoch 447: total training loss 506.04\n",
            "2023-01-19 15:06:15,897 - INFO - joeynmt.training - EPOCH 448\n",
            "2023-01-19 15:06:16,889 - INFO - joeynmt.training - Epoch 448, Step:   155600, Batch Loss:     1.565996, Batch Acc: 0.610988, Tokens per Sec:    15496, Lr: 0.000023\n",
            "2023-01-19 15:06:24,497 - INFO - joeynmt.training - Epoch 448, Step:   155700, Batch Loss:     1.440828, Batch Acc: 0.617884, Tokens per Sec:    15807, Lr: 0.000023\n",
            "2023-01-19 15:06:32,146 - INFO - joeynmt.training - Epoch 448, Step:   155800, Batch Loss:     1.488158, Batch Acc: 0.611768, Tokens per Sec:    15876, Lr: 0.000023\n",
            "2023-01-19 15:06:39,944 - INFO - joeynmt.training - Epoch 448, Step:   155900, Batch Loss:     1.465681, Batch Acc: 0.616262, Tokens per Sec:    15552, Lr: 0.000023\n",
            "2023-01-19 15:06:42,698 - INFO - joeynmt.training - Epoch 448: total training loss 505.90\n",
            "2023-01-19 15:06:42,699 - INFO - joeynmt.training - EPOCH 449\n",
            "2023-01-19 15:06:47,715 - INFO - joeynmt.training - Epoch 449, Step:   156000, Batch Loss:     1.492135, Batch Acc: 0.618244, Tokens per Sec:    15718, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 128.22ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9497.67ex/s] \n",
            "2023-01-19 15:06:48,001 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=156000\n",
            "2023-01-19 15:06:48,002 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:06:52,790 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:06:52,790 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:06:52,791 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:06:52,792 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:06:52,794 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.30, loss:   2.82, ppl:  16.78, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7395[sec], evaluation: 0.0455[sec]\n",
            "2023-01-19 15:06:52,797 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:06:52,800 - INFO - joeynmt.training - \tSource:     کیتابی وه پیغمبرلریمزی گؤندردیکلریمیزی یالان حساب ائدنلر مۆتلق بیلهجکلر ! \n",
            "2023-01-19 15:06:52,801 - INFO - joeynmt.training - \tReference:  کسانی که کتاب خدا و آنچه را که فرستادگان خود را بدان گسیل داشته‌ایم تکذیب کرده‌اند ، به زودی خواهند دانست ؛ \n",
            "2023-01-19 15:06:52,801 - INFO - joeynmt.training - \tHypothesis: و قطعا کسانی را که کتاب آسمانی و فرستادگان ما را دروغ انگاشتند ، زودا که بدان دروغ می روند .\n",
            "2023-01-19 15:06:52,801 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:06:52,803 - INFO - joeynmt.training - \tSource:     روحلارێ بو دیاردان قووماسێن دئیه او ، ایسهآیا چوخ یالواردی . \n",
            "2023-01-19 15:06:52,803 - INFO - joeynmt.training - \tReference:  او بارها از عیسی استدعا کرد که آن‌ها را از آن ناحیه بیرون نکند . \n",
            "2023-01-19 15:06:52,803 - INFO - joeynmt.training - \tHypothesis: او را تمجید کردند که گفت : عیسی را تمجید کردند ، او را تمجید کردند .\n",
            "2023-01-19 15:06:52,804 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:06:52,806 - INFO - joeynmt.training - \tSource:     همین گۆن ربی اونلارێ خبردار ائدهجکدیر ! \n",
            "2023-01-19 15:06:52,806 - INFO - joeynmt.training - \tReference:  در چنان روزی پروردگارشان به حال‌ ایشان نیک آگاه است ؟ \n",
            "2023-01-19 15:06:52,806 - INFO - joeynmt.training - \tHypothesis: در آن روز ، پروردگارشان به آنان می دهد .\n",
            "2023-01-19 15:06:52,806 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:06:52,808 - INFO - joeynmt.training - \tSource:     نئچه دفه اونا بوخوو ، زنجیر وورموشدولار ، لاکین او ، زنجیرلری قێرمێش ، بوخوولارێ پارچالامێشدێ . هئچ کسین اونو ساکت ائتمهیه گۆجۆ چاتمێردێ . \n",
            "2023-01-19 15:06:52,808 - INFO - joeynmt.training - \tReference:  بارها او را با زنجیر و پابندهای آهنی بسته بودند ، اما هر بار زنجیرها را پاره می‌کرد و پابندها را می‌شکست و هیچ کس قدرت مهار کردن او را نداشت . \n",
            "2023-01-19 15:06:52,808 - INFO - joeynmt.training - \tHypothesis: چند بار دیگر از این امر ، او را در زنجیرها گرفته و بلافها کردند ، اما زنجیرها را در بندندندند وی را در آتشی بسته بودند که هیچ کس را بسته بود ، به پایم رسانده بود .\n",
            "2023-01-19 15:07:00,516 - INFO - joeynmt.training - Epoch 449, Step:   156100, Batch Loss:     1.476533, Batch Acc: 0.611501, Tokens per Sec:    15168, Lr: 0.000023\n",
            "2023-01-19 15:07:08,349 - INFO - joeynmt.training - Epoch 449, Step:   156200, Batch Loss:     1.505535, Batch Acc: 0.615816, Tokens per Sec:    15347, Lr: 0.000023\n",
            "2023-01-19 15:07:14,840 - INFO - joeynmt.training - Epoch 449: total training loss 504.21\n",
            "2023-01-19 15:07:14,840 - INFO - joeynmt.training - EPOCH 450\n",
            "2023-01-19 15:07:16,251 - INFO - joeynmt.training - Epoch 450, Step:   156300, Batch Loss:     1.461207, Batch Acc: 0.618906, Tokens per Sec:    15923, Lr: 0.000023\n",
            "2023-01-19 15:07:23,993 - INFO - joeynmt.training - Epoch 450, Step:   156400, Batch Loss:     1.354472, Batch Acc: 0.616524, Tokens per Sec:    15578, Lr: 0.000023\n",
            "2023-01-19 15:07:31,637 - INFO - joeynmt.training - Epoch 450, Step:   156500, Batch Loss:     1.557084, Batch Acc: 0.613050, Tokens per Sec:    15571, Lr: 0.000023\n",
            "2023-01-19 15:07:39,459 - INFO - joeynmt.training - Epoch 450, Step:   156600, Batch Loss:     1.521434, Batch Acc: 0.615592, Tokens per Sec:    15366, Lr: 0.000023\n",
            "2023-01-19 15:07:41,963 - INFO - joeynmt.training - Epoch 450: total training loss 508.00\n",
            "2023-01-19 15:07:41,963 - INFO - joeynmt.training - EPOCH 451\n",
            "2023-01-19 15:07:47,275 - INFO - joeynmt.training - Epoch 451, Step:   156700, Batch Loss:     1.298332, Batch Acc: 0.610467, Tokens per Sec:    15531, Lr: 0.000023\n",
            "2023-01-19 15:07:56,048 - INFO - joeynmt.training - Epoch 451, Step:   156800, Batch Loss:     1.417427, Batch Acc: 0.614625, Tokens per Sec:    13860, Lr: 0.000023\n",
            "2023-01-19 15:08:03,762 - INFO - joeynmt.training - Epoch 451, Step:   156900, Batch Loss:     1.501281, Batch Acc: 0.619052, Tokens per Sec:    15634, Lr: 0.000023\n",
            "2023-01-19 15:08:10,007 - INFO - joeynmt.training - Epoch 451: total training loss 504.88\n",
            "2023-01-19 15:08:10,008 - INFO - joeynmt.training - EPOCH 452\n",
            "2023-01-19 15:08:11,556 - INFO - joeynmt.training - Epoch 452, Step:   157000, Batch Loss:     1.360406, Batch Acc: 0.624700, Tokens per Sec:    15335, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.47ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 4623.89ex/s] \n",
            "2023-01-19 15:08:12,006 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=157000\n",
            "2023-01-19 15:08:12,006 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:08:16,930 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:08:16,931 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:08:16,931 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:08:16,932 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:08:16,935 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.96, loss:   2.77, ppl:  15.97, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.8760[sec], evaluation: 0.0450[sec]\n",
            "2023-01-19 15:08:16,938 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:08:16,941 - INFO - joeynmt.training - \tSource:     سن تشنه اولسان دا ، اینسانلارین اکسریتیتی ایمان گتیرن دئییلدیر ! \n",
            "2023-01-19 15:08:16,942 - INFO - joeynmt.training - \tReference:  و بیشتر مردم هر چند آرزومند باشی ایمان‌آورنده نیستند . \n",
            "2023-01-19 15:08:16,942 - INFO - joeynmt.training - \tHypothesis: و تو باید بیشتر مردم باشد ، و بیشتر بیشتر مردم ایمان آورنده نبود .\n",
            "2023-01-19 15:08:16,942 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:08:16,944 - INFO - joeynmt.training - \tSource:      ائی ابراهیم ! بوندان ال چک ، ربینین امری آرتێق گلمیشدیر . اونلارا مۆتلق قارشیسیآلینماز بیر اعذاب گلهجکدیر . \n",
            "2023-01-19 15:08:16,944 - INFO - joeynmt.training - \tReference:  ای ابراهیم ! از این چون و چرا روی برتاب ، که فرمان پروردگارت آمده و برای آنان عذابی که بی‌بازگشت است خواهد آمد . \n",
            "2023-01-19 15:08:16,945 - INFO - joeynmt.training - \tHypothesis: ای ابراهیم ، دستور پروردگار تو به دستور پروردگارش آمده است ، و قطعا عذابی دردناک خواهد بود .\n",
            "2023-01-19 15:08:16,945 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:08:16,947 - INFO - joeynmt.training - \tSource:     قارداشلار ، سیزه جصارتله دئمهلییم کی ، دوغرودان دا ، اولو بابامێز داوود اؤلوب وه دفن اولونوب . اونون قبری بو گۆنه قدر بورادادێر . \n",
            "2023-01-19 15:08:16,947 - INFO - joeynmt.training - \tReference:   ای برادران ، من می‌توانم آزادانه در مورد جدمان ، داوود با شما صحبت کنم که درگذشت ، دفن شد و مقبره‌اش تا امروز در میان ماست ؛ \n",
            "2023-01-19 15:08:16,947 - INFO - joeynmt.training - \tHypothesis: ای برادران ، از طرف شما به نشان دهید که از زمان داوود بت پرسید و داوود و مردگان همواره در اینجا بمانید .\n",
            "2023-01-19 15:08:16,947 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:08:16,949 - INFO - joeynmt.training - \tSource:     آخێ آللاه سنه گؤره ملکلرنه امر ائدر کی ، سنی قوروسونلار ، \n",
            "2023-01-19 15:08:16,949 - INFO - joeynmt.training - \tReference:  زیرا نوشته شده است ، او در مورد تو به فرشتگان خود فرمان خواهد داد تا تو را حفظ کنند \n",
            "2023-01-19 15:08:16,949 - INFO - joeynmt.training - \tHypothesis: زیرا خدا به فرشتگان تو حکم می کند که تو را از مرگ برتابد ،\n",
            "2023-01-19 15:08:24,649 - INFO - joeynmt.training - Epoch 452, Step:   157100, Batch Loss:     1.462150, Batch Acc: 0.615622, Tokens per Sec:    14786, Lr: 0.000023\n",
            "2023-01-19 15:08:32,282 - INFO - joeynmt.training - Epoch 452, Step:   157200, Batch Loss:     1.424920, Batch Acc: 0.615976, Tokens per Sec:    15742, Lr: 0.000023\n",
            "2023-01-19 15:08:40,057 - INFO - joeynmt.training - Epoch 452, Step:   157300, Batch Loss:     1.415509, Batch Acc: 0.610774, Tokens per Sec:    15669, Lr: 0.000023\n",
            "2023-01-19 15:08:42,266 - INFO - joeynmt.training - Epoch 452: total training loss 504.85\n",
            "2023-01-19 15:08:42,267 - INFO - joeynmt.training - EPOCH 453\n",
            "2023-01-19 15:08:47,760 - INFO - joeynmt.training - Epoch 453, Step:   157400, Batch Loss:     1.370472, Batch Acc: 0.618735, Tokens per Sec:    15821, Lr: 0.000023\n",
            "2023-01-19 15:08:55,270 - INFO - joeynmt.training - Epoch 453, Step:   157500, Batch Loss:     1.398856, Batch Acc: 0.614372, Tokens per Sec:    15988, Lr: 0.000023\n",
            "2023-01-19 15:09:02,862 - INFO - joeynmt.training - Epoch 453, Step:   157600, Batch Loss:     1.369535, Batch Acc: 0.615545, Tokens per Sec:    15992, Lr: 0.000023\n",
            "2023-01-19 15:09:09,742 - INFO - joeynmt.training - Epoch 453: total training loss 504.92\n",
            "2023-01-19 15:09:09,742 - INFO - joeynmt.training - EPOCH 454\n",
            "2023-01-19 15:09:11,656 - INFO - joeynmt.training - Epoch 454, Step:   157700, Batch Loss:     1.510076, Batch Acc: 0.618560, Tokens per Sec:    15514, Lr: 0.000023\n",
            "2023-01-19 15:09:19,261 - INFO - joeynmt.training - Epoch 454, Step:   157800, Batch Loss:     1.516778, Batch Acc: 0.616024, Tokens per Sec:    15983, Lr: 0.000023\n",
            "2023-01-19 15:09:26,821 - INFO - joeynmt.training - Epoch 454, Step:   157900, Batch Loss:     1.406464, Batch Acc: 0.617971, Tokens per Sec:    15822, Lr: 0.000023\n",
            "2023-01-19 15:09:34,385 - INFO - joeynmt.training - Epoch 454, Step:   158000, Batch Loss:     1.434669, Batch Acc: 0.614110, Tokens per Sec:    15888, Lr: 0.000023\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 76.54ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9719.18ex/s]\n",
            "2023-01-19 15:09:34,675 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=158000\n",
            "2023-01-19 15:09:34,675 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:09:39,107 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:09:39,107 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:09:39,107 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:09:39,108 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:09:39,111 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.45, loss:   2.88, ppl:  17.78, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.3896[sec], evaluation: 0.0392[sec]\n",
            "2023-01-19 15:09:39,114 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:09:39,117 - INFO - joeynmt.training - \tSource:     حقیقتا ، او ، درگاهێمێزا یاخین اولاجاق وه اونون قاییدیب گلهجهگی یئر ده گؤزل اولاجاقدێر . \n",
            "2023-01-19 15:09:39,118 - INFO - joeynmt.training - \tReference:  و قطعا برای او در پیشگاه ما تقرب و فرجام نیکوست . \n",
            "2023-01-19 15:09:39,118 - INFO - joeynmt.training - \tHypothesis: در حقیقت ، او نزدیک و پایداری و فرجامی خوش خواهد بود .\n",
            "2023-01-19 15:09:39,118 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:09:39,120 - INFO - joeynmt.training - \tSource:     خئیر ، من اونلارا دا ، آتالارێنا دا اؤزلرینه حاقق وه بیان ائدن بیر پیغمبر گلهنهدک گۆن گۆزران وئردیم ! \n",
            "2023-01-19 15:09:39,121 - INFO - joeynmt.training - \tReference:  بلکه اینان و پدرانشان را برخوداری دادم تا حقیقت و فرستاده‌ای آشکار به سویشان آمد . \n",
            "2023-01-19 15:09:39,121 - INFO - joeynmt.training - \tHypothesis: من برای آنان نیز هستم ، و پدر و مادر خود را بر حقانیت نهاده است . پس امروز فرمانش را بر آنان پوشیده دادم .\n",
            "2023-01-19 15:09:39,121 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:09:39,123 - INFO - joeynmt.training - \tSource:     سونرا فیکریمه گلدی کی جنازه می تکجه آنام گؤرور سه چوخدا اؤنمی یوخ دی . \n",
            "2023-01-19 15:09:39,123 - INFO - joeynmt.training - \tReference:  بعد فكر كردم احتمالا جنازه را فقط مادرم دیده و این نمی بایست زیاد مهم باشد . \n",
            "2023-01-19 15:09:39,124 - INFO - joeynmt.training - \tHypothesis: سپس فستوس كه به من رسیده ام تا كه در لحظه یك مادرم خالی بود .\n",
            "2023-01-19 15:09:39,124 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:09:39,126 - INFO - joeynmt.training - \tSource:     اونلارا اوخوندوغو زامان : بیز اونا ایناندیق . دوغرودان دا ، ربیمیزدن حاقدێر . بیز اوندان اول ده مۆسلمان ایدیک ! دئییرلر . \n",
            "2023-01-19 15:09:39,126 - INFO - joeynmt.training - \tReference:  و چون بر ایشان فرو خوانده می‌شود ، می‌گویند : بدان ایمان آوردیم که آن درست است و از طرف پروردگار ماست ؛ ما پیش از آن هم‌ از تسلیم‌شوندگان بودیم . \n",
            "2023-01-19 15:09:39,126 - INFO - joeynmt.training - \tHypothesis: و چون بر آنان خوانده شود ، می گویند : ما به آن ایمان آوردیم . پیش از این ، پیش از آن ، پیش از این ما بودیم .\n",
            "2023-01-19 15:09:41,024 - INFO - joeynmt.training - Epoch 454: total training loss 504.49\n",
            "2023-01-19 15:09:41,025 - INFO - joeynmt.training - EPOCH 455\n",
            "2023-01-19 15:09:46,900 - INFO - joeynmt.training - Epoch 455, Step:   158100, Batch Loss:     1.494797, Batch Acc: 0.618684, Tokens per Sec:    15548, Lr: 0.000022\n",
            "2023-01-19 15:09:54,427 - INFO - joeynmt.training - Epoch 455, Step:   158200, Batch Loss:     1.432094, Batch Acc: 0.615329, Tokens per Sec:    16012, Lr: 0.000022\n",
            "2023-01-19 15:10:01,995 - INFO - joeynmt.training - Epoch 455, Step:   158300, Batch Loss:     1.531753, Batch Acc: 0.613560, Tokens per Sec:    16063, Lr: 0.000022\n",
            "2023-01-19 15:10:07,495 - INFO - joeynmt.training - Epoch 455: total training loss 503.84\n",
            "2023-01-19 15:10:07,495 - INFO - joeynmt.training - EPOCH 456\n",
            "2023-01-19 15:10:09,653 - INFO - joeynmt.training - Epoch 456, Step:   158400, Batch Loss:     1.425312, Batch Acc: 0.623012, Tokens per Sec:    15419, Lr: 0.000022\n",
            "2023-01-19 15:10:17,259 - INFO - joeynmt.training - Epoch 456, Step:   158500, Batch Loss:     1.376206, Batch Acc: 0.618049, Tokens per Sec:    15820, Lr: 0.000022\n",
            "2023-01-19 15:10:24,743 - INFO - joeynmt.training - Epoch 456, Step:   158600, Batch Loss:     1.431675, Batch Acc: 0.613752, Tokens per Sec:    16096, Lr: 0.000022\n",
            "2023-01-19 15:10:32,206 - INFO - joeynmt.training - Epoch 456, Step:   158700, Batch Loss:     1.375888, Batch Acc: 0.613947, Tokens per Sec:    16201, Lr: 0.000022\n",
            "2023-01-19 15:10:33,822 - INFO - joeynmt.training - Epoch 456: total training loss 505.80\n",
            "2023-01-19 15:10:33,822 - INFO - joeynmt.training - EPOCH 457\n",
            "2023-01-19 15:10:39,838 - INFO - joeynmt.training - Epoch 457, Step:   158800, Batch Loss:     1.595590, Batch Acc: 0.617805, Tokens per Sec:    15805, Lr: 0.000022\n",
            "2023-01-19 15:10:47,473 - INFO - joeynmt.training - Epoch 457, Step:   158900, Batch Loss:     1.328673, Batch Acc: 0.616211, Tokens per Sec:    15806, Lr: 0.000022\n",
            "2023-01-19 15:10:54,973 - INFO - joeynmt.training - Epoch 457, Step:   159000, Batch Loss:     1.370732, Batch Acc: 0.614703, Tokens per Sec:    16340, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 135.90ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10699.45ex/s]\n",
            "2023-01-19 15:10:55,240 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=159000\n",
            "2023-01-19 15:10:55,241 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:11:00,384 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:11:00,384 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:11:00,384 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:11:00,385 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:11:00,388 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.36, loss:   2.82, ppl:  16.80, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0945[sec], evaluation: 0.0453[sec]\n",
            "2023-01-19 15:11:00,391 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:11:00,395 - INFO - joeynmt.training - \tSource:     دۆنیا وه اونداکێ احترآسلار کئچیب گئدیر ، آمما آللاهێن ارادهسینه امل ائدن ابدی قالێر . \n",
            "2023-01-19 15:11:00,395 - INFO - joeynmt.training - \tReference:  به علاوه ، دنیا و خواسته‌های آن گذراست ، اما کسی که خواست خدا را به جا می‌آورد ، تا ابد باقی خواهد ماند . \n",
            "2023-01-19 15:11:00,395 - INFO - joeynmt.training - \tHypothesis: این دنیا و جلالی که از طریق او می روند ، برود ، اما هر که خواست خدا را برای انجام دهد ، برای همیشه وی که انجام دهد .\n",
            "2023-01-19 15:11:00,395 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:11:00,397 - INFO - joeynmt.training - \tSource:     قادێن ایسه یاخینلاشدی وه اونا سجده قێلێب دئدی : یا رب ، منه امداد ائت . \n",
            "2023-01-19 15:11:00,398 - INFO - joeynmt.training - \tReference:  اما آن زن پیش عیسی آمد ، در مقابل او به زانو افتاد و گفت : سرورم ، به من کمک کن ! \n",
            "2023-01-19 15:11:00,398 - INFO - joeynmt.training - \tHypothesis: آن زن برخاست و به او گفت : ای سرور ، من را با شمشیر کن و به من گفت : سرور ، به من بگشای .\n",
            "2023-01-19 15:11:00,398 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:11:00,400 - INFO - joeynmt.training - \tSource:     چوخلو آدام اونون یانینا گلیب دئدی : یهیا هئچ بیر علامت گؤسترمهسه ده ، اونون بو آدام بارهده دئدیگی بۆتۆن سؤزلر دوغرو چێخدێ . \n",
            "2023-01-19 15:11:00,400 - INFO - joeynmt.training - \tReference:  مردمی بسیار نزد او آمدند و گفتند : یحیی یک معجزه هم به ظهور نرساند ، اما هر چه در مورد این مرد گفت ، درست بود . \n",
            "2023-01-19 15:11:00,400 - INFO - joeynmt.training - \tHypothesis: در حالی که بسیاری نزد او آمدند و گفت : یحیی معجزه ای برای او نشانه هایی معجزات نکرده است ، اما مردم این گفته را به درستی صحبت می کردند .\n",
            "2023-01-19 15:11:00,400 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:11:00,402 - INFO - joeynmt.training - \tSource:     اونلار قولاقلارێنێ توتوب هارای قوپاردێلار وه هامێسێ بیرلیکده استئفانێن اۆستۆنه حۆجوم چکدیلر . \n",
            "2023-01-19 15:11:00,403 - INFO - joeynmt.training - \tReference:  آنگاه آن مردان با صدای بلند فریاد کشیدند و دست‌های خود را روی گوش‌هایشان گذاشتند و همگی با هم به طرف او هجوم آوردند . \n",
            "2023-01-19 15:11:00,403 - INFO - joeynmt.training - \tHypothesis: آنان را با شنیدن گوش زدند و همراه تمامی حرفه و تمامیس را بر سر او می کشیدند .\n",
            "2023-01-19 15:11:05,607 - INFO - joeynmt.training - Epoch 457: total training loss 501.55\n",
            "2023-01-19 15:11:05,607 - INFO - joeynmt.training - EPOCH 458\n",
            "2023-01-19 15:11:08,120 - INFO - joeynmt.training - Epoch 458, Step:   159100, Batch Loss:     1.447574, Batch Acc: 0.622594, Tokens per Sec:    16232, Lr: 0.000022\n",
            "2023-01-19 15:11:15,804 - INFO - joeynmt.training - Epoch 458, Step:   159200, Batch Loss:     1.485102, Batch Acc: 0.619104, Tokens per Sec:    15662, Lr: 0.000022\n",
            "2023-01-19 15:11:23,295 - INFO - joeynmt.training - Epoch 458, Step:   159300, Batch Loss:     1.563747, Batch Acc: 0.614238, Tokens per Sec:    16126, Lr: 0.000022\n",
            "2023-01-19 15:11:31,931 - INFO - joeynmt.training - Epoch 458, Step:   159400, Batch Loss:     1.451920, Batch Acc: 0.613050, Tokens per Sec:    13983, Lr: 0.000022\n",
            "2023-01-19 15:11:33,041 - INFO - joeynmt.training - Epoch 458: total training loss 502.08\n",
            "2023-01-19 15:11:33,041 - INFO - joeynmt.training - EPOCH 459\n",
            "2023-01-19 15:11:39,578 - INFO - joeynmt.training - Epoch 459, Step:   159500, Batch Loss:     1.455600, Batch Acc: 0.615513, Tokens per Sec:    15954, Lr: 0.000022\n",
            "2023-01-19 15:11:47,278 - INFO - joeynmt.training - Epoch 459, Step:   159600, Batch Loss:     1.485334, Batch Acc: 0.618202, Tokens per Sec:    15629, Lr: 0.000022\n",
            "2023-01-19 15:11:54,808 - INFO - joeynmt.training - Epoch 459, Step:   159700, Batch Loss:     1.364748, Batch Acc: 0.616341, Tokens per Sec:    16112, Lr: 0.000022\n",
            "2023-01-19 15:11:59,498 - INFO - joeynmt.training - Epoch 459: total training loss 502.51\n",
            "2023-01-19 15:11:59,498 - INFO - joeynmt.training - EPOCH 460\n",
            "2023-01-19 15:12:02,365 - INFO - joeynmt.training - Epoch 460, Step:   159800, Batch Loss:     1.391073, Batch Acc: 0.614997, Tokens per Sec:    15861, Lr: 0.000022\n",
            "2023-01-19 15:12:10,016 - INFO - joeynmt.training - Epoch 460, Step:   159900, Batch Loss:     1.416592, Batch Acc: 0.617345, Tokens per Sec:    15780, Lr: 0.000022\n",
            "2023-01-19 15:12:17,700 - INFO - joeynmt.training - Epoch 460, Step:   160000, Batch Loss:     1.438995, Batch Acc: 0.616738, Tokens per Sec:    15605, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 137.04ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9603.76ex/s]\n",
            "2023-01-19 15:12:17,986 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=160000\n",
            "2023-01-19 15:12:17,987 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:12:23,407 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:12:23,408 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:12:23,408 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:12:23,409 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:12:23,412 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.21, loss:   2.82, ppl:  16.79, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.3733[sec], evaluation: 0.0438[sec]\n",
            "2023-01-19 15:12:23,415 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:12:23,418 - INFO - joeynmt.training - \tSource:     آتاسێنێ یا آناسێنێ مندن آرتێق سئون کس منه لایق دئییل . اوغلونو یا قێزێنێ مندن آرتێق سئون ده منه لایق دئییل . \n",
            "2023-01-19 15:12:23,418 - INFO - joeynmt.training - \tReference:  کسی که پدر یا مادر خود را بیش از من دوست داشته باشد ، لایق من نیست و کسی که پسر یا دختر خود را بیش از من دوست داشته باشد ، لایق من نیست . \n",
            "2023-01-19 15:12:23,418 - INFO - joeynmt.training - \tHypothesis: پس هر که از من درخواست کند ، دیگر لایق نیست که به خانهٔ خود محبت کند ، بلکه پسر نیست که از من دوست داشته باشد .\n",
            "2023-01-19 15:12:23,418 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:12:23,420 - INFO - joeynmt.training - \tSource:     بو حادسا سونرا آداداقێ قالان خستهلر ده گلیب شفا تاپدێ . \n",
            "2023-01-19 15:12:23,421 - INFO - joeynmt.training - \tReference:  پس از این واقعه ، بیماران دیگر آن جزیره نیز نزد او می‌آمدند و شفا می‌یافتند . \n",
            "2023-01-19 15:12:23,421 - INFO - joeynmt.training - \tHypothesis: در این حال ، اگر به نام یاای شهر افتاد ، شفا یافتند .\n",
            "2023-01-19 15:12:23,421 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:12:23,423 - INFO - joeynmt.training - \tSource:      دئ : اونلارێن نه قدر قالدیقلارینی آللاه داها یاخشی بیلیر . گؤیلرین وه یئرین غیبینی بیلمک آنجاق اونا مخصوصدور . او ، هر شئیی نئجه گؤزل گؤرۆر ، نئجه ده یاخشی ائشیدیر ! اونلارێن آللاهدان باشقا هئچ بیر حامثی یوخدور . او ، هئچ کسی اؤز هؤکمونه شریک ائتمز ! \n",
            "2023-01-19 15:12:23,423 - INFO - joeynmt.training - \tReference:  بگو : خدا به آنچه درنگ کردند داناتر است . نهان آسمانها و زمین به او اختصاص دارد . وه ! چه بینا و شنواست . برای آنان یاوری جز او نیست و هیچ کس را در فرمانروایی خود شریک نمی‌گیرد . \n",
            "2023-01-19 15:12:23,424 - INFO - joeynmt.training - \tHypothesis: بگو : تا معلوم دارند که خدا آنچه را که در آن هنگام رستاخیز می ماند ، و آنچه را که علم دارد و اوست ، و آنچه را نیکوست می بیناست و چه نیکوست و آنچه را نیکوست ، و هیچ کس جز او نیست ، و او را در برابر او نیست ، و او شرکات نمی کند ، و هیچ کس را شریک می گرداند ، و او را شریک می سازد .\n",
            "2023-01-19 15:12:23,424 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:12:23,426 - INFO - joeynmt.training - \tSource:      نه اۆچۆن شاگردلرین آغساققاللاردان قالان آدت انهنهنی پوزور وه چؤرک یئدیکلری زامان اللهرینی یومورلار ؟ \n",
            "2023-01-19 15:12:23,426 - INFO - joeynmt.training - \tReference:   چرا شاگردان تو سنت گذشتگان را زیر پا می‌گذارند ؟ مثلا ، پیش از خوردن غذا دست‌هایشان را آب نمی‌کشند ! \n",
            "2023-01-19 15:12:23,426 - INFO - joeynmt.training - \tHypothesis: چرا از طریق پست که از شاگردانت باقی مانده بودند ، شاگردانش را تمسخیر می خورند و دست هایش را خوردند ؟\n",
            "2023-01-19 15:12:30,983 - INFO - joeynmt.training - Epoch 460, Step:   160100, Batch Loss:     1.438588, Batch Acc: 0.618470, Tokens per Sec:    15416, Lr: 0.000022\n",
            "2023-01-19 15:12:31,797 - INFO - joeynmt.training - Epoch 460: total training loss 503.85\n",
            "2023-01-19 15:12:31,797 - INFO - joeynmt.training - EPOCH 461\n",
            "2023-01-19 15:12:38,452 - INFO - joeynmt.training - Epoch 461, Step:   160200, Batch Loss:     1.413210, Batch Acc: 0.619213, Tokens per Sec:    16108, Lr: 0.000022\n",
            "2023-01-19 15:12:47,193 - INFO - joeynmt.training - Epoch 461, Step:   160300, Batch Loss:     1.379891, Batch Acc: 0.615852, Tokens per Sec:    13844, Lr: 0.000022\n",
            "2023-01-19 15:12:54,709 - INFO - joeynmt.training - Epoch 461, Step:   160400, Batch Loss:     1.535211, Batch Acc: 0.619310, Tokens per Sec:    16112, Lr: 0.000022\n",
            "2023-01-19 15:12:59,178 - INFO - joeynmt.training - Epoch 461: total training loss 502.46\n",
            "2023-01-19 15:12:59,178 - INFO - joeynmt.training - EPOCH 462\n",
            "2023-01-19 15:13:02,257 - INFO - joeynmt.training - Epoch 462, Step:   160500, Batch Loss:     1.464222, Batch Acc: 0.622477, Tokens per Sec:    16067, Lr: 0.000022\n",
            "2023-01-19 15:13:09,896 - INFO - joeynmt.training - Epoch 462, Step:   160600, Batch Loss:     1.483030, Batch Acc: 0.615735, Tokens per Sec:    15788, Lr: 0.000022\n",
            "2023-01-19 15:13:17,579 - INFO - joeynmt.training - Epoch 462, Step:   160700, Batch Loss:     1.560609, Batch Acc: 0.614144, Tokens per Sec:    15700, Lr: 0.000022\n",
            "2023-01-19 15:13:25,142 - INFO - joeynmt.training - Epoch 462, Step:   160800, Batch Loss:     1.514387, Batch Acc: 0.615967, Tokens per Sec:    15940, Lr: 0.000022\n",
            "2023-01-19 15:13:25,749 - INFO - joeynmt.training - Epoch 462: total training loss 503.71\n",
            "2023-01-19 15:13:25,749 - INFO - joeynmt.training - EPOCH 463\n",
            "2023-01-19 15:13:32,707 - INFO - joeynmt.training - Epoch 463, Step:   160900, Batch Loss:     1.377983, Batch Acc: 0.618283, Tokens per Sec:    16070, Lr: 0.000022\n",
            "2023-01-19 15:13:40,317 - INFO - joeynmt.training - Epoch 463, Step:   161000, Batch Loss:     1.579107, Batch Acc: 0.617164, Tokens per Sec:    15914, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.36ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10479.86ex/s]\n",
            "2023-01-19 15:13:40,591 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=161000\n",
            "2023-01-19 15:13:40,591 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:13:45,569 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:13:45,570 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:13:45,570 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:13:45,571 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:13:45,574 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.35, loss:   2.79, ppl:  16.34, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9266[sec], evaluation: 0.0476[sec]\n",
            "2023-01-19 15:13:45,577 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:13:45,580 - INFO - joeynmt.training - \tSource:     آللاهێن روحونو بوندان تانییا بیلرسینیز : ایسا مسیحین جسما گلدیگینی اقرار ائدن هر روح آللاهداندێر ، \n",
            "2023-01-19 15:13:45,580 - INFO - joeynmt.training - \tReference:  به این طریق تشخیص می‌دهید که گفته‌ای الهام‌شده از خداست : هر گفتهٔ الهام‌شده که تصدیق کند عیسی مسیح به صورت انسان آمد ، از جانب خداست . \n",
            "2023-01-19 15:13:45,581 - INFO - joeynmt.training - \tHypothesis: همچنین روح خدا را می شناسید و می توانید از او می شناسید : روحی که خداست ، خدا را تصدیق می کند ، آن که روح را تصدیق می کند ، خداست .\n",
            "2023-01-19 15:13:45,581 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:13:45,583 - INFO - joeynmt.training - \tSource:     اونلار اهد باغلادێغێن کیمسهلردیر کی ، سونرا هر دفه اهدلرینی پوزار وه آللاهدان دا قورخمازلار . \n",
            "2023-01-19 15:13:45,583 - INFO - joeynmt.training - \tReference:  همانان که از ایشان پیمان گرفتی ولی هر بار پیمان خود را می‌شکنند و از خدا پروا نمی‌دارند . \n",
            "2023-01-19 15:13:45,583 - INFO - joeynmt.training - \tHypothesis: آنان همان کسانی که پیمان خدا را پس از آنکه پیمان بستند ، و از خدا پروا کنند و آنان بیم ندارند .\n",
            "2023-01-19 15:13:45,584 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:13:45,586 - INFO - joeynmt.training - \tSource:     ساغ طرف صاحبلری ! کیمدیر او ساغ طرف صاحبلری ؟ \n",
            "2023-01-19 15:13:45,586 - INFO - joeynmt.training - \tReference:  و یاران راست ؛ یاران راست کدامند ؟ \n",
            "2023-01-19 15:13:45,586 - INFO - joeynmt.training - \tHypothesis: یاران دست راست ، کدامند یاران راست ؟\n",
            "2023-01-19 15:13:45,586 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:13:45,588 - INFO - joeynmt.training - \tSource:     آللاه گؤیدن بیر یاغیش ائندیرر ، اونونلا یئر اۆزۆنۆ اؤلدوکدن سونرا دیریلدر . قولاق آسانلار اۆچۆن بوندا بیر ابرت واردێر . \n",
            "2023-01-19 15:13:45,588 - INFO - joeynmt.training - \tReference:  و خدا از آسمان آبی فرود آورد و با آن زمین را پس از پژمردنش زنده گردانید ، قطعا در این امر برای مردمی که شنوایی دارند نشانه‌ای است . \n",
            "2023-01-19 15:13:45,589 - INFO - joeynmt.training - \tHypothesis: و خدا از آسمان ، آبی فرود آورد ، پس از آسمان ، زمین را پس از مرگش به وسیله آن ، و برای مردمی که می پندارند می کند ، و در این امر برای مردمی که تعجبیری برای مردمی عبرتی است .\n",
            "2023-01-19 15:13:53,190 - INFO - joeynmt.training - Epoch 463, Step:   161100, Batch Loss:     1.527721, Batch Acc: 0.616444, Tokens per Sec:    15524, Lr: 0.000022\n",
            "2023-01-19 15:13:57,292 - INFO - joeynmt.training - Epoch 463: total training loss 499.96\n",
            "2023-01-19 15:13:57,292 - INFO - joeynmt.training - EPOCH 464\n",
            "2023-01-19 15:14:00,760 - INFO - joeynmt.training - Epoch 464, Step:   161200, Batch Loss:     1.426794, Batch Acc: 0.616241, Tokens per Sec:    16148, Lr: 0.000022\n",
            "2023-01-19 15:14:08,335 - INFO - joeynmt.training - Epoch 464, Step:   161300, Batch Loss:     1.441191, Batch Acc: 0.616138, Tokens per Sec:    15784, Lr: 0.000022\n",
            "2023-01-19 15:14:16,091 - INFO - joeynmt.training - Epoch 464, Step:   161400, Batch Loss:     1.466065, Batch Acc: 0.616823, Tokens per Sec:    15500, Lr: 0.000022\n",
            "2023-01-19 15:14:23,685 - INFO - joeynmt.training - Epoch 464, Step:   161500, Batch Loss:     1.481828, Batch Acc: 0.616148, Tokens per Sec:    15947, Lr: 0.000022\n",
            "2023-01-19 15:14:23,910 - INFO - joeynmt.training - Epoch 464: total training loss 503.88\n",
            "2023-01-19 15:14:23,910 - INFO - joeynmt.training - EPOCH 465\n",
            "2023-01-19 15:14:31,205 - INFO - joeynmt.training - Epoch 465, Step:   161600, Batch Loss:     1.406152, Batch Acc: 0.619747, Tokens per Sec:    16066, Lr: 0.000022\n",
            "2023-01-19 15:14:38,673 - INFO - joeynmt.training - Epoch 465, Step:   161700, Batch Loss:     1.448438, Batch Acc: 0.615097, Tokens per Sec:    16308, Lr: 0.000022\n",
            "2023-01-19 15:14:46,225 - INFO - joeynmt.training - Epoch 465, Step:   161800, Batch Loss:     1.497012, Batch Acc: 0.614624, Tokens per Sec:    15940, Lr: 0.000022\n",
            "2023-01-19 15:14:49,985 - INFO - joeynmt.training - Epoch 465: total training loss 500.89\n",
            "2023-01-19 15:14:49,986 - INFO - joeynmt.training - EPOCH 466\n",
            "2023-01-19 15:14:53,850 - INFO - joeynmt.training - Epoch 466, Step:   161900, Batch Loss:     1.446251, Batch Acc: 0.619207, Tokens per Sec:    15684, Lr: 0.000022\n",
            "2023-01-19 15:15:01,336 - INFO - joeynmt.training - Epoch 466, Step:   162000, Batch Loss:     1.445032, Batch Acc: 0.619162, Tokens per Sec:    15978, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 133.63ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9423.03ex/s] \n",
            "2023-01-19 15:15:01,634 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=162000\n",
            "2023-01-19 15:15:01,634 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:15:07,700 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:15:07,700 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:15:07,700 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:15:07,702 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:15:07,706 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  10.20, loss:   2.76, ppl:  15.78, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 6.0096[sec], evaluation: 0.0516[sec]\n",
            "2023-01-19 15:15:07,708 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:15:07,712 - INFO - joeynmt.training - \tSource:      آللاهێ قویوب یالنیز قادێن بۆتلره تاپێنێر وه یالنیز آسی شیطانا ابادت ائدیرلر . \n",
            "2023-01-19 15:15:07,713 - INFO - joeynmt.training - \tReference:   مشرکان‌ ، به جای او ، جز بتهای مادینه را به دعا نمی‌خوانند ، و جز شیطان سرکش را نمی‌خوانند . \n",
            "2023-01-19 15:15:07,713 - INFO - joeynmt.training - \tHypothesis: و غیر از خدا ، این زن را می خوانند و جز شیطان می پرستند .\n",
            "2023-01-19 15:15:07,713 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:15:07,715 - INFO - joeynmt.training - \tSource:     بیزی اؤز ایزهتی وه عالیجنابلیغینا چاغێرانێن الاهی گۆجۆ اونو تانیمآغیمیز واسطهسیله یاشامآغیمیز وه مؤمین اولماغێمێز اۆچۆن لازێم گلن هر شئیی بیزه بخش ائتدی . \n",
            "2023-01-19 15:15:07,716 - INFO - joeynmt.training - \tReference:  زیرا قدرت خدا هر آنچه را که نیاز داریم تا بر طبق وقفمان به او زندگی کنیم ، به ما ارزانی داشته است . این از طریق شناخت دقیق او امکان‌پذیر شده است که ما را به وسیلهٔ جلال و نیکویی خود فراخوانده است . \n",
            "2023-01-19 15:15:07,716 - INFO - joeynmt.training - \tHypothesis: با این حال ، ما را جلال دهند و ما را در اختیار خود فراخوانده است تا بتوانیم او را به جلالی که از طریق ما آگاه سازیم ، در مورد ما زندگی کنیم و هر آنچه ما را داریم ، باید برای همه چیز مقصود کنیم .\n",
            "2023-01-19 15:15:07,716 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:15:07,718 - INFO - joeynmt.training - \tSource:     ساماریالی قادێن اونا دئدی : سن بیر یهودیسن ، منسه ساماریالی بیر قادێن . نئجه سن مندن سو ایستهیه بیلرسن ؟ چۆنکی یهودیلر ساماریالیلارلا اۆنسیت ائتمیر . \n",
            "2023-01-19 15:15:07,719 - INFO - joeynmt.training - \tReference:  پس زن سامری به او گفت : چطور تو با این که یهودی هستی ، از من که زنی سامری هستم ، آب می‌خواهی ؟ \n",
            "2023-01-19 15:15:07,719 - INFO - joeynmt.training - \tHypothesis: زن به آن زن گفت : من یهودی هستی . اما تو چگونه می توانی با من بنویس ؟ زیرا می توانی به یهودیان تو اجازه دهد ؟ زیرا یهودیان از من می اندیشند .\n",
            "2023-01-19 15:15:07,719 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:15:07,721 - INFO - joeynmt.training - \tSource:      پیغمبرلردن ازم صاحبلری اولانلارێن صبر ائتدیگی کیمی ، سن ده صبر ائت . تئز گلمهسینی ایستهمه . اونلار وه د اولوندوقلارینی گؤرهجکلری گۆن گۆندۆزۆن آنجاق بیر ساعاتێ قدر قالدیقلارینی ساناجاقلار . بیر خبردارلێقدێر . فاصق بیر قؤؤمدن باشقاسێ دا هئچ محو ائدیلرمی \n",
            "2023-01-19 15:15:07,721 - INFO - joeynmt.training - \tReference:  پس همان گونه که پیامبران نستوه ، صبر کردند ، صبر کن ، و برای آنان شتابزدگی به خرج مده . روزی که آنچه را وعده داده می‌شوند بنگرند ، گویی که آنان جز ساعتی از روز را در دنیا نمانده‌اند ؛ این‌ ابلاغی است . پس آیا جز مردم نافرمان هلاکت خواهند یافت ؟ \n",
            "2023-01-19 15:15:07,722 - INFO - joeynmt.training - \tHypothesis: و نیز کسانی که پیامبران را شکیبای کردند ، شکیباییی کن ، و آنان را به شتابزززودی کن ، و روز قیامت فقط آنان را فراموش کردند . روز قیامت فقط روز جزاس نمی فقط روزی که فرق نمی دهند . و هیچ یک از آنچه را که نمی دانند گواهی نمی دهند ، و هیچ انفاق نمی شود .\n",
            "2023-01-19 15:15:15,632 - INFO - joeynmt.training - Epoch 466, Step:   162100, Batch Loss:     1.435066, Batch Acc: 0.617467, Tokens per Sec:    14663, Lr: 0.000022\n",
            "2023-01-19 15:15:23,161 - INFO - joeynmt.training - Epoch 466: total training loss 503.86\n",
            "2023-01-19 15:15:23,161 - INFO - joeynmt.training - EPOCH 467\n",
            "2023-01-19 15:15:23,237 - INFO - joeynmt.training - Epoch 467, Step:   162200, Batch Loss:     1.377611, Batch Acc: 0.634471, Tokens per Sec:    16035, Lr: 0.000022\n",
            "2023-01-19 15:15:30,818 - INFO - joeynmt.training - Epoch 467, Step:   162300, Batch Loss:     1.291552, Batch Acc: 0.618161, Tokens per Sec:    16034, Lr: 0.000022\n",
            "2023-01-19 15:15:38,416 - INFO - joeynmt.training - Epoch 467, Step:   162400, Batch Loss:     1.435789, Batch Acc: 0.618323, Tokens per Sec:    15938, Lr: 0.000022\n",
            "2023-01-19 15:15:46,076 - INFO - joeynmt.training - Epoch 467, Step:   162500, Batch Loss:     1.412148, Batch Acc: 0.618953, Tokens per Sec:    15681, Lr: 0.000022\n",
            "2023-01-19 15:15:49,709 - INFO - joeynmt.training - Epoch 467: total training loss 502.62\n",
            "2023-01-19 15:15:49,710 - INFO - joeynmt.training - EPOCH 468\n",
            "2023-01-19 15:15:53,717 - INFO - joeynmt.training - Epoch 468, Step:   162600, Batch Loss:     1.343646, Batch Acc: 0.621666, Tokens per Sec:    15430, Lr: 0.000022\n",
            "2023-01-19 15:16:01,499 - INFO - joeynmt.training - Epoch 468, Step:   162700, Batch Loss:     1.494483, Batch Acc: 0.617288, Tokens per Sec:    15484, Lr: 0.000022\n",
            "2023-01-19 15:16:09,458 - INFO - joeynmt.training - Epoch 468, Step:   162800, Batch Loss:     1.483657, Batch Acc: 0.617364, Tokens per Sec:    15359, Lr: 0.000022\n",
            "2023-01-19 15:16:16,879 - INFO - joeynmt.training - Epoch 468: total training loss 501.42\n",
            "2023-01-19 15:16:16,880 - INFO - joeynmt.training - EPOCH 469\n",
            "2023-01-19 15:16:17,195 - INFO - joeynmt.training - Epoch 469, Step:   162900, Batch Loss:     1.281543, Batch Acc: 0.638316, Tokens per Sec:    14803, Lr: 0.000022\n",
            "2023-01-19 15:16:25,926 - INFO - joeynmt.training - Epoch 469, Step:   163000, Batch Loss:     1.418133, Batch Acc: 0.618901, Tokens per Sec:    13921, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 142.55ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10525.13ex/s]\n",
            "2023-01-19 15:16:26,201 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=163000\n",
            "2023-01-19 15:16:26,202 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:16:31,720 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:16:31,721 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:16:31,721 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:16:31,722 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:16:31,725 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.69, loss:   2.86, ppl:  17.50, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.4637[sec], evaluation: 0.0521[sec]\n",
            "2023-01-19 15:16:31,728 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:16:31,731 - INFO - joeynmt.training - \tSource:     اورادا ائنئیا آدلێ بیر نفره باش چکدی . ائنئیا افلیج اولموشدو وه سککیز ایل ایدی کی ، یاتاقدا ایدی . \n",
            "2023-01-19 15:16:31,731 - INFO - joeynmt.training - \tReference:  در آنجا مردی را دید به نام اینیاس که برای هشت سال مفلوج و در بستر بود . \n",
            "2023-01-19 15:16:31,731 - INFO - joeynmt.training - \tHypothesis: در آنجا مردی به نام اهالی به نام اهالی قیروس رسید . او ۱۵۰ سال ، ایلیا بود .\n",
            "2023-01-19 15:16:31,732 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:16:31,734 - INFO - joeynmt.training - \tSource:     سیز محض بونون اۆچۆن چاغێرێلدێنێز . چۆنکی مصیح ده سیزین اۆچۆن اعذاب چکدی وه سیزه نمونه اولدو کی ، سیز ده اونون ایزی ایله گئدهسینیز . \n",
            "2023-01-19 15:16:31,734 - INFO - joeynmt.training - \tReference:  در واقع ، شما نیز فراخوانده شده‌اید تا این راه را دنبال کنید ؛ زیرا حتی مسیح برای شما رنج کشید و سرمشقی برای شما قرار داد تا به‌دقت در جای پای او گام بردارید . \n",
            "2023-01-19 15:16:31,734 - INFO - joeynmt.training - \tHypothesis: اما شما که نمی دانید ، این ها را به خاطر مسیح فراخوانده شد ؛ زیرا مسیحْ عیسی رنج هایی که در اتحاد با مسیح رنج می کشید ، می کشید .\n",
            "2023-01-19 15:16:31,734 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:16:31,736 - INFO - joeynmt.training - \tSource:     ایسا اترافینداکیلارا غزبله باخدێ وه اونلارێن اینادکار اۆرکلی اولدوقلارینا گؤره کدرلهنیب او آداما دئدی : الینی اوزات ! او ، الینی اوزاتدێ وه الی اوهلکی هالێنا قایێتدی . \n",
            "2023-01-19 15:16:31,737 - INFO - joeynmt.training - \tReference:  عیسی از سنگدلی‌شان عمیقا اندوهگین شد و با خشم به آنان نظر افکند . سپس به آن مرد گفت : دستت را دراز کن . او دستش را دراز کرد و دست او شفا یافت . \n",
            "2023-01-19 15:16:31,737 - INFO - joeynmt.training - \tHypothesis: عیسی به اطراف صحوم هایشان نگاه کرد و با دیدن دلشان به دلیل سخنان کفرناحوم افزود و در دلشان به آنان گفت : دستت را دراز کن و دستش را دراز کن .\n",
            "2023-01-19 15:16:31,737 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:16:31,739 - INFO - joeynmt.training - \tSource:     ائلهجه سیز ده بو شئیلرین باش وئردیگینی گؤرنده بیلین کی ، آللاهێن پادشاهلێغێ یاخینلاشیب . \n",
            "2023-01-19 15:16:31,739 - INFO - joeynmt.training - \tReference:  به همین ترتیب نیز ، وقتی شما وقوع این چیزها را می‌بینید ، بدانید که پادشاهی خدا نزدیک است . \n",
            "2023-01-19 15:16:31,739 - INFO - joeynmt.training - \tHypothesis: پس شما نیز وقتی این امور را می بینید ، بدانید که پادشاهی خدا نزدیک شده است .\n",
            "2023-01-19 15:16:39,298 - INFO - joeynmt.training - Epoch 469, Step:   163100, Batch Loss:     1.458760, Batch Acc: 0.616790, Tokens per Sec:    15345, Lr: 0.000022\n",
            "2023-01-19 15:16:46,961 - INFO - joeynmt.training - Epoch 469, Step:   163200, Batch Loss:     1.429012, Batch Acc: 0.617107, Tokens per Sec:    15572, Lr: 0.000022\n",
            "2023-01-19 15:16:50,413 - INFO - joeynmt.training - Epoch 469: total training loss 503.18\n",
            "2023-01-19 15:16:50,413 - INFO - joeynmt.training - EPOCH 470\n",
            "2023-01-19 15:16:54,607 - INFO - joeynmt.training - Epoch 470, Step:   163300, Batch Loss:     1.462426, Batch Acc: 0.621452, Tokens per Sec:    16047, Lr: 0.000022\n",
            "2023-01-19 15:17:02,141 - INFO - joeynmt.training - Epoch 470, Step:   163400, Batch Loss:     1.417255, Batch Acc: 0.618905, Tokens per Sec:    15815, Lr: 0.000022\n",
            "2023-01-19 15:17:09,712 - INFO - joeynmt.training - Epoch 470, Step:   163500, Batch Loss:     1.448046, Batch Acc: 0.617720, Tokens per Sec:    15739, Lr: 0.000022\n",
            "2023-01-19 15:17:16,849 - INFO - joeynmt.training - Epoch 470: total training loss 501.93\n",
            "2023-01-19 15:17:16,850 - INFO - joeynmt.training - EPOCH 471\n",
            "2023-01-19 15:17:17,301 - INFO - joeynmt.training - Epoch 471, Step:   163600, Batch Loss:     1.343481, Batch Acc: 0.631913, Tokens per Sec:    15049, Lr: 0.000022\n",
            "2023-01-19 15:17:24,973 - INFO - joeynmt.training - Epoch 471, Step:   163700, Batch Loss:     1.442642, Batch Acc: 0.619367, Tokens per Sec:    15892, Lr: 0.000022\n",
            "2023-01-19 15:17:32,510 - INFO - joeynmt.training - Epoch 471, Step:   163800, Batch Loss:     1.535794, Batch Acc: 0.621069, Tokens per Sec:    16237, Lr: 0.000022\n",
            "2023-01-19 15:17:40,035 - INFO - joeynmt.training - Epoch 471, Step:   163900, Batch Loss:     1.414498, Batch Acc: 0.615299, Tokens per Sec:    16203, Lr: 0.000022\n",
            "2023-01-19 15:17:43,019 - INFO - joeynmt.training - Epoch 471: total training loss 496.27\n",
            "2023-01-19 15:17:43,020 - INFO - joeynmt.training - EPOCH 472\n",
            "2023-01-19 15:17:47,669 - INFO - joeynmt.training - Epoch 472, Step:   164000, Batch Loss:     1.476097, Batch Acc: 0.617865, Tokens per Sec:    15761, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.02ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10464.70ex/s]\n",
            "2023-01-19 15:17:47,934 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=164000\n",
            "2023-01-19 15:17:47,934 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:17:52,190 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:17:52,191 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:17:52,191 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:17:52,192 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:17:52,194 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:  10.77, loss:   2.78, ppl:  16.18, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.2134[sec], evaluation: 0.0401[sec]\n",
            "2023-01-19 15:17:52,409 - INFO - joeynmt.helpers - delete RESULTS/model/106000.ckpt\n",
            "2023-01-19 15:17:52,422 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:17:52,426 - INFO - joeynmt.training - \tSource:     سنه قالان دخیل ده گرک بیر نئچه سیککه اولاردی . \n",
            "2023-01-19 15:17:52,426 - INFO - joeynmt.training - \tReference:  تو قلكی كه‌ برات‌ گذاشتم‌ چند تایی‌ سكه‌ بود . \n",
            "2023-01-19 15:17:52,426 - INFO - joeynmt.training - \tHypothesis: و آنچه را که باقی ماندگانی باقی مانده بود به تو بنیدیم .\n",
            "2023-01-19 15:17:52,426 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:17:52,428 - INFO - joeynmt.training - \tSource:     او کسلردن اؤترو کی ، اونلار اؤز ربیندن اونو گؤرمهدن قورخار وه قیامتدن لرزهیه گلرلر . \n",
            "2023-01-19 15:17:52,428 - INFO - joeynmt.training - \tReference:   همان‌ کسانی که از پروردگارشان در نهان می‌ترسند و از قیامت هراسناکند . \n",
            "2023-01-19 15:17:52,429 - INFO - joeynmt.training - \tHypothesis: و کسانی که بر آنان در باره پروردگارشان تقریبی می کنند ، و کسانی که به حال آنکه به حال گروندگانش می آیند .\n",
            "2023-01-19 15:17:52,429 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:17:52,431 - INFO - joeynmt.training - \tSource:     سیز بیلمهدیگینیزه ابادت ائدیرسینیز ، بیزسه بیلدیگیمیزه ابادت ائدیریک ، چۆنکی خلاص یهودیلردن گلیر . \n",
            "2023-01-19 15:17:52,431 - INFO - joeynmt.training - \tReference:  شما آنچه را که نمی‌شناسید می‌پرستید ، ما آنچه را که می‌شناسیم می‌پرستیم ؛ زیرا نجات از یهودیان آغاز می‌شود . \n",
            "2023-01-19 15:17:52,431 - INFO - joeynmt.training - \tHypothesis: اگر می پرستیم ، ما می پرستیم ، پس ما را می خوانیم ؛ زیرا یهودیانی که در واقع نجات می یابیم .\n",
            "2023-01-19 15:17:52,431 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:17:52,433 - INFO - joeynmt.training - \tSource:     کیتابین آلملرین ربی ترهفیندن نازل ائدیلمهسینده هئچ بیر شک شبههه یوخدور ! \n",
            "2023-01-19 15:17:52,434 - INFO - joeynmt.training - \tReference:  نازل شدن این کتاب که هیچ جای‌ شک در آن نیست از طرف پروردگار جهانهاست . \n",
            "2023-01-19 15:17:52,434 - INFO - joeynmt.training - \tHypothesis: و کتابی که از جانب پروردگار جهانیان به سوی آن فرو فرستاده شده است ، و قطعا در آن تردیدی نیست .\n",
            "2023-01-19 15:18:00,224 - INFO - joeynmt.training - Epoch 472, Step:   164100, Batch Loss:     1.461542, Batch Acc: 0.619453, Tokens per Sec:    14723, Lr: 0.000022\n",
            "2023-01-19 15:18:07,759 - INFO - joeynmt.training - Epoch 472, Step:   164200, Batch Loss:     1.534939, Batch Acc: 0.618556, Tokens per Sec:    15871, Lr: 0.000022\n",
            "2023-01-19 15:18:14,446 - INFO - joeynmt.training - Epoch 472: total training loss 501.88\n",
            "2023-01-19 15:18:14,447 - INFO - joeynmt.training - EPOCH 473\n",
            "2023-01-19 15:18:15,362 - INFO - joeynmt.training - Epoch 473, Step:   164300, Batch Loss:     1.517658, Batch Acc: 0.622670, Tokens per Sec:    15372, Lr: 0.000022\n",
            "2023-01-19 15:18:22,926 - INFO - joeynmt.training - Epoch 473, Step:   164400, Batch Loss:     1.381821, Batch Acc: 0.618206, Tokens per Sec:    15962, Lr: 0.000022\n",
            "2023-01-19 15:18:30,441 - INFO - joeynmt.training - Epoch 473, Step:   164500, Batch Loss:     1.379352, Batch Acc: 0.619831, Tokens per Sec:    16301, Lr: 0.000022\n",
            "2023-01-19 15:18:37,981 - INFO - joeynmt.training - Epoch 473, Step:   164600, Batch Loss:     1.459186, Batch Acc: 0.616270, Tokens per Sec:    15920, Lr: 0.000022\n",
            "2023-01-19 15:18:41,659 - INFO - joeynmt.training - Epoch 473: total training loss 499.34\n",
            "2023-01-19 15:18:41,659 - INFO - joeynmt.training - EPOCH 474\n",
            "2023-01-19 15:18:46,610 - INFO - joeynmt.training - Epoch 474, Step:   164700, Batch Loss:     1.458299, Batch Acc: 0.616011, Tokens per Sec:    15846, Lr: 0.000022\n",
            "2023-01-19 15:18:54,269 - INFO - joeynmt.training - Epoch 474, Step:   164800, Batch Loss:     1.442236, Batch Acc: 0.621195, Tokens per Sec:    15750, Lr: 0.000022\n",
            "2023-01-19 15:19:01,856 - INFO - joeynmt.training - Epoch 474, Step:   164900, Batch Loss:     1.498712, Batch Acc: 0.620754, Tokens per Sec:    15758, Lr: 0.000022\n",
            "2023-01-19 15:19:08,235 - INFO - joeynmt.training - Epoch 474: total training loss 501.16\n",
            "2023-01-19 15:19:08,235 - INFO - joeynmt.training - EPOCH 475\n",
            "2023-01-19 15:19:09,471 - INFO - joeynmt.training - Epoch 475, Step:   165000, Batch Loss:     1.457620, Batch Acc: 0.617699, Tokens per Sec:    16060, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 127.39ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10005.06ex/s]\n",
            "2023-01-19 15:19:09,751 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=165000\n",
            "2023-01-19 15:19:09,751 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:19:14,594 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:19:14,594 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:19:14,594 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:19:14,595 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:19:14,598 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.24, loss:   2.85, ppl:  17.28, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7981[sec], evaluation: 0.0417[sec]\n",
            "2023-01-19 15:19:14,601 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:19:14,604 - INFO - joeynmt.training - \tSource:     آخێ کیم داها بؤیوکدور ، سفرهده اوتوران ، یوخسا خدمت ائدن ؟ سفرهده اوتوران دئییلمی ؟ منسه سیزین آرانێزدا خدمتچی کیمیگم . \n",
            "2023-01-19 15:19:14,605 - INFO - joeynmt.training - \tReference:  زیرا کدام بزرگ‌تر است ، آن که بر سر سفره می‌نشیند یا آن که خدمت می‌کند ؟ آیا آن که بر سر سفره نشسته است ، بزرگ‌تر نیست ؟ اما من در میان شما همچون خدمتگزار هستم . \n",
            "2023-01-19 15:19:14,605 - INFO - joeynmt.training - \tHypothesis: زیرا اگر کسی در سفر بزرگ تر است ، آیا کسی که بر سر سفره نشسته است ؟ آیا کسی که بر آن نشسته است ، خدمتگزارتان نیستم ؟\n",
            "2023-01-19 15:19:14,605 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:19:14,611 - INFO - joeynmt.training - \tSource:      ائی ابراهیم ! بوندان ال چک ، ربینین امری آرتێق گلمیشدیر . اونلارا مۆتلق قارشیسیآلینماز بیر اعذاب گلهجکدیر . \n",
            "2023-01-19 15:19:14,611 - INFO - joeynmt.training - \tReference:  ای ابراهیم ! از این چون و چرا روی برتاب ، که فرمان پروردگارت آمده و برای آنان عذابی که بی‌بازگشت است خواهد آمد . \n",
            "2023-01-19 15:19:14,611 - INFO - joeynmt.training - \tHypothesis: ای ابراهیم ، دستور پروردگار تو به دستور پروردگارش آمده است ، و قطعا عذابی دردناک خواهد بود .\n",
            "2023-01-19 15:19:14,611 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:19:14,613 - INFO - joeynmt.training - \tSource:     ائی ربیمیز ! من اهلی ایالیمدان به زیسینی سنین بیطالحرامینین یاخینلیغیندا ، اکین بیتمز بیر واده ساکین ائتدیم . ائی ربیمیز ! اونلار ناماز قێلسێنلار دئیه بئله ائتدیم . ائله ائت کی ، اینسانلارین بیر قسمینین قلبلری اونلارا مئیل ائتسین . اونلارا مئیوهلریندن روزی وئر کی ، شۆکۆر ائده بیلسینلر ! \n",
            "2023-01-19 15:19:14,613 - INFO - joeynmt.training - \tReference:  پروردگارا ، من یکی از فرزندانم را در دره‌ای بی‌کشت ، نزد خانه محترم تو ، سکونت دادم . پروردگارا ، تا نماز را به پا دارند ، پس دلهای برخی از مردم را به سوی آنان گرایش ده و آنان را از محصولات مورد نیازشان‌ روزی ده ، باشد که سپاسگزاری کنند . \n",
            "2023-01-19 15:19:14,614 - INFO - joeynmt.training - \tHypothesis: پروردگارا ، می گویند : ای اهل مصیبت کن و در حالی که تو را از زندان درربند ، یا دروع ساحل نداد ، تا نماز را بر پا دارید ، و از فضل خویش بیاوده کن تا با آنها سپاسگزاری کنند . پس آنان را شکرگزاری می کنند .\n",
            "2023-01-19 15:19:14,614 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:19:14,616 - INFO - joeynmt.training - \tSource:     او واختدان ایسا وز ائدیب بئله دئمهیه باشلادێ : تؤؤبه ائدین ! چۆنکی سماوی پادشاهلێق یاخینلاشیب . \n",
            "2023-01-19 15:19:14,616 - INFO - joeynmt.training - \tReference:  از آن زمان عیسی موعظه را آغاز کرد . او می‌گفت : توبه کنید ؛ زیرا پادشاهی آسمان‌ها نزدیک شده است . \n",
            "2023-01-19 15:19:14,616 - INFO - joeynmt.training - \tHypothesis: از آن زمان ، عیسی از او چنین گفت : توبه کنید ؛ زیرا به پادشاهی آسمان نزدیک شده است .\n",
            "2023-01-19 15:19:22,309 - INFO - joeynmt.training - Epoch 475, Step:   165100, Batch Loss:     1.436495, Batch Acc: 0.619050, Tokens per Sec:    15349, Lr: 0.000022\n",
            "2023-01-19 15:19:29,835 - INFO - joeynmt.training - Epoch 475, Step:   165200, Batch Loss:     1.478859, Batch Acc: 0.617111, Tokens per Sec:    16013, Lr: 0.000022\n",
            "2023-01-19 15:19:37,420 - INFO - joeynmt.training - Epoch 475, Step:   165300, Batch Loss:     1.417040, Batch Acc: 0.618935, Tokens per Sec:    15998, Lr: 0.000022\n",
            "2023-01-19 15:19:39,713 - INFO - joeynmt.training - Epoch 475: total training loss 497.43\n",
            "2023-01-19 15:19:39,713 - INFO - joeynmt.training - EPOCH 476\n",
            "2023-01-19 15:19:45,023 - INFO - joeynmt.training - Epoch 476, Step:   165400, Batch Loss:     1.388394, Batch Acc: 0.619489, Tokens per Sec:    16173, Lr: 0.000022\n",
            "2023-01-19 15:19:52,587 - INFO - joeynmt.training - Epoch 476, Step:   165500, Batch Loss:     1.429338, Batch Acc: 0.622611, Tokens per Sec:    15899, Lr: 0.000022\n",
            "2023-01-19 15:20:01,205 - INFO - joeynmt.training - Epoch 476, Step:   165600, Batch Loss:     1.519775, Batch Acc: 0.615416, Tokens per Sec:    14088, Lr: 0.000022\n",
            "2023-01-19 15:20:07,009 - INFO - joeynmt.training - Epoch 476: total training loss 498.48\n",
            "2023-01-19 15:20:07,010 - INFO - joeynmt.training - EPOCH 477\n",
            "2023-01-19 15:20:08,755 - INFO - joeynmt.training - Epoch 477, Step:   165700, Batch Loss:     1.441361, Batch Acc: 0.620814, Tokens per Sec:    15696, Lr: 0.000022\n",
            "2023-01-19 15:20:16,376 - INFO - joeynmt.training - Epoch 477, Step:   165800, Batch Loss:     1.440318, Batch Acc: 0.620842, Tokens per Sec:    15778, Lr: 0.000022\n",
            "2023-01-19 15:20:23,903 - INFO - joeynmt.training - Epoch 477, Step:   165900, Batch Loss:     1.399157, Batch Acc: 0.619367, Tokens per Sec:    16167, Lr: 0.000022\n",
            "2023-01-19 15:20:31,475 - INFO - joeynmt.training - Epoch 477, Step:   166000, Batch Loss:     1.439456, Batch Acc: 0.620719, Tokens per Sec:    15927, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 145.29ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10673.11ex/s]\n",
            "2023-01-19 15:20:31,766 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=166000\n",
            "2023-01-19 15:20:31,766 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:20:37,006 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:20:37,007 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:20:37,007 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:20:37,008 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:20:37,011 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.97, loss:   2.80, ppl:  16.43, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.1936[sec], evaluation: 0.0433[sec]\n",
            "2023-01-19 15:20:37,013 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:20:37,017 - INFO - joeynmt.training - \tSource:     اگر بیز سنه صبات وئرمهسیدیک ، یقین کی ، آز دا اولسا ، اونلارا اویاجاقدین ! \n",
            "2023-01-19 15:20:37,017 - INFO - joeynmt.training - \tReference:  و اگر تو را استوار نمی‌داشتیم ، قطعا نزدیک بود کمی به سوی آنان متمایل شوی . \n",
            "2023-01-19 15:20:37,017 - INFO - joeynmt.training - \tHypothesis: و اگر ما به تو نداده ایم ، قطعا آنان بهتر و اندکی از آن درد باز هم اندازه خواهد شد .\n",
            "2023-01-19 15:20:37,017 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:20:37,019 - INFO - joeynmt.training - \tSource:     آمما گؤرونموری . \n",
            "2023-01-19 15:20:37,019 - INFO - joeynmt.training - \tReference:  اما دیده‌ نمی‌شود . \n",
            "2023-01-19 15:20:37,019 - INFO - joeynmt.training - \tHypothesis: اما نيشم .\n",
            "2023-01-19 15:20:37,020 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:20:37,021 - INFO - joeynmt.training - \tSource:     اینسانا بۆتۆن دۆنیانی قازانێب جانێنێ ایتیرمهیینین نه خئیری وار ؟ \n",
            "2023-01-19 15:20:37,022 - INFO - joeynmt.training - \tReference:  به‌راستی چه فایده دارد که کسی تمام دنیا را به دست آورد ، اما جان خود را از دست بدهد ؟ \n",
            "2023-01-19 15:20:37,022 - INFO - joeynmt.training - \tHypothesis: زیرا انسان باید از تمام دنیا حاصل حاصل کنم ، چه حاصل است ؟\n",
            "2023-01-19 15:20:37,022 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:20:37,024 - INFO - joeynmt.training - \tSource:     حقیقتا ، ربین یاراداندیر ، بیلندیر ! \n",
            "2023-01-19 15:20:37,024 - INFO - joeynmt.training - \tReference:  زیرا پروردگار تو همان آفریننده داناست . \n",
            "2023-01-19 15:20:37,024 - INFO - joeynmt.training - \tHypothesis: و راستی که پروردگار تو پدیدآورنده داناست .\n",
            "2023-01-19 15:20:38,915 - INFO - joeynmt.training - Epoch 477: total training loss 499.14\n",
            "2023-01-19 15:20:38,915 - INFO - joeynmt.training - EPOCH 478\n",
            "2023-01-19 15:20:44,618 - INFO - joeynmt.training - Epoch 478, Step:   166100, Batch Loss:     1.417666, Batch Acc: 0.618940, Tokens per Sec:    15912, Lr: 0.000022\n",
            "2023-01-19 15:20:52,120 - INFO - joeynmt.training - Epoch 478, Step:   166200, Batch Loss:     1.655122, Batch Acc: 0.617524, Tokens per Sec:    16154, Lr: 0.000022\n",
            "2023-01-19 15:20:59,710 - INFO - joeynmt.training - Epoch 478, Step:   166300, Batch Loss:     1.318110, Batch Acc: 0.618873, Tokens per Sec:    15986, Lr: 0.000022\n",
            "2023-01-19 15:21:05,131 - INFO - joeynmt.training - Epoch 478: total training loss 497.47\n",
            "2023-01-19 15:21:05,132 - INFO - joeynmt.training - EPOCH 479\n",
            "2023-01-19 15:21:07,245 - INFO - joeynmt.training - Epoch 479, Step:   166400, Batch Loss:     1.476216, Batch Acc: 0.619931, Tokens per Sec:    16258, Lr: 0.000022\n",
            "2023-01-19 15:21:14,800 - INFO - joeynmt.training - Epoch 479, Step:   166500, Batch Loss:     1.443661, Batch Acc: 0.621910, Tokens per Sec:    16188, Lr: 0.000022\n",
            "2023-01-19 15:21:22,401 - INFO - joeynmt.training - Epoch 479, Step:   166600, Batch Loss:     1.278581, Batch Acc: 0.619651, Tokens per Sec:    15822, Lr: 0.000022\n",
            "2023-01-19 15:21:29,990 - INFO - joeynmt.training - Epoch 479, Step:   166700, Batch Loss:     1.375478, Batch Acc: 0.618580, Tokens per Sec:    15917, Lr: 0.000022\n",
            "2023-01-19 15:21:31,478 - INFO - joeynmt.training - Epoch 479: total training loss 496.94\n",
            "2023-01-19 15:21:31,479 - INFO - joeynmt.training - EPOCH 480\n",
            "2023-01-19 15:21:37,674 - INFO - joeynmt.training - Epoch 480, Step:   166800, Batch Loss:     1.544985, Batch Acc: 0.617327, Tokens per Sec:    15808, Lr: 0.000022\n",
            "2023-01-19 15:21:45,228 - INFO - joeynmt.training - Epoch 480, Step:   166900, Batch Loss:     1.439014, Batch Acc: 0.620828, Tokens per Sec:    15848, Lr: 0.000022\n",
            "2023-01-19 15:21:52,833 - INFO - joeynmt.training - Epoch 480, Step:   167000, Batch Loss:     1.469720, Batch Acc: 0.619840, Tokens per Sec:    15899, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 136.89ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10841.81ex/s]\n",
            "2023-01-19 15:21:53,095 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=167000\n",
            "2023-01-19 15:21:53,095 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:21:58,192 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:21:58,193 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:21:58,193 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:21:58,194 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:21:58,197 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.67, loss:   2.85, ppl:  17.29, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0523[sec], evaluation: 0.0420[sec]\n",
            "2023-01-19 15:21:58,200 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:21:58,203 - INFO - joeynmt.training - \tSource:     بونا گؤره ده نئجه دینلهدیگینیزه دقت ائدین . چۆنکی کیمین وارێدێرسا ، اونا داها چوخ وئریلهجک . آمما کیمین یوخودورسا ، اؤزونونکو زن ائتدیگی شئی ده الیندن آلێناجاق . \n",
            "2023-01-19 15:21:58,203 - INFO - joeynmt.training - \tReference:  پس دقت کنید که چگونه می‌شنوید ؛ زیرا هر که دارد ، بیشتر به او داده خواهد شد ، اما آن که ندارد ، حتی آنچه تصور می‌کند دارد نیز از او گرفته خواهد شد . \n",
            "2023-01-19 15:21:58,203 - INFO - joeynmt.training - \tHypothesis: پس به این ترتیب ، سعی کنید که چه سخن بگویید ؛ زیرا اگر کسی همچون خردمندان داده خواهد شد ، اما آن که آنچه از دهانش یافت ، از آنچه به دست داده خواهد شد .\n",
            "2023-01-19 15:21:58,203 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:21:58,205 - INFO - joeynmt.training - \tSource:     آللاه امین آمانلێق یوردونا چاغێرێر وه ایستهدیگینی دوغرو یولا سالێر ! \n",
            "2023-01-19 15:21:58,206 - INFO - joeynmt.training - \tReference:  و خدا شما را به سرای سلامت فرا می‌خواند ، و هر که را بخواهد به راه راست هدایت می‌کند . \n",
            "2023-01-19 15:21:58,206 - INFO - joeynmt.training - \tHypothesis: خدا هر که را بخواهد بخواند ، و هر که را بخواهد بخواند و هر که را بخواهد هدایت کند .\n",
            "2023-01-19 15:21:58,206 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:21:58,208 - INFO - joeynmt.training - \tSource:     اجدادلاریمیز ایمانلاری اۆچۆن مۆسبت شهادت آلدێلار . \n",
            "2023-01-19 15:21:58,208 - INFO - joeynmt.training - \tReference:  در ایام قدیم ، افرادی بودند که به دلیل ایمانشان ، بر آنان به‌نیکویی شهادت داده شد . \n",
            "2023-01-19 15:21:58,208 - INFO - joeynmt.training - \tHypothesis: پس شهادت دادند که ما ایمان آورده بودند .\n",
            "2023-01-19 15:21:58,208 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:21:58,210 - INFO - joeynmt.training - \tSource:     اونلار آللاهێ قویوب اؤزلرینه نه بیر خئیر ، نه ده بیر زرر وئره بیلن بۆتلره ابادت ائدیر وه : بونلار آللاه یانیندا بیزدن اؤترو شفاعت ائدنلردیر ! دئییرلر . دئ : آللاها گؤیلرده وه یئرده بیلمهدیگی بیر شئییمی خبر وئریرسینیز ؟ آللاه اؤزونه شریک قوشولان بۆتلردن اوزاقدێر وه اوجادێر ! \n",
            "2023-01-19 15:21:58,210 - INFO - joeynmt.training - \tReference:  و به جای خدا ، چیزهایی را می‌پرستند که نه به آنان زیان می‌رساند و نه به آنان سود می‌دهد . و می‌گویند : اینها نزد خدا شفاعتگران ما هستند . بگو : آیا خدا را به چیزی که در آسمانها و در زمین نمی‌داند ، آگاه می‌گردانید ؟ او پاک و برتر است از آنچه با وی‌ شریک می‌سازند . \n",
            "2023-01-19 15:21:58,210 - INFO - joeynmt.training - \tHypothesis: و چون به جای خدا را می پرستند ، نه سود و نه زیانی به آنان می رساند ، و می گویند : اینها را برای خدا و نهان و آشکارا به او احسانه است . بگو : آیا از آنچه در آسمانها و آنچه با خداست از آنچه با وی شریک می گردانند ؟\n",
            "2023-01-19 15:22:03,372 - INFO - joeynmt.training - Epoch 480: total training loss 499.85\n",
            "2023-01-19 15:22:03,372 - INFO - joeynmt.training - EPOCH 481\n",
            "2023-01-19 15:22:05,819 - INFO - joeynmt.training - Epoch 481, Step:   167100, Batch Loss:     1.385172, Batch Acc: 0.624821, Tokens per Sec:    15967, Lr: 0.000022\n",
            "2023-01-19 15:22:13,410 - INFO - joeynmt.training - Epoch 481, Step:   167200, Batch Loss:     1.356114, Batch Acc: 0.619569, Tokens per Sec:    16072, Lr: 0.000022\n",
            "2023-01-19 15:22:22,028 - INFO - joeynmt.training - Epoch 481, Step:   167300, Batch Loss:     1.476493, Batch Acc: 0.615365, Tokens per Sec:    13919, Lr: 0.000022\n",
            "2023-01-19 15:22:29,622 - INFO - joeynmt.training - Epoch 481, Step:   167400, Batch Loss:     1.369298, Batch Acc: 0.617638, Tokens per Sec:    15991, Lr: 0.000022\n",
            "2023-01-19 15:22:30,771 - INFO - joeynmt.training - Epoch 481: total training loss 497.54\n",
            "2023-01-19 15:22:30,771 - INFO - joeynmt.training - EPOCH 482\n",
            "2023-01-19 15:22:37,224 - INFO - joeynmt.training - Epoch 482, Step:   167500, Batch Loss:     1.365386, Batch Acc: 0.619976, Tokens per Sec:    15750, Lr: 0.000022\n",
            "2023-01-19 15:22:44,797 - INFO - joeynmt.training - Epoch 482, Step:   167600, Batch Loss:     1.427231, Batch Acc: 0.621721, Tokens per Sec:    15876, Lr: 0.000022\n",
            "2023-01-19 15:22:52,316 - INFO - joeynmt.training - Epoch 482, Step:   167700, Batch Loss:     1.274612, Batch Acc: 0.616627, Tokens per Sec:    16012, Lr: 0.000022\n",
            "2023-01-19 15:22:57,347 - INFO - joeynmt.training - Epoch 482: total training loss 499.26\n",
            "2023-01-19 15:22:57,348 - INFO - joeynmt.training - EPOCH 483\n",
            "2023-01-19 15:23:00,050 - INFO - joeynmt.training - Epoch 483, Step:   167800, Batch Loss:     1.399118, Batch Acc: 0.623570, Tokens per Sec:    16240, Lr: 0.000022\n",
            "2023-01-19 15:23:07,602 - INFO - joeynmt.training - Epoch 483, Step:   167900, Batch Loss:     1.451064, Batch Acc: 0.621372, Tokens per Sec:    15776, Lr: 0.000022\n",
            "2023-01-19 15:23:15,200 - INFO - joeynmt.training - Epoch 483, Step:   168000, Batch Loss:     1.473872, Batch Acc: 0.619963, Tokens per Sec:    15764, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 139.85ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10516.02ex/s]\n",
            "2023-01-19 15:23:15,469 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=168000\n",
            "2023-01-19 15:23:15,469 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:23:20,417 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:23:20,417 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:23:20,417 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:23:20,418 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:23:20,421 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.70, loss:   2.88, ppl:  17.78, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.9014[sec], evaluation: 0.0427[sec]\n",
            "2023-01-19 15:23:20,424 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:23:20,427 - INFO - joeynmt.training - \tSource:     یهودا قبیلهسیندن 12 ، 000 مۆهورلنمیش نفر ؛ رووئن قبیلهسیندن 12 ، 000 نفر ؛ قاد قبیلهسیندن 12 ، 000 نفر ؛ \n",
            "2023-01-19 15:23:20,427 - INFO - joeynmt.training - \tReference:  از طایفهٔ یهودا ۱۲ ۰۰۰ نفر مهر شدند ؛ از طایفهٔ رئوبین ۱۲ ۰۰۰ نفر ؛ از طایفهٔ جاد ۱۲ ۰۰۰ نفر ؛ \n",
            "2023-01-19 15:23:20,427 - INFO - joeynmt.training - \tHypothesis: یهودا ۰۰۰ نفر ؛ از طایفهٔ یساکار ۱۲ ۰۰۰ نفر ؛ از طایفهٔ یساکار ۱۲ ۰۰۰ نفر ؛\n",
            "2023-01-19 15:23:20,428 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:23:20,430 - INFO - joeynmt.training - \tSource:     سیزه خدمت ائتمک اۆچۆن باشقا جمیتلردن تمینات آلاراق سانکی اونلارێ سویدوم . \n",
            "2023-01-19 15:23:20,430 - INFO - joeynmt.training - \tReference:  من با پذیرفتن کمک‌های مالی از جماعت‌های دیگر ، آن‌ها را در محرومیت قرار دادم تا بتوانم به شما خدمت کنم . \n",
            "2023-01-19 15:23:20,430 - INFO - joeynmt.training - \tHypothesis: من اینچنین که جماعتی از شما به تشویق می کنم ، از شما سیلاس ها غرقه ای دیگریهٔ جماعت ها ی خود را به شما نشان دادم .\n",
            "2023-01-19 15:23:20,430 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:23:20,432 - INFO - joeynmt.training - \tSource:     آللاه باشچیلاری وه حاکملری ترک سلاح ائدیب ، اونلارا چارمێخدا قلبه چالاراق آلمده رۆسۆای ائتدی . \n",
            "2023-01-19 15:23:20,432 - INFO - joeynmt.training - \tReference:  او حکومت‌ها و قدرت‌ها را از طریق تیر شکنجه مغلوب و نزد همگان رسوا ساخت و به این ترتیب ، پیروزی خود را بر آن‌ها به نمایش گذاشت . \n",
            "2023-01-19 15:23:20,432 - INFO - joeynmt.training - \tHypothesis: خدا سرسرازان و قدرت ها را ترک کردند و به تیر میخکوب کردند ، با مثل هایشان در دلشان تصمیموت کردند .\n",
            "2023-01-19 15:23:20,432 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:23:20,434 - INFO - joeynmt.training - \tSource:     اؤزو ده چۆرۆمۆش سۆمۆکلر اولدوغوموز زامانمێ \n",
            "2023-01-19 15:23:20,434 - INFO - joeynmt.training - \tReference:  آیا وقتی ما استخوان‌ریزه‌های پوسیده شدیم زندگی را از سر می‌گیریم‌ ؟ \n",
            "2023-01-19 15:23:20,434 - INFO - joeynmt.training - \tHypothesis: و چون نوشد ، نمی تواند رسید ؟\n",
            "2023-01-19 15:23:28,343 - INFO - joeynmt.training - Epoch 483, Step:   168100, Batch Loss:     1.377323, Batch Acc: 0.619723, Tokens per Sec:    14639, Lr: 0.000022\n",
            "2023-01-19 15:23:30,015 - INFO - joeynmt.training - Epoch 483: total training loss 502.75\n",
            "2023-01-19 15:23:30,015 - INFO - joeynmt.training - EPOCH 484\n",
            "2023-01-19 15:23:36,592 - INFO - joeynmt.training - Epoch 484, Step:   168200, Batch Loss:     1.509373, Batch Acc: 0.618311, Tokens per Sec:    15446, Lr: 0.000022\n",
            "2023-01-19 15:23:44,134 - INFO - joeynmt.training - Epoch 484, Step:   168300, Batch Loss:     1.525768, Batch Acc: 0.621490, Tokens per Sec:    16211, Lr: 0.000022\n",
            "2023-01-19 15:23:51,708 - INFO - joeynmt.training - Epoch 484, Step:   168400, Batch Loss:     1.411166, Batch Acc: 0.622058, Tokens per Sec:    16096, Lr: 0.000022\n",
            "2023-01-19 15:23:56,503 - INFO - joeynmt.training - Epoch 484: total training loss 497.35\n",
            "2023-01-19 15:23:56,504 - INFO - joeynmt.training - EPOCH 485\n",
            "2023-01-19 15:23:59,280 - INFO - joeynmt.training - Epoch 485, Step:   168500, Batch Loss:     1.397018, Batch Acc: 0.624485, Tokens per Sec:    15564, Lr: 0.000022\n",
            "2023-01-19 15:24:06,760 - INFO - joeynmt.training - Epoch 485, Step:   168600, Batch Loss:     1.454195, Batch Acc: 0.621662, Tokens per Sec:    16045, Lr: 0.000022\n",
            "2023-01-19 15:24:14,308 - INFO - joeynmt.training - Epoch 485, Step:   168700, Batch Loss:     1.311598, Batch Acc: 0.620229, Tokens per Sec:    16185, Lr: 0.000022\n",
            "2023-01-19 15:24:21,865 - INFO - joeynmt.training - Epoch 485, Step:   168800, Batch Loss:     1.503121, Batch Acc: 0.622152, Tokens per Sec:    15931, Lr: 0.000022\n",
            "2023-01-19 15:24:22,770 - INFO - joeynmt.training - Epoch 485: total training loss 498.79\n",
            "2023-01-19 15:24:22,771 - INFO - joeynmt.training - EPOCH 486\n",
            "2023-01-19 15:24:29,426 - INFO - joeynmt.training - Epoch 486, Step:   168900, Batch Loss:     1.358716, Batch Acc: 0.620080, Tokens per Sec:    15808, Lr: 0.000022\n",
            "2023-01-19 15:24:37,030 - INFO - joeynmt.training - Epoch 486, Step:   169000, Batch Loss:     1.462118, Batch Acc: 0.618404, Tokens per Sec:    15967, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 125.04ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9864.14ex/s]\n",
            "2023-01-19 15:24:37,306 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=169000\n",
            "2023-01-19 15:24:37,306 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:24:42,230 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:24:42,230 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:24:42,231 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:24:42,232 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:24:42,234 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.35, loss:   2.75, ppl:  15.57, acc:   0.44, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.8727[sec], evaluation: 0.0477[sec]\n",
            "2023-01-19 15:24:42,237 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:24:42,240 - INFO - joeynmt.training - \tSource:     سنسه اؤیرندیگین وه گۆۆندیگین شئیلره صادق قال . چۆنکی بونلارێ کیملردن اؤیرندیگینی بیلیرسن ، \n",
            "2023-01-19 15:24:42,241 - INFO - joeynmt.training - \tReference:  اما ، تو آنچه را از دیگران آموختی و با دلیل و برهان به آن متقاعد شدی ، دنبال کن ؛ زیرا می‌دانی آن‌ها را از چه کسانی آموخته‌ای . \n",
            "2023-01-19 15:24:42,241 - INFO - joeynmt.training - \tHypothesis: اما تو از آنچه می کنی ، آنچه را که در آن منطقیع می کنی ، در واقع این ها درکاتاتاتاتات را می داند ،\n",
            "2023-01-19 15:24:42,241 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:24:42,243 - INFO - joeynmt.training - \tSource:     بیر زامان ملکلره : آدمه سجده ائدین ! دئیه بویورموشدوق . ایبلیسدن باشقا هامێسێ سجده ائتدی . او ، بویون قاچێرتدێ . \n",
            "2023-01-19 15:24:42,243 - INFO - joeynmt.training - \tReference:  و یاد کن‌ هنگامی را که به فرشتگان گفتیم : برای آدم سجده کنید . پس ، جز ابلیس که سر باز زد همه‌ سجده کردند . \n",
            "2023-01-19 15:24:42,243 - INFO - joeynmt.training - \tHypothesis: و چون فرشتگان گفت : به راستی برای آدم سجده کنید ، پس همه را سجده کردند و همه جز ابلیس سجده کردند . همه همه از آن گریزان می شد .\n",
            "2023-01-19 15:24:42,244 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:24:42,245 - INFO - joeynmt.training - \tSource:     آللاه هر کسه ائتدیگی امللرین جزاسێنێ وئرمک اۆچۆن بئله ائدهجکدیر ! شبههسیز کی ، آللاه تئزلیکله حاقق حساب چکندیر ! \n",
            "2023-01-19 15:24:42,246 - INFO - joeynmt.training - \tReference:  تا خدا به هر کس هر چه به دست آورده است جزا دهد ، که خدا زودشمار است . \n",
            "2023-01-19 15:24:42,246 - INFO - joeynmt.training - \tHypothesis: خدا هر که را پاداش آنچه انجام می دهد کیفر کیفرش خواهد کرد ، زیرا خدا به زودی خدا زودشمار است .\n",
            "2023-01-19 15:24:42,246 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:24:42,248 - INFO - joeynmt.training - \tSource:     اؤز ائوینی یاخشی اداره ائتمهلی ، تام بیر لیاقتله اوشاقلارێنێ اؤز تابعلیگینده ساخلامالێدێر . \n",
            "2023-01-19 15:24:42,248 - INFO - joeynmt.training - \tReference:  کسی باشد که خانوادهٔ خود را به‌خوبی اداره کند و فرزندانی مطیع و مؤدب داشته باشد\n",
            "2023-01-19 15:24:42,248 - INFO - joeynmt.training - \tHypothesis: به خانهٔ خود اصلاحمت مکن ، بلکه حتی یکی از زنان خود پیروی مکن ، بلکه با خود ملایمت خود را تحسین نمی کنم .\n",
            "2023-01-19 15:24:49,779 - INFO - joeynmt.training - Epoch 486, Step:   169100, Batch Loss:     1.556524, Batch Acc: 0.620820, Tokens per Sec:    15602, Lr: 0.000022\n",
            "2023-01-19 15:24:54,349 - INFO - joeynmt.training - Epoch 486: total training loss 496.17\n",
            "2023-01-19 15:24:54,350 - INFO - joeynmt.training - EPOCH 487\n",
            "2023-01-19 15:24:57,463 - INFO - joeynmt.training - Epoch 487, Step:   169200, Batch Loss:     1.392320, Batch Acc: 0.626298, Tokens per Sec:    15749, Lr: 0.000022\n",
            "2023-01-19 15:25:04,988 - INFO - joeynmt.training - Epoch 487, Step:   169300, Batch Loss:     1.455272, Batch Acc: 0.618509, Tokens per Sec:    15963, Lr: 0.000022\n",
            "2023-01-19 15:25:12,569 - INFO - joeynmt.training - Epoch 487, Step:   169400, Batch Loss:     1.443955, Batch Acc: 0.622425, Tokens per Sec:    15788, Lr: 0.000022\n",
            "2023-01-19 15:25:20,129 - INFO - joeynmt.training - Epoch 487, Step:   169500, Batch Loss:     1.413890, Batch Acc: 0.618948, Tokens per Sec:    15988, Lr: 0.000022\n",
            "2023-01-19 15:25:20,839 - INFO - joeynmt.training - Epoch 487: total training loss 499.72\n",
            "2023-01-19 15:25:20,839 - INFO - joeynmt.training - EPOCH 488\n",
            "2023-01-19 15:25:27,753 - INFO - joeynmt.training - Epoch 488, Step:   169600, Batch Loss:     1.548503, Batch Acc: 0.622921, Tokens per Sec:    16055, Lr: 0.000022\n",
            "2023-01-19 15:25:35,239 - INFO - joeynmt.training - Epoch 488, Step:   169700, Batch Loss:     1.336501, Batch Acc: 0.623096, Tokens per Sec:    15914, Lr: 0.000022\n",
            "2023-01-19 15:25:42,774 - INFO - joeynmt.training - Epoch 488, Step:   169800, Batch Loss:     1.477459, Batch Acc: 0.618282, Tokens per Sec:    16088, Lr: 0.000022\n",
            "2023-01-19 15:25:47,129 - INFO - joeynmt.training - Epoch 488: total training loss 496.86\n",
            "2023-01-19 15:25:47,129 - INFO - joeynmt.training - EPOCH 489\n",
            "2023-01-19 15:25:50,410 - INFO - joeynmt.training - Epoch 489, Step:   169900, Batch Loss:     1.410907, Batch Acc: 0.623186, Tokens per Sec:    15971, Lr: 0.000022\n",
            "2023-01-19 15:25:59,159 - INFO - joeynmt.training - Epoch 489, Step:   170000, Batch Loss:     1.530117, Batch Acc: 0.622555, Tokens per Sec:    13932, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 134.98ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10891.35ex/s]\n",
            "2023-01-19 15:25:59,415 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=170000\n",
            "2023-01-19 15:25:59,415 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:26:04,921 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:26:04,921 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:26:04,921 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:26:04,922 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:26:04,925 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.38, loss:   2.84, ppl:  17.05, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.4433[sec], evaluation: 0.0590[sec]\n",
            "2023-01-19 15:26:04,927 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:26:04,931 - INFO - joeynmt.training - \tSource:     اونلار داوودون یانینا گلیکده اونلاردان قورخدو . اونلار دئدیلر : قورخما ، بیز بیر بیریمیزه حاقسێزلێق ائتمیش ایکی ادیاچییییق . آرامێزدا عدالتله هؤکم ائت ، حاققێ تاپدالاما وه بیزه دوغرو یولو گؤستر ! \n",
            "2023-01-19 15:26:04,931 - INFO - joeynmt.training - \tReference:  وقتی به طور ناگهانی‌ بر داوود درآمدند ، و او از آنان به هراس افتاد ، گفتند : مترس ، ما دو مدعی هستیم‌ که یکی از ما بر دیگری تجاوز کرده ، پس میان ما به حق داوری کن ، و از حق دور مشو ، و ما را به راه راست راهبر باش . \n",
            "2023-01-19 15:26:04,931 - INFO - joeynmt.training - \tHypothesis: و چون داوود را نزد داوود بردند ، از ایشان ترسیدند . گفتند : مترس ، ما مردمی را برای ستم کردیم ، و میان ما گروهی قرار دادیم که میان ما به حق داوری کن و راه راست قرار دهیم .\n",
            "2023-01-19 15:26:04,931 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:26:04,933 - INFO - joeynmt.training - \tSource:     بئواختا قالیرسان . \n",
            "2023-01-19 15:26:04,933 - INFO - joeynmt.training - \tReference:  دیر میرسی . \n",
            "2023-01-19 15:26:04,933 - INFO - joeynmt.training - \tHypothesis: بصریه در افروخته می شود .\n",
            "2023-01-19 15:26:04,934 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:26:04,935 - INFO - joeynmt.training - \tSource:      ائی مبدی داغێدێب اۆچ گۆنده تیکن ! اگر آللاهێن اوغلوسانسا ، اؤزونو خلاص ائت وه چارمێخدان دۆش ! دئییردیلر . \n",
            "2023-01-19 15:26:04,936 - INFO - joeynmt.training - \tReference:  می‌گفتند : تو که می‌خواستی معبد را خراب و در سه روز آن را بنا کنی ، خود را نجات بده ! اگر یکی از پسران خدایی ، از تیر شکنجه پایین بیا ! \n",
            "2023-01-19 15:26:04,936 - INFO - joeynmt.training - \tHypothesis: و گفتند : ای معبد ، در سه روز در سه روزها ، پسر خدا را نجات بده و اگر از تیر شکنجهٔ خود نجات بده .\n",
            "2023-01-19 15:26:04,936 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:26:04,938 - INFO - joeynmt.training - \tSource:     آمما سندن بیر شکایتیم وار : اوهلکی محبتندن ال چکمیسن . \n",
            "2023-01-19 15:26:04,938 - INFO - joeynmt.training - \tReference:  با این حال ، این عیب را در تو می‌بینم که محبت نخستین خود را از دست داده‌ای . \n",
            "2023-01-19 15:26:04,938 - INFO - joeynmt.training - \tHypothesis: اما تو از تو شکایت می کنم که محبتی به وجود آمده ای .\n",
            "2023-01-19 15:26:12,498 - INFO - joeynmt.training - Epoch 489, Step:   170100, Batch Loss:     1.370226, Batch Acc: 0.623078, Tokens per Sec:    15296, Lr: 0.000022\n",
            "2023-01-19 15:26:20,095 - INFO - joeynmt.training - Epoch 489, Step:   170200, Batch Loss:     1.586274, Batch Acc: 0.615345, Tokens per Sec:    15905, Lr: 0.000022\n",
            "2023-01-19 15:26:20,484 - INFO - joeynmt.training - Epoch 489: total training loss 496.52\n",
            "2023-01-19 15:26:20,484 - INFO - joeynmt.training - EPOCH 490\n",
            "2023-01-19 15:26:27,594 - INFO - joeynmt.training - Epoch 490, Step:   170300, Batch Loss:     1.450784, Batch Acc: 0.623905, Tokens per Sec:    15975, Lr: 0.000022\n",
            "2023-01-19 15:26:35,200 - INFO - joeynmt.training - Epoch 490, Step:   170400, Batch Loss:     1.517950, Batch Acc: 0.620545, Tokens per Sec:    15930, Lr: 0.000022\n",
            "2023-01-19 15:26:42,797 - INFO - joeynmt.training - Epoch 490, Step:   170500, Batch Loss:     1.523487, Batch Acc: 0.618966, Tokens per Sec:    15968, Lr: 0.000022\n",
            "2023-01-19 15:26:46,894 - INFO - joeynmt.training - Epoch 490: total training loss 497.66\n",
            "2023-01-19 15:26:46,894 - INFO - joeynmt.training - EPOCH 491\n",
            "2023-01-19 15:26:50,359 - INFO - joeynmt.training - Epoch 491, Step:   170600, Batch Loss:     1.469861, Batch Acc: 0.626584, Tokens per Sec:    16103, Lr: 0.000022\n",
            "2023-01-19 15:26:57,899 - INFO - joeynmt.training - Epoch 491, Step:   170700, Batch Loss:     1.339665, Batch Acc: 0.617782, Tokens per Sec:    16002, Lr: 0.000022\n",
            "2023-01-19 15:27:06,453 - INFO - joeynmt.training - Epoch 491, Step:   170800, Batch Loss:     1.407506, Batch Acc: 0.619749, Tokens per Sec:    14109, Lr: 0.000022\n",
            "2023-01-19 15:27:14,176 - INFO - joeynmt.training - Epoch 491, Step:   170900, Batch Loss:     1.283822, Batch Acc: 0.624168, Tokens per Sec:    15558, Lr: 0.000022\n",
            "2023-01-19 15:27:14,396 - INFO - joeynmt.training - Epoch 491: total training loss 496.07\n",
            "2023-01-19 15:27:14,396 - INFO - joeynmt.training - EPOCH 492\n",
            "2023-01-19 15:27:21,878 - INFO - joeynmt.training - Epoch 492, Step:   171000, Batch Loss:     1.345735, Batch Acc: 0.624963, Tokens per Sec:    15816, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 74.80ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9980.52ex/s] \n",
            "2023-01-19 15:27:22,165 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=171000\n",
            "2023-01-19 15:27:22,165 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:27:27,437 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:27:27,438 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:27:27,438 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:27:27,439 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:27:27,442 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.02, loss:   2.91, ppl:  18.30, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.2223[sec], evaluation: 0.0469[sec]\n",
            "2023-01-19 15:27:27,445 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:27:27,449 - INFO - joeynmt.training - \tSource:     بو دا آد تایفاسیدیر ! اونلار ربینین آیهلرینی اینکار ائتدیلر ، اونون پیغمبرلرینه قارشێ چێخدێلار ، باشلارێنێن اۆستۆنده دوران هر بیر اینادکار بؤیوگون امرینه تابع اولدولار . \n",
            "2023-01-19 15:27:27,449 - INFO - joeynmt.training - \tReference:  و این ، قوم‌ عاد بود که آیات پروردگارشان را انکار کردند ، و فرستادگانش را نافرمانی نمودند ، و به دنبال فرمان هر زورگوی ستیزه‌جوی رفتند . \n",
            "2023-01-19 15:27:27,449 - INFO - joeynmt.training - \tHypothesis: این عقوبت کسانی اند که آیات پروردگارشان انکار کردند ، و آیات او را به دروغ برسانند و به فرمان او مبراجر ورزیدند . پس به فرمان او فرمان دادند که مجادله کنند .\n",
            "2023-01-19 15:27:27,449 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:27:27,451 - INFO - joeynmt.training - \tSource:     همچنین اونلار قوربان تقدیم ائتدی ، نئجه کی ربین قانونوندا امر اولونوب : ایکتی قومرو قوشو یاخود ایکی گؤیرچین بالاسێ . \n",
            "2023-01-19 15:27:27,451 - INFO - joeynmt.training - \tReference:  همچنین ایشان هماهنگ با آنچه در شریعت یهوه گفته شده است ، قربانی گذراندند ؛ یعنی یک جفت قمری یا دو جوجه‌کبوتر . \n",
            "2023-01-19 15:27:27,452 - INFO - joeynmt.training - \tHypothesis: پس آنچه قربانی تقدیم کرده است ، حکم یهوه برای آنان چنین دستور داد : دو یا دو نفر در مذبح ، دو ماهیهٔ دو قرعه ای برای دو برابر خدا قرار دهد .\n",
            "2023-01-19 15:27:27,452 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:27:27,454 - INFO - joeynmt.training - \tSource:     پئتئر ایسه دئدی : باخ بیز هر شئیی قویوب سنین آردێنجا گلمیشیک . \n",
            "2023-01-19 15:27:27,454 - INFO - joeynmt.training - \tReference:  پطرس به او گفت : ما همه چیز خود را رها کرده‌ایم و از تو پیروی می‌کنیم . \n",
            "2023-01-19 15:27:27,454 - INFO - joeynmt.training - \tHypothesis: پطرس گفت : ما همه چیز را رهانیدیم .\n",
            "2023-01-19 15:27:27,454 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:27:27,456 - INFO - joeynmt.training - \tSource:     او کسلر اۆچۆن کی ، بؤیوک گۆناهلاردان رزیل ایشلردن چکینر ، قزبلندیکلری زامان باغیشلایارلار ؛ \n",
            "2023-01-19 15:27:27,456 - INFO - joeynmt.training - \tReference:  و کسانی که از گناهان بزرگ و زشتکاریها خود را به دور می‌دارند و چون به خشم درمی‌آیند درمی‌گذرند . \n",
            "2023-01-19 15:27:27,457 - INFO - joeynmt.training - \tHypothesis: همان کسانی که از گناهان بزرگی که از گناهان بزرگی می شوند ، در نتیجه ستمگران می کنند .\n",
            "2023-01-19 15:27:35,140 - INFO - joeynmt.training - Epoch 492, Step:   171100, Batch Loss:     1.433482, Batch Acc: 0.622756, Tokens per Sec:    15130, Lr: 0.000022\n",
            "2023-01-19 15:27:42,650 - INFO - joeynmt.training - Epoch 492, Step:   171200, Batch Loss:     1.397389, Batch Acc: 0.621621, Tokens per Sec:    16111, Lr: 0.000022\n",
            "2023-01-19 15:27:46,424 - INFO - joeynmt.training - Epoch 492: total training loss 494.69\n",
            "2023-01-19 15:27:46,424 - INFO - joeynmt.training - EPOCH 493\n",
            "2023-01-19 15:27:50,160 - INFO - joeynmt.training - Epoch 493, Step:   171300, Batch Loss:     1.371819, Batch Acc: 0.625718, Tokens per Sec:    16074, Lr: 0.000022\n",
            "2023-01-19 15:27:57,687 - INFO - joeynmt.training - Epoch 493, Step:   171400, Batch Loss:     1.504129, Batch Acc: 0.623699, Tokens per Sec:    16149, Lr: 0.000022\n",
            "2023-01-19 15:28:05,312 - INFO - joeynmt.training - Epoch 493, Step:   171500, Batch Loss:     1.406980, Batch Acc: 0.619859, Tokens per Sec:    15917, Lr: 0.000022\n",
            "2023-01-19 15:28:12,686 - INFO - joeynmt.training - Epoch 493: total training loss 495.99\n",
            "2023-01-19 15:28:12,686 - INFO - joeynmt.training - EPOCH 494\n",
            "2023-01-19 15:28:12,837 - INFO - joeynmt.training - Epoch 494, Step:   171600, Batch Loss:     1.359216, Batch Acc: 0.644560, Tokens per Sec:    15403, Lr: 0.000022\n",
            "2023-01-19 15:28:20,378 - INFO - joeynmt.training - Epoch 494, Step:   171700, Batch Loss:     1.428104, Batch Acc: 0.624186, Tokens per Sec:    15941, Lr: 0.000022\n",
            "2023-01-19 15:28:27,857 - INFO - joeynmt.training - Epoch 494, Step:   171800, Batch Loss:     1.472284, Batch Acc: 0.624348, Tokens per Sec:    15956, Lr: 0.000022\n",
            "2023-01-19 15:28:35,485 - INFO - joeynmt.training - Epoch 494, Step:   171900, Batch Loss:     1.514019, Batch Acc: 0.618566, Tokens per Sec:    16033, Lr: 0.000022\n",
            "2023-01-19 15:28:39,044 - INFO - joeynmt.training - Epoch 494: total training loss 495.12\n",
            "2023-01-19 15:28:39,045 - INFO - joeynmt.training - EPOCH 495\n",
            "2023-01-19 15:28:43,076 - INFO - joeynmt.training - Epoch 495, Step:   172000, Batch Loss:     1.471447, Batch Acc: 0.620985, Tokens per Sec:    16351, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 132.26ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10250.22ex/s]\n",
            "2023-01-19 15:28:43,343 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=172000\n",
            "2023-01-19 15:28:43,344 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:28:48,403 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:28:48,404 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:28:48,404 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:28:48,405 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:28:48,408 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.62, loss:   2.80, ppl:  16.45, acc:   0.42, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 5.0128[sec], evaluation: 0.0435[sec]\n",
            "2023-01-19 15:28:48,410 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:28:48,414 - INFO - joeynmt.training - \tSource:     ائی ایمان گتیرنلر ! پیغمبرله مخفی دانیشآجاغینیز زامان بو دانێشێقدان اول صدقه وئرین . بو سیزین اۆچۆن داها خئیرلی ، داکا پاکدێر . اگر بیر شئی تاپماسانێز . چۆنکی آللاه باغیشلایاندیر ، رحم ائدندیر ! \n",
            "2023-01-19 15:28:48,415 - INFO - joeynmt.training - \tReference:  ای کسانی که ایمان آورده‌اید ، هرگاه با پیامبر خدا گفتگوی محرمانه می‌کنید ، پیش از گفتگوی محرمانه خود صدقه‌ای تقدیم بدارید . این کار برای شما بهتر و پاکیزه‌تر است ؛ و اگر چیزی نیافتید بدانید که خدا آمرزنده مهربان است . \n",
            "2023-01-19 15:28:48,415 - INFO - joeynmt.training - \tHypothesis: ای کسانی که ایمان آورده اید ، چون با سخن بگویید : ما پیش از این ، نبوت را بر شما حفایت می کنیم ، این برای شما بهتر است ؛ و اگر تقوا پیش از آن برای شما بهتر است ، و اگر خدا آمرزنده مهربان است .\n",
            "2023-01-19 15:28:48,415 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:28:48,417 - INFO - joeynmt.training - \tSource:     اونلارێن گؤزلری زلیلجهسینه یئره دیکیلهجک ، اؤزلرینی ده ذلت بۆرویهجکدیر . بو اونلارا وه د اولونموش همین قیامت گۆنۆدۆر ! \n",
            "2023-01-19 15:28:48,417 - INFO - joeynmt.training - \tReference:  دیدگانشان فرو افتاده ، غبار مذلت آنان را فرو گرفته است . این است همان روزی که به ایشان وعده داده می‌شد . \n",
            "2023-01-19 15:28:48,417 - INFO - joeynmt.training - \tHypothesis: دیدگانشان فرو افتاده و خواری آنان را فرو بر آنان فرو گرفته است . این وعده ها راست است .\n",
            "2023-01-19 15:28:48,417 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:28:48,419 - INFO - joeynmt.training - \tSource:     نیه ؟ سیزی سئومهدیگیمه گؤرهمی ؟ آللاه بیلیر کی ، سئویرم . \n",
            "2023-01-19 15:28:48,419 - INFO - joeynmt.training - \tReference:  چرا از شما چیزی نپذیرفته‌ام ؟ آیا به این دلیل است که شما را دوست ندارم ؟ خدا می‌داند که دوستتان دارم . \n",
            "2023-01-19 15:28:48,420 - INFO - joeynmt.training - \tHypothesis: چرا به همین دلیل که به شما محبت می کنم ، خدا دوست می دارم که دوستت به شما محبت می کند .\n",
            "2023-01-19 15:28:48,420 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:28:48,422 - INFO - joeynmt.training - \tSource:     ایسنانین اونا رحمی گلدی . الینی اوزادێب اونا توخوندو وه دئدی : ایستهییرهم ، پاک اول ! \n",
            "2023-01-19 15:28:48,422 - INFO - joeynmt.training - \tReference:  عیسی دلش به حال او سوخت . پس دستش را دراز کرد ، آن مرد را لمس نمود و به او گفت : می‌خواهم . پاک شو . \n",
            "2023-01-19 15:28:48,422 - INFO - joeynmt.training - \tHypothesis: پس عیسی دلش به او سوخت و دستش را دراز کرد . سپس به او گفت : من می خواهم ، پاک شوم .\n",
            "2023-01-19 15:28:56,053 - INFO - joeynmt.training - Epoch 495, Step:   172100, Batch Loss:     1.422222, Batch Acc: 0.624925, Tokens per Sec:    15086, Lr: 0.000022\n",
            "2023-01-19 15:29:03,684 - INFO - joeynmt.training - Epoch 495, Step:   172200, Batch Loss:     1.526348, Batch Acc: 0.619279, Tokens per Sec:    16026, Lr: 0.000022\n",
            "2023-01-19 15:29:10,755 - INFO - joeynmt.training - Epoch 495: total training loss 493.68\n",
            "2023-01-19 15:29:10,755 - INFO - joeynmt.training - EPOCH 496\n",
            "2023-01-19 15:29:11,304 - INFO - joeynmt.training - Epoch 496, Step:   172300, Batch Loss:     1.364275, Batch Acc: 0.617330, Tokens per Sec:    15210, Lr: 0.000022\n",
            "2023-01-19 15:29:18,913 - INFO - joeynmt.training - Epoch 496, Step:   172400, Batch Loss:     1.346886, Batch Acc: 0.622029, Tokens per Sec:    15818, Lr: 0.000022\n",
            "2023-01-19 15:29:26,451 - INFO - joeynmt.training - Epoch 496, Step:   172500, Batch Loss:     1.404290, Batch Acc: 0.620548, Tokens per Sec:    16000, Lr: 0.000022\n",
            "2023-01-19 15:29:35,144 - INFO - joeynmt.training - Epoch 496, Step:   172600, Batch Loss:     1.471348, Batch Acc: 0.620023, Tokens per Sec:    13953, Lr: 0.000022\n",
            "2023-01-19 15:29:38,297 - INFO - joeynmt.training - Epoch 496: total training loss 495.38\n",
            "2023-01-19 15:29:38,298 - INFO - joeynmt.training - EPOCH 497\n",
            "2023-01-19 15:29:42,740 - INFO - joeynmt.training - Epoch 497, Step:   172700, Batch Loss:     1.406692, Batch Acc: 0.622388, Tokens per Sec:    16180, Lr: 0.000022\n",
            "2023-01-19 15:29:50,235 - INFO - joeynmt.training - Epoch 497, Step:   172800, Batch Loss:     1.385639, Batch Acc: 0.624797, Tokens per Sec:    16091, Lr: 0.000022\n",
            "2023-01-19 15:29:57,829 - INFO - joeynmt.training - Epoch 497, Step:   172900, Batch Loss:     1.490721, Batch Acc: 0.619093, Tokens per Sec:    15746, Lr: 0.000022\n",
            "2023-01-19 15:30:04,564 - INFO - joeynmt.training - Epoch 497: total training loss 494.90\n",
            "2023-01-19 15:30:04,564 - INFO - joeynmt.training - EPOCH 498\n",
            "2023-01-19 15:30:05,418 - INFO - joeynmt.training - Epoch 498, Step:   173000, Batch Loss:     1.425586, Batch Acc: 0.617454, Tokens per Sec:    15381, Lr: 0.000022\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 127.92ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 8714.39ex/s]\n",
            "2023-01-19 15:30:05,718 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=173000\n",
            "2023-01-19 15:30:05,718 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:30:10,242 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:30:10,242 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:30:10,242 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:30:10,243 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:30:10,246 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   8.53, loss:   2.84, ppl:  17.10, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.4793[sec], evaluation: 0.0415[sec]\n",
            "2023-01-19 15:30:10,249 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:30:10,252 - INFO - joeynmt.training - \tSource:     سیز بیر زامانلار تقصیرلرینیز وه گۆناهلارینیز اۆزۆندن اؤلو ایدینیز . \n",
            "2023-01-19 15:30:10,252 - INFO - joeynmt.training - \tReference:  به علاوه ، خدا شما را که به سبب نافرمانی‌ها و گناهان خود مرده بودید ، زنده ساخت . \n",
            "2023-01-19 15:30:10,252 - INFO - joeynmt.training - \tHypothesis: شما چون بر مرگتان گناهی شده اید و گناه شما در آن زمان بودید .\n",
            "2023-01-19 15:30:10,253 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:30:10,254 - INFO - joeynmt.training - \tSource:     آمما استئفانێن سؤزوندکی هیکمتین وه روحون قارشیسیندا دایانا بیلمهدیلر . \n",
            "2023-01-19 15:30:10,255 - INFO - joeynmt.training - \tReference:  اما نتوانستند با حکمت او و روحی که او را در سخن گفتن هدایت می‌کرد ، مقابله کنند . \n",
            "2023-01-19 15:30:10,255 - INFO - joeynmt.training - \tHypothesis: اما او از نفعی که کلام الهام شده بود ، به نحوی و آنجا را نیافتند .\n",
            "2023-01-19 15:30:10,255 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:30:10,257 - INFO - joeynmt.training - \tSource:     ایمهانلی بیر قادێنێن دول قادێن یاخینلاری وارسا ، قوی اونلارا یاردیم ائتسین . ایمهنلیلار جمیتی بئله یۆکۆن آلتێنا گیرمهسین کی ، حقیقی دول قادێنلارا یاردیم ائده بیلسین . \n",
            "2023-01-19 15:30:10,257 - INFO - joeynmt.training - \tReference:  اگر زنی ایماندار خویشاوندانی بیوه دارد ، باید به آنان کمک کند تا سربار جماعت نباشند . به این ترتیب ، جماعت می‌تواند به بیوه‌زنانی که واقعا بی‌کس و نیازمندند ، کمک کند . \n",
            "2023-01-19 15:30:10,257 - INFO - joeynmt.training - \tHypothesis: آنان باید بیوه زنی از زنی باشد که شوهرشان در جماعت باشد ، خود را باید به خود بندندند و در جماعت های خود نهیباند .\n",
            "2023-01-19 15:30:10,257 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:30:10,259 - INFO - joeynmt.training - \tSource:     یانینیزدا اولارکن احتیاجیم اولدوغو حالدا هئچ بیرینیزه یۆک اولمادێم . چۆنکی ماکئدونایادان گلن قارداشلار احتیاجیمی تامامیله اؤدهدیلر . سیزه یۆک اولماقدان اؤزومو ساخلادێم وه گلهجکده ده اؤزومو ساخلایاجاغام . \n",
            "2023-01-19 15:30:10,259 - INFO - joeynmt.training - \tReference:  با این حال ، وقتی نزد شما بودم هر گاه به چیزی نیاز داشتم ، سربار هیچ کس نشدم ؛ زیرا برادرانی که از مقدونیه آمدند ، همهٔ نیازهای مرا برآورده ساختند . آری ، از هر جهت مراقب بودم که سربار شما نباشم و در آینده نیز نخواهم بود . \n",
            "2023-01-19 15:30:10,260 - INFO - joeynmt.training - \tHypothesis: در واقع ، حتی زمانی که نزد شما می آیم ، خود را نیز بر دوشق قرار دادم ؛ زیرا اکنون لازم را از طرف برادرانم ، یعنی از از از طرف دیگران خود نگاه داشته باشم و خود رازم و خود رازم .\n",
            "2023-01-19 15:30:17,870 - INFO - joeynmt.training - Epoch 498, Step:   173100, Batch Loss:     1.530388, Batch Acc: 0.623785, Tokens per Sec:    15272, Lr: 0.000021\n",
            "2023-01-19 15:30:25,394 - INFO - joeynmt.training - Epoch 498, Step:   173200, Batch Loss:     1.516198, Batch Acc: 0.623679, Tokens per Sec:    15918, Lr: 0.000021\n",
            "2023-01-19 15:30:32,993 - INFO - joeynmt.training - Epoch 498, Step:   173300, Batch Loss:     1.380648, Batch Acc: 0.620060, Tokens per Sec:    15901, Lr: 0.000021\n",
            "2023-01-19 15:30:36,046 - INFO - joeynmt.training - Epoch 498: total training loss 495.90\n",
            "2023-01-19 15:30:36,046 - INFO - joeynmt.training - EPOCH 499\n",
            "2023-01-19 15:30:40,855 - INFO - joeynmt.training - Epoch 499, Step:   173400, Batch Loss:     1.390847, Batch Acc: 0.625392, Tokens per Sec:    15668, Lr: 0.000021\n",
            "2023-01-19 15:30:49,395 - INFO - joeynmt.training - Epoch 499, Step:   173500, Batch Loss:     1.358000, Batch Acc: 0.621588, Tokens per Sec:    14144, Lr: 0.000021\n",
            "2023-01-19 15:30:56,981 - INFO - joeynmt.training - Epoch 499, Step:   173600, Batch Loss:     1.363469, Batch Acc: 0.622525, Tokens per Sec:    15855, Lr: 0.000021\n",
            "2023-01-19 15:31:03,473 - INFO - joeynmt.training - Epoch 499: total training loss 492.16\n",
            "2023-01-19 15:31:03,473 - INFO - joeynmt.training - EPOCH 500\n",
            "2023-01-19 15:31:04,651 - INFO - joeynmt.training - Epoch 500, Step:   173700, Batch Loss:     1.521472, Batch Acc: 0.632097, Tokens per Sec:    15273, Lr: 0.000021\n",
            "2023-01-19 15:31:12,319 - INFO - joeynmt.training - Epoch 500, Step:   173800, Batch Loss:     1.419293, Batch Acc: 0.621727, Tokens per Sec:    15767, Lr: 0.000021\n",
            "2023-01-19 15:31:19,930 - INFO - joeynmt.training - Epoch 500, Step:   173900, Batch Loss:     1.351249, Batch Acc: 0.621222, Tokens per Sec:    15879, Lr: 0.000021\n",
            "2023-01-19 15:31:27,528 - INFO - joeynmt.training - Epoch 500, Step:   174000, Batch Loss:     1.364528, Batch Acc: 0.623701, Tokens per Sec:    16045, Lr: 0.000021\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 131.21ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10543.23ex/s]\n",
            "2023-01-19 15:31:27,791 - INFO - joeynmt.training - Sample random subset from dev set: n=200, seed=174000\n",
            "2023-01-19 15:31:27,791 - INFO - joeynmt.prediction - Predicting 200 example(s)... (Greedy decoding with min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:31:32,602 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:31:32,603 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:31:32,603 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:31:32,604 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:31:32,607 - INFO - joeynmt.prediction - Evaluation result (greedy) bleu:   9.82, loss:   2.80, ppl:  16.39, acc:   0.43, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 4.7641[sec], evaluation: 0.0431[sec]\n",
            "2023-01-19 15:31:32,609 - INFO - joeynmt.training - Example #0\n",
            "2023-01-19 15:31:32,613 - INFO - joeynmt.training - \tSource:     قورد اولسان گرگ قاچاسان و قوواسان . \n",
            "2023-01-19 15:31:32,613 - INFO - joeynmt.training - \tReference:  گرگ كه میشوی باید دنبالشان كنی . \n",
            "2023-01-19 15:31:32,614 - INFO - joeynmt.training - \tHypothesis: اگر به این قرآن گوش می دهی ، باید تو یسای .\n",
            "2023-01-19 15:31:32,614 - INFO - joeynmt.training - Example #1\n",
            "2023-01-19 15:31:32,616 - INFO - joeynmt.training - \tSource:     بونلارین هامیسی من ده واردیلار\n",
            "2023-01-19 15:31:32,616 - INFO - joeynmt.training - \tReference:  همه‌ی این‌ها در من هستند \n",
            "2023-01-19 15:31:32,616 - INFO - joeynmt.training - \tHypothesis: اینان همهٔ این ها هم هم هم هستم .\n",
            "2023-01-19 15:31:32,616 - INFO - joeynmt.training - Example #2\n",
            "2023-01-19 15:31:32,618 - INFO - joeynmt.training - \tSource:     یالنیز آللاهێن آیهلرینی اینکار ائدنلر بئله دؤندهریلرلر ! \n",
            "2023-01-19 15:31:32,618 - INFO - joeynmt.training - \tReference:  کسانی که نشانه‌های خدا را انکار می‌کردند ، این گونه از خدا رویگردان می‌شوند . \n",
            "2023-01-19 15:31:32,618 - INFO - joeynmt.training - \tHypothesis: مگر کسانی که آیات خدا را انکار کردند ،\n",
            "2023-01-19 15:31:32,618 - INFO - joeynmt.training - Example #3\n",
            "2023-01-19 15:31:32,620 - INFO - joeynmt.training - \tSource:     ائی کور رحبر ! آغجاقانادی سۆزگجله چێخارێر ، دوهنی ایسه اودورسونوز ! \n",
            "2023-01-19 15:31:32,620 - INFO - joeynmt.training - \tReference:  ای راهنمایان کور ، شما پشه را با صافی می‌گیرید ، اما شتر را می‌بلعید ! \n",
            "2023-01-19 15:31:32,621 - INFO - joeynmt.training - \tHypothesis: ای نابینا ، کوران را بیرون می آورد و می اندیشید که آن را بیرون می آورد و در آن سرزمین می آورد .\n",
            "2023-01-19 15:31:35,138 - INFO - joeynmt.training - Epoch 500: total training loss 492.28\n",
            "2023-01-19 15:31:35,144 - INFO - joeynmt.training - Training ended after 500 epochs.\n",
            "2023-01-19 15:31:35,145 - INFO - joeynmt.training - Best validation result (greedy) at step   119000:  11.08 bleu.\n",
            "2023-01-19 15:31:35,159 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2023-01-19 15:31:35,243 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2023-01-19 15:31:35,245 - INFO - joeynmt.model - Total params: 4199424\n",
            "2023-01-19 15:31:35,405 - INFO - joeynmt.helpers - Load model from /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/119000.ckpt.\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 123.68ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 9443.01ex/s]\n",
            "2023-01-19 15:31:35,712 - INFO - joeynmt.prediction - Decoding on dev set...\n",
            "2023-01-19 15:31:35,712 - INFO - joeynmt.prediction - Predicting 1497 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:32:35,524 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:32:35,524 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:32:35,525 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:32:35,531 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:32:35,555 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  10.61, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 59.4789[sec], evaluation: 0.3184[sec]\n",
            "2023-01-19 15:32:35,571 - INFO - joeynmt.prediction - Translations saved to: /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/00119000.hyps.dev.\n",
            "Dropping NaN...: 100% 2/2 [00:00<00:00, 138.12ba/s]\n",
            "Preprocessing...: 100% 1497/1497 [00:00<00:00, 10439.07ex/s]\n",
            "2023-01-19 15:32:35,822 - INFO - joeynmt.prediction - Decoding on test set...\n",
            "2023-01-19 15:32:35,822 - INFO - joeynmt.prediction - Predicting 1497 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-19 15:33:37,502 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-19 15:33:37,503 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-19 15:33:37,503 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-19 15:33:37,510 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-19 15:33:37,534 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:  10.34, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 61.3603[sec], evaluation: 0.3112[sec]\n",
            "2023-01-19 15:33:37,542 - INFO - joeynmt.prediction - Translations saved to: /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/00119000.hyps.test.\n"
          ]
        }
      ],
      "source": [
        "!python3 -m joeynmt train {data_dir}/config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7qULQu0B5Yl"
      },
      "source": [
        "## Continue training after interruption\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr3mEDga2BiJ"
      },
      "outputs": [],
      "source": [
        "resume_config = config\\\n",
        "  .replace('#load_model:', 'load_model:')\\\n",
        "  .replace('#reset_best_ckpt: False', 'reset_best_ckpt: False')\\\n",
        "  .replace('#reset_scheduler: False', 'reset_scheduler: False')\\\n",
        "  .replace('#reset_optimizer: False', 'reset_optimizer: False')\\\n",
        "  .replace('#reset_iter_state: False', 'reset_iter_state: False')\\\n",
        "  .replace(f'model_dir: \"{model_dir}\"', f'model_dir: \"{model_dir}_resume\"')\n",
        "\n",
        "with (Path(data_dir) / \"resume_config.yaml\").open('w') as f:\n",
        "    f.write(resume_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgH1vAsV2Bkw"
      },
      "outputs": [],
      "source": [
        "!python3 -m joeynmt train {data_dir}/resume_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVv1ja0eCk66"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5T0OEp22BnX",
        "outputId": "b88bb5aa-0294-42d0-e3db-3fac2e0fc54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-13 18:51:38,465 - INFO - root - Hello! This is Joey-NMT (version 2.0.0).\n",
            "2023-01-13 18:51:38,466 - INFO - joeynmt.data - Building tokenizer...\n",
            "2023-01-13 18:51:38,478 - INFO - joeynmt.tokenizers - azb tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
            "2023-01-13 18:51:38,478 - INFO - joeynmt.tokenizers - fa tokenizer: SentencePieceTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=SentencePieceProcessor, nbest_size=5, alpha=0.0)\n",
            "2023-01-13 18:51:38,478 - INFO - joeynmt.data - Building vocabulary...\n",
            "2023-01-13 18:51:38,535 - INFO - joeynmt.data - Loading dev set...\n",
            "2023-01-13 18:51:38,669 - INFO - numexpr.utils - NumExpr defaulting to 2 threads.\n",
            "/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
            "  warnings.warn(\n",
            "2023-01-13 18:51:39,268 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/data/validation/cache-65fa863bdf5b960f.arrow\n",
            "2023-01-13 18:51:39,304 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/data/validation/cache-7c7db85b5c73c62f.arrow\n",
            "2023-01-13 18:51:39,307 - INFO - joeynmt.data - Loading test set...\n",
            "2023-01-13 18:51:39,349 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/data/test/cache-27c8214ca0df0b20.arrow\n",
            "2023-01-13 18:51:39,385 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/data/test/cache-e57b330fd3b1d8ac.arrow\n",
            "2023-01-13 18:51:39,387 - INFO - joeynmt.data - Data loaded.\n",
            "2023-01-13 18:51:39,387 - INFO - joeynmt.helpers - Train dataset: None\n",
            "2023-01-13 18:51:39,388 - INFO - joeynmt.helpers - Valid dataset: HuggingfaceDataset(len=1497, src_lang=azb, trg_lang=fa, has_trg=True, random_subset=200, split=validation)\n",
            "2023-01-13 18:51:39,388 - INFO - joeynmt.helpers -  Test dataset: HuggingfaceDataset(len=1497, src_lang=azb, trg_lang=fa, has_trg=True, random_subset=-1, split=test)\n",
            "2023-01-13 18:51:39,388 - INFO - joeynmt.helpers - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁، (5) ی (6) ▁. (7) ه (8) ا (9) ▁و\n",
            "2023-01-13 18:51:39,388 - INFO - joeynmt.helpers - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁، (5) ی (6) ▁. (7) ه (8) ا (9) ▁و\n",
            "2023-01-13 18:51:39,388 - INFO - joeynmt.helpers - Number of unique Src tokens (vocab_size): 2000\n",
            "2023-01-13 18:51:39,389 - INFO - joeynmt.helpers - Number of unique Trg tokens (vocab_size): 2000\n",
            "2023-01-13 18:51:39,389 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2023-01-13 18:51:39,468 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2023-01-13 18:51:39,469 - INFO - joeynmt.model - Total params: 4199424\n",
            "2023-01-13 18:51:41,223 - INFO - joeynmt.helpers - Load model from /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/model/18000.ckpt.\n",
            "2023-01-13 18:51:41,291 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/data/validation/cache-dd6e321906ac1907.arrow\n",
            "2023-01-13 18:51:41,328 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/data/validation/cache-1764725bf76d0890.arrow\n",
            "2023-01-13 18:51:41,331 - INFO - joeynmt.prediction - Decoding on dev set...\n",
            "2023-01-13 18:51:41,331 - INFO - joeynmt.prediction - Predicting 1497 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-13 18:52:51,062 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-13 18:52:51,063 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-13 18:52:51,063 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-13 18:52:51,069 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-13 18:52:51,093 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:   8.19, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 69.3922[sec], evaluation: 0.3262[sec]\n",
            "2023-01-13 18:52:51,141 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/data/test/cache-9c6df08dbc801f6c.arrow\n",
            "2023-01-13 18:52:51,181 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /content/drive/.shortcut-targets-by-id/1kwR57fdWwIFTDwwKUg0P4t7ohSi2r3i4/Azari/1401Bahar-azb2fa-translation/RESULTS/data/test/cache-1ce96ada79298b51.arrow\n",
            "2023-01-13 18:52:51,184 - INFO - joeynmt.prediction - Decoding on test set...\n",
            "2023-01-13 18:52:51,184 - INFO - joeynmt.prediction - Predicting 1497 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=100, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)\n",
            "2023-01-13 18:54:00,311 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2023-01-13 18:54:00,312 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-01-13 18:54:00,312 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "2023-01-13 18:54:00,319 - INFO - joeynmt.metrics - nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\n",
            "2023-01-13 18:54:00,342 - INFO - joeynmt.prediction - Evaluation result (beam search) bleu:   8.30, nltk_bleu:   0.00, nltk_gleu:   0.00, nltk_nist:   0.00, generation: 68.8118[sec], evaluation: 0.3065[sec]\n"
          ]
        }
      ],
      "source": [
        "!python3 -m joeynmt test {data_dir}/config.yaml --ckpt {model_dir}/best.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEcyEwpS1Pvi"
      },
      "outputs": [],
      "source": [
        "!python3 -m joeynmt translate {data_dir}/config.yaml --ckpt {model_dir}/best.ckpt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l7qULQu0B5Yl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "036b9b7baa9e4e17b2af8bb6c7085f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3ff0ecbce794a4da8b5f6f59f0084bb",
              "IPY_MODEL_9bb410da850d4f3fa4f4acff023df102",
              "IPY_MODEL_ac5d345c7ee646efb47b460fd4b946fe"
            ],
            "layout": "IPY_MODEL_be72bda1ce9c463aa523934407aaf9b4"
          }
        },
        "e3ff0ecbce794a4da8b5f6f59f0084bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ce517ae36a64d279dcc2cfc6ffbdf9c",
            "placeholder": "​",
            "style": "IPY_MODEL_39476d0a1c194b8da9a6650b3e0a3ac4",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "9bb410da850d4f3fa4f4acff023df102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef5f6024ae5f4ff4b4e3176a50d1c22e",
            "max": 11978,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e480deda15e04e808e5cfdd0a8622251",
            "value": 11978
          }
        },
        "ac5d345c7ee646efb47b460fd4b946fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e925ea4ebbf040a4ade80f6a21be1862",
            "placeholder": "​",
            "style": "IPY_MODEL_c73cc904e62146ec86b07c4375d09db6",
            "value": " 11978/11978 [00:00&lt;00:00, 218496.66 examples/s]"
          }
        },
        "be72bda1ce9c463aa523934407aaf9b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "3ce517ae36a64d279dcc2cfc6ffbdf9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39476d0a1c194b8da9a6650b3e0a3ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef5f6024ae5f4ff4b4e3176a50d1c22e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e480deda15e04e808e5cfdd0a8622251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e925ea4ebbf040a4ade80f6a21be1862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c73cc904e62146ec86b07c4375d09db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21b5636aeb4c4e34ad6d204b6b612583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8043dc1ff02a4481bc528a1a6f8ff072",
              "IPY_MODEL_2d52b1faf61f4d3b9c520050c0a488de",
              "IPY_MODEL_6b62d7c0d7f84736b73b2d5f36aad769"
            ],
            "layout": "IPY_MODEL_75e49c5551c7438d83371a87375b6d73"
          }
        },
        "8043dc1ff02a4481bc528a1a6f8ff072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a765fbff31a1466c84aad17dc5d47d50",
            "placeholder": "​",
            "style": "IPY_MODEL_b5d9eff48e66421f96006061f26ddb9b",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "2d52b1faf61f4d3b9c520050c0a488de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc851c481b7045e18847b34f0df2d466",
            "max": 1497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9fdf7ee5ad924f1aa6fe34281699d1e3",
            "value": 1497
          }
        },
        "6b62d7c0d7f84736b73b2d5f36aad769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e7d6734845a4b35a56a9cd44d21a1ea",
            "placeholder": "​",
            "style": "IPY_MODEL_240f2c21ce7e46f6bb7df482b83cbbe0",
            "value": " 1497/1497 [00:00&lt;00:00, 40925.51 examples/s]"
          }
        },
        "75e49c5551c7438d83371a87375b6d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a765fbff31a1466c84aad17dc5d47d50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5d9eff48e66421f96006061f26ddb9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc851c481b7045e18847b34f0df2d466": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fdf7ee5ad924f1aa6fe34281699d1e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e7d6734845a4b35a56a9cd44d21a1ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "240f2c21ce7e46f6bb7df482b83cbbe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b96b2c1679a45318ae3cce2d4b7108a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c05bf4480034e5fa8d74dfd5d42b57f",
              "IPY_MODEL_5f0b346485f44b83a88bf2d2be9a07d4",
              "IPY_MODEL_6976b922730e40179b892b5a4a293840"
            ],
            "layout": "IPY_MODEL_ba70e67ee73749608d49e0b6ae94b1d0"
          }
        },
        "8c05bf4480034e5fa8d74dfd5d42b57f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85b9b1bd34334f278350aa113e446183",
            "placeholder": "​",
            "style": "IPY_MODEL_3418e0c3414f4cce897067215325be54",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "5f0b346485f44b83a88bf2d2be9a07d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e060a4d4aed349618959b9cb434f6a0b",
            "max": 1497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd9241dc0f0e4e248d6076d347e6e253",
            "value": 1497
          }
        },
        "6976b922730e40179b892b5a4a293840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_898e70ccdb8848818ef8b6d6821b6fb4",
            "placeholder": "​",
            "style": "IPY_MODEL_f69412e8a2f54c22adbf57dfa33794ec",
            "value": " 1497/1497 [00:00&lt;00:00, 47389.15 examples/s]"
          }
        },
        "ba70e67ee73749608d49e0b6ae94b1d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "85b9b1bd34334f278350aa113e446183": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3418e0c3414f4cce897067215325be54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e060a4d4aed349618959b9cb434f6a0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd9241dc0f0e4e248d6076d347e6e253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "898e70ccdb8848818ef8b6d6821b6fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f69412e8a2f54c22adbf57dfa33794ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
